\documentclass[11pt,t,usepdftitle=false,aspectratio=169]{beamer}
\usetheme[nototalframenumber,license]{uibk}

\title{Random Forests}
\subtitle{Supervised Learning: Algorithmic Modeling}
\author{Lisa Schlosser, Achim Zeileis}

%% forest header image
\renewcommand{\headerimage}[1]{%
   \IfStrEqCase{#1}{%
      {1}{%
         \gdef\myheaderimageid{#1}%
         \gdef\myheaderimageposition{nw}%
         \gdef\myheaderimage{forest.jpg}%
      }}[%
         \gdef\myheaderimageid{1}%
         \gdef\myheaderimageposition{nw}%
         \gdef\myheaderimage{forest.jpg}%
      ]%
}
\headerimage{1}

%% custom subsection slides
\setbeamercolor*{subsectionfade}{use={normal text},parent={normal text},fg=structure.fg!30!normal text.bg}
\AtBeginSubsection[]{%
  \begin{frame}[c]
    \begin{center}
      \usebeamercolor[fg]{subsectionfade}
      \Large \insertsection \\[2ex]
      \usebeamercolor[fg]{structure}
      \huge\bfseries\insertsubsection
    \end{center}
  \end{frame}
}

\usepackage[utf8]{inputenc}

\setbeamertemplate{caption}{\insertcaption} 
%% includes a replacement for \usepackage{Sweave}
%\usepackage{Sweave}
\usepackage{changepage}
\usepackage{amsmath,tikz}
\usepackage{calc}
\usepackage{graphicx}
\usetikzlibrary{positioning,shapes,arrows,decorations.pathreplacing,calc,automata,mindmap,trees,tikzmark,decorations.pathreplacing}
\usepackage{xcolor}
\newcommand{\argmin}{\operatorname{argmin}\displaylimits}

%% colors
\definecolor{HighlightOrange}{rgb}{0.9490196,0.5725490,0.0000000}
\definecolor{HighlightBlue}{rgb}{0.4784314,0.7490196,0.9803922}
\definecolor{forestred}{RGB}{206,73,81}
\definecolor{treegreen}{RGB}{0,143,0}
\definecolor{lightblue}{RGB}{34,151,230}
\definecolor{lightorange}{RGB}{255,165,0}


\SweaveOpts{engine=R, eps=FALSE, keep.source=TRUE}
<<preliminaries, echo=FALSE, results=hide>>=
options(prompt = "R> ", continue = "+  ", useFancyQuotes = FALSE, width = 70)

set.seed(7)

invisible(.Call(grDevices:::C_palette, grDevices::hcl(
  h = c(0,   5, 125, 245, 195, 315,  65,   0),
  c = c(0, 100,  90,  85,  63, 105,  90,   0),
  l = c(0,  55,  75,  60,  82,  48,  80,  65)
)))

library("partykit")
library("Formula")
library("latex2exp")
library("lattice")
library("MASS")
library("colorspace")



## HCL palette
pal <- hcl(c(10, 128, 260, 290, 50), 100, 50)
names(pal) <- c("forest", "tree", "gamlss", "gamboostLSS", "EMOS")

pallight <- hcl(c(10, 128, 260, 290, 70), 100, 50, alpha = 0.25)
names(pallight) <- c("forest", "tree", "gamlss", "gamboostLSS", "EMOS")

transpgray <- rgb(0.190,0.190,0.190, alpha = 0.2)

lightblue <- "#2297E6"
lightorange <- "#FFA500"
@





<<motivation_tree, echo=FALSE, results=hide>>=
## Reg. Tree
set.seed(7)
nobs <- 200
kappa <- 12
x <- c(1:nobs)/nobs
ytrue <- numeric(length = length(x))
for(i in 1:(nobs)) ytrue[i] <- if(x[i]<1/3) 0.5 else 1+(1-plogis(kappa*(2*(x[i]-0.2)-1)))
y <- ytrue + rnorm(nobs,0,0.3)
# more points for precise illustration of true function
x100 <- c(1:(100*nobs))/(100*nobs)
ytrue <- ytree <- ytree2 <- ytree3 <- ytree4 <- yforest <- numeric(length = length(x100))
for(i in 1:(nobs*100)) ytrue[i] <- if(x100[i]<1/3) 0.5 else 1+(1-plogis(kappa*(2*(x100[i]-0.2)-1)))
for(i in 1:(nobs*100)) ytree[i] <- if(x100[i]<1/3) 0.5 else {if(x100[i]<2/3) 2 else 1}
for(i in 1:(nobs*100)) ytree2[i] <- if(x100[i]<0.31) 0.47 else {if(x100[i]<0.69) 1.9 else 1.1}
for(i in 1:(nobs*100)) ytree3[i] <- if(x100[i]<0.34) 0.52 else {if(x100[i]<0.54) 2.2 else {if(x100[i]<0.74) 1.74 else 0.9}}
for(i in 1:(nobs*100)) ytree4[i] <- if(x100[i]<0.32) 0.51 else {if(x100[i]<0.63) 1.8 else 1.15}
for(i in 1:(nobs*100)) yforest[i] <- if(x100[i]<=0.33) 0.52 else {if(x100[i]<=0.34) 1.5 else {if(x100[i]<=0.54) 2 else {if(x100[i]<=0.63) 1.9 else {if(x100[i]<=0.69) 1.6 else {if(x100[i]<=0.74) 1.3 else 1}}}}}
@


\begin{document}

\section{Random forests}

\subsection{Motivation}
\begin{frame}[fragile]
\frametitle{Motivation}

{\bf Idea:} \\
\begin{itemize}
\item Combine an ensemble of trees. 
\item Each tree can capture non-linear and non-additive effects and select covariates and possible interactions automatically.
\item Combining trees to a forest model allows for an approximation of smooth effects.
\item A forest can regularize and stabilize the model.
\end{itemize}

%\bigskip

\end{frame}


\begin{frame}
\frametitle{Motivation}

\vspace{0.2cm}

\begin{minipage}{0.4\textwidth}
\only<1>{\textbf{Tree model:\\}}\only<2-8>{\textbf{Subsampling:\\}}\only<9->{\textbf{Forest model:\\}}
\vspace{-0.4cm}
%\vspace{-0.8cm}
\begin{center}
\only<1>{
%\vspace{0.2cm}
\resizebox{0.63\textwidth}{!}{
\begin{tikzpicture}
  \node[ellipse, fill=HighlightBlue!70, align=center] (n0) at (2, 3) {};
  \node[] (n00) at (1, 1.5) {};
  \node[rectangle, fill=HighlightOrange!70, align=center] (n1) at (0, 0) {$\hat{Y}_1$};
  \draw[-] (n0) -- (n00) node [midway, left] {\small $X \leq p_1$};
  \draw[-] (n0) -- (n1);
  \node[ellipse, fill=HighlightBlue!70, align=center] (n2) at (3, 1.5) {};
  \draw[-] (n0) -- (n2) node [midway, right] {\small $X > p_1$};
  \node[rectangle, fill=HighlightOrange!70, align=center] (n3) at (2, 0) {$\hat{Y}_2$};
  \draw[-] (n2) -- (n3) node [midway, left] {\small $X \leq p_2$};
  \node[rectangle, fill=HighlightOrange!70, align=center] (n4) at (4, 0) {$\hat{Y}_3$};
  \draw[-] (n2) -- (n4) node [midway, right] {\small $X > p_2$};
\end{tikzpicture}}

\vspace{0.475cm}

}
\only<2->{
%\vspace{0.2cm}
\resizebox{0.6\textwidth}{!}{
\begin{tikzpicture}
%\draw[ellipse, draw=black, align=center, scale = 0.7, minimum width=170pt, minimum height = 35pt] (set) at (1, 3) {};
\draw[gray,rounded corners=10pt] (-1.05,4.35) -- (4.05,4.35) -- (4.05,2.7) -- (-2.05,2.7) -- (-2.05,4.35) -- (0.05,4.35);
%\draw[gray] (1,3.5) ellipse (3 and 0.8);
%\visible<4>{
\begin{scope}
\clip (-1,4.3) -- (4,4.3) -- (4,2.7) -- (-2,2.7) -- (-2,4.3) -- (0,4.3);
%\clip (1,3.5) ellipse (3 and 0.8);
\pgfmathsetseed{7}
\foreach \p in {1,...,200} {\fill[gray] (1+3*rand,3.5+0.8*rand) circle (0.04);}
\end{scope}
%}
%%% color points
%\visible<5-6>{
%\begin{scope}
%\clip (-1,4.3) -- (4,4.3) -- (4,2.7) -- (-2,2.7) -- (-2,4.3) -- (0,4.3);
%\pgfmathsetseed{7}
%\foreach \p in {1,...,100} {\fill[treegreen] (1+3*rand,3.5+0.8*rand) circle (0.07);}
%\end{scope}
%}
\visible<3->{
\node[ellipse, draw=treegreen, align=center, minimum width=80pt, minimum height = 40pt, line width = 3pt] (subset1) at (0.1, 3.5) {};
}
%\visible<7-8>{
%\begin{scope}
%\clip (-1,4.3) -- (4,4.3) -- (4,2.7) -- (-2,2.7) -- (-2,4.3) -- (0,4.3);
%\pgfmathsetseed{7}
%\foreach \p in {1,...,49} {\fill[gray] (1+3*rand,3.5+0.8*rand) circle (0.04);}
%\foreach \p in {50,...,150} {\fill[lightblue] (1+3*rand,3.5+0.8*rand) circle (0.07);}
%\end{scope}
%}
\visible<5->{
\node[ellipse, draw = lightblue, align=center, minimum width=100pt, minimum height = 30pt, line width = 3pt] (subset2) at (1.2, 3.6) {};
}
%\visible<9-10>{
%\begin{scope}
%\clip (-1,4.3) -- (4,4.3) -- (4,2.7) -- (-2,2.7) -- (-2,4.3) -- (0,4.3);
%\pgfmathsetseed{7}
%\foreach \p in {1,...,99} {\fill[gray] (1+3*rand,3.5+0.8*rand) circle (0.04);}
%\foreach \p in {100,...,200} {\fill[lightorange] (1+3*rand,3.5+0.8*rand) circle (0.07);}
%\end{scope}
%}
\visible<7->{
\node[ellipse, draw = lightorange, align=center, minimum width=120pt, minimum height = 25pt, line width = 3pt] (subset3) at (1.6, 3.3) {};
}
\visible<4->{
\node[ellipse, fill=treegreen, align=center] (n00) at (-1, 2) {};
\node[ellipse, fill=treegreen, align=center] (n01) at (-1.25, 1.25) {};
\draw[-, line width=1pt] (n00) -- (n01);
\node[rectangle, fill=treegreen, align=center] (n02) at (-1.5, 0.5) {};
\draw[-, line width=1pt] (n01) -- (n02);
\node[rectangle, fill=treegreen, align=center] (n03) at (-1, 0.5) {};
\draw[-, line width=1pt] (n01) -- (n03);
\node[rectangle, fill=treegreen, align=center] (n04) at (-0.5, 0.5) {};
\draw[-, line width=1pt] (n00) -- (n04);
}
\visible<6->{
\node[ellipse, fill=lightblue, align=center] (n10) at (1, 2) {};
\node[ellipse, fill=lightblue, align=center] (n11) at (0.5, 1.25) {};
\draw[-, line width=1pt] (n10) -- (n11);
\node[rectangle, fill=lightblue, align=center] (n12) at (0.25, 0.5) {};
\draw[-, line width=1pt] (n11) -- (n12);
\node[rectangle, fill=lightblue, align=center] (n13) at (0.75, 0.5) {};
\draw[-, line width=1pt] (n11) -- (n13);
\node[ellipse, fill=lightblue, align=center] (n14) at (1.5, 1.25) {};
\draw[-, line width=1pt] (n10) -- (n14);
\node[rectangle, fill=lightblue, align=center] (n15) at (1.75, 0.5) {};
\draw[-, line width=1pt] (n14) -- (n15);
\node[rectangle, fill=lightblue, align=center] (n16) at (1.25, 0.5) {};
\draw[-, line width=1pt] (n14) -- (n16);
}
\visible<8->{
\node[ellipse, fill=lightorange, align=center] (n20) at (3, 2) {};
\node[rectangle, fill=lightorange, align=center] (n21) at (2.5, 0.5) {};
\draw[-, line width=1pt] (n20) -- (n21);
\node[ellipse, fill=lightorange, align=center] (n22) at (3.25, 1.25) {};
\draw[-, line width=1pt] (n20) -- (n22);
\node[rectangle, fill=lightorange, align=center] (n23) at (3, 0.5) {};
\draw[-, line width=1pt] (n22) -- (n23);
\node[rectangle, fill=lightorange, align=center] (n24) at (3.5, 0.5) {};
\draw[-, line width=1pt] (n22) -- (n24);
}
\visible<10->{
\node[ellipse, draw = black, align=center, minimum width=15pt, minimum height = 15pt, line width = 2pt] (B1) at (-1.5, 0.5) {};
\node[ellipse, align=center, line width = 3pt, text=treegreen] (Y1) at (-1.5, -0.5) {\LARGE $\hat{y}_1$};
}
\visible<11->{
\node[ellipse, draw = black, align=center, minimum width=15pt, minimum height = 15pt, line width = 2pt] (B2) at (1.25, 0.5) {};
\node[ellipse, align=center, line width = 3pt, text=lightblue] (Y2) at (1.25, -0.5) {\LARGE $\hat{y}_2$};
}
\visible<12->{
\node[ellipse, draw = black, align=center, minimum width=15pt, minimum height = 15pt, line width = 2pt] (B2) at (3.5, 0.5) {};
\node[ellipse, align=center, line width = 3pt, text=lightorange] (Y3) at (3.5, -0.5) {\LARGE $\hat{y}_3$};(3.5, 0.5)
}
\end{tikzpicture}
}
}
\end{center}
\end{minipage}
\hspace{0.7cm}
\begin{minipage}{0.36\textwidth}
%\begin{center}
%\vspace{0.1cm}
\only<1-2>{
%\vspace{0.01cm}
<<plot_motivation_tree, fig=TRUE, echo=FALSE, width=7>>=
par(mar=c(3,3,2,0))
#par(mar=c(3,3,2,5.5))
plot(x = x, y = y, xaxt="n", yaxt="n", ann=FALSE, col = "slategray", pch = 19)
box(lwd=5)
#lines(x = x, y = ytrue, col = "gray", lwd=5, main = "")
lines(x = x100, y = ytree, col = pal["forest"], lwd=7)
mtext(text = "X", side = 1, cex = 2.5, line = 2)  
mtext(text = "Y", side = 2, cex = 2.5, line = 1)  
@
}
\only<3>{
%\vspace{0.01cm}
%\hspace{-0.1cm}
<<plot_motivation_randforest_data, fig=TRUE, echo=FALSE, width=7>>=
par(mar=c(3,3,2,0))
#par(mar=c(3,3,2,5.5))
plot(x = x, y = y, xaxt="n", yaxt="n", ann=FALSE, col = "slategray", pch = 19)
box(lwd=5)
mtext(text = "X", side = 1, cex = 2.5, line = 2)  
mtext(text = "Y", side = 2, cex = 2.5, line = 1)  
@
}
\only<4-5>{
%\vspace{0.01cm}
%\hspace{-0.1cm}
<<plot_motivation_randforest1, fig=TRUE, echo=FALSE, width=7>>=
<<plot_motivation_randforest_data>>
lines(x = x100, y = ytree2, col = pal["tree"], lwd=7)
@
}
\only<6-7>{
%\vspace{0.01cm}
%\hspace{-0.1cm}
<<plot_motivation_randforest2, fig=TRUE, echo=FALSE, width=7>>=
<<plot_motivation_randforest1>>
lines(x = x100, y = ytree3, col = lightblue, lwd=7)
@
}
\only<8>{
%\vspace{0.01cm}
%\hspace{-0.1cm}
<<plot_motivation_randforest3, fig=TRUE, echo=FALSE, width=7>>=
<<plot_motivation_randforest2>>
lines(x = x100, y = ytree4, col = lightorange, lwd=7)
@
}
\only<9-12>{
<<plot_motivation_randforest_x, fig=TRUE, echo=FALSE, width=7>>=
par(mar=c(3,3,2,0))
#par(mar=c(3,3,2,5.5))
plot(x = x, y = y, xaxt="n", yaxt="n", ann=FALSE, col = "slategray", pch = 19)
box(lwd=5)
lines(x = x100, y = ytree2, col = pal["tree"], lwd=7)
lines(x = x100, y = ytree3, col = lightblue, lwd=7)
lines(x = x100, y = ytree4, col = lightorange, lwd=7)
lines(x = c(0.6,0.6), y = c(-1, 1.8), type = "l", lty = 2, lwd = 4)
mtext(text = "Y", side = 2, cex = 2.5, line = 1)  
mtext(text = "x", side = 1, cex = 4, line = 2, at=0.6)  
@
}
\only<13->{
<<plot_motivation_randforest4, fig=TRUE, echo=FALSE, width=7>>=
par(mar=c(3,3,2,0))
plot(x = x, y = y, xaxt="n", yaxt="n", ann=FALSE, col = "slategray", pch = 19)
box(lwd=5)
lines(x = x100, y = yforest, col = pal["forest"], lwd=7)
lines(x = c(0.6,0.6), y = c(-1, 1.8), type = "l", lty = 2, lwd = 4)
mtext(text = "x", side = 1, cex = 4, line = 2, at=0.6)  
mtext(text = "Y", side = 2, cex = 2.5, line = 1)  
@
}
\end{minipage}

\vspace{0.5cm}

\only<13->{
$\hat{y}= \frac{1}{3} (\hat{y}_1 + \hat{y}_2 + \hat{y}_3)$
}

\vspace{0.3cm}

\visible<13->{
for $\hat{y}_t$ being the average response value in the segment $\mathcal{B}^t_x$ which the observed covariate $x$ is assigned to in the $t$-th tree with learning data $\{(y_i,x_i)\}_{i=1,\ldots,n}$:

\vspace{0.3cm}

$\hat{y}_t= \frac{1}{|\mathcal{B}^t_x|} \sum_{i: y_i \in \mathcal{B}^t_x} y_i$
\visible<14->{
$\ =\ \sum_{i=1}^n \frac{I(x_i \in \mathcal{B}^t_x)}{|\mathcal{B}^t_x|} \cdot y_i$
}
\visible<15->{
$\ =\ \sum_{i=1}^n w_i(x) \cdot y_i$
}
}
\end{frame}


\subsection{Aggregation of trees\\
\&\\
random sampling}

\begin{frame}
\frametitle{Aggregation of trees}
{\bf Simple tree model:} Obtain the predicted response $\hat{y}$ for an observed covariate~$x$ by averaging over observations in the same terminal node $\mathcal{B}_x$: 
$$
\hat{y}= \frac{1}{|\mathcal{B}_x|} \sum_{i: y_i \in \mathcal{B}_x} y_i
\visible<2->{
\ =\ \sum_{i=1}^n w_i(x) \cdot y_i$$
}

{\bf Model-based tree:} Obtain the predicted model parameter(s) $\hat{\theta}$ for an observed covariate~$x$ by optimizing an objective function $f$ for all observations in the same terminal node $\mathcal{B}_x$:
$$
\hat{\theta}= \argmin_{\theta} \sum_{i: y_i \in \mathcal{B}_x} f(\theta; y_i)
\visible<2->{
\ =\ \argmin_{\theta} \sum_{i=1}^n w_i(x) \cdot f(\theta; y_i)$$
}

{\bf Note:} The model-based approach allows for fitting complex models but also includes the specific case of averaging over all observations which corresponds to minimizing the residual sum of squares $\sum (y_i - \theta)^2$.


\end{frame}



\begin{frame}
\frametitle{Aggregation of trees}
{\bf Strategy:} Combine trees to a forest model by employing tree-based weights.

The predicted model parameter (vector) $\hat{\theta}$ 
is the argument of the parameter space $\Theta$ 
that minimizes the weighted sum of the objective function $f$ evaluated for learning data $\{(y_i,x_i)\}_{i=1,\ldots,n}$:

\vspace{-0.2cm}

\[
\hat{\theta}(x) =  \argmin_{\theta} \sum_{i=1}^n w_i(x) \cdot f(\theta; y_i)
\]

\medskip

\only<2-4>{

\vspace{0.13cm}

\textbf{Weights:}

\vspace{-1.14cm}

\begin{eqnarray*}
w^{\text{base}}_i(x)   & = & 1 \\[0.2cm]
\visible<3-4>{
w^{\text{tree}}_i(x)   & = & \sum_{b=1}^B I((x_i \in \mathcal{B}_b) \land (x \in \mathcal{B}_b)) \\[0.1cm]
\visible<4>{
w^{\text{forest}}_i(x) & = & \frac{1}{T} \sum_{t=1}^T \sum_{b=1}^{B^t} \frac{I((x_i \in \mathcal{B}^t_b) \land (x \in \mathcal{B}^t_b))}{\left|\mathcal{B}^t_b\right|}
\end{eqnarray*}
}}}


\end{frame}



\begin{frame}
\frametitle{Random sampling}
{\bf Idea:} Build each tree on a different subset of the full data set.

\medskip

{\bf Sampling observations:}
\begin{itemize}
\item Bootstrapping: resampling with replacement, usually ``n out of n''.
\item Subsampling: draw a subset without replacement (typically of the size of 63.2\% of the full data set, but can be a smaller fraction for large data sets).
\end{itemize}

\bigskip

{\bf Sampling covariates:}\\
\smallskip
In order to reduce the correlation among trees random input variable sampling is employed, i.e., for each split only a subset of covariates is selected randomly and provided as possible split variables (typical sampling size: $\sqrt{\text{\#Var}}$).
\end{frame}

\begin{frame}
\frametitle{Hyperparameter selection}

Apart from the size and type of sampling two further control parameters have to be set for a forest model: 
\begin{itemize}
\item {\bf Number of trees:} A high number of trees allows for a more stable model and for a better approximation of smooth effects, however, at the cost of higher computational effort.
\item {\bf Size of the trees:} The size of each single tree can be controlled by setting stopping criteria such as a minimal number of observations in a node, maximum tree depth or a significance level if a statistical test is employed in the tree algorithm. However, in a forest model each tree is usually built as large as possible which can lead to overfitting for a single tree, but this is compensated by aggregating the trees to a forest model.
\end{itemize}
\end{frame}

<<data_wages, eval=TRUE, echo=FALSE, results=hide>>=
data("CPS1985", package = "AER")
@

\begin{frame}[fragile]
\frametitle{Example: wages}

\textbf{Data:} Random sample from the May 1985 US Current Population Survey.
A data frame containing \Sexpr{nrow(CPS1985)} observations on \Sexpr{ncol(CPS1985)} variables.

\bigskip

\begin{tabular}{ll}
\hline 
Variable & Description\\
\hline 
\code{wage}       & Wage (in US dollars per hour). \\
\code{education}  & Education (in years).  \\
\code{experience} & Potential work experience (in years, \code{age - education - 6}). \\
\code{age}        & Age (in years). \\
\code{ethnicity}  & Factor: Caucasian, Hispanic, Other. \\
\code{gender}     & Factor: Male, Female. \\
\code{union}      & Factor. Does the individual work on a union job? \\
\hline 
\end{tabular}

\bigskip

\textbf{Model formula:} \code{log(wage) ~ education + experience + age + ethnicity + gender + union}.

\end{frame}



<<example_wages, eval=TRUE, echo=FALSE, results=hide>>=
set.seed(4)
f <- log(wage) ~ education + experience + age + ethnicity + gender + union
ct_cps <- ctree(f, data = CPS1985, alpha = 0.01)
cf <- cforest(f, data = CPS1985, ntree = 5) #, control =ctree_control(maxdepth = 5))
vimp <- varimp(cf , risk = "loglik")
@

\begin{frame}[fragile]
\frametitle{Example: wages}
Single tree model:
\setkeys{Gin}{width=0.99\textwidth}
<<example_wages_ctree, eval=TRUE, echo=FALSE, fig=TRUE, width=13>>=
plot(ct_cps)
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Example: wages}
Forest model - tree I:
\setkeys{Gin}{width=0.99\textwidth}
<<example_wages_cforest1, eval=TRUE, echo=FALSE, fig=TRUE, width=13>>=
#par(mfrow=c(2,2))
plot(party(cf[[1]][[1]], data = CPS1985), drop_terminal = TRUE)
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Example: wages}
Forest model - tree II:
\setkeys{Gin}{width=0.99\textwidth}
<<example_wages_cforest2, eval=TRUE, echo=FALSE, fig=TRUE, width=13>>=
#par(mfrow=c(2,2))
plot(party(cf[[1]][[2]], data = CPS1985), drop_terminal = TRUE)
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Example: wages}
Forest model - tree III:
\setkeys{Gin}{width=0.99\textwidth}
<<example_wages_cforest3, eval=TRUE, echo=FALSE, fig=TRUE, width=13>>=
#par(mfrow=c(2,2))
plot(party(cf[[1]][[3]], data = CPS1985), drop_terminal = TRUE)
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Example: wages}
Forest model - tree IV:
\setkeys{Gin}{width=0.99\textwidth}
<<example_wages_cforest4, eval=TRUE, echo=FALSE, fig=TRUE, width=13>>=
#par(mfrow=c(2,2))
plot(party(cf[[1]][[4]], data = CPS1985), drop_terminal = TRUE)
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Example: wages}
\vspace{-0.5cm}
\setkeys{Gin}{width=0.94\textwidth}
<<example_wages_forest_prediction, eval=TRUE, echo=FALSE, fig=TRUE, width = 10>>=
nd <- data.frame(education = 13.02, #c(2:18),
                 experience = 17.82,
                 age = c((18:64)/1), #36.83,
                 ethnicity = "cauc",
                 region = "other",
                 gender = "male",
                 occupation = "worker",
                 sector = "other",
                 union = "no",
                 married = "yes")

{
  par(mfrow = c(2,2))
  set.seed(74)
  cf <- cforest(f, data = CPS1985, ntree = 2)
  predwage <- cbind(c((18:64)/1), predict(cf, newdata = nd))
  colnames(predwage) <- c("age", "predicted log(wage)")
  plot(x = predwage[,"age"], y = exp(predwage[,"predicted log(wage)"]), type = 'l',
       ylab = "predicted wage", xlab = "age", main = "Forest with 2 trees")
  
  cf <- cforest(f, data = CPS1985, ntree = 4)
  predwage <- cbind(c((18:64)/1), predict(cf, newdata = nd))
  colnames(predwage) <- c("age", "predicted log(wage)")
  plot(x = predwage[,"age"], y = exp(predwage[,"predicted log(wage)"]), type = 'l',
       ylab = "predicted wage", xlab = "age", main = "Forest with 4 trees")
  
  cf <- cforest(f, data = CPS1985, ntree = 20)
  predwage <- cbind(c((18:64)/1), predict(cf, newdata = nd))
  colnames(predwage) <- c("age", "predicted log(wage)")
  plot(x = predwage[,"age"], y = exp(predwage[,"predicted log(wage)"]), type = 'l',
       ylab = "predicted wage", xlab = "age", main = "Forest with 20 trees")
  
  cf <- cforest(f, data = CPS1985, ntree = 200)
  predwage <- cbind(c((18:64)/1), predict(cf, newdata = nd))
  colnames(predwage) <- c("age", "predicted log(wage)")
  plot(x = predwage[,"age"], y = exp(predwage[,"predicted log(wage)"]), type = 'l',
       ylab = "predicted wage", xlab = "age", main = "Forest with 200 trees")
}
@
\end{frame}


\begin{frame}
\frametitle{Variable importance}
{\bf Problem:} In a single tree model the effect of each covariate can easily be investigated based on a simple illustration of the tree structure. However, in a forest model this would require to investigate each tree on its own including its specific setting (induced by random sampling).

\bigskip

{\bf Idea:} Asses the influence of a covariate on the forest model by evaluating the change in performance of the model (mean decrease in accuracy) after permutating the selected covariate. 

\bigskip


%\bigskip

% {\bf Implementations:}
% \begin{itemize}
% \item \fct{importance} in the R package randomForest.
% \item \fct{importance.ranger} in the R package ranger.
% \item \fct{varimp} in the R package partykit.
% \end{itemize}

% \only<2->{
% \setkeys{Gin}{width=0.5\textwidth}
% <<example_wages_vimp, eval=TRUE, echo=FALSE, fig=TRUE, width=6, height=3>>=
% par(mar = c(4,5.5,0,2))
% barplot(sort(vimp, decreasing = FALSE),
%         horiz = TRUE, las = 1, axes = FALSE,
%         xlab = "Variable importance: mean decrease in log-likelihood")
% axis(1, at = seq(0,1,0.02), las = 1, mgp=c(0,1,0))
% @

Depending on the fitted model the corresponding objective function can be employed as \emph{risk function} to measure accuracy by calculating importance scores (e.g., log-likelihood, number of misclassifications, \ldots).
%}
\end{frame}



\begin{frame}
\frametitle{Example: wages}
Evaluation of variable importance in the fitted forest model:

\bigskip

\setkeys{Gin}{width=0.57\textwidth}
<<example_wages_vimp, eval=TRUE, echo=FALSE, fig=TRUE, width=6, height=3>>=
par(mar = c(4,5.4,1,2))
barplot(sort(vimp, decreasing = FALSE),
        horiz = TRUE, las = 1, axes = FALSE,
        xlab = "Variable importance: mean decrease in log-likelihood")
axis(1, at = seq(0,1,0.02), las = 1, mgp=c(0,1,0))
@
\end{frame}

\subsection{Implementations}

\begin{frame}[fragile]
\frametitle{R software for forest models}

{\bf randomForest:}
\begin{itemize}
  \item R package for classic random forests.
  \item Based on implementations of CART.
  \item First implementation of random forests in R.
\end{itemize}

% \medskip
% 
% <<call_randomForest, eval=FALSE, echo=TRUE>>=
% randomForest(formula, data=NULL, ..., subset, na.action=na.fail)
% randomForest(x, y=NULL,  ...)
% @
% 
% \medskip
% 
% Optional control arguments:
% \begin{itemize}
% \item \code{ntree}: nr. of trees, 
% \item \code{mtry}: nr. of randomly sampled split variable candidates, 
% \item \code{replace}: type of subsetting, 
% \item \code{sampsize}: size of drawn subsamples, 
% \item \code{nodesize}: minimum size of terminal nodes, 
% \item \code{maxnodes}: maximum number of terminal nodes, 
% \item \ldots 
% \end{itemize}

\medskip

{\bf ranger}:
\begin{itemize}
\item RANdom  forest  GEneRator.
\item R package providing a fast implementation of classic random forests.
\end{itemize}

\medskip

{\bf partykit:}
\begin{itemize}
  \item R package offering a toolkit for unbiased recursive partitioning. 
  \item Provides the forest-building function \fct{cforest}, based on conditional inference trees (Ctree).
\end{itemize}

\end{frame}


\begin{frame}[fragile]
\frametitle{R software for forest models}

{\bf ranger}:

<<call_ranger, eval=TRUE, echo=TRUE>>=
library("ranger")
rf <- ranger(log(wage) ~ education + experience + age + ethnicity + gender + union,
             data = CPS1985)
@

Optional control arguments:
\begin{itemize}
\item \code{num.trees}: number of trees, 
\item \code{mtry}: number of randomly sampled split variable candidates, 
\item \code{replace}: sample with replacement, 
\item \code{sample.fraction}: fraction of observations to sample, 
\item \code{min.node.size}: minimal size of terminal nodes, 
\item \code{max.depth}: maximal tree depth, 
\item \code{splitrule}: splitting rule, 
\item \ldots 
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{R software for forest models}

{\bf ranger}:

<<call_ranger_predict, eval=TRUE, echo=TRUE, results=verbatim>>=
newdata <- data.frame(education = 13, experience = 17,
                      age = 36, ethnicity = "cauc",
                      region = "other", gender = "male",
                      occupation = "worker", sector = "other",
                      union = "no", married = "yes")
pred_rf <- predict(rf, data = newdata)
pred_rf$predictions     
exp(pred_rf$predictions) 
rf <- ranger(log(wage) ~ education + experience + age + ethnicity + gender + union,
             data = CPS1985, importance = "impurity")
importance(rf)
@

\end{frame}

\begin{frame}[fragile]
\frametitle{R software for forest models}

{\bf partykit:}

<<call_cforest, eval = FALSE, echo=TRUE>>=
library("partykit")
cf <- cforest(log(wage) ~ education + experience + age + ethnicity + gender + union,
              data = CPS1985)
@

\only<1>{
Optional forest-specific control arguments:
\begin{itemize}
\item \code{ytrafo}: transformation function applied to the response,
\item \code{ntree}: number of trees, 
\item \code{mtry}: number of randomly sampled split variable candidates, 
\item \code{perturb}: type and size of subsetting, 
\item \ldots 
\end{itemize}
%\vspace{0.9cm}
}
\only<2->{
Optional tree-specific arguments handed over via \fct{ctree\_control}:
\begin{itemize}
\item \code{minsplit}: minimum number of observations to perform a split, 
\item \code{minbucket}: minimum number of observations in a node after splitting, 
\item \code{maxdepth}: maximum depth of the tree, 
\item \code{alpha}: significance level for split variable selection,
\item \ldots 
\end{itemize}
}
% \only<3>{
% 
% <<call_cforest_control, eval = FALSE, echo=TRUE>>=
% cf <- cforest(log(wage) ~ education + experience + age + ethnicity + gender + union,
%             data = CPS1985, control = ctree_control(minsplit = 20))
% @
% }

\end{frame}


\begin{frame}[fragile]
\frametitle{R software for forest models}

{\bf partykit:}

<<call_cforest, eval = FALSE, echo=TRUE>>=
library("partykit")
cf <- cforest(log(wage) ~ education + experience + age + ethnicity + gender + union,
              data = CPS1985)
@


Optional tree-specific arguments handed over via \fct{ctree\_control}:
\begin{itemize}
\item \code{minsplit}: minimum number of observations to perform a split, 
\item \code{minbucket}: minimum number of observations in a node after splitting, 
\item \code{maxdepth}: maximum depth of the tree, 
\item \code{alpha}: significance level for split variable selection,
\item \ldots 
\end{itemize}

<<call_cforest_control, eval = FALSE, echo=TRUE>>=
cf <- cforest(log(wage) ~ education + experience + age + ethnicity + gender + union,
            data = CPS1985, control = ctree_control(minsplit = 20))
@

\end{frame}
\begin{frame}[fragile]
\frametitle{R software for forest models}

{\bf partykit:}

<<call_cforest_predict, eval = TRUE, results=verbatim>>=
newdata <- data.frame(education = 13, experience = 17,
                      age = 36, ethnicity = "cauc",
                      region = "other", gender = "male",
                      occupation = "worker", sector = "other",
                      union = "no", married = "yes")
pred_cf <- predict(cf, newdata = newdata)
pred_cf
exp(pred_cf)
varimp(cf)
@

\end{frame}



\begin{frame}
\frametitle{R software for forest models}

\textbf{Further R packages:}
%
\begin{itemize}
  \item grf: generalized random forests,
  \item disttree: distributional forests, based on partykit,
  \item \ldots
\end{itemize}

\end{frame}



\subsection{References}

\begin{frame}
\frametitle{References}

\vspace{-0.2cm}

\scriptsize

Breiman L (2001).
  \dquote{Random {Forests}.}
  \emph{Machine Learning}, 
  \textbf{45}(1), 5--32.
  \doi{10.1023/A:1010933404324}

\medskip
  
Breiman L, Cutler A (2004).
 \dquote{Random Forests.},
 \url{https://www.stat.berkeley.edu/~breiman/RandomForests}
 (Accessed: 2018-02-22)
 
\medskip

%Hothorn T, Zeileis A (2017).
%  \dquote{Transformation Forests.}
%  \emph{arXiv 1701.02110}, arXiv.org E-Print Archive.
%  \url{http://arxiv.org/abs/1701.02110}
%  
%\smallskip
  
Hothorn T, Hornik K, Zeileis A (2006).
 \dquote{Unbiased Recursive Partitioning: A Conditional Inference Framework.}
 \emph{Journal of Computational and Graphical Statistics},
 \textbf{15}(3), 651--674.
 \doi{10.1198/106186006X133933}
 
\medskip

Hothorn T, Zeileis A (2015).
 \dquote{{partykit}: A Modular Toolkit for Recursive Partytioning in \textsf{R}.}
 \emph{Journal of Machine Learning Research},
 \textbf{16}, 3905--3909.
 \url{http://www.jmlr.org/papers/v16/hothorn15a.html}
  
\medskip  

Liaw A, Wiener M (2002).
 \dquote{Classification and Regression by randomForest.}
 \emph{R News},
 \textbf{2}(3), 18--22.
 \url{https://CRAN.R-project.org/doc/Rnews/}

\medskip

Wright M N, Ziegler A (2017).
\dquote{{ranger}: A Fast Implementation of Random Forests for High
Dimensional Data in {C++} and {R}.}
\emph{Journal of Statistical Software}, \textbf{77}{1}, 1--17.
\doi{10.18637/jss.v077.i01}

\medskip

% Zeileis A, Hothorn T, Hornik K (2008).
%  \dquote{Model-Based Recursive Partitioning.}
%   \emph{Journal of Computational and Graphical Statistics},
%   \textbf{17}(2), 492--514.
%   \doi{10.1198/106186008X319331}
% 
% \medskip

Athey S, Tibshirani J, Wager S (2019). 
 \dquote{Generalized Random Forests.} 
 \emph{Annals of Statistics}, \textbf{47}{2}, 1148--1178.
 \doi{10.1214/18-AOS1709"}

\medskip

Schlosser L, Hothorn T, Stauffer R, Zeileis A (2019).
\dquote{Distributional Regression Forests for Probabilistic Precipitation Forecasting in Complex Terrain.}
\emph{The Annals of Applied Statistics}, \textbf{13}(3), 1564--1589.
\doi{10.1214/19-AOAS1247}

\end{frame}

\end{document}
