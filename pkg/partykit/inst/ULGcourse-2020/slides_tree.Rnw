\documentclass[11pt,t,usepdftitle=false,aspectratio=169]{beamer}
\usetheme[nototalframenumber,license]{uibk}

\title{Classification and Regression Trees and Beyond}
\subtitle{Supervised Learning: Algorithmic Modeling}
\author{Lisa Schlosser, Achim Zeileis}

%% forest header image
\renewcommand{\headerimage}[1]{%
   \IfStrEqCase{#1}{%
      {1}{%
         \gdef\myheaderimageid{#1}%
         \gdef\myheaderimageposition{nw}%
         \gdef\myheaderimage{forest.jpg}%
      }}[%
         \gdef\myheaderimageid{1}%
         \gdef\myheaderimageposition{nw}%
         \gdef\myheaderimage{forest.jpg}%
      ]%
}
\headerimage{1}

%% custom subsection slides
\setbeamercolor*{subsectionfade}{use={normal text},parent={normal text},fg=structure.fg!30!normal text.bg}
\AtBeginSubsection[]{%
  \begin{frame}[c]
    \begin{center}
      \usebeamercolor[fg]{subsectionfade}
      \Large \insertsection \\[2ex]
      \usebeamercolor[fg]{structure}
      \huge\bfseries\insertsubsection
    \end{center}
  \end{frame}
}

%% for \usepackage{Sweave}
\SweaveOpts{engine=R, eps=FALSE, echo=FALSE, results=hide, keep.source=TRUE}

<<preliminaries, echo=FALSE, results=hide>>=
options(prompt = "R> ", continue = "+  ", useFancyQuotes = FALSE, width = 70)

library("rpart")
library("partykit")
library("coin")

set.seed(7)
@

<<data>>=
data("CPS1985", package = "AER")
data("Titanic", package = "datasets")
ttnc <- as.data.frame(Titanic)
ttnc <- ttnc[rep(1:nrow(ttnc), ttnc$Freq), 1:4]
names(ttnc)[2] <- "Gender"
@



\begin{document}

\section{Classification and regression trees and beyond}

\subsection{Motivation}
%\subsectionpage

\begin{frame}[fragile]
\frametitle{Motivation}

\smallskip

\textbf{Idea:} ``Divide and conquer.''
\begin{itemize}
  \item \emph{Goal:} Split the data into small(er) and (rather) homogenous subgroups.
  \item \emph{Inputs:} Explanatory variables (or covariates/regressors) used for splitting.
  \item \emph{Output:} Prediction for dependent (or target) variable(s).
\end{itemize}

\bigskip
\pause

\textbf{Formally:}
\begin{itemize}
  \item Dependent variable $Y$ (possibly multivariate).
  \item Based on explanatory variables $X_1, \dots, X_m$.
  \item ``Learn'' subgroups of data by combining splits in $X_1, \dots, X_m$.
  \item Predict $Y$ with (simple) model in the subgroups, often simply the mean.
\end{itemize}
  
\bigskip
\pause

\textbf{Key features:}
\begin{itemize}
  \item Predictive power in nonlinear regression relationships.
  \item Interpretability (enhanced by tree visualization), i.e., no ``black box''.
\end{itemize}

\end{frame}


\begin{frame}[fragile]
\frametitle{Motivation}

\textbf{Example:} Survival (yes/no) on the Titanic. Women and children first?

\begin{center}
\setkeys{Gin}{width=0.9\textwidth}
<<ctree_Titanic, echo=FALSE, fig=TRUE, width=11>>=
ct_ttnc <- ctree(Survived ~ Gender + Age + Class, data = ttnc, alpha = 0.01)
plot(ct_ttnc)
@
\end{center}

\end{frame}


\begin{frame}[fragile]
\frametitle{Motivation}

\textbf{Model formula:} \code{Survived ~ Gender + Age + Class}.

\bigskip

\textbf{Data:} Information on the survival of 2201~passengers on board the
ill-fated maiden voyage of the RMS~Titanic in 1912. A data frame containing
\Sexpr{nrow(ttnc)} observations on \Sexpr{ncol(ttnc)} variables.

\bigskip

\begin{tabular}{ll}
\hline 
Variable        & Description\\
\hline 
\code{Class}    & Factor: 1st, 2nd, 3rd, or Crew.\\
\code{Gender}   & Factor: Male, Female.\\
\code{Age}      & Factor: Child, Adult.\\
\code{Survived} & Factor: No, Yes.\\
\hline 
\end{tabular}

\end{frame}


\begin{frame}[fragile]
\frametitle{Motivation}

\textbf{Example:} Determinants of wages and returns to education.

\begin{center}
\setkeys{Gin}{width=0.99\textwidth}
<<ctree_Wages1985, echo=FALSE, fig=TRUE, width=13>>=
ct_cps <- ctree(log(wage) ~ education + experience + age + ethnicity + gender + union, data = CPS1985, alpha = 0.01)
plot(ct_cps)
@
\end{center}

\end{frame}


\begin{frame}[fragile]
\frametitle{Motivation}

\textbf{Model formula:} \code{log(wage) ~ education + experience + age + ethnicity + gender + union}.

\bigskip

\textbf{Data:} Random sample from the May 1985 US Current Population Survey.
A data frame containing \Sexpr{nrow(CPS1985)} observations on \Sexpr{ncol(CPS1985)} variables.

\bigskip

\begin{tabular}{ll}
\hline 
Variable & Description\\
\hline 
\code{wage}       & Wage (in US dollars per hour). \\
\code{education}  & Education (in years).  \\
\code{experience} & Potential work experience (in years, \code{age - education - 6}). \\
\code{age}        & Age (in years). \\
\code{ethnicity}  & Factor: Caucasian, Hispanic, Other. \\
\code{gender}     & Factor: Male, Female. \\
\code{union}      & Factor. Does the individual work on a union job? \\
\hline 
\end{tabular}

\end{frame}


\begin{frame}[fragile]
\frametitle{Motivation}

\textbf{Alternatively:} Linear regression tree.

\begin{center}
\setkeys{Gin}{width=0.9\textwidth}
<<lmtree_Wages1985, echo=FALSE, fig=TRUE, width=11>>=
mob_cps <- lmtree(log(wage) ~ education | experience + age + ethnicity + gender + union, data = CPS1985)
plot(mob_cps)
@
\end{center}

\end{frame}

\begin{frame}[fragile]
\frametitle{Motivation}

\textbf{Model formula:} \code{log(wage) ~ education | experience + age + ethnicity + gender + union}.

\bigskip

\textbf{Model fit:}
\begin{itemize}
  \item Not just a group-specific mean.
  \item But group-specific intercept and slope (i.e., returns to education).
\end{itemize}

\end{frame}


\begin{frame}[fragile]
\frametitle{Motivation}

\textbf{Example:} Nowcasting (1--3 hours ahead) of wind direction at Innsbruck Airport.

\setkeys{Gin}{width=0.87\linewidth}
\begin{center}
\includegraphics{circtree_ibk.pdf}
\end{center}

\end{frame}

\begin{frame}[fragile]
\frametitle{Motivation}

\textbf{Data:} 41,979 data points for various weather observations.
\begin{itemize}
  \item Dependent variable: Wind direction 1--3 hours ahead.
  \item Explanatory variables: Current weather observations including
    wind direction, wind speed, temperature, (reduced) air pressure, relative humidity.
  \item Circular response in $[0^{\circ}, 360^{\circ})$ with $0^{\circ} = 360^{\circ}$.
\end{itemize}

\bigskip

\textbf{Model fit:} Circular distribution (von Mises), fitted by maximum likelihood.

\end{frame}


% \begin{frame}[fragile]
% \frametitle{Motivation}
% {\small Wage data:  \quad $log(wage) \; \sim  \; education + experience + ethnicity + region + parttime$}
% \begin{center}
% \setkeys{Gin}{width=0.99\textwidth}
% <<ctree_Wages1988, echo=FALSE, fig=TRUE, width=13>>=
% data("CPS1988", package = "AER")
% ## ctree
% plot(ctree(log(wage)~education+experience+ethnicity+region+parttime,data=CPS1988,
%            control = ctree_control(alpha = 0.01, minsplit = 7000)))
% @
% \end{center}
% \end{frame}

% \begin{frame}[fragile]
% \frametitle{Motivation}
% {\small Boston housing:  \quad $medv \; \sim  \; lstat + crim + rm + age + black$}
% \begin{center}
% \setkeys{Gin}{width=0.99\textwidth}
% <<ctree_Boston, echo=FALSE, fig=TRUE, width=13>>=
% data("Boston")
% plot(ctree(medv~lstat + crim + rm + age + black, 
%            data = Boston, control = ctree_control(alpha = 0.01, minsplit = 150)))
% @
% \end{center}
% \end{frame}



\begin{frame}[fragile]
\frametitle{Motivation}

\textbf{Original idea:} Trees are purely algorithmic models without assumptions.
\begin{itemize}
  \item Data-driven ``learning'' of homogenous subgroups.
  \item Simple constant fit in each group, e.g., an average or proportion.
\end{itemize}

\bigskip
\pause

\textbf{Subsequently:} Trees are well-suited for combination with classical models.
\begin{itemize}
  \item Group-wise models, e.g., fitted by least squares or maximum likelihood.
  \item Model-based learning accounts for model differences across subgroups.
\end{itemize}

\bigskip
\pause

\textbf{Trade-off:} 
\begin{itemize}
  \item Assume simple model and learn larger tree \emph{vs.}
  \item More complex model and potentially smaller tree.
\end{itemize}

\end{frame}
 

\subsection{Tree algorithm}

\begin{frame}[fragile]
\frametitle{Tree algorithm}

\textbf{Base algorithm:}
\begin{enumerate}
\item Fit a model to the response $Y$.
\item Assess association of $Y$ (or a corresponding transformation/goodness-of-fit measure) and each possible split variable $X_j$.
\item Split sample along the $X_{j^{\ast}}$ with strongest association: Choose split point with highest improvement of the model fit.
\item Repeat steps 1--3 recursively in the subsamples until some stopping criterion is met.
\item \emph{Optionally:} Reduce size of the tree by pruning branches of splits that do not improve the model fit sufficiently.
\end{enumerate}

\end{frame}

\begin{frame}[fragile]
\frametitle{Tree algorithm}

\textbf{Specific algorithms:} Many (albeit not all) can be derived from the base algorithm
by combining suitable building blocks.

\bigskip
\pause

\textbf{Models for $Y$:} Simple constant fits vs.\ more complex statistical models.

\bigskip
\pause

\textbf{Goodness of fit:} Suitable measure depends on type of response variable(s) $Y$
and the corresponding model.

\end{frame}

\begin{frame}[fragile]
\frametitle{Tree algorithm}

\textbf{Goodness of fit:}
\begin{itemize}
  \item Numeric response: 
  \begin{itemize}
    \item $Y$ or ranks of $Y$.
    \item (Absolute) deviations $Y - \bar Y$.
    \item Residual sum of squares $\sum (Y - \hat{Y})^2$.
  \end{itemize}
  \pause
  \item Categorical response: 
  \begin{itemize}
    \item Dummy variables for categories.
    \item Number of misclassifications.
    \item Gini impurity.
  \end{itemize}
  \pause
  \item Survival response:
  \begin{itemize}
    \item Log-rank scores.
  \end{itemize}
  \pause
  \item Parametric model:
  \begin{itemize}
    \item Residuals.
    \item Model scores (gradient contributions).
  \end{itemize}
\end{itemize}

\end{frame}

\begin{frame}[fragile]
\frametitle{Tree algorithm}

\textbf{Split variable selection:} Optimize some criterion over all $X_j$ ($j = 1, \dots, m$).
\begin{itemize}
  \item Objective function (residual sum of squares, log-likelihood, misclassification rate, impurity, \dots).
  \item Test statistic or corresponding $p$-value.
\end{itemize}

\bigskip
\pause

\textbf{Split point selection:} Optimize some criterion over all (binary) splits in $X_{j*}$.
\begin{itemize}
  \item Objective function.
  \item Two-sample test statistic or corresponding $p$-value.
\end{itemize}

\bigskip
\pause

\textbf{Stopping criteria:}
\begin{itemize}
  \item Constraints: Number of observations per node, tree depth, \dots
  \item Lack of improvement: Significance, information criteria, \dots
\end{itemize}

\end{frame}


\subsection{Split variable selection}

\begin{frame}[fragile]
\frametitle{Split variable selection}

\textbf{Idea:}
\begin{itemize}
  \item Select variable $X_j$ ($j = 1, \dots, m$) most associated with heterogeneity in $Y$.
  \item Heterogeneity captured by goodness-of-fit measure.
  \item \emph{Often:} Maximum association over all possible binary splits.
  \item \emph{Alternatively:} Overall association.
\end{itemize}

\end{frame}


\begin{frame}[fragile]
\frametitle{Split variable selection}

\textbf{Potential bias:} Variables with many potential splits may yield greater
association ``by chance'', e.g., continuous $X_j$ or categorical with many levels.

\bigskip
\pause

\textbf{Unbiased recursive partitioning:} Accounts for potential random variation by
employing $p$-values from appropriate statistical tests.

\bigskip
\pause

\textbf{Possible tests:} Depend on scales of $Y$, $X_j$, and the adopted model.
\begin{itemize}
  \item $\chi^2$ test, Pearson correlation test, two-sample t-test, ANOVA.
  \item Maximally-selected two-sample tests.
  \item Parameter instability tests.
  \item \dots
\end{itemize}

\end{frame}


\begin{frame}[fragile]
\frametitle{Split variable selection}

\textbf{Examples:}
\begin{itemize}
  \item Titanic survival and wage determinants data.
  \item Selection of first split variable in root node.
  \item Employ classical statistical tests.
  \item Does not exactly match a particular tree algorithm but similar to CTree.
\end{itemize}

\end{frame}


\begin{frame}[fragile]
\frametitle{Split variable selection}
\textbf{Assess pairwise associations:} \code{Survived ~ Gender} (Titanic).

\begin{minipage}[t]{0.43\linewidth}
<<survival-gender, echo=TRUE, eval=FALSE>>=
plot(Survived ~ Gender, data = ttnc)
@
\vspace{-0.7cm}
<<survival-gender-plot, echo=FALSE, fig=TRUE, width=4.5, height=3.4>>=
<<survival-gender>>
@
\end{minipage}
\hfill
\pause
\begin{minipage}[t]{0.5\linewidth}
<<echo=TRUE, results=verbatim>>=
xtabs(~ Survived + Gender, data = ttnc)
@
\end{minipage}

\pause

<<echo=TRUE, results=verbatim>>=
chisq.test(xtabs(~ Survived + Gender, data = ttnc))
@

\end{frame}



\begin{frame}[fragile]
\frametitle{Split variable selection}
\textbf{Assess pairwise associations:} \code{Survived ~ Age} (Titanic).

\begin{minipage}[t]{0.43\linewidth}
<<survival-age, echo=TRUE, eval=FALSE>>=
plot(Survived ~ Age, data = ttnc)
@
\vspace{-0.7cm}
<<survival-age-plot, echo=FALSE, fig=TRUE, width=4.5, height=3.4>>=
<<survival-age>>
@
\end{minipage}
\hfill
\pause
\begin{minipage}[t]{0.5\linewidth}
<<echo=TRUE, results=verbatim>>=
xtabs(~ Survived + Age, data = ttnc)
@
\end{minipage}

<<echo=TRUE, results=verbatim>>=
chisq.test(xtabs(~ Survived + Age, data = ttnc))
@

\end{frame}


\begin{frame}[fragile]
\frametitle{Split variable selection}
\textbf{Assess pairwise associations:} \code{Survived ~ Class} (Titanic).

\begin{minipage}[t]{0.43\linewidth}
<<survival-class, echo=TRUE, eval=FALSE>>=
plot(Survived ~ Class, data = ttnc)
@
\vspace{-0.7cm}
<<survival-class-plot, echo=FALSE, fig=TRUE, width=4.5, height=3.4>>=
<<survival-class>>
@
\end{minipage}
\hfill
\pause
\begin{minipage}[t]{0.5\linewidth}
<<echo=TRUE, results=verbatim>>=
xtabs(~ Survived + Class, data = ttnc)
@
\end{minipage}

<<echo=TRUE, results=verbatim>>=
chisq.test(xtabs(~ Survived + Class, data = ttnc))
@

\end{frame}


% 
% \begin{frame}[fragile]
% \frametitle{Split variable selection} 
% \textbf{Compare pairwise associations} (Titanic): 
% Based on independence tests, e.g., $\chi^2$-test for categorical variables.
% \vspace{0.4cm}
% <<echo=TRUE, results=verbatim>>=
% chisq_test(Survived ~ Gender, data = ttnc)
% @
% 
% \vspace{0.2cm}
% 
% <<echo=TRUE, results=verbatim>>=
% chisq_test(Survived ~ Age, data = ttnc)
% @
% 
% \vspace{0.7cm}
% 
% $\Rightarrow$ Select the split variable showing the lowest $p$-value.
% 
% \end{frame}


\begin{frame}[fragile]
\frametitle{Split variable selection}

\begin{center}
\setkeys{Gin}{width=0.95\textwidth}
<<echo=FALSE, fig=TRUE, width=11>>=
<<ctree_Titanic>>
@
\end{center}
\end{frame}


\begin{frame}[fragile]
\frametitle{Split variable selection}
\textbf{Assess pairwise associations:} \code{log(wage) ~ education} (Wages).

\begin{minipage}[t]{0.39\linewidth}
<<logwage-education, echo=TRUE, eval=FALSE>>=
plot(log(wage) ~ education,
  data = CPS1985)
@
\vspace{-0.7cm}
\setkeys{Gin}{width=\textwidth}
<<logwage-education-plot, echo=FALSE, fig=TRUE, width=4.5, height=3.4>>=
<<logwage-education>>
@
\end{minipage}
\hfill
\pause
\begin{minipage}[t]{0.59\linewidth}
<<echo=TRUE, eval=FALSE>>=
cor.test(~ log(wage) + education, data = CPS1985)
@
<<echo=FALSE, results=verbatim>>=
out <- capture.output(cor.test(~ log(wage) + education, data = CPS1985))
out <- gsub("alternative hypothesis: ", "alternative hypothesis:\n", out, fixed = TRUE)
writeLines(out)
@
\end{minipage}

\end{frame}


\begin{frame}[fragile]
\frametitle{Split variable selection}
\textbf{Assess pairwise associations:} \code{log(wage) ~ gender} (Wages).

\begin{minipage}[t]{0.39\linewidth}
<<logwage-gender, echo=TRUE, eval=FALSE>>=
plot(log(wage) ~ gender,
  data = CPS1985)
@
\vspace{-0.7cm}
\setkeys{Gin}{width=\textwidth}
<<logwage-gender-plot, echo=FALSE, fig=TRUE, width=4.5, height=3.4>>=
<<logwage-gender>>
@
\end{minipage}
\hfill
\pause
\begin{minipage}[t]{0.59\linewidth}
<<echo=TRUE, eval=FALSE>>=
t.test(log(wage) ~ gender, data = CPS1985)
@
<<echo=FALSE, results=verbatim>>=
out <- capture.output(t.test(log(wage) ~ gender, data = CPS1985))
out <- gsub("alternative hypothesis: ", "alternative hypothesis:\n", out, fixed = TRUE)
writeLines(out)
@
\end{minipage}

\end{frame}


\begin{frame}[fragile]
\frametitle{Split variable selection}

\begin{center}
\setkeys{Gin}{width=0.99\textwidth}
<<echo=FALSE, fig=TRUE, width=13>>=
<<ctree_Wages1985>> 
@
\end{center}

\end{frame}


\subsection{Split point selection \& pruning}

\begin{frame}[fragile]
\frametitle{Split point selection}

\textbf{Idea:}
\begin{itemize}
  \item Split $Y$ into most homogenous subgroups with respect to selected $X_{j*}$.
  \item Homogeneity captured by goodness-of-fit measure.
  \item \emph{Often:} Consider only binary splits.
  \item Trivial for binary $X_{j*}$.
  \item Otherwise typically exhaustive search over all binary splits.
  \item Possibly already done in split variable selection.
\end{itemize}

\bigskip
\pause

\textbf{Goodness-of-fit measure:} Depends on scale of $Y$ and the adopted model.
\begin{itemize}
  \item Objective function (residual sum of squares, log-likelihood, misclassification rate, impurity, \dots).
  \item Two-sample test statistic or corresponding $p$-value.
\end{itemize}

\end{frame}


\begin{frame}[fragile]
\frametitle{Split point selection}

\textbf{Example:}
\begin{itemize}
  \item Wage determinants data.
  \item Selection of best split point in education in root node.
  \item Residual sum of squares vs.\ normal log-likelihood.
  \item Two-sample ANOVA-type $\chi^2$ test and corresponding $p$-value.
\end{itemize}

\end{frame}


<<split_point_search>>=
evalsplit <- function(sp) {
  m <- lm(log(wage) ~ factor(education <= sp), data = CPS1985)
  s <- independence_test(log(wage) ~ factor(education <= sp), data = CPS1985, teststat = "quadratic") 
  c(
    rss = deviance(m),
    loglik = logLik(m),
    logpval = pchisq(statistic(s), df = 1, log = TRUE, lower.tail = FALSE),
    teststatistic = statistic(s)
  )
}  
sp <- head(sort(unique(CPS1985$education)), -1)
eval <- sapply(sp, evalsplit)
@


\begin{frame}[fragile]
\frametitle{Split point selection}

<<plot_split_point_search1, fig=TRUE, width = 7.5, height = 5>>=
par(mar = c(5, 3, 1, 3))
plot(sp, eval["loglik",], type = "n", xlab = "education", ylab = "", axes = FALSE)
abline(v = 13, lty = 2, lwd = 1.5)
lines(sp, eval["loglik", ], type = "o", col = 2, lwd = 1.5)
axis(2, col.ticks = 2, col.axis = 2)
par(new = TRUE)
plot(sp, eval["teststatistic",], type = "o",
  col = 4, lwd = 1.5,
  xlab = "", ylab = "", axes = FALSE)
axis(4, col.ticks = 4, col.axis = 4)
axis(1)
axis(1, at = 13)
box()
legend("topleft", c("Log-likelihood", "Test statistic"), col = c(2, 4), lwd = 1.5, pch = 1, bty = "n")
@

\end{frame}


\begin{frame}[fragile]
\frametitle{Split point selection}

<<plot_split_point_search2, fig=TRUE, width = 7.5, height = 5>>=
par(mar = c(5, 3, 1, 3))
plot(sp, eval["rss",], type = "n", xlab = "education", ylab = "", axes = FALSE)
abline(v = 13, lty = 2, lwd = 1.5)
lines(sp, eval["rss", ], type = "o", col = 2, lwd = 1.5)
axis(2, col.ticks = 2, col.axis = 2)
par(new = TRUE)
plot(sp, eval["logpval",], type = "o",
  col = 4, lwd = 1.5,
  xlab = "", ylab = "", axes = FALSE)
axis(4, col.ticks = 4, col.axis = 4)
axis(1)
axis(1, at = 13)
box()
legend("bottomleft", c("Residual sum of squares", "Log-p-value"), col = c(2, 4), lwd = 1.5, pch = 1, bty = "n")
@

\end{frame}

\begin{frame}[fragile]
\frametitle{Split point selection}

\begin{center}
\setkeys{Gin}{width=0.99\textwidth}
<<echo=FALSE, fig=TRUE, width=13>>=
<<ctree_Wages1985>> 
@
\end{center}

\end{frame}


\begin{frame}[fragile]
\frametitle{Pruning}

\textbf{Goal:} Avoid overfitting.

\medskip

\textbf{Pre-pruning:} Internal stopping criterium. Stop splitting when there is no significant association to any of the possible split variables $X_j$.

\medskip

\textbf{Post-pruning:} Grow large tree and prune splits that do not improve the model fit (e.g., via cross-validation or information criteria).

\end{frame}


\begin{frame}[fragile]
\frametitle{Pruning}

\textbf{Pre-pruning:}
\begin{itemize}
\item[+] Does not require additional calculations as only already provided information from statistical test is used.
\item[+] Computationally less expensive as trees are prevented from getting too large.
\item[-] Performance depends on the power of the selected statistical test.
\end{itemize}

\vspace{0.4cm}

\textbf{Post-pruning:}
\begin{itemize}
\item[+] Stable method to avoid overfitting, regardless of the power of the employed statistical test.
\item[+] Can employ information criteria such as AIC or BIC for model-based partitioning. 
\item[-] Computationally more expensive as very large trees are grown and additional (out-of-bag) evaluations are required.
\end{itemize}

\end{frame}


\subsection{Algorithms \& implementations}

\begin{frame}
\frametitle{CTree}

\begin{itemize} 
\item Conditional inference trees.
\item Flexible and general framework for unbiased recursive partitioning,
originally proposed for nonparametric modeling but also allowing for a model based approach via transformations of the response. 
\item Split variable selection based on conditional permutation tests.
\item Split point selection based on two-sample test statistics.
\item Employs pre-pruning.
\item Implemented in the R package partykit.
\end{itemize}

\bigskip

\textbf{Note:} The CTree algorithm has been employed for all previous tree illustrations.

\end{frame}


\begin{frame}[fragile]
\frametitle{CTree}

Apply \fct{ctree} to preprocessed \code{ttnc} data:

<<ctree, echo=TRUE, eval=FALSE>>=
ct <- ctree(Survived ~ Gender + Age + Class, data = ttnc)
@

\medskip

Plot:

<<ctree-plot, echo=TRUE, eval=FALSE>>=
plot(ct)
@

\medskip

Add control arguments via \fct{ctree\_control}:

<<ctree_control, echo=TRUE, eval=FALSE>>=
ct <- ctree(Survived ~ Gender + Age + Class, data = ttnc,
            control = ctree_control(alpha = 0.01,
                                    minsplit = 20, 
                                    maxdepth = 3))
@
\end{frame}


\begin{frame}[fragile]
\frametitle{CTree}

\setkeys{Gin}{width=0.9\linewidth}
<<ctree_control_plot, echo=TRUE, eval=TRUE, fig=TRUE, width=10, height=5>>=
ct <- ctree(Survived ~ Gender + Age + Class, data = ttnc,
            control = ctree_control(alpha = 0.01, minsplit = 20, maxdepth = 3))
plot(ct)
@

\end{frame}



\begin{frame}[fragile]
\frametitle{CTree}

\textbf{Predictions:} For a (possibly new) set of observations the method \fct{predict} can be employed to obtain the corresponding predicted values of the response variable. 

<<ctree_predict, echo=TRUE, eval=TRUE, results=verbatim>>=
nd <- data.frame(Class = "2nd", Gender = c("Female", "Male"), Age = "Child")
nd
predict(ct, newdata = nd)
@

\end{frame}



\begin{frame}
\frametitle{rpart}

\begin{itemize} 
\item Recursive PARTitioning.
\item Based on ideas of CART (Classification And Regression Trees).
\item Split variable and split point selection based on optimizing an objective function.
\item Classification trees: reduce number of misclassifications.
\item Regression trees: reduce objective function for numeric response.
\item Post-pruning can be employed.
\item Implemented in the R package rpart.
\end{itemize}

\end{frame}


\begin{frame}[fragile]
\frametitle{rpart}

Apply \fct{rpart} to preprocessed \code{ttnc} data:

<<rpart, echo=TRUE, eval=TRUE>>=
rp <- rpart(Survived ~ Gender + Age + Class, data = ttnc)
@

\medskip

Standard plot:

<<rpart-plot, echo=TRUE, eval=FALSE>>=
plot(rp)
text(rp)
@

Visualization via partykit:

<<rpart-party-plot, echo=TRUE, eval=FALSE>>=
plot(as.party(rp))
@
\end{frame}

\begin{frame}[fragile]
\frametitle{rpart}

<<rpart-plot_eval, echo=TRUE, eval=FALSE>>=
plot(rp)
text(rp)
@

\vspace{-0.7cm}

\setkeys{Gin}{width=0.9\linewidth}
<<rpart-plot_eval, echo=FALSE, eval=TRUE, fig=TRUE, width=10, height = 6>>=
plot(rp)
text(rp)
@

\end{frame}



\begin{frame}[fragile]
\frametitle{rpart}

<<rpart-party-plot_eval, echo=TRUE, eval=FALSE>>=
plot(as.party(rp))
@

%\vspace{-0.4cm}

\setkeys{Gin}{width=0.9\linewidth}
<<rpart-party-plot_eval, echo=FALSE, eval=TRUE, fig=TRUE, width = 12>>=
plot(as.party(rp))
@
\end{frame}


\begin{frame}[fragile]
\frametitle{rpart}

Standard output: 
<<rpart-print, echo=TRUE, results=verbatim>>=
rp
@

\end{frame}



\begin{frame}[fragile]
\frametitle{rpart}

Output via partykit: 
<<rpart-party_print, echo=TRUE, results=verbatim>>=
as.party(rp)
@

\end{frame}

\begin{frame}[fragile]
\frametitle{rpart}

\textbf{Post-pruning:} Cost-complexity pruning of an rpart-object with complexity parameter \texttt{cp}.

%\medskip

<<rpart_pruning, echo=TRUE, eval=TRUE, results=verbatim>>=
rp_pruned <- prune(rp, cp = 0.01)
@

\setkeys{Gin}{width=0.6\linewidth}
<<rpart_pruning_plot, echo=TRUE, eval=TRUE, fig=TRUE, width = 8, height=5>>=
plot(as.party(rp_pruned))
@
% plot(rp_pruned)
% text(rp_pruned)

\end{frame}

\begin{frame}[fragile]
\frametitle{rpart}

\textbf{Post-pruning:} Cost-complexity pruning of an rpart-object with complexity parameter \texttt{cp}.

%\medskip

<<rpart_pruning2, echo=TRUE, eval=TRUE, results=verbatim>>=
rp_pruned <- prune(rp, cp = 0.02)
@

\setkeys{Gin}{width=0.6\linewidth}
<<rpart_pruning2_plot, echo=TRUE, eval=TRUE, fig=TRUE, width = 8, height=5>>=
plot(as.party(rp_pruned))
@
% plot(rp_pruned)
% text(rp_pruned)

\end{frame}

\begin{frame}[fragile]
\frametitle{rpart}

\textbf{Post-pruning:} Cost-complexity pruning of an rpart-object with complexity parameter \texttt{cp}.

%\medskip

<<rpart_pruning3, echo=TRUE, eval=TRUE, results=verbatim>>=
rp_pruned <- prune(rp, cp = 0.03)
@

\setkeys{Gin}{width=0.6\linewidth}
<<rpart_pruning3_plot, echo=TRUE, eval=TRUE, fig=TRUE, width =8, height=5>>=
plot(as.party(rp_pruned))
@
% plot(rp_pruned)
% text(rp_pruned)

\end{frame}




\begin{frame}
\frametitle{MOB}

\begin{itemize} 
\item Model-based recursive partitioning using M-estimation (ML, OLS, CRPS, \ldots).
\item Split variable selection based on asymptotic parameter instability tests.
\item Split point selection based on exhaustive search.
\item Employs pre-pruning, but can also use post-pruning based on AIC or BIC.
\item Implemented in the R package partykit.
\end{itemize}

\end{frame}


% \begin{frame}
% \frametitle{R software for tree models}
% 
% \textbf{rpart:} 
% \begin{itemize}
% \item R package for Recursive PARTitioning.
% \item Implementation based on ideas of CART (Classification And Regression Trees).
% \item Classification trees: reduce number of misclassifications.
% \item Regression trees: reduce objective function for numeric response.
% \end{itemize}
% 
% \vspace{0.4cm}
% 
% \textbf{partykit:} 
% \begin{itemize} 
% \item R package offering a toolkit for unbiased recursive partitioning. 
% \item Implementations of the tree algorithms CTree (Conditional Inference Trees) and MOB (Model-based Recursive Partitioning).
% \item Provides a very general framework with a unifying infrastructure.
% \item Can import rpart.
% \end{itemize}
% 
% \end{frame}








\begin{frame}[fragile]
\frametitle{MOB}

Is ``women and children first'' applied equally for all passenger classes?

\bigskip

Add a factor variable distinguishing women and children from male adults:
<<ttnc_treatment, echo=TRUE, eval=TRUE>>=
ttnc <- transform(ttnc, Treatment = factor(Gender == "Female" | Age == "Child", 
                                           levels = c(FALSE, TRUE),
                                           labels = c("Male&Adult", "Female|Child")))
@

\bigskip

Assess differences in the preferential treatment effect over the remaining covariates by applying \fct{glmtree} which employs the MOB algorithm for generalized linear models in the tree nodes:

<<glmtree, echo=TRUE, eval=TRUE, results=verbatim>>=
glmt <- glmtree(Survived ~ Treatment | Class + Gender + Age,
                data = ttnc, family = binomial, alpha = 0.01)
@

<<glmtree_print, echo=TRUE, eval=FALSE>>=
glmt
@
\end{frame}


\begin{frame}[fragile]
\frametitle{MOB}
\vspace{-0.4cm}
\small
<<glmtree_print_eval, echo=FALSE, eval=TRUE, results=verbatim>>=
glmt
@
\end{frame}


\begin{frame}[fragile]
\frametitle{MOB}
\setkeys{Gin}{width=0.99\textwidth}
<<glmtree-plot, echo=TRUE, eval=TRUE, fig=TRUE, width=11, height=5>>=
plot(glmt)
@
\end{frame}




\begin{frame}
\frametitle{R software for tree models}

\textbf{Further R packages based on partykit:}
\begin{itemize}
  \item evtree (evolutionary trees),
  \item model4you (personalised trees),
  \item disttree (distributional trees),
  \item stablelearner (stability assessment),
  \item \ldots
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{R software for tree models}

\textbf{Further R packages:}
\begin{itemize}
  \item mvpart (multivariate CART),
  \item RWeka (J4.8, M5', LMT),
  \item and many more (C50, quint, stima, \dots).
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{R software for tree models}

\textbf{Furthermore:} Tree algorithms/software without R interface, e.g.,
%
\begin{itemize}
  \item QUEST,
  \item GUIDE,
  \item LOTUS,
  \item CRUISE,
  \item \dots
\end{itemize}

\end{frame}



% \subsection*{References}

\begin{frame}
\frametitle{References}

%\vspace{-0.2cm}

\small

Breiman L, Friedman JH, Olshen RA, Stone CJ (1984).
  \dquote{Classification and Regression Trees.}
  \emph{Wadsworth, California.}, 

\medskip

% Breiman L (2001).
%   \dquote{Random {Forests}.}
%   \emph{Machine Learning}, 
%   \textbf{45}(1), 5--32.
%   \doi{10.1023/A:1010933404324}
% 
% \medskip

Hothorn T, Hornik K, Zeileis A (2006).
 \dquote{Unbiased Recursive Partitioning: A Conditional Inference Framework.}
 \emph{Journal of Computational and Graphical Statistics},
 \textbf{15}(3), 651--674.
 \doi{10.1198/106186006X133933}
 
\medskip

Zeileis A, Hothorn T, Hornik K (2008).
 \dquote{Model-Based Recursive Partitioning.}
  \emph{Journal of Computational and Graphical Statistics},
  \textbf{17}(2), 492--514.
  \doi{10.1198/106186008X319331}

\medskip

Hothorn T, Zeileis A (2015).
 \dquote{{partykit}: A Modular Toolkit for Recursive Partytioning in \textsf{R}.}
 \emph{Journal of Machine Learning Research},
 \textbf{16}, 3905--3909.
 \url{http://www.jmlr.org/papers/v16/hothorn15a.html}
 
\medskip

Therneau T, Atkinson B (2019). 
  \dquote{{rpart}: Recursive Partitioning and Regression Trees.}
  \url{https://CRAN.R-project.org/package=rpart}

% medskip
%  
% Schlosser L, Hothorn T, Stauffer R, Zeileis A (2019).
% \dquote{Distributional Regression Forests for Probabilistic Precipitation Forecasting in Complex Terrain.}
% \emph{The Annals of Applied Statistics}, \textbf{13}(3), 1564--1589.
% \doi{10.1214/19-AOAS1247}
 
% \medskip

% Breiman L (2001).
%   \dquote{Statistical Modeling: The Two Cultures.}
%   \emph{Statistical Science}, 
%   \textbf{16}(3), 199--231.
%   \doi{10.1214/ss/1009213726}

\end{frame}

\end{document}
