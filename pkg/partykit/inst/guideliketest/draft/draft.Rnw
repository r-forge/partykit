\documentclass[nojss]{jss}

%% packages
\usepackage{amstext,amsfonts,amsmath,amssymb,bm,thumbpdf,lmodern,hyperref}
\usepackage[all]{hypcap}

%% need no \usepackage{Sweave} ?
%\usepackage{Sweave}
\SweaveOpts{engine = R, concordance = FALSE, eps = FALSE, keep.source = TRUE, eval = TRUE}

%% commands
\newcommand{\fixme}[1]{\emph{\marginpar{FIXME} (#1)}}
\renewcommand{\Prob}{\mathbb{P} }
\renewcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\Var}{\mathbb{V}}
\newcommand{\R}{\mathbb{R} }
\newcommand{\N}{\mathbb{N} }
\newcommand{\C}{\mathbb{C} }
\newcommand{\argmin}{\operatorname{argmin}\displaylimits}
\newcommand{\argmax}{\operatorname{argmax}\displaylimits}
\newcommand{\LS}{\mathcal{L}_n}
\newcommand{\TS}{\mathcal{T}_n}
\newcommand{\LSc}{\mathcal{L}_{\text{comb},n}}
\newcommand{\LSbc}{\mathcal{L}^*_{\text{comb},n}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\yn}{y_{\text{new}}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\sX}{\mathcal{X}}
\newcommand{\sY}{\mathcal{Y}}
\newcommand{\T}{\mathbf{T}}
\newcommand{\x}{\mathbf{x}}
\renewcommand{\a}{\mathbf{a}}
\newcommand{\xn}{\mathbf{x}_{\text{new}}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\ws}{\mathbf{w}_\cdot}
\renewcommand{\t}{\mathbf{t}}
\newcommand{\M}{\mathbf{M}}
\renewcommand{\vec}{\text{vec}}
\newcommand{\B}{\mathbf{B}}
\newcommand{\K}{\mathbf{K}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\cellx}{\pi_n[\x]}
\newcommand{\partn}{\pi_n(\mathcal{L}_n)}
\newcommand{\err}{\text{Err}}
\newcommand{\ea}{\widehat{\text{Err}}^{(a)}}
\newcommand{\ecv}{\widehat{\text{Err}}^{(cv1)}}
\newcommand{\ecvten}{\widehat{\text{Err}}^{(cv10)}}
\newcommand{\eone}{\widehat{\text{Err}}^{(1)}}
\newcommand{\eplus}{\widehat{\text{Err}}^{(.632+)}}
\newcommand{\eoob}{\widehat{\text{Err}}^{(oob)}}
\newcommand{\bft}{\mathbf{t}}

<<preliminaries, echo=FALSE, results=hide>>=
# setwd("~/svn/partykit/pkg/partykit/inst/guideliketest/draft/")
options(prompt = "R> ", continue = "+  ", useFancyQuotes = FALSE, width = 70)
library("partykit")
library("Formula")
library("parallel")
library("lattice")
library("scales")
# setwd("~/svn/partykit/pkg/partykit/inst/guideliketest/draft/")
source("../guidelike.R")
source("../tests.R")
#load("../sim/simres20180422_1step.rda")
load("../sim/simres20181223_stump.rda")
sim_stump <- simres$res
#load("../sim/simres20180422_2step.rda")
#sim2step <- simres
rm(simres)
load("../sim/simres20181223_tree.rda")
sim_tree <- simres$res
rm(simres)

## HCL palette
pal <- hcl(c(140, 80, 260, 70, 10), 300, 60)
names(pal) <- c("CTree", "CTree_max", "MOB", "GUIDE", "GUIDE_full")

@


\title{The Power of Unbiased Recursive Partitioning:\\
A Unifying View of CTree, MOB, and GUIDE}
\Shorttitle{The Power of Unbiased Recursive Partitioning}
%Comparing Testing Strategies of Unbiased Tree Algorithms

\author{Lisa Schlosser\\Universit\"at Innsbruck
\And Torsten Hothorn\\Universit\"at Z\"urich
\And Achim Zeileis\\Universit\"at Innsbruck}
\Plainauthor{Lisa Schlosser, Torsten Hothorn, Achim Zeileis}

\Abstract{
A core step of every algorithm for learning regression trees is the decision if, 
how, and where to split the underlying data - or, in other words, the selection 
of the best splitting variable from the available covariates and the corresponding 
split point. Early tree algorithms (e.g., AID, CART) employ greedy search strategies 
that directly compare all possible split points in all available covariates. However, 
subsequent research showed that this is biased towards selection of covariates with 
more potential split points. Therefore, unbiased recursive partitioning algorithms 
have been suggested (e.g., QUEST, GUIDE, CTree, MOB) that first select the covariate 
based on statistical inference using $p$-values that are appropriately adjusted for 
the possible split points. In a second step a split point optimizing some objective 
function is selected in the chosen split variable. However, different unbiased tree 
algorithms employ different inference frameworks for computing these $p$-values and 
their relative advantages or disadvantages are not well understood, yet.

Therefore, three different approaches are considered here and embedded into a common 
modeling framework with special emphasis to linear model trees: classical categorical 
association tests (GUIDE), conditional inference (CTree), parameter instability 
tests (MOB). It is assessed how different building blocks affect the power of the 
tree algorithms to select the appropriate covariates for splitting: residuals vs. full 
model scores, binarization of residuals/scores at zero, binning of covariates, 
conditional vs.\ unconditional approximations of the null distribution.
}
\Keywords{classification and regression trees, independence tests, recursive partitioning}

\Address{
Lisa Schlosser, Achim Zeileis \\
Universit\"at Innsbruck \\
Department of Statistics \\
Faculty of Economics and Statistics \\
Universit\"atsstr.~15 \\
6020 Innsbruck, Austria \\
E-mail: \email{Lisa.Schlosser@uibk.ac.at}, \email{Achim.Zeileis@R-project.org} \\
URL: \url{https://www.uibk.ac.at/statistics/personal/schlosser-lisa/},\\
\phantom{URL: }\url{https://eeecon.uibk.ac.at/~zeileis/}\\

Torsten Hothorn\\
Universit\"at Z\"urich \\
Institut f\"ur Epidemiologie, Biostatistik und Pr\"avention \\
Hirschengraben 84\\
CH-8001 Z\"urich, Switzerland \\
E-mail: \email{Torsten.Hothorn@R-project.org}\\
URL: \url{http://user.math.uzh.ch/hothorn/}\\

}


\begin{document}

\section{Introduction}\label{sec:introduction}
In many situations fitting one global model to a data set can be very challenging, 
especially if this data contains lots of different features with high/extreme variations 
or complex interactions. Therefore, separating the data into more homogeneous subgroups 
based on (a set of) covariates first can simplify the task and fitting a local model to 
each of the resulting subgroups often leads to better results. This separation of the 
data can be done by applying a tree algorithm. Many different algorithms have been developed
in this research field. While they all follow the general idea of splitting the data such 
that some objective function is minimized/maximized, they differ in their 
specific/particular approaches to selecting a split variable and the corresponding split point. 
Some of the first tree algorithms (e.g., AID \citealp{Morgan+Sonquist:1963}; 
CART \citealp{Breiman+Friedman+Stone:1984}) rely on exhaustive search procedures to find both, 
the best split point and split variable in one step by directly comparing all possible split
points in all possible split variables. However, it has been shown that this is not only
computationally expensive but also biased towards split variables with many possible split
points \citep{Doyle:1973, Kim+Loh:2001}. 
Therefore, selecting a split variable in a first step and then searching for the 
best split point only within this variable in a separated second step is a more 
promising strategy as applied for example by the algorithms QUEST \citep{Loh+Shih:1997},
CHAID \citep{Kass:1980}, GUIDE \citep{Loh:2002}, CTree \citep{Hothorn+Hornik+Zeileis:2006} 
and MOB \citep{Zeileis+Hothorn+Hornik:2008}.
For the first step of selecting a split variable they all follow the idea of 
choosing the covariate which shows the highest association to/influence on the response variable 
based on a statistical test. While QUEST, CHAID and GUIDE employ statistical significance
tests for contingency tables, CTree applies permutation tests in a conditional inference
framework and MOB uses fluctuation tests based on asymptotic theory.

In this paper the focus will be put on this first step, i.e. the task of selecting 
the right/best split variable. In particular, the approach of the GUIDE algorithm is
compared to the one of the CTree algorithm and the MOB algorithm. For this purpose, a 
unifying framework for unbiased model based tree algorithms is presented such that each 
of the three algorithms can be obtained by a specific combination of the available building 
blocks. In that way, the effects of each of these characteristic features of the GUIDE,
CTree and MOB algorithm can be investigated separately by varying the switches in
the tool box provided by this general framework. 
Depending on the setup different effects can be observed for the investigated characteristics.
However, none of the results contradicts the conclusion that in general including all available 
model scores is at least as efficient but mostly clearly advantageous over considering only 
residuals. Moreover, the effects of binarizing residuals or scores at $0$ or categorizing
the values of possible split variables are investigated and the impact of applying a conditional
or an unconditional approximation of the null distribution is analyzed.

In the first part of Section~\ref{sec:urp} a basic scheme on how to build a tree is given. 
Then, after explaining the investigated testing strategies more in detail in 
Sections~\ref{sec:ctree}, \ref{sec:mob} and \ref{sec:guide}, the unifying framework for model 
based unbiased recursive partitioning is presented in Section~\ref{sec:unify}.
The setting for the simulation study is introduced in Section~\ref{sec:simulation}
and the results are discussed in Section~\ref{sec:results}.



\section{Unbiased Recursive Partitioning}
\label{sec:urp}

\subsection{Generic Algorithm}
The basic idea of building a regression tree model is to partition the data into smaller and 
more homogeneous subgroups based on (a set of) covariates. Various different tree algorithms 
have been developed, all following the same general structure:
\begin{enumerate}
\item For the whole underlying data set in the current node some form of discrepancy 
measure is calculated. 
%This can for example be the sum of deviations from a typical or average value of the 
%data set or it can be based on a fitted model.
\item Among all available covariates one is selected as split variable and a value 
within the range/measurable space of this variable is chosen as split point such that 
after splitting the data the discrepancy measure is minimal within the resulting subgroups.
\item Steps 1 and 2 are repeated within each new subgroup until either a stopping criterion
is reached or the subgroup consists of only one or all equal observations. (In the second
case the resulting tree is of maximal size and can be reduced by (post-)pruning.)
\end{enumerate}


The employed discrepancy measure can for example be the sum of deviations from a typical 
or average value, but it can also be constructed based on a statistical model. For instance,
different types of residuals or their signs can be applied (see e.g., \cite{Loh:2002}) as well as
rank sums or logrank scores as in \cite{Hothorn+Hornik+Wiel:2006}.
(In the simulation study of this paper trees with linear models fitted in each node are 
investigated with the residual sum of squares (RSS) employed as discrepancy measure. However, 
the discussed methodology and the presented results also apply for a wide range of other 
model types.)

As explained in Section~\ref{sec:introduction} the considered tree algorithms CTree, MOB, and 
GUIDE first select the split variable and then the split point in a separate step. 
For the first step of selecting a split variable they all apply statistical (independence) tests
following the same general strategy: 
Based on the discrepancy measure an $n \times k$ - matrix $D$ is obtained giving information
on the goodness of fit of the model for a data set of $n \in \mathbb{N}$ observations in the 
current node and $k \in \mathbb{N}$.
Using this matrix, the algorithm-specific test statistic is calculated for each possible split 
variable $Z_j$, $j \in \{1,\ldots,m\}$ for $m \in \mathbb{N}$, evaluating the dependency
of $Z_j$ on $D$. As the variables are usually not all measured at the same scale the resulting
values of the test statistics can not be compared directly in an unbiased way. Therefore, it is
necessary to calculate the corresponding $p$-values in order to allow for an unbiased
variable selection on this standardized/unified scale. Then the variable corresponding to the smallest
$p$-value is selected as split variable as it has shown to be the variable with the strongest
influence on the goodness of fit of the model.
% Hinweis: pruning-Strategien
%Once the split variable is chosen the corresponding split point is then selected such that it 
%minimizes the discrepancy measure.

While this basic approach is the same for the GUIDE, the CTree and the MOB algorithm they differ 
in their strategies on how to calculate test statistics. In order to point out these specific 
characteristics in Section~\ref{sec:unify} the strategies of the three tree algorithms are first 
explained in detail in Sections~\ref{sec:ctree}.


%To emphasize the differences between these three algorithms in how they select split variables,
%their testing strategies are explained in detail in Section~\ref{sec:strategies} 
%using the example of tree models with a linear model fitted in each node as discussed
%in the simulation study. Again, this is done only for means of simplification and the same 
%methodology can be applied for a wide variety of models.


\subsection{CTree}\label{sec:ctree}

The CTree algorithm is based on the idea of providing non-parametric regression tree models in 
a conditional inference framework by applying permutation tests. 
To select a split variable it is tested whether there is any association between the transformed 
response $h(Y)$ and $g(Z)$ for a so-called influence function $g$ and each possible split variable $Z$.
For different types of response variables different functions $h$ which depend on $Y$ in a 
permutation-symmetric way and might also be multidimensional can be selected.
For a numeric response the identity function $h(Y) = Y$ is a common choice, while a categorical
response can be mapped to a unity vector indicating the corresponding category by an indicator
function $h(Y) = (0,\ldots,1,\ldots,0)^{\top}$. Alternatively, the function $h$ can return a location
and scale parameter of $Y$, e.g. $h(Y) = (Y, (Y-\bar{Y})^2)^{\top}$. 
If a parametric model is fit to the values of the response variable~$Y$ and regressor variable(s)~$X$
the corresponding model scores can be obtained by setting $h(Y) = s(Y,X,\hat{\theta})$
where $\hat{\theta}$ is the estimated model parameter and $s$ is the score function:
$$
s(Y, X, \hat{\theta}) = \frac{\partial \psi(Y, X, \hat{\theta})}{\partial \theta}
$$
with $\psi$ being the objective function which is minimized to estimate the model parameter
$\theta$. Hence, for a data set of $n$ observations containing the response vector 
$y = (y_1,\ldots,y_n)^\top$ and regressor(s) $x = (x_1,\ldots,x_n)^\top$ and a $k$-dimensional 
parameter vector $\theta$ the score function $s$ returns a $n \times k$ - matrix.
In that way a score value indicating how well the model fits the data is given separately
for each tuple of observation and parameter element $(y_i, x_i, \beta_l)$ with $i \in \{1,\dots,n\}$
and $l \in \{1,\ldots,k\}$. 
(To estimate $\theta$ many different M-type estimators can be applied including maximum likelihood 
or ordinary least squares. Therefore, a model-based discrepancy measure considered for the tree 
algorithm can be employed as objective function.)

Similarly, different types of functions can be chosen for the influence function $g$ depending
on a possible split variable $Z$. A simple choice in case of $Z$ being a numeric variable is again
the identity function $g(Z) = Z$. For categorical variables $g$ can also map its values to the 
corresponding unity vectors by $g(Z) = (0,\ldots,1,\ldots,0)$.

To test for independence of $h(Y)$ and $g(Z)$ a linear test statistic is calculated.
Following \cite{Strasser+Weber:1999} the conditional expectation and covariance of the test 
statistic given all permutations of the response variable can be calculated and used to standardize 
the test statistic. In order to map multivariate test statistics into the real line two options are 
given: either the maximum of the absolute values of the standardized statistic or a quadratic form 
is considered. The resulting values can then be transformed to the scale of $p$-values where they 
can be compared in an unbiased way to select a split variable.

If both variables $Y$ and $Z$ are numeric the applied independence test corresponds to a Pearson 
correlation test. For one of the variables being categorical and the other numeric a 1-way ANOVA
is employed while for two categorical variables a $\chi^2$-test is performed. Hence, the CTree algorithm
provides a very general framework where the choice of the type of variables and the corresponding 
functions $g$ and $h$ determine the type of the applied statistical test.

In the CTree algorithm the number of possible breakpoints is fixed as splits can only be made at
observed values or optionally at an average value lying between two observations. To select the
best split point two-sample linear statistics of the same structure as in the independence test 
are evaluated for all possible subsets of the sample space of the selected split variable.


\subsection{MOB}\label{sec:mob}
While CTree offers to apply model-based discrepancy measures as one possible choice, the MOB algorithm 
was developed with the aim of embedding parametric models into the framework of regression trees
and is hence purely intended for a model-based approach. The testing strategy for selecting a split
variable is based on a fitted model, i.e. on an estimated parameter (vector) $\hat{\theta}$ for each
node of the tree, and on the application of the Central Limit Theorem. However, the fitted parametric 
models can be of a very basic and simple form such as an average value. The algorithm only requires the 
corresponding model scores $s(Y,X,\hat{\theta})$ in order to test for any instability in the model 
parameters over each particular ordering induced by each of the possible partitioning variables $Z$.
If a variable $Z$ has no specific influence on the parameters the scores would always fluctuate randomly
around their mean 0. Therefore, these deviations are investigated by measuring them using an 
empirical fluctuation process. Applying a scalar function which captures the fluctuation in this process 
yields the test statistic of this so called generalized M-fluctuation test. To obtain the distribution of
the resulting test statistic asymptotic theory, in particular the Central Limit Theorem is applied.
In this parametric unconditional framework $p$-values can be calculated and the covariate
with the smallest $p$-value is selected as split variable. 
For $Z$ being a numeric variable a maximally selected split point search is employed while for a 
categorical variable all possible splits are evaluated to find the best one.
Hence, the number of possible breakpoints is not fixed unlike in the CTree algorithm which can make 
this operation computationally expensive.



\subsection{GUIDE}\label{sec:guide}
Similar to the MOB algorithm, parametric models are fit to the nodes of a tree in the GUIDE algorithm.
For the statistical test applied to select a split variable in each node it requires some type of 
residuals $r(Y,X,\hat{\theta})$ from the models such as for example 
$r(Y,X,\hat{\theta}) = Y - X^{\top}\hat{\theta}$. 
Before applying a statistical test two additional steps are performed:
The residuals are binarized, i.e. depending on whether the sign of a residual is 
positive or not it is replaced by $1$ or $2$  % FIX ME: +1 and -1 in the implementation? 
and for each possible split variable the observed values are categorized into 4 groups 
separated by the corresponding 1st, 2nd and 3rd quartile.
Then, a $\chi^2$-test of independence is performed for the binarized residuals and
each categorized split variable.
After choosing the split variable showing the highest dependency by yielding the lowest $p$-value,
the split point minimizing the discrepancy measure is selected. The possible number of breakpoints
is determined by the categorization of the possible split variables such that the quartiles are
the only three possible split points.

\bigskip

\textit{Note:}
While GUIDE originally constructs large trees and applies post-pruning with the cross-validation 
method of CART, CTree and MOB employ pre-pruning by setting a significance level for the 
independence tests such that no split is performed if the lowest $p$-value is not smaller than 
the significance level. The impact of the choice of pruning strategy is further discussed in 
Section~\ref{sec:discussion}.



\section{Unifying Framework}\label{sec:unify}
As all three algorithms follow the same basic structure their approaches can be presented together
in a unifying framework for a model based tree algorithm from which each specific algorithm can be 
obtained by switching on or off certain features. By applying such a general framework each of the 
characteristic features of the three algorithms and its impact can be investigated in detail. 
These switches are explained in this Section and their setting for the algorithms
CTree, MOB, and GUIDE are listed in Table~\ref{tab:switches}.

\subsection{Switches}
\begin{itemize}
\item  Transformations of Y:\\
The selection of a split variable is based on a statistical test investigating the dependency of the
response $Y$ on each possible split variables $Z$. However, before evaluating such a test a model-based
transformation of $Y$ is applied to obtain a discrepancy measure. This can be residuals 
$r(Y,X,\hat{\theta})$ as in the GUIDE algorithm or scores $s(Y,X,\hat{\theta})$ as in the MOB algorithm
or, more generally, a function $h(Y,X)$ as in the CTree algorithm including both of the foregoing cases.
Since the first column of the score matrix $S = s(Y,X,\hat{\theta})$ is the vector of residuals the 
GUIDE algorithm selects only this first column while the MOB algorithm uses all available score values,
hence the full model matrix $S$.
%providing information on the goodness of fit for each observation and model parameter.
(By including all available scores the goodness of fit of the model is investigated regarding 
each parameter rather than just regarding the mean/location parameter. In that way changes in 
parameters other than the mean/location parameter should be detected more easily.)

\item Binarization of scores:\\
Rather than applying the independence test directly on the selected (columns of) scores a
binarized version of them can be used as well. In particular, the score values are binarized
at $0$ such that only their signs are considered.

\item Categorization of covariates:\\
Similarly, the covariates can also be categorized. For this purpose, the corresponding 1st, 2nd 
and 3rd quartile are used to bin each possible split variable into four categories. In that way
a $\chi^2$-test can be applied and the quartiles are the only possible split points while otherwise maximally selected splits over $Z$ are considered. 

\end{itemize}
%\subsubsection{Conditional vs unconditional}


\begin{table}[t!]
\centering
\caption[Table caption text]{CTree, MOB, and GUIDE with the corresponding setting of
the switches in the unifying framework and the approximation type of the underlying null distribution.}
\label{tab:switches}
\begin{tabular}{ l c c c c }
\hline 
& Scores            & Binarization  & Categorization  & Null distribution \\ 
\hline
CTree & full model scores & -           & -             & conditional\\
MOB   & full model scores & -           & -             & unconditional\\
GUIDE & residuals         & \checkmark  & \checkmark    & unconditional\\
\hline
\end{tabular}
\end{table}




\subsection{Linear model tree}
As an example a linear model tree is considered as this allows 
for an easy and well structured description of the investigated subjects/issues. 
For that reason, this model is also considered for the simulation study in Section~\ref{sec:simulation}.
However, the conclusions drawn for this example also apply for a wide range of other model
types. The models fitted in the nodes of the considered linear tree model are of the form
$$
Y = \beta_0 + \beta_1 \cdot X + \epsilon
$$
for a response variable $Y$, a regressor variable $X$ and an error $\epsilon$. 
As for each node the type of the fitted model is the same no node-specific features are considered
and the observations of the current (arbitrarily chosen) node are denoted by 
$\{(y_i,x_i)\}_{i=1,\ldots,n}$ for $n \in \mathbb{N}$.

A very basic form of a discrepency measure can be obtained by simply calculating deviations of the 
response values from the mean value within the node, for example by $\sum_{i=1}^n (\bar{y} - y_i)^2$.
However, this includes only the response variable $Y$ but not the fitted model.
For a model-based approach residual can be considered such as in the residual sum of squares (RSS)
$$
r(y,x,\boldsymbol{\beta}) = RSS(y, x, \boldsymbol{\beta}) = 
\sum_{i=1}^n (y_i - \beta_0 - \beta_1 \cdot x_i)^2.
$$ 
which is also the objective function that is minimized for the estimation of the coefficient vector 
$\boldsymbol{\beta} = (\beta_0, \beta_1)^\top$, 
i.e. to obtain $\boldsymbol{\hat{\beta}} = (\hat{\beta_0}, \hat{\beta_1})^\top$.
The corresponding scores are defined by
$$
s(y_i, x_i, \hat{\boldsymbol{\beta}}) = 
\frac{\partial \textit{RSS}}{\partial \boldsymbol{\beta}}(y_i, x_i, \hat{\boldsymbol{\beta}})
$$
such that the score matrix $S$ is in this case a $n \times 2$ - matrix as $\boldsymbol{\beta}$ is 
two dimensional. In that way a score value indicating how well the model fits the data is given separately
for each tuple of observation and coefficient $(y_i,x_i,\beta_j)$ with $i \in \{1,\dots,n\}$
and $j \in \{1,2\}$.

In this example considering only residuals, i.e. only the first column of the score matrix $S$,
corresponds to including the derivatives of $RSS$ only regarding the intercept parameter~$\beta_0$.
On the contrary, using full model scores means including the whole $n \times 2$ matrix $S$, hence,
the derivatives of $RSS$ regarding the intercept $\beta_0$ (first column) and the slope parameter 
$\beta_1$ (second column).



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%All three algorithms apply statistical tests yielding $p$-values based on which
%the most appropriate split variable is chosen. One main difference lies in
%the way how scores are included. While CTree and MOB directly include the values
%of all available scores the GUIDE algorithm only uses residuals, in particular,
%the signs of residuals and binned versions of the covariates.
%The advantages of including all scores or only residuals as well as the impact
%of the binarization of scores at zero and the binning of covariates is
%investigated in detail in Section~\ref{sec:results}.
%Hence, the GUIDE algorithm differs mainly in the setting of the applied test
%in which the MOB and CTree algorithms are very similar. However, one essential difference
%between the testing strategies of MOB and CTree lies in the underlying approximation 
%of the null distribution where MOB employs an unconditional approach contrary
%to the conditional framework of CTree.

%Once the split variable is found they choose the split point minimizing the discrepancy measure, hence, 
%the RSS in this case.

%Once the split variable is chosen the search area for the optimal split point 
%is reduced to the range of this split variable. A possible but computationally 
%demanding approach to finding the best split point is an exhaustive search over 
%all possible split points in the split variable. However, other
%strategies have been developed as well, for example based on two-sample statistics such as 
%applied in the CTree algorithm.




\section{Simulation Setting and Evaluation}
\label{sec:simulation}

Two different scenarios are considered in this simulation study. First, the underlying
tree structure based on which the data is generated is a stump, i.e. a tree with only one 
split. By keeping the complexity of the model as low as possible the features/advantages and
disadvantages of the different testing strategies can be elaborated more precisely and 
the focus can be put on their power in terms of selecting the correct split variable.

In the second scenario the true tree structure contains two splits in two different variables
yielding a tree with three terminal nodes. After having investigated the characteristics of 
the employed algorithms in detail for the stump scenario this additional evaluation aims at
showing that the same conclusions can be drawn for more complex structures.

In the following the data generating process is explained in detail for the first scenario.
The second, more complex scenario employs the same basic structure simply adding another split
as illustrated in Figure~\ref{fig:dgp_tree}.

\subsection{Data generating process}
Each generated data set consists of a response variable, one regressor, one split 
variable and nine noise variables. In particular, the variables listed in 
Table~\ref{tab:variables} together with the corresponding distributions are 
included in the data generating process.

\begin{table}
\begin{center}
\caption[Table caption text]{Variables included in the data generating process of 
the first scenario (stump).}
\label{tab:variables}
\begin{tabular}{l l l}
\hline
Name                    & Notation                   & Specification \\
\hline
\textit{Variables:}     &                            & \\ 
Response                & $Y$                        & $=\beta_0(Z_1) + \beta_1(Z_1) \cdot X + \epsilon$ \\
Regressor               & $X$                        & $\mathcal{U}([-1,1])$ \\
Error                   & $\epsilon$                 & $\mathcal{N}(0,1)$ \\
True split variable     & $Z_1$                      & $\mathcal{U}([-1,1])$  or $\mathcal{N}(0, 1)$ \\
Noise split variables   & $Z_2, Z_3, \dots, Z_{10}$  & $\mathcal{U}([-1,1])$  or $\mathcal{N}(0, 1)$ \\
\hline
\textit{Parameters/functions:} &            & \\ 
Intercept                      & $\beta_0$  & $0$ or $\pm \delta$\\
Slope                          & $\beta_1$  & $1$ or $\pm \delta$\\
True split point               & $\xi$      & $\in \{0, 0.2, 0.5, 0.8\}$ \\
Effect size                    & $\delta$   & $\in \{0, 0.1, 0.2, \ldots , 1\}$ \\
\hline
\end{tabular}
\end{center}
\end{table}

The location parameter $\mu$ of the normally distributed response variable $Y$ depends
linearly on the regressor variable $X$ which is a uniformly distributed variable taking 
values between $-1$ and $1$.
% can either be a binary variable (taking either $0$ or $1$) or a uniformly distributed 
% variable (taking values between $-1$ and $1$).
Moreover, the intercept $\beta_0$ and the slope parameter $\beta_1$ of this linear equation
(can) depend on the true split variable $Z_1$. 
%in the following way:
%\begin{center}
%$\mu(X, Z_1) = \beta_0(Z_1) + \beta_1(Z_1) \cdot X$.
%\end{center}
%
In particular, three different cases are considered for the coefficients $\beta_0$ 
and $\beta_1$:
\begin{enumerate}
\item The intercept $\beta_0$ varies depending on $Z_1$ while $\beta_1$ is fixed (at $1$).
\item The slope coefficient $\beta_1$ varies depending on $Z_1$ while $\beta_0$ is fixed 
(at $0$).
\item Both coefficients $\beta_0$ and $\beta_1$ vary depending on $Z_1$.
\end{enumerate}

The varying coefficient(s) $\beta_i$, $i\in\{0,1\}$, are binary taking either the positive or
the negative value of the effect size, i.e. $\delta$ or $-\delta$.
% can either be binary (taking either $\delta$ or $-\delta$), or a continuous function. 
% In the binary case the following definition is applied for the true split point $\xi$
\begin{align*}
\beta_{i}(Z_1) = \begin{cases}
-\delta \cdot (-1)^{i}  \quad \text{if } Z_1 < \xi \\
+\delta \cdot (-1)^{i}  \quad \text{if } Z_1 \geq \xi
\end{cases}
\end{align*}
%while in the continuous case a simple linear function is considered in the form of
%$$\beta_{i}(Z_1) = Z_1 \cdot \delta \cdot (-1)^{i}.$$
In that way, the type of variation is the same for $\beta_0$ and $\beta_1$, however, in 
opposite directions.
An illustration of the three different scenarios 
%for binary coefficients and a continuous regressor 
can be found in Figure~\ref{fig:dgp_stump}.

%\newpage
<<dgp, echo=FALSE>>=
set.seed(7)

nobs <- 250
sigma <- 0.5
xi <- 0
delta <- 1
beta0 <- 0
beta1 <- 1

d <- dgp_stump(nobs = nobs, binary_beta = TRUE, binary_regressor = FALSE, only_intercept = FALSE,
               vary_beta = "beta0", beta1 = beta1, delta = delta, xi = xi, sigma = sigma)

sp_xi1 <- partysplit(3L, breaks = 0)
 
pn <- partynode(1L, split = sp_xi1, 
                kids = list(
                  #partynode(2L, info = c("true parameters:", 
                  #                       " ",
                  #                       expression(beta[0] == 0~" , "~beta[1] == -delta),
                  #                       "or",
                  #                       expression(beta[0] == +delta~" , "~beta[1] == 1),
                  #                       "or",
                  #                       expression(beta[0] == +delta~" , "~beta[1] == -delta))),
                  #partynode(3L, info = c("true parameters:", 
                  #                       " ",
                  #                       expression(beta[0] == 0~" , "~beta[1] == +delta),
                  #                       "or",
                  #                       expression(beta[0] == -delta~" , "~beta[1] == 1),
                  #                       "or",
                  #                      expression(beta[0] == -delta~" , "~beta[1] == +delta)))))
                  
                  partynode(2L, info = c("true parameters:", 
                                         expression(beta[0] ==~" "~0~" or "~-delta), 
                                         expression(beta[1] ==~" "~1~" or "~+delta))),
                  partynode(3L, info = c("true parameters:", 
                                         expression(beta[0] ==~" "~0~" or "~+delta), 
                                         expression(beta[1] ==~" "~1~" or "~-delta)))))

p <- party(pn, d)

#par(mfrow = c(2,2))
#par(mar=c(1,1,1,1), oma=c(1,1,1,1))
paltrees <- rgb(c(1, 0.9, 1), c(1, 0.9, 1), c(1, 0.9, 1))

ep <- function (obj, digits = 3, abbreviate = FALSE, justmin = Inf, 
    just = c("alternate", "increasing", "decreasing", "equal"), 
    fill = "white") 
{
  meta <- obj$data
  justfun <- function(i, split) {
    myjust <- if (mean(nchar(split)) > justmin) {
      match.arg(just, c("alternate", "increasing", "decreasing", "equal"))
    } else {
      "equal"
    }
    k <- length(split)
    rval <- switch(myjust, equal = rep.int(0, k), 
                   alternate = rep(c(0.5, -0.5), length.out = k), 
                   increasing = seq(from = -k/2, to = k/2, by = 1), 
                   decreasing = seq(from = k/2, to = -k/2, by = -1))
    unit(0.5, "npc") + unit(rval[i], "lines")
  }
  function(node, i) {
    split <-  c("<= xi", "> xi")
    y <- justfun(i, split)
    split <- split[i]
    if (any(grep(">", split) > 0) | any(grep("<", split) > 0)) {
      tr <- suppressWarnings(try(parse(text = paste("phantom(0)", split)), silent = TRUE))
      if (!inherits(tr, "try-error")) split <- tr
    }
    grid.rect(y = y, gp = gpar(fill = fill, col = 0), width = unit(1, "strwidth", split))
    grid.text(split, y = y, just = "center")
  }
}
class(ep) <- "grapcon_generator"
@

\begin{figure}%[h!]
\setkeys{Gin}{width=1\linewidth}
\minipage{0.45\textwidth}
<<dgp_stump1, fig=TRUE, echo=FALSE, width=4, height=4>>=
#par(mar=c(3,3,0,0), oma = c(3,3,0,0))
plot(p, 
     tp_args = list(FUN = identity, width = 15, fill = paltrees[c(1, 3)]), 
     ip_args = list(fill = paltrees[c(2, 3)]),
     drop_terminal = TRUE, tnex = 1, edge_panel = ep)
@
\endminipage
\minipage{0.45\textwidth}
<<dgp_stump2, fig=TRUE, echo=FALSE, width=4, height=4>>=
par(mar = c(4,4,2,1))
plot(x = d$x[d$z1<=xi], y = d$y[d$z1<=xi], ylim = c(-5,5), ylab = "y", xlab = "x", 
     main = expression("varying"~beta[0]),
     pch = 19, col = alpha("blue", 0.4), cex = 0.7)
abline(coef = c(-1,1), col = "blue", lwd = 2)
abline(coef = c(1,1), col = "red", lwd = 2)
points(x = d$x[d$z1>xi], y = d$y[d$z1>xi], ylim = c(-5,5), pch = 19, col = alpha("red", 0.4), cex = 0.7)
legend("top", c(expression(z[1]<=xi), expression(z[1]>xi)), col = c("blue", "red"), pch = c(19), bty = "n", cex = 0.9)
@
\endminipage

\minipage{0.45\textwidth}
<<dgp_stump3, fig=TRUE, echo=FALSE, width=4, height=4>>=
d <- dgp_stump(nobs = nobs, binary_beta = TRUE, binary_regressor = FALSE, only_intercept = FALSE,
               vary_beta = "beta1", beta0 = beta0, delta = delta, xi = xi, sigma = sigma)

par(mar = c(4,4,2,1))
plot(x = d$x[d$z1<=xi], y = d$y[d$z1<=xi], ylim = c(-5,5), ylab = "y", xlab = "x", main = expression("varying"~beta[1]),
     pch = 19, col = alpha("blue", 0.4), cex = 0.7)
abline(coef = c(0,1), col = "blue", lwd = 2)
abline(coef = c(0,-1), col = "red", lwd = 2)
points(x = d$x[d$z1>xi], y = d$y[d$z1>xi], ylim = c(-5,5), pch = 19, col = alpha("red", 0.4), cex = 0.7)
legend("top", c(expression(z[1]<=xi), expression(z[1]>xi)), col = c("blue", "red"), pch = c(19), bty = "n", cex = 0.9)
@
\endminipage
\minipage{0.45\textwidth}
<<dgp_stump4, fig=TRUE, echo=FALSE, width=4, height=4>>=
d <- dgp_stump(nobs = nobs, binary_beta = TRUE, binary_regressor = FALSE, only_intercept = FALSE,
               vary_beta = "all", delta = delta, xi = xi, sigma = sigma)
par(mar = c(4,4,2,1))
plot(x = d$x[d$z1<=xi], y = d$y[d$z1<=xi], ylim = c(-5,5), ylab = "y", xlab = "x",
     main = expression("varying"~beta[0]~"and"~beta[1]),
     pch = 19, col = alpha("blue", 0.4), cex = 0.7)
abline(coef = c(-1,1), col = "blue", lwd = 2)
abline(coef = c(1,-1), col = "red", lwd = 2)
points(x = d$x[d$z1>xi], y = d$y[d$z1>=xi], pch = 19, col = alpha("red", 0.4), cex = 0.7)
legend("top", c(expression(z[1]<=xi), expression(z[1]>xi)), col = c("blue", "red"), pch = c(19), bty = "n", cex = 0.9)
@
\endminipage
\caption{\label{fig:dgp_stump}{Top left panel: True stump structure with the true model parameters $\beta_0$ and $\beta_1$, either varying or being fixed at $0$ or $1$.
Top right and bottom panels: Bivariate plot of the response $Y$ on regressor $X$ 
with effect size $\delta = 1$ and for three scenarios: 
varying $\beta_0$ and fixed $\beta_1$ (top right),
fixed $\beta_0$ and varying $\beta_1$ (bottom left),
varying $\beta_0$ and varying $\beta_1$ (bottom right).}}
\end{figure}

For the second simulation scenario the same basic setting with the same variables is used
with an additional split in $Z_2$ resulting in a tree with three terminal nodes. The first
split (in $Z_2$, at~$\xi$) induces a change in the slope parameter $\beta_1$ while the second
split (in $Z_1$, also at~$\xi$) corresponds to a change in the intercept $\beta_0$.
The underlying tree structure for the data generating process is illustrated in 
Figure~\ref{fig:dgp_tree}.

<<true_tree_dgp, eval = TRUE, echo=FALSE, results=hide>>=

# d <- dgp_tree(nobs = 250)
set.seed(7)
d <- dgp_tree(nobs = 250, xi = 0, binary_regressor = FALSE, vary_beta = "all", 
              binary_beta = TRUE, nrsplitvar = 2, sigma = 0.5)

sp_xi2 <- partysplit(4L, breaks = 0)
sp_xi1 <- partysplit(3L, breaks = 0)
 
pn <- partynode(1L, split = sp_xi2, 
                kids = list(
                  partynode(2L, info = c("true parameters:", 
                                         expression(beta[0] == 0), 
                                         expression(beta[1] == +delta))),
                  partynode(3L, split = sp_xi1, 
                            kids = list(
                              partynode(4L, info = c("true parameters:", 
                                                     expression(beta[0] == -delta), 
                                                     expression(beta[1] == -delta))),
                              partynode(5L, info = c("true parameters:", 
                                                     expression(beta[0] == +delta), 
                                                     expression(beta[1] == -delta)))))))

p <- party(pn, d)

#ctest <- evaltests(y~x|z1+z2+z3+z4+z5+z6+z7+z8+z9+z10, data = d, testfun = "ctree", stump = FALSE)
@


\begin{figure}[h!]
\begin{center}
\setkeys{Gin}{width=1\linewidth}
\minipage{0.5\textwidth}
<<dgp_tree1, fig=TRUE, echo=FALSE, width=5, height=4.5>>=
par(mar=c(0,0,0,0), oma=c(0,0,0.4,0))
#paltrees <- rgb(c(0.97, 0.64, 1), c(0.70, 0.83, 1), c(0.30, 0.99, 1))
paltrees <- rgb(c(1, 0.9, 1), c(1, 0.9, 1), c(1, 0.9, 1))
plot(p, edge_panel = ep,
     tp_args = list(FUN = identity, width = 15, fill = paltrees[c(1, 3)]), 
     ip_args = list(fill = paltrees[c(2, 3)]),
     drop_terminal = TRUE, tnex = 1)
@
\endminipage
\minipage{0.45\textwidth}
<<dgp_tree2, fig=TRUE, echo=FALSE, width=4, height=4>>=
par(mar = c(4,4,2,1))
plot(x = d$x[d$z2<=xi], y = d$y[d$z2<=xi], ylim = c(-5,5), ylab = "y", xlab = "x",
     pch = 19, col = alpha("blue", 0.4), cex = 0.7)
abline(coef = c(0,1), col = "blue", lwd = 2)
abline(coef = c(-1,-1), col = "red", lwd = 2)
abline(coef = c(1,-1), col = "green", lwd = 2)
points(x = d$x[d$z2>xi & d$z1<=xi], y = d$y[d$z2>xi & d$z1<=xi], pch = 19, 
       col = alpha("red", 0.4), cex = 0.7)
points(x = d$x[d$z2>xi & d$z1>xi], y = d$y[d$z2>xi & d$z1>xi], pch = 19, 
       col = alpha("green", 0.4), cex = 0.7)
legend("top", c(expression(z[2]<=xi), 
                expression(z[2]>xi~" and "~z[1]<=xi),
                expression(z[2]>xi~" and "~z[1]>xi)), 
       col = c("blue", "red", "green"), pch = c(19), bty = "n", cex = 0.9)
@
\endminipage
\caption{\label{fig:dgp_tree}Left panel: True tree structure with the true model parameters 
$\beta_0$ and $\beta_1$ for each terminal panel.
Right panel: Bivariate plot of the response $Y$ on regressor $X$ 
with effect size $\delta = 1$.}
\end{center}
\end{figure}

%\begin{figure}[h!]
%\begin{center}
%\setkeys{Gin}{width=0.6\linewidth}
%<<true_tree_biv, fig=TRUE, echo=FALSE, width=5.4, height=4>>=
%plot(ctest, terminal_panel = node_bivplot, drop_terminal = TRUE, edge_panel = ep,
%     tp_args = list(margins = c(0,1,0,1), pointcol = "white", pointcex = 0.7, lwd = 2),
%     ip_args = list(pval = FALSE, fill = paltrees[c(2, 3)]))
%@
%\caption{\label{fig:true_tree_biv}True tree structure for split point $\xi = 0$ and 
%effect size $\delta = 1$} with the corresponding linear functions being plotted
%in each terminal node.
%\end{center}
%\end{figure}




\subsection{Evaluation}
The testing strategies are evaluated over a stepwise increasing effect size $\delta$, 
on 100 data sets per step each consisting of 250 observations. 

By applying the unifying framework presented in Section~\ref{sec:unify} the effect of
each switch can be investigated on its own. In Table~\ref{tab:switches} the setting of these 
switches is listed for the algorithms CTree, MOB and GUIDE in their original form.
However, in order to be able to focus on particular features rather than the whole algorithms
other combinations as presented in Table~\ref{tab:switches}, hence variations of the original 
algorithms, are evaluated as well. 

Moreover, all of the considered statistical tests are applied as significance tests answering 
two questions at once: (1) Should a split be performed at all? (2) If so, in which variable?
For the first question the $p$-values regarding all available covariates are compared 
to a predefined level of significance $\alpha$. Only if the smallest $p$-value is smaller
than $\alpha$ a split is performed and the covariate corresponding to this $p$-value is 
selected. Including this criterion of significance in the building process of a tree is often
referred to as \textit{pre-pruning}, however, this is not always applied in practice as 
significance is often not considered. As mentioned before, the GUIDE algorithm usually builds
large trees which are then pruned afterwards. However, for the purpose of comparing the
testing strategies rather than the whole tree algorithms in this simulation study a 
significance level is considered for all statistical tests, including the $\xi^2$-test in 
the GUIDE algorithm such that all algorithms apply pre-pruning rather than post-pruning.

To compare the performance of the evaluated testing strategies for each step the proportion
of data sets for which the true splitting variable $Z_1$ is detected and also selected 
(for a given significance level $\alpha = 0.05$) is considered for the first scenario 
(stump).
For the second scenario (tree with 2 splits) the adjusted Rand index (ARI) is calculated 
as a measure of similarity between the true tree structure and the model tree.



\section{Results}
\label{sec:results}

To illustrate the effects of the switches in the unifying framework presented in 
Section~\ref{sec:unify} each of them is investigated separately for the first
simulation scenario (stump) in Sections~\ref{sec:scores}, \ref{sec:categorization}
and \ref{sec:binarization}. Then the testing strategies are evaluated on a more
complex tree structure in Section~\ref{sec:tree}. The corresponding results are shown 
in the following plots.

\subsection{Residuals vs full model scores}
\label{sec:scores}
To assess the impact of using only residuals or the full score matrix four testing
strategies are compared: CTree, GUIDE, MOB (all three in their original form),
and additionally the GUIDE strategy using the full score matrix (denoted by GUIDE\_full).
Thus, the four strategies are obtained from the unifying framework by setting the switches 
as listed in Table~\ref{tab:switches_scores}.

\begin{table}[h!]
\centering
\caption[Table caption text]{CTree, MOB, GUIDE, and GUIDE\_full with the corresponding setting of
the switches in the unifying framework and the approximation type of the underlying null distribution.}
\label{tab:switches_scores}
\begin{tabular}{ l c c c c }
\hline 
& Scores            & Binarization  & Categorization  & Null distribution \\ \hline
CTree       & full model scores & -           & -               & conditional\\
MOB         & full model scores & -           & -               & unconditional\\
GUIDE       & residuals         & \checkmark  & \checkmark      & unconditional\\
GUIDE\_full & full model scores & \checkmark  & \checkmark      & unconditional\\ \hline
\end{tabular}
\end{table}

Figure~\ref{fig:scores_stump} shows the evaluation results for the three different
scenarios of varying only the intercept $\beta_0$ or only the slope parameter $\beta_1$
or both of them. If the intercept varies (left and right panel) all strategies perform 
similarly with CTree and MOB being slightly ahead. However, this advantage increases if 
the slope parameter varies and the intercept is kept at a fixed value (middle panel). 
In particular, the original strategy of GUIDE fails to detect any changes in $\beta_1$ 
while GUIDE\_full performs reasonably well in this scenario.
These results clearly illustrate that overall the decision to include all available scores
when testing for dependencies in order to select split variables is advantageous over 
considering only the scores regarding the intercept as changes not only in the mean but also 
in all other parameters can be detected in that way.


\begin{figure}
\setkeys{Gin}{width=\linewidth}
<<scores_stump, fig=TRUE, echo=FALSE, height=4, width=8>>=
subdata <- subset(sim_stump, test %in% c("ctree","mfluc",
                                         "guide_sum_12", "guide_sum_1_cor"))
subdata$test <- factor(subdata$test, 
                       levels = c("ctree","mfluc",
                                  "guide_sum_12", "guide_sum_1_cor"),
                       labels = c("CTree","MOB",
                                  "GUIDE_full", "GUIDE"))

par.settings <- list(superpose.symbol = list(col = pal[c("CTree", "MOB", "GUIDE_full", "GUIDE")], 
                                             fill = pal[c("CTree", "MOB", "GUIDE_full", "GUIDE")]), 
                     superpose.line = list(col = pal[c("CTree", "MOB", "GUIDE_full", "GUIDE")]))

xyplot(prop_Tsplit ~ delta | vary_beta, groups = ~ test, 
       data = subdata, 
       subset = binary_beta == TRUE & binary_regressor == FALSE & 
         only_intercept == FALSE & xi == 0, 
       type = "l", lty = c(1,1,1,1),
       key =  list(text = list(levels(subdata$test)),
                   lines = list(lty = c(1,1,1,1), type = "l", pch = 1, cex = 0.8,
                                col = pal[c("CTree", "MOB", "GUIDE_full", "GUIDE")], lwd = 1)),
       layout= c(3,1),
       index.cond = list(c(2,3,1)),
       xlab = expression(delta),
       ylab = expression("Proportion of selecting"~Z[1]),
       strip = strip.custom(factor.levels = c(expression("vary"~beta[0]~"and"~beta[1]), 
                                              expression("vary"~beta[0]), 
                                              expression("vary"~beta[1]))),
       par.settings = par.settings)
@
\caption{\label{fig:scores_stump}Proportion of cases where the true split variable $Z_1$ is
selected over increasing effect size $\delta$ for $\sigma = 1$, 250 observations and 100 data 
sets per step. Each panel 
represents one scenario for the coefficients: varying intercept 
$\beta_0$ (left panel), varying slope parameter $\beta_1$ (middle panel) and both 
coefficients varying (right panel). The performances of four algorithms are compared: 
CTree, MOB, GUIDE, GUIDE\_full.}
\end{figure}


\subsection{Binarization of scores}
\label{sec:binarization}
To find out whether a binarization of the scores at 0 leads to an improvement or a deterioration
of performance the two algorithms CTree and MOB are applied, once in their original version
without a binarization and once in an adapted version with a binarization of scores at 0 
(CTree\_bin, MOB\_bin). 
Moreover, they are compared to the adapted GUIDE version which includes all available 
scores (GUIDE\_full).


Hence, the five strategies are obtained from the unifying framework by setting the switches 
as listed in Table~\ref{tab:switches_bin}.

\begin{table}[h!]
\centering
\caption[Table caption text]{CTree, CTree\_bin, MOB, MOB\_bin, and GUIDE\_full with the 
corresponding setting of the switches in the unifying framework and the approximation type 
of the underlying null distribution.}
\label{tab:switches_bin}
\begin{tabular}{ l c c c c }
\hline 
& Scores            & Binarization  & Categorization  & Null distribution \\ 
\hline
CTree       & full model scores & -           & -             & conditional\\
CTree\_bin  & full model scores & \checkmark  & -             & conditional\\
MOB         & full model scores & -           & -             & unconditional\\
MOB\_bin    & full model scores & \checkmark  & -             & unconditional\\
GUIDE\_full & full model scores & \checkmark  & \checkmark    & unconditional\\
\hline
\end{tabular}
\end{table}

Figure~\ref{fig:bin_stump} shows the effect of binarizing the score values over
different values for the true split point $\xi$, however, all four situations
lead to the same conclusions. Binarizing the score values decreases the proportion
of cases where the split variable was selected correctly.


%% FIX ME: ctree or ctree_max (1 step: ctree better, 2 steps: ctree_max better)
\begin{figure}%[h!]
\setkeys{Gin}{width=\linewidth}
<<bin_stump, fig=TRUE, echo=FALSE, height=4, width=8>>=
subdata <- subset(sim_stump, test %in% c("ctree", "ctree_bin", 
                                         #"ctree_max", "ctree_max_bin",
                                         "mfluc", "mfluc_bin",
                                         "guide_sum_12"))

subdata <- subset(subdata, delta %in% c(0, 0.1, 0.2, 0.3, 0.4, 0.5))
subdata$test <- factor(subdata$test, 
                       levels = c("ctree", "ctree_bin", 
                                  #"ctree_max", "ctree_max_bin",
                                  "mfluc", "mfluc_bin",
                                  "guide_sum_12"),
                       labels = c("CTree","CTree_bin",
                                  #"CTree_max","CTree_max_bin",
                                  "MOB", "MOB_bin",
                                  "GUIDE_full"))

par.settings <- list(superpose.symbol = list(col = pal[c("CTree", "CTree", 
                                                         #"CTree_max", "CTree_max", 
                                                         "MOB", "MOB",
                                                         "GUIDE_full")], 
                                             fill = pal[c("CTree", "CTree", 
                                                          #"CTree_max", "CTree_max", 
                                                          "MOB", "MOB",
                                                          "GUIDE_full")]), 
                     superpose.line = list(col = pal[c("CTree", "CTree", 
                                                       #"CTree_max", "CTree_max", 
                                                       "MOB", "MOB",
                                                       "GUIDE_full")]))

xyplot(prop_Tsplit ~ delta | xi, groups = ~ test, 
       data = subdata, 
       subset = binary_beta == TRUE & binary_regressor == FALSE & 
         only_intercept == FALSE & vary_beta == "all", 
       type = "b",
       lty = c(1,2,#1,2,
               1,2,1),
       layout= c(4,1),
       key =  list(text = list(levels(subdata$test)),
                   lines = list(lty = c(1,2,#1,2,
                                        1,2,1), 
                                type = "b", pch = 1, cex = 0.8,
                                col = pal[c("CTree", "CTree", 
                                            #"CTree_max", "CTree_max", 
                                            "MOB", "MOB",
                                            "GUIDE_full")], lwd = 1)), 
       #index.cond = list(c(1,2,3,4)),
       xlab = expression(delta),
       ylab = expression("Proportion of selecting"~Z[1]),
       strip = strip.custom(factor.levels = c(expression(xi==0), 
                                              expression(xi==0.2), 
                                              expression(xi==0.5), 
                                              expression(xi==0.8))),
       par.settings = par.settings)

@
\caption{\label{fig:bin_stump}Proportion of cases where the true split variable $Z_1$ is
detected over increasing effect size $\delta$ for both parameter varying, $\sigma = 1$, 
250 observations and 100 data sets per step. Each panel represents a different value of the 
split point $\xi$ (0, 0.2, 0.5 and 0.8 from left to right). The performances of five testing 
strategies are compared: CTree, CTree\_bin, MOB, MOB\_bin, GUIDE\_full.}
\end{figure}




\subsection{Categorization of splitting variables}
\label{sec:categorization}
To find out whether a categorization of the possible splitting variables leads to 
an improvement or a deterioration of performance the two algorithms CTree and MOB are 
applied, once in their original version without a categorization and once in an adapted 
version with a categorization of the possible split variables (CTree\_cat and MOB\_cat). 
Moreover, they are compared to GUIDE\_full which includes all available scores.

Hence, the five strategies are obtained from the unifying framework by setting the switches 
as listed in Table~\ref{tab:switches_cat}.

\begin{table}[h!]
\centering
\caption[Table caption text]{CTree, CTree\_cat, MOB, MOB\_cat, and GUIDE\_full with the 
corresponding setting of the switches in the unifying framework and the approximation type 
of the underlying null distribution.}
\label{tab:switches_cat}
\begin{tabular}{ l c c c c }
\hline 
& Scores            & Binarization  & Categorization  & Null distribution \\ 
\hline
CTree       & full model scores & -           & -               & conditional\\
CTree\_cat  & full model scores & -           & \checkmark      & conditional\\
MOB         & full model scores & -           &                 & unconditional\\
MOB\_cat    & full model scores & -           & \checkmark      & unconditional\\
GUIDE\_full & full model scores & \checkmark  & \checkmark      & unconditional\\
\hline
\end{tabular}
\end{table}

In Figure~\ref{fig:cat_stump} the effect of categorizing split variables on the performance 
is illustrated for four different values of the true split point $\xi$. 
For both, CTree and MOB, it can be stated that overall they perform better in their
original form without any categorization. Only if the true split point $\xi$
is close to the quartiles used as breakpoints for the categorizations both versions
lead to a similar proportion of cases where the true splitting variable is detected
(e.g., for CTree and $\xi=0.5$)

Therefore, it can be summarized that a categorization of the values of the split variable
does not lead to any advantages unless the true split point corresponds to one of the 
quartiles used for the categorization. In most situations it even causes the performance to 
get worse.

\begin{figure}%[h!]
\setkeys{Gin}{width=\linewidth}
<<cat_stump, fig=TRUE, echo=FALSE, height=4, width=8>>=
subdata <- subset(sim_stump, test %in% c("ctree", "ctree_cat",
                                         #"ctree_max", "ctree_max_cat",
                                         "mfluc", "mfluc_cat",
                                         "guide_sum_12"))
subdata <- subset(subdata, delta %in% c(0, 0.1, 0.2, 0.3, 0.4, 0.5))
subdata$test <- factor(subdata$test, 
                       levels = c("ctree", "ctree_cat", 
                                  #"ctree_max","ctree_max_cat",
                                  "mfluc", "mfluc_cat",
                                  "guide_sum_12"),
                       labels = c("CTree","CTree_cat",
                                  #"CTree_max","CTree_max_cat",
                                  "MOB", "MOB_cat",
                                  "GUIDE_full"))

par.settings <- list(superpose.symbol = list(col = pal[c("CTree", "CTree", 
                                                         #"CTree_max", "CTree_max", 
                                                         "MOB", "MOB",
                                                         "GUIDE_full")], 
                                             fill = pal[c("CTree", "CTree", 
                                                          #"CTree_max", "CTree_max", 
                                                          "MOB", "MOB",
                                                          "GUIDE_full")]), 
                     superpose.line = list(col = pal[c("CTree", "CTree", 
                                                       #"CTree_max", "CTree_max", 
                                                       "MOB", "MOB",
                                                       "GUIDE_full")]))

xyplot(prop_Tsplit ~ delta | xi, groups = ~ test, 
       data = subdata, 
       subset = binary_beta == TRUE & binary_regressor == FALSE & 
         only_intercept == FALSE & vary_beta == "all", 
       #xlim = c(0,0.1,0.5),
       type = "b", 
       lty = c(1,2,#1,2,
               1,2,1),
       layout= c(4,1),
       key =  list(text = list(levels(subdata$test)),
                   lines = list(lty = c(1,2,#1,2,
                                        1,2,1), 
                                type = "b", pch = 1, cex = 0.8,
                                col = pal[c("CTree", "CTree", 
                                            #"CTree_max", "CTree_max", 
                                            "MOB", "MOB",
                                            "GUIDE_full")], lwd = 1)), 
       #index.cond = list(c(1,2,3,4)),
       xlab = expression(delta),
       ylab = expression("Proportion of selecting"~Z[1]),
       strip = strip.custom(factor.levels = c(expression(xi==0), 
                                              expression(xi==0.2), 
                                              expression(xi==0.5), 
                                              expression(xi==0.8))),
       par.settings = par.settings)

@
\caption{\label{fig:cat_stump}Proportion of cases where the true split variable $Z_1$ is
selected over increasing effect size $\delta$ for both parameters varying, $\sigma = 1$, 
250 observations and 100 data sets per step. Each panel represents a different value of the 
split point $\xi$ ($0$, $0.2$, $0.5$ and $0.8$ from left to right). The performances of five 
testing strategies are compared: CTree, CTree\_cat, MOB, MOB\_cat, GUIDE\_full.}
\end{figure}





\subsection{Tree}
\label{sec:tree}
To compare the performance of testing strategies in a more complex scenario they have also
been evaluated on data sets based on a tree structure with two different split variables 
($Z_1$ and $Z_2$), both with the same split point $\xi$ as illustrated in 
Figure~\ref{fig:dgp_tree}.

Figure~\ref{fig:tree_ari} shows that based on the resulting ARI values CTree and MOB perform 
almost equally well for the true split points $0$, $0.2$ and $0.5$ while GUIDE\_full
a bit worse but still achieves an ARI above 0.5 for the highest considered effect size
of $\delta = 1$. However, the original version of GUIDE is clearly behind which is mainly
due to the fact that it doesn't detect the first split in $Z_2$ corresponding to a changing
slope parameter as it only considers residuals in its testing strategy.
For a split point farther away from the center such as for $\xi = 0.8$ MOB outperforms all other
methods and both GUIDE versions fail to find the correct tree structure.
Based on these results it can be concluded once again that including all available score in the
testing strategy leads to a better performance while binarizing scores and categorizing
split variables both do not cause any improvement but can even have a negative effect in many situations. 

\begin{figure}[h!]
\setkeys{Gin}{width=\linewidth}
<<tree_ari, fig=TRUE, echo=FALSE, height=4, width=8>>=
subdata <- subset(sim_tree, test %in% c("ctree","mfluc",
                                        "guide_sum_12", "guide_sum_1_cor"))
subdata$test <- factor(subdata$test, 
                       levels = c("ctree", "mfluc",
                                  "guide_sum_12", "guide_sum_1_cor"),
                       labels = c("CTree","MOB",
                                  "GUIDE_full", "GUIDE"))

par.settings <- list(superpose.symbol = list(col = pal[c("CTree", "MOB", "GUIDE_full", "GUIDE")], 
                                             fill = pal[c("CTree", "MOB", "GUIDE_full", "GUIDE")]), 
                     superpose.line = list(col = pal[c("CTree", "MOB", "GUIDE_full", "GUIDE")]))


xyplot(ari ~ delta | xi, groups = ~ test, 
       data = subdata, 
       type = "l",
       lty = c(1,1,1,1),
       key =  list(text = list(levels(subdata$test)),
                   lines = list(lty = c(1,1,1,1), type = "l", pch = 1, cex = 0.8,
                                col = pal[c("CTree", "MOB", "GUIDE_full", "GUIDE")], lwd = 1)),
       layout= c(4,1),
       #index.cond = list(c(1,2,3,4)),
       xlab = expression(delta),
       ylab = "Adjusted Rand Index",
       strip = strip.custom(factor.levels = c(expression(xi==0), 
                                              expression(xi==0.2), 
                                              expression(xi==0.5), 
                                              expression(xi==0.8))),
       par.settings = par.settings)

@
\caption{\label{fig:tree_ari}Adjusted Rand Index over increasing effect size $\delta$ for
250 observations and 100 data sets per step.
Each panel represents a different value of the split point $\xi$ ($0$, $0.2$, $0.5$ and 
$0.8$ from left to right). The performances of four testing strategies are compared: CTree,
MOB, GUIDE, GUIDE\_full.}
\end{figure}


\section{Discussion}
\label{sec:discussion}
FIX ME: only pre-pruning applied, but same results if significance level is ignored (maybe show one plot).
Optionally compare GUIDE with pre-pruning to GUIDE with post-pruning on tree structure?
Check implementation of pruning.

FIX ME: LM-specific results

FIX ME: results without loss of generalization

\subsection{Pruning}
<<pruning_data, echo=FALSE, results=hide>>=
load("../sim/simres20181227_pruning.rda")
pr <- simres$res
## extract data for xyplot comparing ARI and ARI_p
pr <- pr[,c(1,2,7,8,9,12,15)]
## extended data set with post-pruning results being represented as separated tests
prex <- pr[,c(1,2,3,4,6)]
prex <- rbind(prex,prex)

prex$test <- factor(prex$test,
                    levels = c("ctree","mfluc",
                              "guide_sum_12", "guide_sum_1_cor",
                              "ctree_p","mfluc_p",
                              "guide_sum_12_p", "guide_sum_1_cor_p"),
                    labels = c("CTree","MOB",
                               "GUIDE_full", "GUIDE",
                               "CTree_p","MOB_p",
                               "GUIDE_full_p", "GUIDE_p"))

for(i in c((length(pr$test)+1) : (2*length(pr$test)))){
  if(prex$test[i] == "CTree") prex$test[i] <- "CTree_p"
  if(prex$test[i] == "MOB") prex$test[i] <- "MOB_p"
  if(prex$test[i] == "GUIDE_full") prex$test[i] <- "GUIDE_full_p"
  if(prex$test[i] == "GUIDE") prex$test[i] <- "GUIDE_p"
}

prex$nrsubgr[(length(pr$nrsubgr)+1) : (2*length(pr$nrsubgr))] <- pr$nrsubgr_p
prex$ari[(length(pr$ari)+1) : (2*length(pr$ari))] <- pr$ari_p

par.settings <- list(superpose.symbol = list(col = pal[c("CTree", "MOB", "GUIDE_full", "GUIDE")], 
                                             fill = pal[c("CTree", "MOB", "GUIDE_full", "GUIDE")]), 
                     superpose.line = list(col = pal[c("CTree", "MOB", "GUIDE_full", "GUIDE")]))

@

\begin{figure}[h!]
\setkeys{Gin}{width=\linewidth}
<<prune_ari, fig=TRUE, echo=FALSE, height=4, width=8>>=
xyplot(ari ~ delta | xi, groups = ~ test, 
       data = prex, 
       type = "l", lty = c(1,1,1,1,2,2,2,2),
       key =  list(text = list(levels(prex$test)[1:4]),
                   lines = list(lty = c(1,1,1,1), type = "l", pch = 1, cex = 0.8,
                                col = pal[c("CTree", "MOB", "GUIDE_full", "GUIDE")], lwd = 1)),
       layout= c(4,1),
       xlab = expression(delta),
       ylab = "Adjusted Rand Index",
       strip = strip.custom(factor.levels = c(expression(xi==0), 
                                              expression(xi==0.2), 
                                              expression(xi==0.5), 
                                              expression(xi==0.8))),
       par.settings = par.settings
)
@
\caption{\label{fig:pruning_ari}Adjusted Rand Index over increasing effect size $\delta$ for
250 observations and 100 data sets per step.
Each panel represents a different value of the split point $\xi$ ($0$, $0.2$, $0.5$ and 
$0.8$ from left to right). The performances of four testing strategies are compared: CTree,
MOB, GUIDE, GUIDE\_full.}
\end{figure}

\begin{figure}[h!]
\setkeys{Gin}{width=\linewidth}
<<prune_nrsubgr, fig=TRUE, echo=FALSE, height=4, width=8>>=
xyplot(nrsubgr ~ delta | xi, groups = ~ test, 
       data = prex, 
       type = "l", lty = c(1,1,1,1,2,2,2,2),
       key =  list(text = list(levels(prex$test)[1:4]),
                   lines = list(lty = c(1,1,1,1), type = "l", pch = 1, cex = 0.8,
                                col = pal[c("CTree", "MOB", "GUIDE_full", "GUIDE")], lwd = 1)),
       layout= c(4,1),
       xlab = expression(delta),
       ylab = "Adjusted Rand Index",
       strip = strip.custom(factor.levels = c(expression(xi==0), 
                                              expression(xi==0.2), 
                                              expression(xi==0.5), 
                                              expression(xi==0.8))),
       par.settings = par.settings
)

@
\caption{\label{fig:pruning_nrsubgr}Adjusted Rand Index over increasing effect size $\delta$ for
250 observations and 100 data sets per step.
Each panel represents a different value of the split point $\xi$ ($0$, $0.2$, $0.5$ and 
$0.8$ from left to right). The performances of four testing strategies are compared: CTree,
MOB, GUIDE, GUIDE\_full. Each of them is once applied with pre-pruning as before and once with 
post-pruning (dashed lines)}
\end{figure}

\newpage
\bibliography{ref.bib}

\end{document}