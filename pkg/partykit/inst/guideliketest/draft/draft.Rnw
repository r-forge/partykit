\documentclass[nojss]{jss}

%% packages
\usepackage{amstext,amsfonts,amsmath,amssymb,bm,thumbpdf,lmodern,hyperref}
\usepackage[all]{hypcap}
\usepackage{enumitem}
\usepackage{color,soul}

%% need no \usepackage{Sweave} 
\SweaveOpts{engine = R, concordance = FALSE, eps = FALSE, keep.source = TRUE, eval = TRUE}

%% commands
\newcommand{\fixme}[1]{\emph{\marginpar{FIXME} (#1)}}
\renewcommand{\Prob}{\mathbb{P} }
\renewcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\Var}{\mathbb{V}}
\newcommand{\R}{\mathbb{R} }
\newcommand{\N}{\mathbb{N} }
\newcommand{\C}{\mathbb{C} }
\newcommand{\argmin}{\operatorname{argmin}\displaylimits}
\newcommand{\argmax}{\operatorname{argmax}\displaylimits}
\newcommand{\LS}{\mathcal{L}_n}
\newcommand{\TS}{\mathcal{T}_n}
\newcommand{\LSc}{\mathcal{L}_{\text{comb},n}}
\newcommand{\LSbc}{\mathcal{L}^*_{\text{comb},n}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\yn}{y_{\text{new}}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\sX}{\mathcal{X}}
\newcommand{\sY}{\mathcal{Y}}
\newcommand{\T}{\mathbf{T}}
\newcommand{\x}{\mathbf{x}}
\renewcommand{\a}{\mathbf{a}}
\newcommand{\xn}{\mathbf{x}_{\text{new}}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\ws}{\mathbf{w}_\cdot}
\renewcommand{\t}{\mathbf{t}}
\newcommand{\M}{\mathbf{M}}
\renewcommand{\vec}{\text{vec}}
\newcommand{\B}{\mathbf{B}}
\newcommand{\K}{\mathbf{K}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\cellx}{\pi_n[\x]}
\newcommand{\partn}{\pi_n(\mathcal{L}_n)}
\newcommand{\err}{\text{Err}}
\newcommand{\ea}{\widehat{\text{Err}}^{(a)}}
\newcommand{\ecv}{\widehat{\text{Err}}^{(cv1)}}
\newcommand{\ecvten}{\widehat{\text{Err}}^{(cv10)}}
\newcommand{\eone}{\widehat{\text{Err}}^{(1)}}
\newcommand{\eplus}{\widehat{\text{Err}}^{(.632+)}}
\newcommand{\eoob}{\widehat{\text{Err}}^{(oob)}}
\newcommand{\bft}{\mathbf{t}}

\definecolor{changes}{rgb}{1,1,0.7}
\newcommand{\hlc}[1]{{\sethlcolor{changes}\hl{#1}}}

<<preliminaries, echo=FALSE, results=hide>>=
# setwd("~/svn/partykit/pkg/partykit/inst/guideliketest/draft/")
options(prompt = "R> ", continue = "+  ", useFancyQuotes = FALSE, width = 70)

library("partykit")
library("Formula")
library("parallel")
library("lattice")
library("scales")
library("colorspace")

source("../guidelike.R")
source("../tests_guideprune.R")
source("../ccprune.R")

if(file.exists("stump.rda")){
  load("stump.rda")
} else {
  simres <- simwrapper_p(nobs = 250, nrep = 100, seed = 7,
                         nrsplitvar = 1, nrsteps = 1, nrlevels = 2,
                         delta = seq(from = 0, to = 1, by = 0.1),
                         xi = c(0, 0.2, 0.5, 0.8), vary_beta = c("all", "beta0", "beta1"),
                         binary_regressor = FALSE, binary_beta = TRUE, only_intercept = FALSE,
                         test = c("ctree", "mfluc", "ctree_max",
                                  "ctree_cat", "mfluc_cat", "ctree_max_cat",
                                  "ctree_bin", "mfluc_bin", "ctree_max_bin",
                                  "ctree_cat_bin", "mfluc_cat_bin", "ctree_max_cat_bin",
                                  "guide_sum_12", "guide_coin_12", "guide_sum_1_cor"),
                         beta0 = 0, beta1 = 1,
                         stump = TRUE, z1dist = "unif", sigma = 1, alpha = 0.05)
  
  save(simres, file = "stump.rda")
}
sim_stump <- simres$res
rm(simres)

if(file.exists("tree.rda")){
  load("tree.rda")
} else {
  simres <- simwrapper_p(nobs = 250, nrep = 100, seed = 7,
                         nrsplitvar = 2, nrsteps = 1, nrlevels = 2,
                         delta = seq(from = 0, to = 1, by = 0.1),
                         xi = c(0, 0.2, 0.5, 0.8), vary_beta = "all",
                         binary_regressor = FALSE, binary_beta = TRUE,
                         only_intercept = FALSE,
                         test = c("ctree", "mfluc", "ctree_max",
                                  "ctree_cat", "mfluc_cat", "ctree_max_cat",
                                  "ctree_bin", "mfluc_bin", "ctree_max_bin",
                                  "ctree_cat_bin", "mfluc_cat_bin", "ctree_max_cat_bin",
                                  "guide_sum_12", "guide_coin_12", "guide_sum_1_cor"),
                         beta0 = NULL, beta1 = NULL,
                         stump = FALSE, z1dist = "unif", sigma = 1, alpha = 0.05)
  
  save(simres, file = "tree.rda")
}
sim_tree <- simres$res
rm(simres)

## HCL palette
pal <- qualitative_hcl(5, "Dark 3")
names(pal) <- c("CTree", "MOB", "GUIDE", "GUIDE_scores", "CTree_max")

## HCL palette for illustrations of data generating process
pal_dgp_light <- qualitative_hcl(3, "Set 2")
pal_dgp <- qualitative_hcl(3, "Dark 3")

## HCL palette for tree panels
paltrees <- rgb(c(0.9, 1), c(0.9, 1), c(0.9, 1))

@


\title{The Power of Unbiased Recursive Partitioning:\\
A Unifying View of CTree, MOB, and GUIDE}
\Shorttitle{The Power of Unbiased Recursive Partitioning}
%Comparing Testing Strategies of Unbiased Tree Algorithms

\author{Lisa Schlosser\\Universit\"at Innsbruck
\And Torsten Hothorn\\Universit\"at Z\"urich
\And Achim Zeileis\\Universit\"at Innsbruck}
\Plainauthor{Lisa Schlosser, Torsten Hothorn, Achim Zeileis}

\Abstract{
A core step of every algorithm for learning regression trees is the decision if, 
how, and where to split the underlying data - or, in other words, the selection 
of the best splitting variable from the available covariates and the corresponding 
split point. Early tree algorithms (e.g., AID, CART) employ greedy search strategies 
that directly compare all possible split points in all available covariates. However, 
subsequent research showed that this is biased towards selection of covariates with 
more potential split points. Therefore, unbiased recursive partitioning algorithms 
have been suggested (e.g., QUEST, GUIDE, CTree, MOB) that first select the covariate 
based on statistical inference using $p$-values that are appropriately adjusted for 
the possible split points. In a second step a split point optimizing some objective 
function is selected in the chosen split variable. However, different unbiased tree 
algorithms employ different inference frameworks for computing these $p$-values and 
their relative advantages or disadvantages are not well understood, yet.

Therefore, three different approaches are considered here and embedded into a common 
modeling framework with special emphasis to linear model trees: classical categorical 
association tests (GUIDE), conditional inference (CTree), parameter instability 
tests (MOB). It is assessed how different building blocks affect the power of the 
tree algorithms to select the appropriate covariates for splitting: residuals vs.\ full 
model scores, binarization of residuals/scores at zero, binning of covariates, 
conditional vs.\ unconditional approximations of the null distribution.
}
\Keywords{classification and regression trees, independence tests, recursive partitioning}

\Address{
Lisa Schlosser, Achim Zeileis \\
Universit\"at Innsbruck \\
Department of Statistics \\
Faculty of Economics and Statistics \\
Universit\"atsstr.~15 \\
6020 Innsbruck, Austria \\
E-mail: \email{Lisa.Schlosser@uibk.ac.at}, \email{Achim.Zeileis@R-project.org} \\
URL: \url{https://www.uibk.ac.at/statistics/personal/schlosser-lisa/},\\
\phantom{URL: }\url{https://eeecon.uibk.ac.at/~zeileis/}\\

Torsten Hothorn\\
Universit\"at Z\"urich \\
Institut f\"ur Epidemiologie, Biostatistik und Pr\"avention \\
Hirschengraben 84\\
CH-8001 Z\"urich, Switzerland \\
E-mail: \email{Torsten.Hothorn@R-project.org}\\
URL: \url{http://user.math.uzh.ch/hothorn/}\\

}


\begin{document}

\section{Introduction}\label{sec:introduction}
In many situations fitting one global model to a given data set can be very challenging, 
especially if the data contains lots of different features with strong variation 
and complex interactions. Therefore, separating the data into more homogeneous subgroups 
based on a set of covariates first can simplify the task and fitting a local model to 
each of the resulting subgroups often leads to better results. This separation can be 
done by applying a tree algorithm. While all of the many developed algorithms 
follow the general idea of splitting the data such that some objective function is optimized, 
they differ in their specific approaches to selecting a split variable and the corresponding 
split point. 
Some of the first tree algorithms (e.g., AID, \citealp{Morgan+Sonquist:1963}; 
CART, \citealp{Breiman+Friedman+Stone:1984}) rely on exhaustive search procedures to find both, 
the best split point and split variable in one step by directly comparing all possible split
points in all possible split variables. However, it has been shown that this is not only
computationally expensive but also biased towards split variables with many possible split
points \citep{Doyle:1973, Kim+Loh:2001}. 
Therefore, selecting a split variable in a first step and then searching for the 
best split point only within this variable in a separated second step is a more 
promising strategy as applied for example by the algorithms QUEST \citep{Loh+Shih:1997},
CHAID \citep{Kass:1980}, GUIDE \citep{Loh:2002}, CTree \citep{Hothorn+Hornik+Zeileis:2006} 
and MOB \citep{Zeileis+Hothorn+Hornik:2008}.
For the first step of selecting a split variable they all share the same basic concept of 
choosing the covariate which shows the highest association to the response variable 
based on $p$-values provided by a statistical test. While QUEST, CHAID and GUIDE employ 
statistical significance tests for contingency tables, CTree applies permutation tests in 
a conditional inference framework and MOB uses fluctuation tests based on asymptotic theory. 
All these approaches have shown to work well for various situations, however, the specific 
advantages and disadvantages of each of the testing strategies have not yet been 
investigated and compared in detail.

Therefore, in this paper the focus will be put on that first step of tree algorithms,
i.e. the task of selecting the best split variable. In particular, the approach of the 
GUIDE algorithm is compared to the one of the CTree algorithm and the MOB algorithm
by investigating the building blocks of their testing strategies in which they differ:
(1)~Transformations of the response: using residuals or full model scores
(2)~Binarization of residuals/scores
(3)~Categorization of covariates.
For this purpose, a unifying framework for testing strategies in unbiased model-based tree 
algorithms is presented such that each of the three strategies GUIDE, CTree, and MOB can be 
obtained by a specific combination of the available building blocks. In that way, the 
effects of each of these features can be investigated separately by varying the building 
blocks in the tool box provided by this general framework. 
Additionally, the impact of applying either a pre- or postpruning strategy is analyzed.

Depending on the setup different effects can be observed for the investigated building blocks.
However, none of the results contradicts the conclusion that in general using all available 
model scores is at least as efficient but mostly clearly advantageous over considering only 
residuals. Also regarding the effects of binarizing residuals or scores at $0$ or categorizing
the values of possible split variables clear recommendations can be given based on the presented 
results.

In the first part of Section~\ref{sec:urp} a basic scheme on how to build a tree model is given. 
After explaining the investigated algorithms CTree (Section~\ref{sec:ctree}), 
MOB (\ref{sec:mob}), and GUIDE (\ref{sec:guide}) more in detail the applied pruning strategies 
(\ref{sec:pruning}) are discussed.
In Section~\ref{sec:unify} the unifying framework for testing strategies in model-based unbiased 
recursive partitioning algorithms is presented.
The setting for the simulation study is introduced in Section~\ref{sec:simulation}
and the results are illustrated and discussed in Section~\ref{sec:results}.



\section{Unbiased Recursive Partitioning}
\label{sec:urp}

\subsection{Generic Algorithm}
\label{sec:generic_alg}
The basic idea of building a regression tree model is to partition the data into smaller 
and more homogeneous subgroups based on a set of covariates. Various different tree 
algorithms have been developed, all following the same general structure starting at 
the root of the tree which is simply the first node containing all available data:
\begin{enumerate}
\item For the whole underlying data set in the current node some form of discrepancy 
measure is calculated. 
\item Among all available covariates one is selected as split variable and a value 
within the range/measurable space of this variable is chosen as split point such that 
after splitting the data into subgroups the discrepancy measure is minimal within the 
resulting nodes.
\item Steps 1 and 2 are repeated within each new node until either a stopping criterion
is reached or the data in the node consists of only one or all equal observations. 
\end{enumerate}


The employed discrepancy measure can for example be the sum of deviations from a typical 
or average value, but it can also be constructed based on a parametric model fitted in each 
node. For instance, different types of residuals or their signs can be applied 
\citep[see e.g.,][]{Loh:2002} as well as rank sums or logrank scores as 
in \cite{Hothorn+Hornik+Wiel:2006}.
%(In the simulation study of this paper trees with linear models fitted in each node are 
%investigated with the residual sum of squares (RSS) employed as discrepancy measure.)

As explained in Section~\ref{sec:introduction} the considered tree algorithms CTree, MOB, and 
GUIDE first select the split variable and then, in a separate step, the split point. 
For the first step of selecting a split variable they all apply statistical tests
following the same general strategy: 
\begin{enumerate}
\item A discrepancy measure is obtained in the form of an $n \times k$-matrix $D$ giving information
on the goodness of fit of the model for a data set of $n \in \mathbb{N}$ observations in the 
current node and the parameter $k \in \mathbb{N}$ defined by the algorithm.
\item Using this matrix the algorithm-specific test statistic is evaluated for each possible 
split variable $Z_j$, $j \in \{1,\ldots,m\}$ for $m \in \mathbb{N}$, assessing the dependency
of $D$ on $Z_j$.
\item For the resulting values of the test statistics the corresponding $p$-values are calculated
in order to allow for their comparison on a standardized/unified scale and in that way for
an unbiased variable selection.
\item The variable corresponding to the smallest $p$-value is selected as split variable as 
it has shown to be the variable with the strongest influence on the goodness of fit of the model.
\end{enumerate}

While this basic approach is the same for the GUIDE, the CTree and the MOB algorithm they differ 
in their strategies on how to calculate test statistics. In order to point out these specific 
characteristics in Section~\ref{sec:unify} the strategies of the three tree algorithms are first 
explained in detail in Sections~\ref{sec:ctree}, \ref{sec:mob}, and \ref{sec:guide} . 



\subsection{CTree}\label{sec:ctree}

The CTree algorithm \citep{Hothorn+Hornik+Zeileis:2006} is based on the idea of providing 
non-parametric regression tree models in a conditional inference framework by applying 
permutation tests. 
To select a split variable it is tested whether there is any association between the transformed 
response $h(Y)$ and $g(Z)$ for a so-called influence function $g$ and each possible split 
variable~$Z$.
The function $h$ has to depend on $Y$ in a permutation-symmetric way but can be of various forms
and may also be multidimensional. For a numeric response the identity function $h(Y) = Y$ is a common 
choice while a categorical response can be mapped to a unity vector by an indicator function 
$h(Y) = (0,\ldots,1,\ldots,0)^{\top}$.
Alternatively, the function $h$ can return a location and scale parameter of $Y$, 
e.g. $h(Y) = (Y, (Y-\bar{Y})^2)^{\top}$. 
If a parametric model is fit to the values of the response variable~$Y$ and regressor 
variable(s)~$X$ the function $h$ can be a model-based transformation returning the 
corresponding model scores. This is done by setting $h(Y) = s(Y,X,\hat{\beta})$
where $\beta$ is the possibly multidimensional model parameter and $s$ is the score function
$$
s(Y, X, \beta) = \frac{\partial \psi(Y, X, \beta)}{\partial \beta}
$$
with the sum of $\psi$ over all observations being the objective function that is optimized 
to obtain the estimated model parameter $\hat{\beta}$. For this purpose many different M-type 
estimators can be applied including maximum likelihood or ordinary least squares such that the 
corresponding objective function can be employed as model-based discrepancy measure for the 
tree algorithm. Evaluating the score function $s$ on a data set of $n$ observations 
%containing the 
%response vector $\boldsymbol{y} = (y_1,\ldots,y_n)^\top$ and 
%regressor(s) $\boldsymbol{x} = (x_1,\ldots,x_n)^\top$ and on
and a $k$-dimensional estimated parameter vector 
%$\hat{\boldsymbol{\beta}} = (\hat{\beta_1},\ldots,\hat{\beta_k})^\top$ 
yields a $n \times k$-matrix.
In that way a score value indicating how well the model fits the data is given separately
for each tuple of observation and estimated parameter element 
$(y_i, x_i, \hat{\beta_l})$ with $i \in \{1,\dots,n\}$
and $l \in \{1,\ldots,k\}$. 

Similarly, different types of functions can be chosen for the influence function $g$ depending
on a possible split variable $Z$. A simple choice in case of $Z$ being a numeric variable is again
the identity function $g(Z) = Z$. For categorical variables $g$ can also map its values to the 
corresponding unity vectors by an indicator function $g(Z) = (0,\ldots,1,\ldots,0)$.

To test for independence of $h(Y)$ and $g(Z)$ a linear test statistic is calculated.
Following \cite{Strasser+Weber:1999} the conditional expectation and covariance of the linear
test statistic given all permutations of the response variable can be calculated and used to 
standardize the test statistic. In order to map multivariate test statistics into the real 
line two options are given: either the maximum of the absolute values of the standardized 
statistic or a quadratic form. The resulting values can then be transformed to 
the scale of $p$-values allowing for an unbiased comparison in order to select a split variable.

If both variables $Y$ and $Z$ are numeric the applied independence test corresponds to a Pearson 
correlation test. For one of the variables being categorical and the other numeric a 1-way ANOVA
is employed while for two categorical variables a $\chi^2$-test is performed. Hence, the CTree 
algorithm provides a very general framework where the type of variables and the corresponding 
functions $g$ and $h$ determine the type of the applied statistical test.
(See \cite{Hothorn+Hornik+VanDeWiel:2006} for examples on how to use the different options of this toolbox.)

In the CTree algorithm the number of possible breakpoints is fixed as splits can only be made at
observed values or optionally at an average value lying between two observations. To select the
best split point two-sample linear statistics of the same structure as in the independence test 
are evaluated for all possible subsets of the sample space of the selected split variable.


\subsection{MOB}\label{sec:mob}

While CTree offers to apply model-based discrepancy measures as one possible choice, the 
MOB algorithm \citep{Zeileis+Hothorn+Hornik:2008} was developed with the aim of embedding 
parametric models into the framework of regression trees and is hence purely intended for 
a model-based approach. 

The testing strategy for selecting a split variable is based on a fitted model, i.e. on 
an estimated parameter (vector) $\hat{\beta}$ for each node of the tree, and on the 
application of the central limit theorem. 
The fitted model can be of a very basic and simple form such as an average value of the response 
vector but can also be a more complex parametric model.
In \cite{Zeileis+Hothorn+Hornik:2008} generalized linear models (GLMs) and survival regression models 
are used to exemplify the algorithm. Subsequently, various other types of models have been applied as well,
for example beta regression models \citep{Gruen+Kosmidis+Zeileis:2012}, 
psychometric models such as Rasch trees \citep{Strobl+Kopf+Zeileis:2015} and
mixed effects models \citep{Fokkema+Smits+Zeileis:2018}.

Regardless of the applied model type the algorithm only requires the corresponding model 
score matrix in order to test for any instability in the model parameters over each 
particular ordering induced by each of the possible partitioning variables $Z$.
If a variable $Z$ has no specific influence on the parameters the ordered scores would 
always fluctuate randomly around their mean 0. Therefore, these deviations are investigated 
by measuring them using an empirical fluctuation process. Applying a scalar function which 
captures the fluctuation in this process yields the test statistic of this so called 
generalized M-fluctuation test \citep{Zeileis+Hornik:2007}. 
To obtain the distribution of the resulting test statistic asymptotic theory, in particular 
the central limit theorem is applied. In this parametric unconditional framework $p$-values 
can be calculated and the covariate with the smallest $p$-value is selected as split variable. 

For a categorical split variable all possible splits are evaluated to find the best one while 
for numeric split variable a maximally selected split point search is employed. Hence, 
the number of possible breakpoints is not fixed unlike in the CTree algorithm.


\subsection{GUIDE}\label{sec:guide}
Similar to the MOB algorithm, parametric models are fit to the nodes of a tree in 
the GUIDE algorithm \citep{Loh:2002}. Again, simple models such as 
a constant fit with the parameter $\beta$ being the mean of the response, hence 
$\hat{\beta} = \bar{y} = \frac{1}{n}\sum_{i=1,\ldots,n}^n y_i$, 
%for a response vector $\boldsymbol{y} = (y_1,\ldots,y_n)^\top$, 
can be employed as well as more complex models.
For the statistical test applied to select a split variable some type of model-based
residual is required. In \cite{Loh:2002} linear models are used as an example where the
residuals are defined by
$$
r(Y,X,\hat{\beta}) = Y - \hat{\beta_0} - \hat{\beta_1} \cdot X.
$$
In related literature other models together with an appropriate choice of residuals
have been applied such as in
regression trees for longitudinal and multiresponse data \citep{Loh+Zheng:2013}, 
%(http://www.stat.rice.edu/~jrojo/4th-Lehmann/slides/Loh.pdf)
quantile regression models \citep{Chaudhuri+Loh:2002} 
and proportional hazards modeling via Poisson regression \citep{Loh+He+Man:2015} among many others.
%Poisson regression trees \citep{Chaudhuri+Lo+Loh:1995}, %(adjusted Anscombe residual (Pierce and Schafer 1986))

Before applying a statistical test two additional steps are performed:
(1)~The residuals are binarized, i.e. depending on whether the sign of a residual is 
positive or not it is replaced by $1$ or $2$.  % FIX ME: +1 and -1 in the implementation? 
(2)~For each possible split variable the observed values are categorized into four groups 
separated by the corresponding 1st, 2nd and 3rd quartile unless the split variable is already
categorical.
Then, a $\chi^2$-test of independence is performed for the binarized residuals and
each categorized/categorical split variable.
After choosing the split variable showing the highest dependency by yielding the lowest $p$-value,
the split point minimizing the discrepancy measure is selected. The possible number of breakpoints
is determined by the categorization of the possible split variables and is in that way fixed.

\newpage
\subsection{Pruning}
\label{sec:pruning}
In early developed methods of unbiased recursive partitioning such as 
FACT \citep{Loh+Vanichsetakul:1988}, QUEST \citep{Loh+Shih:1997}, 
CRUISE \citep{Kim+Loh:2001} or GUIDE \citep{Loh:2002}
the $p$-values of applied significance tests are used purely for the selection of a split variable 
based on a unified scale. All other steps are performed in the same way as in classic tree 
algorithms, such as finding the best split point by optimizing an objective function.

In subsequent approaches as in \cite{Hothorn+Hornik+Zeileis:2006}
it is shown that these $p$-values can also be used as a basis for a
pruning strategy namely \textit{prepruning}: Only if the $p$-value corresponding to the selected split
variable (hence the smallest $p$-value) is smaller than a given significance level a split is
performed while otherwise no further splits are made in the current node.
In that way the information provided by the significance test is exploited in a more profound way.
However, prepruning requires the applied test to perform reasonably well as a significance test, 
i.e. it has to have power but also avoid overfitting.

On the contrary, when employing postpruning no significance level is included leading to very large 
trees which are then pruned afterwards, for example by applying cost-complexity pruning as in the 
CART algorithm \citep{Breiman+Friedman+Stone:1984}. 
Thus, for postpruning the test only needs to be able to select the best split variable but does 
not have to be effective as a significance test.

Originally the GUIDE algorithm was introduced applying postpruning while CTree and MOB
were both presented employing prepruning (see Table~\ref{tab:combinations} for a more detailed
overview). Alternatively, for model-based partitioning a pruning strategy can also use model-based 
information criteria such as AIC or BIC \citep[see for example][]{Su+Wang+Fan:2004}.
However, all testing strategies can be combined with each of these pruning strategies.
If a test performs well the choice of a pruning strategy has hardly any impact on the results, 
but as prepruning only requires already provided information it is usually less computationally
expensive than postpruning needing additional calculations. 
However, if the test performs poorly in terms of power and overfitting, prepruning suffers from 
this deficiency while postpruning can compensate it at least partly and improve the results.
Therefore, being aware of the power of the selected testing strategy is essential for choosing 
an appropriate pruning strategy.
For that reason we are focusing on the testing strategies applied in tree algorithms 
and investigate how effective they are as significance tests.





\section{Unifying Framework}\label{sec:unify}
Each of the algorithms CTree, GUIDE, and MOB can be characterized by its combination
of the type of fitted models, the type of applied tests and the employed pruning strategy
as listed in Table~\ref{tab:combinations}. These settings represent the algorithms in their
original form. However, in principle all other combinations of the listed features are possible 
as well. As explained in Section~\ref{sec:pruning} the focus of this paper is on the testing 
strategies (second column of Table~\ref{tab:combinations}). They are here discussed for the case 
of parametric models together with prepruning as the properties of this pruning strategy
are already determined by the testing strategy. Subsequently combinations
with postpruning are evaluated as well to show the impact of the choice of pruning strategy.
From here on the names CTree, MOB, and GUIDE are used to refer to the algorithm-specific
testing strategies rather than the complete algorithms.

\begin{table}[t]
\centering
\caption[Table caption text]{Combinations of fitted model type, test type and pruning strategy
for the algorithms CTree, MOB, and GUIDE.}
\label{tab:combinations}
\begin{tabular}{ l l l l }
\hline 
      & Fit             & Test                     & Pruning \\ 
\hline
CTree & non-parametric  & conditional inference    & pre\\
MOB   & parametric      & score-based fluctuation  & pre (or post with AIC/BIC)\\
GUIDE & parametric      & residual-based $\chi^2$  & post (cost-complexity pruning)\\
\hline
\end{tabular}
\end{table}

\subsection{Unification of Testing Strategies}

Even though CTree, MOB, and GUIDE differ in the specific tests they apply (see second column of
Table~\ref{tab:combinations}) their approaches on how to select a split variable follow the same
basic structure as explained in Section~\ref{sec:generic_alg}. 
Thus they can be presented together in a unifying framework for testing strategies
in model-based unbiased recursive partitioning. 
In this framework each specific testing strategy can be obtained by combining the provided 
building blocks accordingly. But apart from that, variations of the commonly known testing 
strategies can be obtained as this general toolbox allows for all possible combinations of the 
building blocks. Moreover, by exchanging only one of them its impact can be investigated in detail.
These building blocks are explained in Section~\ref{sec:buildingblocks} and their setting
for the standard setup of CTree, MOB, and GUIDE are listed in Table~\ref{tab:buildingblocks} which 
gives a more detailed overview of the differences in the second column of Table~\ref{tab:combinations}.


\subsection{Building Blocks of Testing Strategies}\label{sec:buildingblocks}
The three following building blocks all refer to the way how the given data is prepared/transformed
before employing a statistical test to select a split variable.
\begin{itemize}
\item  Transformations of Y:\\
A model-based transformation of $Y$ is applied to obtain a discrepancy measure giving 
information about the goodness of fit of the model. This can be residuals $r(Y,X,\hat{\beta})$
as in the GUIDE algorithm or scores $s(Y,X,\hat{\beta})$ as in the MOB algorithm or, 
more generally, a function $h(Y,X)$ as in the CTree algorithm including both of the foregoing 
cases.
By including all available scores rather than just residuals the goodness of fit of the model is 
investigated regarding each parameter instead of just the mean. 
%Therefore, this facilitates the detection of changes in parameters other than the mean/location parameter.

\item Binarization of residuals/scores:\\
Rather than applying the independence test directly on the residuals/scores a
binarized version of them can be used as well. In particular, the values are binarized
at $0$ such that only their signs are considered.

\item Categorization of covariates:\\
Similarly, the covariates can also be categorized. For this purpose, the corresponding 1st, 2nd 
and 3rd quartile are considered to bin each possible split variable into four categories. In that way,
depending on whether the scores are binarized or not, a $\chi^2$-test or 1-way ANOVA can be applied
where the quartiles are the only possible split points while otherwise maximally selected splits 
over $Z$ are considered. 

\end{itemize}

\textit{Note:}
Apart from these three building blocks, the algorithms CTree, MOB, and GUIDE differ in the type of test 
statistic they apply and their approximation of the underlying null distribution of the corresponding 
test statistic. While the permutation tests in CTree work in a conditional framework both MOB (M-fluctuation tests) 
and GUIDE ($\chi^2$-tests) apply an unconditional approach. Furthermore, the algorithms CTree and MOB were 
introduced employing prepruning while GUIDE originally applies a postpruning strategy.
However, these differences refer to the standard versions of the three algorithms and all of them
allow for certain modifications of this standard setup. For example, a different form of test statistic
can be selected in CTree considering a maximum value as in MOB.


\begin{table}[t]
\centering
\caption[Table caption text]{Testing strategies of CTree, MOB, and GUIDE with the corresponding
setting of the building blocks in the unifying framework and the type of test statistic.}
\label{tab:buildingblocks}
\begin{tabular}{ l c c c c }
\hline 
      & Scores       & Binarization  & Categorization  & Statistic\\
\hline
CTree & Model scores & --	           & -- 	           & Sum of squares\\
MOB   & Model scores & --	           & -- 	           & Maximally selected\\
GUIDE & Residuals    & \checkmark    & \checkmark      & Sum of squares\\
\hline
\end{tabular}
\end{table}


%\newpage

\subsection{Linear Model Tree}\label{sec:lmtrees}
As an example a linear model tree is considered as this allows for an easy and well structured 
description of the investigated subjects. However, the conclusions drawn from this example also 
hold for a wide range of other model types. 
The models fitted in the nodes of the linear model tree as considered for the simulation 
study in Section~\ref{sec:simulation} are of the form
$$
Y = \beta_0 + \beta_1 \cdot X + \epsilon
$$
for a response variable $Y$, a regressor variable $X$ and an error $\epsilon$. 
For each node the type of the fitted model is the same such that no node-specific features are 
considered and the observations of the (arbitrarily chosen) current node are denoted by 
$\{(y_i,x_i)\}_{i=1,\ldots,n}$ for $n \in \mathbb{N}$.

A very basic form of a discrepancy measure can be obtained for each observation by simply 
calculating deviations of the response values from the mean value within the node, for example 
by $(\bar{y} - y_i)$.
However, this includes only the response variable $Y$ but not the fitted linear model.
For a model-based approach residuals can be considered defined by
$$
r(y_i,x_i,\beta_0,\beta_1) = y_i - \beta_0 - \beta_1 \cdot x_i.
$$
The appropriate choice of the function $\psi$ is then
$$
\psi(y_i,x_i,\beta_0,\beta_1) = r(y_i,x_i,\beta_0,\beta_1)^2
$$
such that $\sum_{i=1}^n \psi(y_i,x_i,\beta_0,\beta_1)$ is the objective function
which is minimized to obtain the estimated coefficient vector
$\boldsymbol{\hat{\beta}} = (\hat{\beta_0}, \hat{\beta_1})^\top$.
The corresponding score function is defined by
$$
s(y_i, x_i, \beta_0, \beta_1) = 
\frac{\partial \psi}{\partial \boldsymbol{\beta}}(y_i, x_i, \beta_0, \beta_1) = 
-2 \cdot r(y_i,x_i, \beta_0, \beta_1) \cdot (1,x_i)^\top
$$
where the factor $-2$ is often omitted such that the first column of the $n \times 2$-dimensional
score matrix corresponds to the residuals. 
Therefore, including only residuals (i.e. only the first column of the score matrix)
corresponds to including the derivatives of $\psi$ only regarding the intercept 
parameter~$\beta_0$.
On the contrary, by using full model scores (i.e. the whole $n \times 2$ matrix) 
the derivatives of $\psi$ regarding the intercept $\beta_0$ (first column) and the slope 
parameter $\beta_1$ (second column) are considered.


\newpage

\section{Simulation Setting and Evaluation}
\label{sec:simulation}

In this simulation study two different scenarios are considered for the linear model trees
presented in Section~\ref{sec:lmtrees}. First, the underlying tree structure based on which the
data is generated is a stump, i.e. a tree with only one split. 
By keeping the complexity of the model as low as possible the advantages and
disadvantages of the different testing strategies can be elaborated more precisely and 
the focus can be put on their power in terms of selecting the correct split variable.

In the second scenario the true tree structure contains two splits in two different variables
yielding a tree with three terminal nodes. After having investigated the characteristics of 
the employed algorithms in detail for the stump scenario this additional evaluation aims at
showing that the same conclusions can be drawn for more complex structures.

In the following the data generating process is explained in detail for the first scenario.
The second, more complex scenario employs the same basic structure simply adding another split
as illustrated in Figure~\ref{fig:dgp_tree}.

\subsection{Data Generating Process}
Each generated data set consists of a response variable, one regressor, one split 
variable and nine noise variables as listed in Table~\ref{tab:variables} 
together with the corresponding distributions.

\begin{table}
\begin{center}
\caption[Table caption text]{Variables included in the data generating process of 
the first scenario (stump).}
\label{tab:variables}
\begin{tabular}{l l l}
\hline
Name                    & Notation                   & Specification \\
\hline
\textit{Variables:}     &                            & \\ 
Response                & $Y$                        & $=\beta_0(Z_1) + \beta_1(Z_1) \cdot X + \epsilon$ \\
Regressor               & $X$                        & $\mathcal{U}([-1,1])$ \\
Error                   & $\epsilon$                 & $\mathcal{N}(0,1)$ \\
True split variable     & $Z_1$                      & $\mathcal{U}([-1,1])$  or $\mathcal{N}(0, 1)$ \\
Noise split variables   & $Z_2, Z_3, \dots, Z_{10}$  & $\mathcal{U}([-1,1])$  or $\mathcal{N}(0, 1)$ \\
\hline
\textit{Parameters/functions:} &            & \\ 
Intercept                      & $\beta_0$  & $0$ or $\pm \delta$\\
Slope                          & $\beta_1$  & $1$ or $\pm \delta$\\
True split point               & $\xi$      & $\in \{0, 0.2, 0.5, 0.8\}$ \\
Effect size                    & $\delta$   & $\in \{0, 0.1, 0.2, \ldots , 1\}$ \\
\hline
\end{tabular}
\end{center}
\end{table}

The location parameter $\mu$ of the normally distributed response variable $Y$ depends
linearly on the regressor variable $X$.
% which is a uniformly distributed variable taking values between $-1$ and $1$.
% can either be a binary variable (taking either $0$ or $1$) or a uniformly distributed 
% variable (taking values between $-1$ and $1$).
Moreover, the intercept $\beta_0$ and the slope parameter $\beta_1$ of this linear equation
(can) depend on the true split variable $Z_1$. 
%in the following way:
%\begin{center}
%$\mu(X, Z_1) = \beta_0(Z_1) + \beta_1(Z_1) \cdot X$.
%\end{center}
%
In particular, three different cases are considered for the coefficients $\beta_0$ 
and $\beta_1$:
\begin{enumerate}
\item The intercept $\beta_0$ varies depending on $Z_1$ while $\beta_1$ is fixed (at $1$).
\item The slope coefficient $\beta_1$ varies depending on $Z_1$ while $\beta_0$ is fixed 
(at $0$).
\item Both coefficients $\beta_0$ and $\beta_1$ vary depending on $Z_1$.
\end{enumerate}

The varying coefficient(s) $\beta_i$, $i\in\{0,1\}$, are binary taking either the positive or
the negative value of the effect size, i.e. $\delta$ or $-\delta$.
% can either be binary (taking either $\delta$ or $-\delta$), or a continuous function. 
% In the binary case the following definition is applied for the true split point $\xi$
\begin{align*}
\beta_{i}(Z_1) = \begin{cases}
-\delta \cdot (-1)^{i}  \quad \text{if } Z_1 < \xi \\
+\delta \cdot (-1)^{i}  \quad \text{if } Z_1 \geq \xi
\end{cases}
\end{align*}
%while in the continuous case a simple linear function is considered in the form of
%$$\beta_{i}(Z_1) = Z_1 \cdot \delta \cdot (-1)^{i}.$$
In that way, the type of variation is the same for $\beta_0$ and $\beta_1$, however, in 
opposite directions.
An illustration of the three different cases 
%for binary coefficients and a continuous regressor 
can be found in Figure~\ref{fig:dgp_stump}.

<<dgp, echo=FALSE>>=
set.seed(7)

nobs <- 250
sigma <- 0.5
xi <- 0
delta <- 1
beta0 <- 0
beta1 <- 1

d <- dgp_stump(nobs = nobs, binary_beta = TRUE, binary_regressor = FALSE, only_intercept = FALSE,
               vary_beta = "beta0", beta1 = beta1, delta = delta, xi = xi, sigma = sigma)

sp_xi1 <- partysplit(3L, breaks = 0)

pn <- partynode(1L, split = sp_xi1, 
                kids = list(
                  partynode(2L, info = c("true parameters:", 
                                         expression(beta[0] ==~" "~0~" or "~-delta), 
                                         expression(beta[1] ==~" "~1~" or "~+delta))),
                  partynode(3L, info = c("true parameters:", 
                                         expression(beta[0] ==~" "~0~" or "~+delta), 
                                         expression(beta[1] ==~" "~1~" or "~-delta)))))

p <- party(pn, d)

ep <- function (obj, digits = 3, abbreviate = FALSE, justmin = Inf, 
                just = c("alternate", "increasing", "decreasing", "equal"), 
                fill = "white") 
{
  meta <- obj$data
  justfun <- function(i, split) {
    myjust <- if (mean(nchar(split)) > justmin) {
      match.arg(just, c("alternate", "increasing", "decreasing", "equal"))
    } else {
      "equal"
    }
    k <- length(split)
    rval <- switch(myjust, equal = rep.int(0, k), 
                   alternate = rep(c(0.5, -0.5), length.out = k), 
                   increasing = seq(from = -k/2, to = k/2, by = 1), 
                   decreasing = seq(from = k/2, to = -k/2, by = -1))
    unit(0.5, "npc") + unit(rval[i], "lines")
  }
  function(node, i) {
    split <-  c("<= xi", "> xi")
    y <- justfun(i, split)
    split <- split[i]
    if (any(grep(">", split) > 0) | any(grep("<", split) > 0)) {
      tr <- suppressWarnings(try(parse(text = paste("phantom(0)", split)), silent = TRUE))
      if (!inherits(tr, "try-error")) split <- tr
    }
    grid.rect(y = y, gp = gpar(fill = fill, col = 0), width = unit(1, "strwidth", split))
    grid.text(split, y = y, just = "center")
  }
}
class(ep) <- "grapcon_generator"


tp <- function (obj, digits = 3, abbreviate = FALSE, 
                fill = c("lightgray", "white"), id = TRUE, 
                just = c("center", "top"), top = 0.85, 
                align = c("center", "left", "right"), gp = NULL, FUN = NULL, 
                height = NULL, width = NULL) 
{
  nam <- names(obj)
  extract_label <- function(node) formatinfo_node(node, FUN = FUN, 
                                                  default = c("terminal", "node"))
  maxstr <- function(node) {
    lab <- extract_label(node)
    klab <- if (is.terminal(node)) 
      ""
    else unlist(lapply(kids_node(node), maxstr))
    lab <- c(lab, klab)
    lab <- try(unlist(lapply(lab, function(x) strsplit(x, 
                                                       "\n"))), silent = TRUE)
    if (inherits(lab, "try-error")) {
      paste(rep("a", 9L), collapse = "")
    }
    else {
      return(lab[which.max(nchar(lab))])
    }
  }
  nstr <- if (is.null(width)) 
    maxstr(node_party(obj))
  else paste(rep("a", width), collapse = "")
  just <- match.arg(just[1L], c("center", "centre", "top"))
  if (just == "centre") 
    just <- "center"
  align <- match.arg(align[1L], c("center", "centre", "left", 
                                  "right"))
  if (align == "centre") 
    align <- "center"
  rval <- function(node) {
    if(node$id == 2) fill <- c("white", pal_dgp_light[1])
    if(node$id == 3 | node$id == 4) fill <- c("white", pal_dgp_light[2])
    if(node$id == 5) fill <- c("white", pal_dgp_light[3])
    fill <- rep(fill, length.out = 2)
    lab <- extract_label(node)
    if (!is.null(gp)) {
      outer_vp <- viewport(gp = gp)
      pushViewport(outer_vp)
    }
    if (is.null(height)) 
      height <- length(lab) + 1L
    node_vp <- viewport(x = unit(0.5, "npc"), y = unit(if (just == 
                                                           "top") 
      top
      else 0.5, "npc"), just = c("center", just), width = unit(1, 
                                                               "strwidth", nstr) * 1.1, height = unit(height, "lines"), 
      name = paste("node_terminal", id_node(node), sep = ""), 
      gp = if (is.null(gp)) 
        gpar()
      else gp)
    pushViewport(node_vp)
    grid.rect(gp = gpar(fill = fill[1]))
    for (i in seq_along(lab)) grid.text(x = switch(align, 
                                                   center = unit(0.5, "npc"), left = unit(1, "strwidth", 
                                                                                          "a"), right = unit(1, "npc") - unit(1, "strwidth", 
                                                                                                                              "a")), y = unit(length(lab) - i + 1, "lines"), 
                                        lab[i], just = align)
    if (id) {
      nodeIDvp <- viewport(x = unit(0.5, "npc"), y = unit(1, 
                                                          "npc"), width = max(unit(1, "lines"), unit(1.3, 
                                                                                                     "strwidth", nam[id_node(node)])), height = max(unit(1, 
                                                                                                                                                         "lines"), unit(1.3, "strheight", nam[id_node(node)])))
      pushViewport(nodeIDvp)
      grid.rect(gp = gpar(fill = fill[2], lty = "solid"))
      grid.text(nam[id_node(node)])
      popViewport()
    }
    if (is.null(gp)) 
      upViewport()
    else upViewport(2)
  }
  return(rval)
}

class(tp) <- "grapcon_generator"

@

\begin{figure}%[t]
\setkeys{Gin}{width=1\linewidth}
\minipage{0.45\textwidth}
<<dgp_stump1, fig=TRUE, echo=FALSE, width=4, height=3>>=
#par(mar=c(3,3,0,0), oma = c(3,3,0,0))
plot(p, 
     tp_args = list(FUN = identity, width = 15), 
     ip_args = list(fill = paltrees[c(1, 2)]),
     drop_terminal = TRUE, tnex = 1, edge_panel = ep, terminal_panel = tp)
@
\endminipage
\minipage{0.45\textwidth}
<<dgp_stump2, fig=TRUE, echo=FALSE, width=4, height=3>>=
par(mar = c(4,4,2,4))
plot(x = d$x[d$z1<=xi], y = d$y[d$z1<=xi], ylim = c(-5,5), ylab = "y", xlab = "x", 
     main = expression("varying"~beta[0]),
     pch = 19, col = alpha(pal_dgp[1], 0.4), cex = 0.7)
abline(coef = c(-1,1), col = pal_dgp[1], lwd = 2)
abline(coef = c(1,1), col = pal_dgp[2], lwd = 2)
points(x = d$x[d$z1>xi], y = d$y[d$z1>xi], ylim = c(-5,5), pch = 19, col = alpha(pal_dgp[2], 0.4), cex = 0.7)
legend("top", c(expression(z[1]<=xi), expression(z[1]>xi)), col = c(pal_dgp[1], pal_dgp[2]), pch = c(19), bty = "n", cex = 0.9)
mtext(expression(beta[0]==+delta), side = 4, col = pal_dgp[2], las = 1, line = 0.5, at = 3.2)
mtext(expression(beta[1]== 1), side = 4, col = pal_dgp[2], las = 1, line = 0.5, at = 2.2)
mtext(expression(beta[0]==-delta), side = 4, col = pal_dgp[1], las = 1, line = 0.5, at = 0.2)
mtext(expression(beta[1]== 1), side = 4, col = pal_dgp[1], las = 1, line = 0.5, at = -0.8)
@
\endminipage

\minipage{0.45\textwidth}
<<dgp_stump3, fig=TRUE, echo=FALSE, width=4, height=3>>=
d <- dgp_stump(nobs = nobs, binary_beta = TRUE, binary_regressor = FALSE, only_intercept = FALSE,
               vary_beta = "beta1", beta0 = beta0, delta = delta, xi = xi, sigma = sigma)

par(mar = c(4,4,2,4))
plot(x = d$x[d$z1<=xi], y = d$y[d$z1<=xi], ylim = c(-5,5), ylab = "y", xlab = "x", main = expression("varying"~beta[1]),
     pch = 19, col = alpha(pal_dgp[1], 0.4), cex = 0.7)
abline(coef = c(0,1), col = pal_dgp[1], lwd = 2)
abline(coef = c(0,-1), col = pal_dgp[2], lwd = 2)
points(x = d$x[d$z1>xi], y = d$y[d$z1>xi], ylim = c(-5,5), pch = 19, col = alpha(pal_dgp[2], 0.4), cex = 0.7)
legend("top", c(expression(z[1]<=xi), expression(z[1]>xi)), col = c(pal_dgp[1], pal_dgp[2]), pch = c(19), bty = "n", cex = 0.9)
mtext(expression(beta[0]==0), side = 4, col = pal_dgp[2], las = 1, line = 0.5, at = -1)
mtext(expression(beta[1]==-delta), side = 4, col = pal_dgp[2], las = 1, line = 0.5, at = -2)
mtext(expression(beta[0]==0), side = 4, col = pal_dgp[1], las = 1, line = 0.5, at = 2)
mtext(expression(beta[1]==+delta), side = 4, col = pal_dgp[1], las = 1, line = 0.5, at = 1)
@
\endminipage
\minipage{0.45\textwidth}
<<dgp_stump4, fig=TRUE, echo=FALSE, width=4, height=3>>=
d <- dgp_stump(nobs = nobs, binary_beta = TRUE, binary_regressor = FALSE, only_intercept = FALSE,
               vary_beta = "all", delta = delta, xi = xi, sigma = sigma)
par(mar = c(4,4,2,4))
plot(x = d$x[d$z1<=xi], y = d$y[d$z1<=xi], ylim = c(-5,5), ylab = "y", xlab = "x",
     main = expression("varying"~beta[0]~"and"~beta[1]),
     xlim = c(-1,1.1),
     pch = 19, col = alpha(pal_dgp[1], 0.4), cex = 0.7)
abline(coef = c(-1,1), col = pal_dgp[1], lwd = 2)
abline(coef = c(1,-1), col = pal_dgp[2], lwd = 2)
points(x = d$x[d$z1>xi], y = d$y[d$z1>=xi], pch = 19, col = alpha(pal_dgp[2], 0.4), cex = 0.7)
legend("top", c(expression(z[1]<=xi), expression(z[1]>xi)), col = c(pal_dgp[1], pal_dgp[2]), pch = c(19), bty = "n", cex = 0.9)
mtext(expression(beta[0]==-delta), side = 4, col = pal_dgp[1], las = 1, line = 0.5, at = 1.7)
mtext(expression(beta[1]==+delta), side = 4, col = pal_dgp[1], las = 1, line = 0.5, at = 0.7)
mtext(expression(beta[0]==+delta), side = 4, col = pal_dgp[2], las = 1, line = 0.5, at = -0.7)
mtext(expression(beta[1]==-delta), side = 4, col = pal_dgp[2], las = 1, line = 0.5, at = -1.7)
@
\endminipage
\caption{\label{fig:dgp_stump}{Top left panel: True stump structure with the true model parameters $\beta_0$ and $\beta_1$, either varying or being fixed at $0$ or $1$ respectively.
Top right and bottom panels: Bivariate plot of the response $Y$ on regressor $X$ 
with effect size $\delta = 1$ and for three cases: 
varying $\beta_0$ and fixed $\beta_1$ (top right),
fixed $\beta_0$ and varying $\beta_1$ (bottom left),
varying $\beta_0$ and varying $\beta_1$ (bottom right).}}
\end{figure}

For the second simulation scenario the same basic setting with the same variables is used
with an additional split in $Z_2$ resulting in a tree with three terminal nodes. The first
split (in $Z_2$, at~$\xi$) induces a change in the slope parameter $\beta_1$ while the second
split (in $Z_1$, also at~$\xi$) corresponds to a change in the intercept $\beta_0$.
The underlying tree structure for the data generating process is illustrated in 
Figure~\ref{fig:dgp_tree}.

<<true_tree_dgp, eval = TRUE, echo=FALSE, results=hide>>=

# d <- dgp_tree(nobs = 250)
set.seed(7)
d <- dgp_tree(nobs = 250, xi = 0, binary_regressor = FALSE, vary_beta = "all", 
              binary_beta = TRUE, nrsplitvar = 2, sigma = 0.5)

sp_xi2 <- partysplit(4L, breaks = 0)
sp_xi1 <- partysplit(3L, breaks = 0)

pn <- partynode(1L, split = sp_xi2, 
                kids = list(
                  partynode(2L, info = c("true parameters:", 
                                         expression(beta[0] == 0), 
                                         expression(beta[1] == +delta))),
                  partynode(3L, split = sp_xi1, 
                            kids = list(
                              partynode(4L, info = c("true parameters:", 
                                                     expression(beta[0] == -delta), 
                                                     expression(beta[1] == -delta))),
                              partynode(5L, info = c("true parameters:", 
                                                     expression(beta[0] == +delta), 
                                                     expression(beta[1] == -delta)))))))

p <- party(pn, d)

#ctest <- evaltests(y~x|z1+z2+z3+z4+z5+z6+z7+z8+z9+z10, data = d, testfun = "ctree", stump = FALSE)
@


\begin{figure}%[t]
\begin{center}
\setkeys{Gin}{width=1\linewidth}
\minipage{0.5\textwidth}
<<dgp_tree1, fig=TRUE, echo=FALSE, width=5, height=4.5>>=
par(mar=c(0,0,0,0), oma=c(0,0,0.4,0))
plot(p, edge_panel = ep, terminal_panel = tp,
     tp_args = list(FUN = identity, width = 15), 
     ip_args = list(fill = paltrees[c(1, 2)]),
     drop_terminal = TRUE, tnex = 1)
@
\endminipage
\minipage{0.45\textwidth}
<<dgp_tree2, fig=TRUE, echo=FALSE, width=4, height=4>>=
par(mar = c(4,4,2,4))
plot(x = d$x[d$z2<=xi], y = d$y[d$z2<=xi], ylim = c(-5,5), ylab = "y", xlab = "x",
     pch = 19, col = alpha(pal_dgp[1], 0.4), cex = 0.7)
abline(coef = c(0,1), col = pal_dgp[1], lwd = 2)
abline(coef = c(-1,-1), col = pal_dgp[2], lwd = 2)
abline(coef = c(1,-1), col = pal_dgp[3], lwd = 2)
points(x = d$x[d$z2>xi & d$z1<=xi], y = d$y[d$z2>xi & d$z1<=xi], pch = 19, 
       col = alpha(pal_dgp[2], 0.4), cex = 0.7)
points(x = d$x[d$z2>xi & d$z1>xi], y = d$y[d$z2>xi & d$z1>xi], pch = 19, 
       col = alpha(pal_dgp[3], 0.4), cex = 0.7)
legend("top", c(expression(z[2]<=xi), 
                expression(z[2]>xi~" and "~z[1]<=xi),
                expression(z[2]>xi~" and "~z[1]>xi)), 
       col = c(pal[1], pal[2], pal_dgp[3]), pch = c(19), bty = "n", cex = 0.9)
mtext(expression(beta[0]==0), side = 4, col = pal_dgp[1], las = 1, line = 0.5, at = 2)
mtext(expression(beta[1]==+delta), side = 4, col = pal_dgp[1], las = 1, line = 0.5, at = 1.3)
mtext(expression(beta[0]==+delta), side = 4, col = pal_dgp[3], las = 1, line = 0.5, at = 0.1)
mtext(expression(beta[1]==-delta), side = 4, col = pal_dgp[3], las = 1, line = 0.5, at = -0.6)
mtext(expression(beta[0]==-delta), side = 4, col = pal_dgp[2], las = 1, line = 0.5, at = -2)
mtext(expression(beta[1]==-delta), side = 4, col = pal_dgp[2], las = 1, line = 0.5, at = -2.7)
@
\endminipage
\caption{\label{fig:dgp_tree}Left panel: True tree structure for the second simulation scenario 
with the true model parameters $\beta_0$ and $\beta_1$ for each terminal panel.
Right panel: Bivariate plot of the response $Y$ on regressor $X$ 
with effect size $\delta = 1$.}
\end{center}
\end{figure}




\subsection{Evaluation}
\label{sec:evaluation}
The testing strategies are evaluated over a stepwise increasing effect size $\delta$, 
on 100 data sets per step each consisting of 250 observations. 
To compare the performance of the evaluated testing strategies for each step 
the $p$-values corresponding to the true split variable $Z_1$ and the proportion
of data sets for which $Z_1$ is detected and also selected 
%(for a given significance level $\alpha = 0.05$) 
is considered for the first scenario (stump).
For the second scenario (tree with 2 splits) the Adjusted Rand Index~(ARI) is calculated 
as a measure of similarity between the true tree structure and the fitted model tree.

As explained before, the aim of this simulation study is to investigate the effects
of each particular building block of the unifying framework presented in Section~\ref{sec:unify} 
rather than the whole testing strategies. Therefore, other combinations as presented in 
Table~\ref{tab:buildingblocks}, hence adapted versions of GUIDE, CTREE, and MOB are evaluated as well
as their original versions. Moreover, as prepruning is employed all of the considered statistical 
tests are applied as significance tests answering two questions at once: 
(1)~Should a split be performed at all? 
(2)~If so, in which variable?
For the first question the $p$-values regarding all available covariates are compared 
to a predefined level of significance $\alpha = 0.05$. Only if the smallest $p$-value is smaller
than $\alpha$ a split is performed and the covariate corresponding to this $p$-value is selected. 




%\newpage
\section{Results}
\label{sec:results}

To illustrate the effects of the building blocks in the unifying framework presented in 
Section~\ref{sec:unify} different combinations of them are investigated for the first
simulation scenario (stump) in Sections ~\ref{sec:scores} and \ref{sec:3way}. 
Then the testing strategies are evaluated on the more
complex tree structure of the second scenario in Section~\ref{sec:tree}. 
The corresponding results are shown in the following plots.

\subsection{Residuals vs. Full Model Scores}
\label{sec:scores}
To assess the impact of using only residuals or the full score matrix four testing
strategies are compared: CTree, GUIDE, MOB (all three in their original form),
and additionally the GUIDE strategy using the full score matrix (denoted by GUIDE\_scores).
%Thus, the four strategies are obtained from the unifying framework by setting the switches 
%as listed in Table~\ref{tab:buildingblocks_scores}.

%\begin{table}%[t]
%\centering
%\caption[Table caption text]{CTree, MOB, GUIDE, and GUIDE\_scores with the corresponding setting of
%the switches in the unifying framework and the approximation type of the underlying null 
%distribution.}
%\label{tab:buildingblocks_scores}
%\begin{tabular}{ l c c c c }
%\hline 
%            & Scores            & Binarization  & Categorization  & Null distribution \\ \hline
%CTree       & full model scores & --          & --              & conditional\\
%MOB         & full model scores & --          & --              & unconditional\\
%GUIDE       & residuals         & \checkmark  & \checkmark      & unconditional\\
%GUIDE\_scores & full model scores & \checkmark  & \checkmark      & unconditional\\ \hline
%\end{tabular}
%\end{table}


\begin{figure}
\setkeys{Gin}{width=\linewidth}
<<scores_stump, fig=TRUE, echo=FALSE, height=5, width=8>>=
subdata <- subset(sim_stump, test %in% c("ctree","mfluc",
                                         "guide_sum_12", "guide_sum_1_cor"))
subdata$test <- factor(subdata$test, 
                       levels = c("ctree","mfluc",
                                  "guide_sum_12", "guide_sum_1_cor"),
                       labels = c("CTree","MOB",
                                  "GUIDE_scores", "GUIDE"))

subdataxi <- subset(subdata, xi %in% c(0,0.8))

par.settings <- list(superpose.symbol = list(col = pal[c("CTree", "MOB", "GUIDE_scores", "GUIDE")], 
                                             fill = pal[c("CTree", "MOB", "GUIDE_scores", "GUIDE")]), 
                     superpose.line = list(col = pal[c("CTree", "MOB", "GUIDE_scores", "GUIDE")]))

library("latticeExtra")
useOuterStrips(xyplot(prop_Tsplit ~ delta | vary_beta + xi, groups = ~ test, 
                      data = subdataxi, 
                      subset = binary_beta == TRUE & binary_regressor == FALSE & 
                        only_intercept == FALSE,# & xi == 0, 
                      type = "l", lty = c(1,1,1,1),
                      lwd = 2,
                      scales=list(x=list(at=seq(1,11,2))),
                      key =  list(text = list(levels(subdataxi$test)),
                                  lines = list(lty = c(1,1,1,1), type = "l", pch = 1, cex = 0.8,
                                               col = pal[c("CTree", "MOB", "GUIDE_scores", "GUIDE")], lwd = 2)),
                      layout= c(3,2),
                      index.cond = list(c(2,3,1),c(2,1)),
                      xlab = expression(delta),
                      ylab = expression("Proportion of selecting"~Z[1]),
                      par.settings = par.settings),
               
               strip =strip.custom(factor.levels = c(expression("vary"~beta[0]~"and"~beta[1]), 
                                                     expression("vary"~beta[0]), 
                                                     expression("vary"~beta[1])),
                                   bg = "gray90"),
               strip.left=strip.custom(factor.levels = c(expression(xi == 0),                                                              expression(xi == 0.8)),
                                   bg = "gray90"))


@
\caption{\label{fig:scores_stump}Proportion of cases where the true split variable $Z_1$ is
selected over increasing effect size $\delta$ (100 data sets of 250 observations per step),
once for a true split point $\xi = 0$ (top row) and once for $\xi = 0.8$ (bottom row). 
Each column represents one scenario for the coefficients: 
varying intercept $\beta_0$ (left), varying slope parameter $\beta_1$ (middle) and 
both coefficients varying (right). The performances of four testing strategies are compared: 
CTree, MOB, GUIDE, GUIDE\_scores.}
\end{figure}


Figure~\ref{fig:scores_stump} shows the evaluation results for the three different
scenarios (varying only the intercept $\beta_0$, only the slope parameter $\beta_1$
or both of them) once for a true split point $\xi=0$ and once for $\xi=0.8$. 
As explained before, the tests are applied as significance tests with significance 
level $\alpha=0.05$. Therefore, the proportion of cases in which the true split variable $Z_1$
is detected with a $p$-value smaller than $\alpha$ is compared.
(To show that in general the drawn conclusions do not change if no significance level is included 
the same setting is evaluated again without any significance level and the results are presented
in Appendix~\ref{app:significance}.)

For a central split point ($\xi=0$, upper panels) all strategies perform similarly if the 
intercept varies (left and right panels) where CTree and MOB are only slightly ahead. However, 
this advantage increases if the slope parameter varies and the intercept is kept at a fixed value 
(middle panel). In particular, the original strategy of GUIDE fails to detect any changes 
in $\beta_1$. 
This is due to the fact that GUIDE only includes residuals, i.e. only the scores 
regarding the intercept, such that changes in the slope parameter are not detected unless the 
corresponding scores are also included as in GUIDE\_scores. This disadvantage of using 
only scores has already been discussed and elaborated in the literature of structural change
analysis \citep[see e.g.][]{Ploberger+Kramer:1992}.

Looking at the results for the split point being close to the upper edge of the considered
interval ($\xi=0.8$, lower panels) all of the applied strategies perform worse than for the
central split point. In this case MOB is clearly ahead leading to the best results. CTree 
performs second best while both GUIDE versions struggle to detect the correct split variable 
even for a high effect size. 
(In order to show that the here observed advantage of MOB over CTree is mainly due to the 
abrupt shift in the model parameters $\beta_0$ and $\beta_1$ an additional evaluation is 
presented in Appendix~\ref{app:contbeta} where these parameters change continuously.)

Altogether these results clearly illustrate that overall the decision to include all available
scores when testing for dependencies in order to select split variables is advantageous over
considering only residuals, i.e. the scores regarding only the intercept, as changes not only in
the mean but also in all other parameters can be detected in that way. This is particularly obvious
when comparing GUIDE and GUIDE\_scores. Moreover, from the fact that these two version are both
performing worse than MOB and CTree it can be deduced that categorizing split variables and
binarizing scores has a negative effect on the ability to detect the right split variable. 
To elaborate these effects more in detail the results for all possible combinations of the 
building blocks are discussed in a full factorial 3-way analysis.

\subsection{Full Factorial Analysis}\label{sec:3way}
In Figure~\ref{fig:3way} the effects of the building blocks are illustrated for the most general
case where the intercept and slope parameter are both varying and the true split point is in the
center, i.e. $\xi=0$. Based on the results shown in Figure~\ref{fig:scores_stump} the effect size 
$\delta = 0.3$ is considered making the differences particularly well observable 
(additional easing $\delta$ as in Section~\ref{sec:scores} 
can be found in Appendix~\ref{app:increasingdelta}).
The power of the different combinations is measured by the mean $p$-value for the true split 
variable~$Z_1$.
The possible levels for each of the three building blocks are listed in Table~\ref{tab:levels}.
In total 12 combinations are applied including the original testing strategies CTree, MOB, and GUIDE
and variations of them. A more detailed overview of all 12 combinations can be found in 
Appendix~\ref{app:combinations}, Table~\ref{tab:combinations_appendix}.

\begin{table}[h!]
\centering
\caption[Table caption text]{Available levels of the building blocks.}
\label{tab:levels}
\begin{tabular}{ l l l }
\hline
Building Block         & Levels              & Effect \\
\hline
Residuals vs.\ Scores  & \textit{residuals}  & use only residuals\\
                       & \textit{scores}     & use full score matrix\\
Categorization         & \textit{cat}        & categorize split variables\\
                       & \textit{max}        & selected split variables maximally w.o. categorization\\
                       & \textit{lin}        & selected split variables linearly w.o. categorization\\
Binarization           & \textit{bin}        & binarize scores\\
                       & \textit{lin}        & use residuals/score values without binarization\\
\hline
\end{tabular}
\end{table}


<<3way_data, results=hide, echo=FALSE>>=
if(file.exists("3way_xi00_delta03.rda")){
  load("3way_xi00_delta03.rda")
} else {
  simres <- sim(nobs = 250, nrep = 100, seed = 7, stump = TRUE, nrsteps = 1,
                delta = 0.3,
                xi = 0, 
                binary_regressor = FALSE,
                binary_beta = TRUE,
                only_intercept = FALSE,
                nrlevels = 2, nrsplitvar = 1,
                vary_beta = "all", beta0 = 0, beta1 = 1, alpha = 0.05,
                return_matrices = TRUE,
                test = c("guide_sum_1_cor",
                         "mfluc_resid_bin",
                         "ctree_resid_bin",
                         "mfluc_resid_cat",
                         "mfluc_resid",
                         "ctree_resid",
                         "guide_sum_12",
                         "mfluc_bin",
                         "ctree_bin",
                         "mfluc_cat",
                         "mfluc",
                         "ctree"))
  d00 <- prep_3way(simres)
  save(d00, file = "3way_xi00_delta03.rda")
}


if(file.exists("3way_xi08_delta1.rda")){
  load("3way_xi08_delta1.rda")
} else {
  simres <- sim(nobs = 250, nrep = 100, seed = 7, stump = TRUE, nrsteps = 1,
                delta = 1,
                xi = 0.8, 
                binary_regressor = FALSE,
                binary_beta = TRUE,
                only_intercept = FALSE,
                nrlevels = 2, nrsplitvar = 1,
                vary_beta = "all", beta0 = 0, beta1 = 1, alpha = 0.05,
                return_matrices = TRUE,
                test = c("guide_sum_1_cor",
                         "mfluc_resid_bin",
                         "ctree_resid_bin",
                         "mfluc_resid_cat",
                         "mfluc_resid",
                         "ctree_resid",
                         "guide_sum_12",
                         "mfluc_bin",
                         "ctree_bin",
                         "mfluc_cat",
                         "mfluc",
                         "ctree"))
  d08 <- prep_3way(simres)
  save(d08, file = "3way_xi08_delta1.rda")
}

d00$xi <- 0
d08$xi <- 0.8

d3way <- rbind(d00, d08)
d3way$xi <- factor(d3way$xi, levels = c(0,0.8), labels = c(0,0.8))
@


\begin{figure}%[t]
\setkeys{Gin}{width=1\linewidth}
<<3way, fig=TRUE, echo=FALSE, height=4, width=8>>=
xyplot(pval_z1 ~ cat | xi, 
       groups = interaction(bin, res_scores), 
       data = d3way, 
       ylim = c(0,0.65),
       type = c("a"), 
       col = c(pal[1], pal[3], pal[1], pal[3]),
       lty = c(1,1,2,2),
       lwd = 2,
       xlab = "Categorization",
       ylab = expression("p-value of"~Z[1]),
       #index.cond = list(c(1,2,3,4)),
       strip = strip.custom(factor.levels = c(expression(xi==0~"and"~delta==0.3), 
                                              expression(xi==0.8~"and"~delta==1)),
                                   bg = "gray90"),
       key = list(text = list(levels(interaction(d3way$bin, d3way$res_scores))),
                  lines = list(lty = c(1,1,2,2), 
                               type = "l", lwd = 2,
                               col = c(pal[1], pal[3], pal[1], pal[3]))))
       

#par(mar = c(3,5,1,5.7))
#with(d00, interaction.plot(res_scores, interaction(cat, bin), pval_z1, ylim = c(0,1),
#                           ylab = expression("Mean"~p~"-value for"~Z[1])))
#with(d00, interaction.plot(interaction(cat, bin), res_scores, pval_z1))
#with(d00, interaction.plot(cat, interaction(bin, res_scores), pval_z1))
#with(d08, interaction.plot(res_scores, interaction(cat, bin), pval_z1, ylim = c(0,1)))
#with(d08, interaction.plot(interaction(cat, bin), res_scores, pval_z1))
#with(d08, interaction.plot(cat, interaction(bin, res_scores), pval_z1))
@
\caption{\label{fig:3way} Mean $p$-values corresponding to the true split variable $Z_1$
over different levels of the building block \textit{Categorization}. 
Each line represents
a combination of the levels of the other two building blocks \textit{Binarization} and 
\textit{Residuals vs.\ Scores}.
Left panel: true split point $\xi=0$ and effect size $\delta = 0.3$.
Right panel: true split point $\xi=0.8$ and effect size $\delta = 1$.}
\end{figure}

The results presented in Figure~\ref{fig:3way} clearly show that binarizing scores/residuals
reduces the power of the testing strategy as it leads to higher $p$-values corresponding
to the true split variable $Z_1$. 
This effect can be observed for a central split point (left panel, $\xi=0$) as well as for a
a split point closer to upper end of the considered interval (right panel, $\xi=0.8$).
However, the effect of the building block \textit{Categorization} varies over these two settings.
For a central split point all three levels perform similarly. The linear selection of split variables
is slightly ahead even though the true parameter function is a step function. 
On the contrary, clear differences can be seen for the outward split point. Here, the maximal selection
performs best while the linear selection shows its disadvantage in detecting abrupt changes
and the categorized split variables lead to even higher $p$-values and thus lower power.
Overall, it depends on the situation whether a linear or a maximal selection of a split variable
is advantageous. However, categorizing possible split variables does not lead to any improvements
but even declines the performance of a testing strategy unless the true split point is close
to a break point used for the binarization.



%\newpage
\subsection{Tree}
\label{sec:tree}
To compare the performance of testing strategies in a more complex scenario they have
been evaluated on data sets based on the tree structure with two different split variables 
($Z_1$ and $Z_2$) both with the same split point $\xi$ as illustrated in 
Figure~\ref{fig:dgp_tree}.

Figure~\ref{fig:tree_ari} shows that based on the resulting ARI values CTree and MOB perform 
almost equally well for the true split points $0$, $0.2$ and $0.5$. GUIDE\_scores performs
slightly worse but it still achieves an ARI above $0.5$ for the highest considered effect size
of $\delta = 1$. However, the original version of GUIDE is clearly behind which is mainly
due to the fact that it doesn't detect the first split in $Z_2$ corresponding to a changing
slope parameter as it only considers residuals in its testing strategy.
For a split point farther away from the center such as for $\xi = 0.8$ MOB outperforms all other
methods and both GUIDE versions fail to find the correct tree structure.
Based on these results it can be concluded once again that including all available scores in the
testing strategy leads to a better performance while binarizing scores and categorizing
split variables both do not cause any improvement but in many situations even have a negative
effect on the power of a testing strategy. 

\begin{figure}%[t]
\setkeys{Gin}{width=\linewidth}
<<tree_ari, fig=TRUE, echo=FALSE, height=4, width=8>>=
subdata <- subset(sim_tree, test %in% c("ctree","mfluc",
                                        "guide_sum_12", "guide_sum_1_cor"))
subdata$test <- factor(subdata$test, 
                       levels = c("ctree", "mfluc",
                                  "guide_sum_12", "guide_sum_1_cor"),
                       labels = c("CTree","MOB",
                                  "GUIDE_scores", "GUIDE"))

par.settings <- list(superpose.symbol = list(col = pal[c("CTree", "MOB", "GUIDE_scores", "GUIDE")], 
                                             fill = pal[c("CTree", "MOB", "GUIDE_scores", "GUIDE")]), 
                     superpose.line = list(col = pal[c("CTree", "MOB", "GUIDE_scores", "GUIDE")]))


xyplot(ari ~ delta | xi, groups = ~ test, 
       data = subdata, 
       type = "l",
       lty = c(1,1,1,1),
       lwd = 2,
       key =  list(text = list(levels(subdata$test)),
                   lines = list(lty = c(1,1,1,1), type = "l", pch = 1, cex = 0.8,
                                col = pal[c("CTree", "MOB", "GUIDE_scores", "GUIDE")], lwd = 2)),
       layout= c(4,1),
       #index.cond = list(c(1,2,3,4)),
       scales=list(x=list(at=seq(1,11,2))),
       xlab = expression(delta),
       ylab = "Adjusted Rand Index",
       strip = strip.custom(factor.levels = c(expression(xi==0), 
                                              expression(xi==0.2), 
                                              expression(xi==0.5), 
                                              expression(xi==0.8)),
                                   bg = "gray90"),
       par.settings = par.settings)

@
\caption{\label{fig:tree_ari}Adjusted Rand Index over increasing effect size $\delta$ 
(100 data sets of 250 observations per step).
Each panel represents a different value of the true split point $\xi$ ($0$, $0.2$, $0.5$ and 
$0.8$ from left to right). The performances of four testing strategies are compared: CTree,
MOB, GUIDE, GUIDE\_scores.}
\end{figure}



\subsection{Prepruning vs. Postpruning}

To elaborate the impact of the choice of a pruning strategy
the tree models presented in Figure~\ref{fig:tree_ari} are built again, but this time
applying postpruning (building very large trees without any significance level and employing 
cost-complexity pruning as in the CART algorithm \citep{Breiman+Friedman+Stone:1984}). 
In Figure~\ref{fig:pruning_ari} the results are compared to those obtained with prepruning,
again based on the Adjusted Rand Index.

For MOB and CTree it has been shown that the tests work very well as significance tests.
Therefore, it can be recommended to take advantage of this power and use the information
given by the tests for prepruning. As it can be seen in Figure~\ref{fig:pruning_ari}
in most cases the results of these two testing strategies do not vary significantly over 
changing the applied pruning strategy.
On the contrary, overall the tests in GUIDE have less power. For that reason, the pruning
strategy should not be based on their results which argues for the application of postpruning.
This can also be observed in Figure~\ref{fig:pruning_ari} showing that postpruning improves
the results for both GUIDE versions.

<<pruning_data, echo=FALSE, results=hide>>=
if(file.exists("pruning.rda")){
  load("pruning.rda")
} else {
  simres <- simwrapper_p(nobs = 250, nrep = 100, seed = 7, 
                         nrsteps = 1, nrlevels = 2,
                         delta = seq(from = 0, to = 1, by = 0.1),
                         xi = c(0, 0.2, 0.5, 0.8), 
                         vary_beta = "all", binary_regressor = FALSE,
                         binary_beta = TRUE, only_intercept = FALSE,
                         compare_pruning = TRUE,
                         test = c("ctree", "mfluc",
                                  "guide_sum_12", 
                                  "guide_sum_1_cor"),
                         beta0 = NULL, beta1 = NULL,
                         stump = FALSE, z1dist = "unif", sigma = 1, alpha = 0.05,
                         nrsplitvar = 2)
  
  save(simres, file = "pruning.rda")  
}

pr <- simres$res
## extract data for xyplot comparing ARI and ARI_p
pr <- pr[,c(1,2,7,8,9,12,15)]
## extended data set with postpruning results being represented as separated tests
prex <- pr[,c(1,2,3,4,6)]
prex <- rbind(prex,prex)

prex$test <- factor(prex$test,
                    levels = c("ctree","mfluc",
                               "guide_sum_12", "guide_sum_1_cor",
                               "ctree_p","mfluc_p",
                               "guide_sum_12_p", "guide_sum_1_cor_p"),
                    labels = c("CTree","MOB",
                               "GUIDE_scores", "GUIDE",
                               "CTree_p","MOB_p",
                               "GUIDE_scores_p", "GUIDE_p"))

for(i in c((length(pr$test)+1) : (2*length(pr$test)))){
  if(prex$test[i] == "CTree") prex$test[i] <- "CTree_p"
  if(prex$test[i] == "MOB") prex$test[i] <- "MOB_p"
  if(prex$test[i] == "GUIDE_scores") prex$test[i] <- "GUIDE_scores_p"
  if(prex$test[i] == "GUIDE") prex$test[i] <- "GUIDE_p"
}

prex$nrsubgr[(length(pr$nrsubgr)+1) : (2*length(pr$nrsubgr))] <- pr$nrsubgr_p
prex$ari[(length(pr$ari)+1) : (2*length(pr$ari))] <- pr$ari_p

par.settings <- list(superpose.symbol = list(col = pal[c("CTree", "MOB", "GUIDE_scores", "GUIDE")], 
                                             fill = pal[c("CTree", "MOB", "GUIDE_scores", "GUIDE")]), 
                     superpose.line = list(col = pal[c("CTree", "MOB", "GUIDE_scores", "GUIDE")]))

@

\begin{figure}%[t]
\setkeys{Gin}{width=\linewidth}
<<prune_ari, fig=TRUE, echo=FALSE, height=4, width=8>>=
xyplot(ari ~ delta | xi, groups = ~ test, 
       data = prex, 
       type = "l", lty = c(2,2,2,2,1,1,1,1),
       lwd = 2,
       key =  list(text = list(levels(prex$test)[1:4]),
                   lines = list(lty = c(1,1,1,1), type = "l", pch = 1, cex = 0.8,
                                col = pal[c("CTree", "MOB", "GUIDE_scores", "GUIDE")], lwd = 2)),
       layout= c(4,1),
       scales=list(x=list(at=seq(1,11,2))),
       xlab = expression(delta),
       ylab = "Adjusted Rand Index",
       strip = strip.custom(factor.levels = c(expression(xi==0), 
                                              expression(xi==0.2), 
                                              expression(xi==0.5), 
                                              expression(xi==0.8)),
                                   bg = "gray90"),
       par.settings = par.settings
)
@
\caption{\label{fig:pruning_ari}Adjusted Rand Index over increasing effect size $\delta$ 
(100 data sets of 250 observations per step).
Each panel represents a different value of the true split point $\xi$ ($0$, $0.2$, $0.5$ and 
$0.8$ from left to right). 
The performances of four testing strategies are compared: CTree, MOB, GUIDE, GUIDE\_scores. 
The dashed lines represent the results from applying prepruning and the continuous lines 
the results from applying postpruning.}
\end{figure}




%\newpage

\section{Discussion}
\label{sec:discussion}

From comparing the effects of the building blocks three main conclusions can be drawn: 
(1)~In many cases including all available score values rather than just residuals leads to 
clearly better results and performs in general at least equally well.
(2)~Binarizing scores/residuals does not lead to any improvement in performance.
(3)~Categorizing possible split variables also doesn't improve the performance 
unless the breakpoints used for the categorization are very close to the true split points.
However, even then the results are very similar to those achieved without any categorization.
But it has to be noted that a categorization of possible split variables can simplify
evaluations. Moreover, depending on the focus of interest, the results can be more
straightforward, for example in case of clearly separated subgroups being defined by the values 
of a continuous split variable. Therefore, categorizing possible split variable can have
advantages in practice, however, it has to be considered that in general
important effects might be neglected.

Linear models as presented in this study have been selected as they allow for a simple illustration
of the investigated subjects. However, the obtained results also hold for a variety of other models
as the introduced building blocks can be applied in the same way for all model types.
Specific characteristics of linear models regarding structural changes have been investigated
in detail in previous literature.
\cite{Ploberger+Kramer:1992} have shown that residual-based tests can detect a change in the parameters
only if it also causes a shift in the expected value $\mathbb{E}(Y)$. 
This is not the case if changes are orthogonal to the mean regressor $(1,0)^{\top}$, hence, if
the parameter vector $\bm{\beta}$ is shifted 
%to $\bm{\beta} + \bm{\Delta}$ 
by $\bm{\Delta} = (0, \Delta_1)^{\top}$.
For that reason GUIDE breaks down when the slope parameter $\beta_1$ changes while the intercept
$\beta_0$ is fixed as seen in the middle panel of Figure~\ref{fig:scores_stump}.
While this might be an extreme example similar scenarios can appear in many applications, see, e.g., 
Figure~2 in \cite{Loh+He+Man:2015} where an additional separation of the data in each node is
required in order to fit a reasonable model. This can be avoided by including model scores
allowing for the detection of changes not only in the location parameter but also in all other 
parameters, i.e., not only in the expected value but also in the variance.
In situations where shifts appear only in the intercept testing procedures purely based on residuals 
perform best. However, the cost in terms of power that comes with including all scores, in particular
with testing more than one parameter, is negligibly small. 

Also in more general models residual-based and score-based procedures are expected to perform equally 
well if all parameters are highly correlated. On the other hand, if parameters are orthogonal changes 
that are not connected to the mean might be missed when only considering residuals. This can be prevented 
by including scores which are a standard measurement of goodness of fit that can be easily employed for 
all model types.
Even though it also depends on the type of the applied residuals 
(see, e.g., \cite{Loh+Zheng:2013}, \cite{Chaudhuri+Loh:2002} 
and \cite{Loh+He+Man:2015})
whether parameter changes can be detected successfully, a one-dimensional measurement of goodness of 
fit such as residuals provides less power than full model scores. 
Whether this difference affects the performance of a testing strategy and if so, to what extent, is 
determined by the way how residuals are connected to parameter shifts as well as the type of parameter
shift and the type of model. Therefore, a general statement for all types of models and residuals can 
hardly be made. However, the results from previous literature such as \cite{Ploberger+Kramer:1992} and
the here presented simulation show that considering only residuals provides small gains in certain 
situations but remarkable deficits in general. On the contrary, the benefits of including full model 
scores clearly outweigh the price of of testing all parameters.


%Furthermore, with only two parameters determining a linear model the importance of including scores 
%regarding each parameter could already be pointed out clearly while for an increasing number of 
%parameters in a model this aspect becomes even more important. 




%\newpage

\section*{Computational Details}
\label{sec:comp}
The applied implementation is based on the \textsf{R} package \textbf{partykit}
(version~\Sexpr{packageVersion("partykit")}) which is available 
on \textsf{R}-Forge at (\url{https://R-Forge.R-project.org/projects/partykit/})
where the code to reproduce the simulation study can be found as well.

The functions \code{ctree} and \code{mob} provide an implementation of the
two tree algorithms in their original form. For their adapted versions additional
modifications have been applied within these functions allowing for a categorization
of possible split variables and a binarization of scores.
To evaluate the GUIDE algorithm a reimplementation of this algorithm has been 
established/constructed using the basic framework of \code{ctree} and \code{mob}.



\section*{Acknowledgements}
\hlc{An extended research stay of Torsten Hothorn in Innsbruck (August
2017 to January 2018) was financially supported by the Swiss National
Science Foundation, grant number SNF IZSEZ0 177091.}



\newpage
\section*{Appendix}
\begin{appendix}

\section{Applied Combinations of Building Blocks}\label{app:combinations}
For the evaluation of all 12 possible combinations of the presented building blocks
the original testing strategies CTree, MOB, and GUIDE have been applied together
with modified versions of them. The employed versions and the corresponding combination
of building blocks is listed in Table~\ref{tab:combinations_appendix}.
Each suffix indicates a change of the original form of the considered testing strategy.
In particular, it relates to the transformations of the data before applying a statistical
test as represented by the different building blocks. In that way,
``\_res'' refers to using only residuals (instead of all scores),
``\_scores'' to using all scores (instead of only residuals),
``\_cat'' to applying a categorization of split variables,
``\_bin'' to applying a binarization of scores/residuals.

\begin{table}%[t]
\centering
\caption[Table caption text]{Applied testing strategies representing the 12 combinations
of building blocks.}
\label{tab:combinations_appendix}
\begin{tabular}{ l c c c }
\hline 
Testing Strategy  & Residuals/Scores   & Binarization & Categorization \\
\hline
GUIDE             & residuals          & bin          & cat \\
MOB\_bin\_res     & residuals          & bin          & max \\
CTree\_bin\_res   & residuals          & bin          & lin \\
MOB\_cat\_res    & residuals          & lin          & cat \\
MOB\_res          & residuals          & lin          & max \\
CTree\_res        & residuals          & lin          & lin \\
GUIDE\_scores     & scores             & bin          & cat \\
MOB\_bin          & scores             & bin          & max \\
CTree\_bin        & scores             & bin          & lin \\
MOB\_cat          & scores             & lin          & cat \\
MOB               & scores             & lin          & max \\
CTree             & scores             & lin          & lin \\
\hline
\end{tabular}
\end{table}


\section{Significance Level}\label{app:significance}
In Section~\ref{sec:scores} the investigated testing strategies are compared based
on the number of cases in which the true split variable $Z_1$ is detected with a $p$-value
smaller than the predefined significance level $\alpha = 0.05$. This choice of measurement
is due to the aim of investigating how well the tests perform as significance test.
However, ignoring this significance level yields the same conclusions 
which can be seen when comparing Figures~\ref{fig:scores_stump_wosign}
and \ref{fig:scores_stump}.
In Figure~\ref{fig:scores_stump_wosign} the proportion of cases 
in which the true splitting variable $Z_1$ is detected, but not necessarily with a $p$-value 
smaller than the significance level, is illustrated. In Figure~\ref{fig:scores_stump} 
the exact same simulation setting is applied with a significance level.


\begin{figure}[t]
\setkeys{Gin}{width=\linewidth}
<<scores_stump_wosign, fig=TRUE, echo=FALSE, height=5, width=8>>=
subdata <- subset(sim_stump, test %in% c("ctree","mfluc",
                                         "guide_sum_12", "guide_sum_1_cor"))
subdata$test <- factor(subdata$test, 
                       levels = c("ctree","mfluc",
                                  "guide_sum_12", "guide_sum_1_cor"),
                       labels = c("CTree","MOB",
                                  "GUIDE_scores", "GUIDE"))

subdataxi <- subset(subdata, xi %in% c(0,0.8))

par.settings <- list(superpose.symbol = list(col = pal[c("CTree", "MOB", "GUIDE_scores", "GUIDE")], 
                                             fill = pal[c("CTree", "MOB", "GUIDE_scores", "GUIDE")]), 
                     superpose.line = list(col = pal[c("CTree", "MOB", "GUIDE_scores", "GUIDE")]))

library("latticeExtra")
useOuterStrips(xyplot(prop_z1 ~ delta | vary_beta + xi, groups = ~ test, 
                      data = subdataxi, 
                      subset = binary_beta == TRUE & binary_regressor == FALSE & 
                        only_intercept == FALSE,# & xi == 0, 
                      type = "l", lty = c(1,1,1,1),
                      lwd = 2,
                      scales=list(x=list(at=seq(1,11,2))),
                      key =  list(text = list(levels(subdataxi$test)),
                                  lines = list(lty = c(1,1,1,1), type = "l", pch = 1, cex = 0.8,
                                               col = pal[c("CTree", "MOB", "GUIDE_scores", "GUIDE")], lwd = 2)),
                      layout= c(3,2),
                      index.cond = list(c(2,3,1),c(2,1)),
                      xlab = expression(delta),
                      ylab = expression("Prop. of"~Z[1]~"having lowest p-value"),
                      par.settings = par.settings),
               
               strip =strip.custom(factor.levels = c(expression("vary"~beta[0]~"and"~beta[1]), 
                                                     expression("vary"~beta[0]), 
                                                     expression("vary"~beta[1])),
                                   bg = "gray90"),
               strip.left=strip.custom(factor.levels = c(expression(xi == 0),                                                              expression(xi == 0.8)),
                                   bg = "gray90"))

@
\caption{\label{fig:scores_stump_wosign}Proportion of cases where the true split variable $Z_1$ 
shows the smallest $p$-value, hence, where $Z_1$ is selected without any significance level, 
over increasing effect size $\delta$ (100 data sets of 250 observations per step), once for a 
true split point $\xi = 0$ (top row) and once for $\xi = 0.8$ (bottom row).
Each column represents one scenario for the coefficients: 
varying intercept $\beta_0$ (left), varying slope parameter $\beta_1$ (middle) and 
both coefficients varying (right). The performances of four testing strategies are compared: 
CTree, MOB, GUIDE, GUIDE\_scores.}
\end{figure}




\section{Simulation Results for Continuous Model Parameters}
\label{app:contbeta}
In the simulation study presented in Sections~\ref{sec:simulation} and \ref{sec:results} the intercept
and slope parameters $\beta_0$ and $\beta_1$ are both either fixed or binary variables taking
either the positive or the negative value of the effect size $\delta$. Therefore, these parameters
are step functions for which MOB has shown to perform better than CTree. However, in case of
monotonous functions CTree is advantageous as it is constructed to detect monotonous effects.
To point out that the difference in performance of CTree and MOB depends on the type of effect
the same setting of which the results are presented in Figure~\ref{fig:scores_stump} is
evaluated again but this time with the varying parameter(s) changing continuously.
In particular, the parameters $\beta_0$ and $\beta_1$ are linear functions of the true
split variable $Z_1$:
\begin{align*}
\beta_{i}(Z_1) = \begin{cases}
-\delta \cdot (-1)^{i} \cdot Z_1\\
+\delta \cdot (-1)^{i} \cdot Z_1.
\end{cases}
\end{align*}
Moreover, a modified version of CTree considering maximum values in the test statistic is evaluated
(denoted by CTree\_max). 
Looking at the results presented in Figure~\ref{fig:stump_contbeta} it can be observed that in this setting the 
original version of CTree is ahead while CTree\_max and MOB perform almost equally well. Hence, the comparison of 
Figures \ref{fig:scores_stump} and \ref{fig:stump_contbeta} points out that it clearly depends on the type of 
effect whether the maximal selection (MOB, CTree\_max) or the linear selection (CTree) is advantageous.


\begin{figure}%%[t]
\setkeys{Gin}{width=\linewidth}
<<contbeta, fig=TRUE, echo=FALSE, height=4, width=8>>=
subdata <- subset(sim_stump, test %in% c("ctree", "ctree_max","mfluc", "guide_sum_1_cor","guide_sum_12"))
#subdata <- subset(subdata, delta %in% c(0, 0.1, 0.2, 0.3, 0.4, 0.5))
subdata$test <- factor(subdata$test, 
                       levels = c("ctree", "ctree_max","mfluc", "guide_sum_1_cor","guide_sum_12"),
                       labels = c("CTree","CTree_max","MOB", "GUIDE","GUIDE_scores"))
par.settings <- list(superpose.symbol = list(col = pal[c("CTree","CTree_max","MOB", "GUIDE","GUIDE_scores")], 
                                             fill = pal[c("CTree","CTree_max","MOB", "GUIDE","GUIDE_scores")]), 
                     superpose.line = list(col = pal[c("CTree","CTree_max","MOB", "GUIDE","GUIDE_scores")]))

xyplot(prop_Tsplit ~ delta | vary_beta, groups = ~ test, 
       data = subdata, 
       subset = binary_beta == FALSE & binary_regressor == FALSE & 
         only_intercept == FALSE, 
       scales=list(x=list(at=seq(1,11,2))),
       type = "l", 
       layout= c(3,1),
       lwd = 2,
       key =  list(text = list(levels(subdata$test)),
                   lines = list(lty = c(1,1,1,1,1), 
                                type = "l", 
                                col = pal[c("CTree","CTree_max","MOB", "GUIDE","GUIDE_scores")], 
                                lwd = 2)), 
       xlab = expression(delta),
       ylab = expression("Proportion of selecting"~Z[1]),
       index.cond = list(c(2,3,1)),
       strip = strip.custom(factor.levels = c(expression("vary "~beta[0]~" and "~beta[1]),
                                              expression("vary "~beta[0]), 
                                              expression("vary "~beta[1])),
                                    bg = "gray90"),
       par.settings = par.settings)
@
\caption{\label{fig:stump_contbeta}
Proportion of cases where the true split variable $Z_1$ is
selected over increasing effect size $\delta$ (100 data sets of 250 observations per step)
with the varying coefficient(s) being continuous. 
Each panel represents one scenario for the coefficients: 
varying intercept $\beta_0$ (left), varying slope parameter $\beta_1$ (middle) and 
both coefficients varying (right). The performances of five testing strategies are compared: 
CTree, CTree\_max, MOB, GUIDE, GUIDE\_scores.}
\end{figure}



\section{Results Over Increasing Effect Size}
\label{app:increasingdelta}

\subsection{Binarization of Residuals/Scores}
\label{app:binarization}
To elaborate over an increasing effect size whether a binarization of the residuals/scores at $0$ 
leads to an improvement or a deterioration of performance CTree and MOB are applied, once in their 
original version without a binarization and once in an adapted version with a binarization of 
scores at $0$ (CTree\_bin, MOB\_bin). Moreover, they are compared to the adapted GUIDE version 
which includes all available scores (GUIDE\_scores).
%Hence, the five strategies are obtained from the unifying framework by setting the switches 
%as listed in Table~\ref{tab:buildingblocks_appendix}.

%\begin{table}%[t]
%\centering
%\caption[Table caption text]{CTree, CTree\_bin, MOB, MOB\_bin, and GUIDE\_scores with the 
%corresponding setting of the switches in the unifying framework and the approximation type 
%of the underlying null distribution.}
%\label{tab:buildingblocks_appendix}
%\begin{tabular}{ l c c c c }
%\hline 
%& Scores            & Binarization  & Categorization  & Null distribution \\ 
%\hline
%CTree       & full model scores & --          & --            & conditional\\
%CTree\_bin  & full model scores & \checkmark  & --            & conditional\\
%MOB         & full model scores & --          & --            & unconditional\\
%MOB\_bin    & full model scores & \checkmark  & --            & unconditional\\
%GUIDE\_scores & full model scores & \checkmark  & \checkmark    & unconditional\\
%\hline
%\end{tabular}
%\end{table}

Figure~\ref{fig:bin_stump} shows the effect of binarizing the score values over
different values for the true split point $\xi$, however, all four situations
lead to the same conclusions. Binarizing the score values decreases the proportion
of cases where the split variable was selected correctly.


\begin{figure}%[t]
\setkeys{Gin}{width=\linewidth}
<<bin_stump, fig=TRUE, echo=FALSE, height=4, width=8>>=
subdata <- subset(sim_stump, test %in% c("ctree", "ctree_bin", 
                                         #"ctree_max", "ctree_max_bin",
                                         "mfluc", "mfluc_bin",
                                         "guide_sum_12"))

subdata <- subset(subdata, delta %in% c(0, 0.1, 0.2, 0.3, 0.4, 0.5))
subdata$test <- factor(subdata$test, 
                       levels = c("ctree", "ctree_bin", 
                                  #"ctree_max", "ctree_max_bin",
                                  "mfluc", "mfluc_bin",
                                  "guide_sum_12"),
                       labels = c("CTree","CTree_bin",
                                  #"CTree_max","CTree_max_bin",
                                  "MOB", "MOB_bin",
                                  "GUIDE_scores"))

par.settings <- list(superpose.symbol = list(col = pal[c("CTree", "CTree", 
                                                         #"CTree_max", "CTree_max", 
                                                         "MOB", "MOB",
                                                         "GUIDE_scores")], 
                                             fill = pal[c("CTree", "CTree", 
                                                          #"CTree_max", "CTree_max", 
                                                          "MOB", "MOB",
                                                          "GUIDE_scores")]), 
                     superpose.line = list(col = pal[c("CTree", "CTree", 
                                                       #"CTree_max", "CTree_max", 
                                                       "MOB", "MOB",
                                                       "GUIDE_scores")]))

xyplot(prop_Tsplit ~ delta | xi, groups = ~ test, 
       data = subdata, 
       subset = binary_beta == TRUE & binary_regressor == FALSE & 
         only_intercept == FALSE & vary_beta == "all", 
       type = "l",
       lty = c(1,2,#1,2,
               1,2,1),
       lwd = 2,
       layout= c(4,1),
       key =  list(text = list(levels(subdata$test)),
                   lines = list(lty = c(1,2,#1,2,
                                        1,2,1), 
                                type = "l", pch = 1, cex = 0.8,
                                col = pal[c("CTree", "CTree", 
                                            #"CTree_max", "CTree_max", 
                                            "MOB", "MOB",
                                            "GUIDE_scores")], lwd = 2)), 
       #index.cond = list(c(1,2,3,4)),
       xlab = expression(delta),
       ylab = expression("Proportion of selecting"~Z[1]),
       strip = strip.custom(factor.levels = c(expression(xi==0), 
                                              expression(xi==0.2), 
                                              expression(xi==0.5), 
                                              expression(xi==0.8)),
                                   bg = "gray90"),
       par.settings = par.settings)

@
\caption{\label{fig:bin_stump}Proportion of cases where the true split variable $Z_1$ is
detected over increasing effect size $\delta$ (100 data sets of 250 observations per step)
with both parameter varying. Each panel represents a different value of the true 
split point $\xi$ (0, 0.2, 0.5 and 0.8 from left to right). The performances of five testing 
strategies are compared: CTree, CTree\_bin, MOB, MOB\_bin, GUIDE\_scores.}
\end{figure}




\subsection{Categorization of Splitting Variables}
\label{app:categorization}
To investigate the effect of categorizing the possible splitting variables CTree and MOB are
applied, once in their original version without a categorization and once in an adapted version
with a categorization of the possible split variables (CTree\_cat and MOB\_cat). 
Moreover, they are also compared to GUIDE\_scores which includes all available scores.
%The corresponding setting of the switches in the unifying framework is listed in 
%Table~\ref{tab:buildingblocks_cat}.

%\begin{table}%[t]
%\centering
%\caption[Table caption text]{CTree, CTree\_cat, MOB, MOB\_cat, and GUIDE\_scores with the 
%corresponding setting of the switches in the unifying framework and the approximation type 
%of the underlying null distribution.}
%\label{tab:buildingblocks_cat}
%\begin{tabular}{ l c c c c }
%\hline 
%& Scores            & Binarization  & Categorization  & Null distribution \\ 
%\hline
%CTree       & full model scores & --          & --              & conditional\\
%CTree\_cat  & full model scores & --          & \checkmark      & conditional\\
%MOB         & full model scores & --          &                 & unconditional\\
%MOB\_cat    & full model scores & --          & \checkmark      & unconditional\\
%GUIDE\_scores & full model scores & \checkmark  & \checkmark      & unconditional\\
%\hline
%\end{tabular}
%\end{table}

In Figure~\ref{fig:cat_stump} the impact of categorizing split variables on the performance 
is illustrated over increasing effect size for four different values of the true split 
point $\xi$. For both, CTree and MOB, it can be stated that overall they perform better in their
original form without any categorization. Only if the true split point $\xi$
is close to the quartiles used as breakpoints for the categorizations both versions
lead to a similar proportion of cases where the true splitting variable is detected
(e.g., for CTree and $\xi=0.5$)

Therefore, it can be summarized that a categorization of the values of the split variable
does not lead to any advantages unless the true split point corresponds to one of the 
quartiles used for the categorization. In most situations it even causes the performance to 
get worse.

\begin{figure}%%[t]
\setkeys{Gin}{width=\linewidth}
<<cat_stump, fig=TRUE, echo=FALSE, height=4, width=8>>=
subdata <- subset(sim_stump, test %in% c("ctree", "ctree_cat",
                                         #"ctree_max", "ctree_max_cat",
                                         "mfluc", "mfluc_cat",
                                         "guide_sum_12"))
subdata <- subset(subdata, delta %in% c(0, 0.1, 0.2, 0.3, 0.4, 0.5))
subdata$test <- factor(subdata$test, 
                       levels = c("ctree", "ctree_cat", 
                                  #"ctree_max","ctree_max_cat",
                                  "mfluc", "mfluc_cat",
                                  "guide_sum_12"),
                       labels = c("CTree","CTree_cat",
                                  #"CTree_max","CTree_max_cat",
                                  "MOB", "MOB_cat",
                                  "GUIDE_scores"))

par.settings <- list(superpose.symbol = list(col = pal[c("CTree", "CTree", 
                                                         #"CTree_max", "CTree_max", 
                                                         "MOB", "MOB",
                                                         "GUIDE_scores")], 
                                             fill = pal[c("CTree", "CTree", 
                                                          #"CTree_max", "CTree_max", 
                                                          "MOB", "MOB",
                                                          "GUIDE_scores")]), 
                     superpose.line = list(col = pal[c("CTree", "CTree", 
                                                       #"CTree_max", "CTree_max", 
                                                       "MOB", "MOB",
                                                       "GUIDE_scores")]))

xyplot(prop_Tsplit ~ delta | xi, groups = ~ test, 
       data = subdata, 
       subset = binary_beta == TRUE & binary_regressor == FALSE & 
         only_intercept == FALSE & vary_beta == "all", 
       #xlim = c(0,0.1,0.5),
       type = "l", 
       lty = c(1,2,#1,2,
               1,2,1),
       lwd = 2,
       layout= c(4,1),
       key =  list(text = list(levels(subdata$test)),
                   lines = list(lty = c(1,2,#1,2,
                                        1,2,1), 
                                type = "l", pch = 1, cex = 0.8,
                                col = pal[c("CTree", "CTree", 
                                            #"CTree_max", "CTree_max", 
                                            "MOB", "MOB",
                                            "GUIDE_scores")], lwd = 2)), 
       #index.cond = list(c(1,2,3,4)),
       xlab = expression(delta),
       ylab = expression("Proportion of selecting"~Z[1]),
       strip = strip.custom(factor.levels = c(expression(xi==0), 
                                              expression(xi==0.2), 
                                              expression(xi==0.5), 
                                              expression(xi==0.8)),
                                   bg = "gray90"),
       par.settings = par.settings)

@
\caption{\label{fig:cat_stump}Proportion of cases where the true split variable $Z_1$ is
detected over increasing effect size $\delta$ (100 data sets of 250 observations per step)
with both parameter varying. Each panel represents a different value of the true 
split point $\xi$ (0, 0.2, 0.5 and 0.8 from left to right). The performances of five testing 
strategies are compared: CTree, CTree\_cat, MOB, MOB\_cat, GUIDE\_scores.}
\end{figure}


\end{appendix}


%\newpage
\bibliography{ref.bib}

\end{document}