\documentclass[11pt,t,usepdftitle=false,aspectratio=169]{beamer}
\usetheme[url,license]{uibk}

\title{The Power of Unbiased Recursive Partitioning:\\
A Unifying View of CTree, MOB, and GUIDE}
\author{Lisa Schlosser, Torsten Hothorn, Achim Zeileis}
\setbeamerfont{url}{size*={11.5pt}{13pt},series=\mdseries}
%\renewcommand{\mysize}{\fontsize{7.7pt}{9pt}\selectfont}
\URL{http://www.partykit.org/} 
\headerimage{3}


\setbeamertemplate{caption}{\insertcaption} 
%% includes a replacement for \usepackage{Sweave}
\usepackage{changepage}
\usepackage{amsmath,tikz}
\usetikzlibrary{positioning,shapes,arrows,decorations.pathreplacing,calc,automata,mindmap,trees,tikzmark,decorations.pathreplacing}
%\graphicspath{{plots/}}

\newcommand{\argmax}{\operatorname{argmax}\displaylimits}

%% colors
\definecolor{HighlightOrange}{rgb}{0.9490196,0.5725490,0.0000000}
\definecolor{HighlightBlue}{rgb}{0.4784314,0.7490196,0.9803922}

 
\SweaveOpts{engine=R, eps=FALSE, keep.source=TRUE}
<<preliminaries, echo=FALSE, results=hide>>=
set.seed(7)
# setwd("~/svn/partykit/pkg/partykit/inst/guideliketest/DAGStat2019/")
options(prompt = "R> ", continue = "+  ", useFancyQuotes = FALSE, width = 70)
library("partykit")
library("Formula")
library("parallel")
library("lattice")
library("latticeExtra")
library("scales")
library("colorspace")
#source("../simulation/guidelike.R")
source("../simulation/tests_guideprune.R")
#source("../simulation/ccprune.R")

load("../simulation/results/sim_stump.rda")
load("../simulation/results/sim_tree.rda")
load("../simulation/results/sim_3way.rda")


## HCL palette
pal <- qualitative_hcl(5, "Dark 3")
names(pal) <- c("CTree", "MOB", "GUIDE", "GUIDE+scores", "CTree+max")

## HCL palette for illustrations of data generating process
pal_dgp_light <- qualitative_hcl(3, "Dark 3", c1 = 50, l1 = 75)
pal_dgp <- qualitative_hcl(3, "Dark 3")

## HCL palette for tree panels
paltrees <- rgb(c(0.9, 1), c(0.9, 1), c(0.9, 1))


#produce plots
set.seed(7)
x <- runif(70,-1,1)
e1 <- rnorm(70,0,1)
e2 <- rnorm(70,0,1)
y1 <- x*6 + e1 
y2 <- x*1 + e2
y <- c(y1,y2)

jpeg("lm1.jpeg")
par(mar=c(3.2,3.2,0,0))
plot(x,y1, ylab = "", xlab = "", xaxt="n", yaxt="n", ylim = c(-7,7))
mtext("Y",side = 2, las = 2, line = 2, cex = 2.5, col = gray.colors(1,0.3))
mtext("X",side = 1, las = 1, line = 2, cex = 2.5, col = gray.colors(1,0.3))
points(x, y2)
abline(lm(y~c(x,x)), col = "red", lwd = 3)
dev.off()

jpeg("lm2.jpeg")
par(mar=c(3.2,3.2,0,0))
plot(x,y1, ylab = "", xlab = "", xaxt="n", yaxt="n", ylim = c(-7,7))
mtext("Y",side = 2, las = 2, line = 2, cex = 2.5, col = gray.colors(1,0.3))
mtext("X",side = 1, las = 1, line = 2, cex = 2.5, col = gray.colors(1,0.3))
abline(lm(y1~x), col = "red", lwd = 3)
dev.off()

jpeg("lm3.jpeg")
par(mar=c(3.2,3.2,0,0))
plot(x,y2, ylab = "", xlab = "", xaxt="n", yaxt="n", ylim = c(-7,7))
mtext("Y",side = 2, las = 2, line = 2, cex = 2.5, col = gray.colors(1,0.3))
mtext("X",side = 1, las = 1, line = 2, cex = 2.5, col = gray.colors(1,0.3))
abline(lm(y2~x), col = "red", lwd = 3)
dev.off()


@




\begin{document}

\section{Unbiased Recursive Partitioning}

\begin{frame}%[fragile]
\frametitle{Motivation}

\vspace{-0.5cm}

\center
\begin{tikzpicture}
\visible<1-3>{
\node[inner sep=0pt] (lm1) at (4, 6)
    {\includegraphics[width=0.2\textwidth]{lm1.jpeg}};
}
\visible<2>{
\node[scale=1] at (4.15, 3.7) {Other covariates $Z_1, \ldots, Z_p$?};
}
\visible<3>{
\node[inner sep=0pt] (lm2) at (2,1.7)
    {\includegraphics[width=0.2\textwidth]{lm2.jpeg}};
\node[inner sep=0pt] (lm3) at (6,1.7)
    {\includegraphics[width=0.2\textwidth]{lm3.jpeg}};
\draw[-, line width=0.2pt] (3.5,4.85) -- (2.5,3.1) node [midway, left] {\scriptsize $Z_j \leq \xi$};
\draw[-, line width=0.2pt] (4.7,4.85) -- (5.7,3.1) node [midway, right] {\scriptsize $Z_j > \xi$};
}
\visible<4->{
\node[ellipse, fill=HighlightBlue!70, align=center, scale = 1, minimum width=90pt, minimum height = 30pt] (model1) at (4.15, 5.35) {$\mathcal{M}(Y,X;\hat{\beta}$)};
\node[rectangle, fill=HighlightOrange!70, align=center, scale = 1, minimum width=80pt, minimum height = 30pt] (model2) at (2.15, 2.55) {$\mathcal{M}(Y_1,X_1;\hat{\beta_1}$)};
\node[rectangle, fill=HighlightOrange!70, align=center, scale = 1, minimum width=80pt, minimum height = 30pt] (model3) at (6.15, 2.55) {$\mathcal{M}(Y_2,X_2;\hat{\beta_2}$)};
\draw[-, line width=0.2pt] (3.5,4.85) -- (2.5,3.1) node [midway, left] {\scriptsize $Z_j \leq \xi$};
\draw[-, line width=0.2pt] (4.7,4.85) -- (5.7,3.1) node [midway, right] {\scriptsize $Z_j > \xi$};
}
\visible<5>{
\node[scale=1] at (4.15, 1) {$\mathcal{M}$ can also be a more general model (possibly without $X$).};
}
\end{tikzpicture}
\end{frame}


\begin{frame}%[fragile]
\frametitle{Unbiased Recursive Partitioning}
{\bf Basic tree algorithm:}
\medskip
\begin{enumerate}
\item Fit a model $\mathcal{M}(Y,X;\hat{\beta})$ to the response $Y$ and possible covariates $X$.
\item Assess association of $\mathcal{M}(Y,X;\hat{\beta})$
and each possible split variable~$Z_j$ and select the split~variable~$Z_{j^\ast}$ showing the strongest association.
\item Choose the corresponding split point leading to the highest improvement of model fit and split the data.
\item Repeat steps 1--3 recursively in each of the resulting subgroups until some stopping criterion is met.
\end{enumerate}

\vspace{0.8cm}

{\bf Here:} Focus on split variable selection (step 2).

\end{frame}



\begin{frame}%[fragile]
\frametitle{Split Variable Selection}

{\bf General testing strategy:}
\begin{enumerate}
\item Evaluate a discrepancy measure capturing the observation-wise goodness of fit of $\mathcal{M}(Y,X;\hat{\beta})$.
\item Apply a statistical test assessing dependency of the discrepancy measure to each possible split variable $Z_j$.
\item Select the split variable $Z_j^\ast$ showing the smallest $p$-value.
\end{enumerate}

\bigskip

{\bf Discrepancy measures:} (Model-based) transformations of $Y$ (and $X$, if any), possibly for each model parameter.
\begin{itemize}
\item (Ranks of) $Y$.
\item (Absolute) deviations $Y - \bar Y$.
\item Residuals of $\mathcal{M}(Y,X;\hat{\beta})$.
\item Score matrix of $\mathcal{M}(Y,X;\hat{\beta})$.
\item \ldots
\end{itemize}
\end{frame}



\begin{frame}%[fragile]
\frametitle{Discrepancy Measures}

\textbf{Example:} Simple linear regression $\mathcal{M}(Y,X;\beta_0, \beta_1)$,
fitted via ordinary least squares~(OLS).

\bigskip

{\bf Residuals:}
\begin{flalign*}
\hspace{0.7cm}
r(Y,X,\hat{\beta_0},\hat{\beta_1}) = Y - \hat{\beta_0} - \hat{\beta_1} \cdot X &&
\end{flalign*}

\bigskip

{\bf Model scores:} Based on log-likelihood or residual sum of squares.
\begin{flalign*}
\hspace{0.7cm}
s(Y, X, \hat{\beta_0}, \hat{\beta_1}) = 
\left(\frac{\partial r^2(Y, X, \hat{\beta_0}, \hat{\beta_1})}{\partial \beta_0} \quad , \quad  
\frac{\partial r^2(Y, X, \hat{\beta_0}, \hat{\beta_1})}{\partial \beta_1}\right) 
&&
\end{flalign*}

\visible<2>{
\vspace{-0.3cm}
$$\hspace{-0.3cm}{\color{red}\Downarrow} \hspace{3.4cm} {\color{red}\Downarrow} $$
$$
{\color{red} -2 \cdot r(Y, X, \hat{\beta_0},\hat{\beta_1})}  \hspace{0.7cm} {\color{red} -2 \cdot r(Y, X, \hat{\beta_0},\hat{\beta_1}) \cdot X}
$$
}
\end{frame}

\begin{frame}
\frametitle{A Unifying View}

\textbf{Algorithms:} CTree, MOB, GUIDE are all `flavors' of the general framework.

\bigskip

\textbf{Building blocks:} For standard setup.

\bigskip

\begin{tabular}{lllll}
\hline 
      & Scores       & Binarization  & Categorization  & Statistic \\ 
\hline
CTree & Model scores & --	           & -- 	           & Sum of squares\\
MOB   & Model scores & --	           & -- 	           & Maximally selected\\
GUIDE & Residuals    & \checkmark    & \checkmark      & Sum of squares\\
\hline
\end{tabular}

\bigskip

\textbf{Remarks:}
\begin{itemize}
  \item All three algorithms allow for certain modifications of standard setup.
  \item Further differences, e.g., null distribution, pruning strategy, etc.
\end{itemize}

\end{frame}

\begin{frame}%[fragile]
\frametitle{General Framework}

{\bf{Building blocks:}}

\begin{itemize}
\item {\only<4->{\color{lightgray}} Residuals vs.\ full model scores.}
\item {\only<1-3,6->{\color{lightgray}} Binarization of residuals/scores.}
\item {\only<1-5>{\color{lightgray}} Categorization of possible split variables.}
\end{itemize}

\bigskip


\only<2>{
\small
$$
s(Y, X, \hat{\beta_0}, \hat{\beta_1}) = 
-2 \cdot \begin{pmatrix} 
r(Y_1, X_1, \hat{\beta_0}, \hat{\beta_1}) & {\color{lightgray} r(Y_1, X_1, \hat{\beta_0}, \hat{\beta_1}) \cdot X_1}\\
r(Y_2, X_2, \hat{\beta_0}, \hat{\beta_1}) & {\color{lightgray} r(Y_2, X_2, \hat{\beta_0}, \hat{\beta_1}) \cdot X_2}\\
\vdots & {\color{lightgray} \vdots} \\
r(Y_n, X_n, \hat{\beta_0}, \hat{\beta_1}) & {\color{lightgray} r(Y_n, X_n, \hat{\beta_0}, \hat{\beta_1}) \cdot X_n}
\end{pmatrix}
%\begin{pmatrix}
%\partial_{\beta_0} r^2(Y_1, X_1, \hat{\beta_0}, \hat{\beta_1}) & {\color{lightgray} \partial_{\beta_1} r^2(Y_1, X_1, \hat{\beta_0}, \hat{\beta_1})}\\
%\partial_{\beta_0} r^2(Y_2, X_2, \hat{\beta_0}, \hat{\beta_1}) & {\color{lightgray} \partial_{\beta_1} r^2(Y_2, X_2, \hat{\beta_0}, \hat{\beta_1})}\\
%\vdots & {\color{lightgray} \vdots} \\
%\partial_{\beta_0} r^2(Y_n, X_n, \hat{\beta_0}, \hat{\beta_1}) & {\color{lightgray} \partial_{\beta_1} r^2(Y_n, X_n, \hat{\beta_0}, \hat{\beta_1})}
%\end{pmatrix}
$$
}
\only<3>{
\small
$$
s(Y, X, \hat{\beta_0}, \hat{\beta_1}) = 
-2 \cdot \begin{pmatrix} 
r(Y_1, X_1, \hat{\beta_0}, \hat{\beta_1}) & r(Y_1, X_1, \hat{\beta_0}, \hat{\beta_1}) \cdot X_1\\
r(Y_2, X_2, \hat{\beta_0}, \hat{\beta_1}) & r(Y_2, X_2, \hat{\beta_0}, \hat{\beta_1}) \cdot X_2\\
\vdots & \vdots \\
r(Y_n, X_n, \hat{\beta_0}, \hat{\beta_1}) & r(Y_n, X_n, \hat{\beta_0}, \hat{\beta_1}) \cdot X_n
\end{pmatrix}
%\begin{pmatrix} 
%\partial_{\beta_0} r^2(Y_1, X_1, \hat{\beta_0}, \hat{\beta_1}) & \partial_{\beta_1} r^2(Y_1, X_1, \hat{\beta_0}, \hat{\beta_1})\\
%\partial_{\beta_0} r^2(Y_2, X_2, \hat{\beta_0}, \hat{\beta_1}) & \partial_{\beta_1} r^2(Y_2, X_2, \hat{\beta_0}, \hat{\beta_1})\\
%\vdots & \vdots \\
%\partial_{\beta_0} r^2(Y_n, X_n, \hat{\beta_0}, \hat{\beta_1}) & \partial_{\beta_1} r^2(Y_n, X_n, \hat{\beta_0}, \hat{\beta_1})
%\end{pmatrix}
$$
}
\only<4-5>{
\small
$$
r(Y, X, \hat{\beta_0}, \hat{\beta_1}) = 
\begin{pmatrix}
r(Y_1, X_1, \hat{\beta_0}, \hat{\beta_1})\\
r(Y_2, X_2, \hat{\beta_0}, \hat{\beta_1})\\
\vdots \\
r(Y_n, X_n, \hat{\beta_0}, \hat{\beta_1})
\end{pmatrix}
%
\visible<5>{
\quad \Rightarrow \quad
\small
\begin{pmatrix}
> 0\\
\le 0\\
\vdots \\
> 0
\end{pmatrix}
}
$$
}
\only<6-7>{
\small
$$
Z_j = 
\begin{pmatrix}
Z_{j1}\\
Z_{j2}\\
\vdots \\
Z_{jn}\\
\end{pmatrix}
%
\visible<7>{
\quad \Rightarrow \quad
\small
\begin{pmatrix}
\text{Q}3\\
\text{Q}1\\
\vdots \\
\text{Q}2
\end{pmatrix}
}
$$
}
\end{frame}



\begin{frame}%[fragile]
\frametitle{Pruning}

{\bf Goal:} Avoid overfitting.

\bigskip

{\bf Two strategies:}

\begin{itemize}
\item \emph{Pre-pruning:}
Internal stopping criterion based on Bonferroni-corrected $p$-values of the applied tests. Stop splitting when there is no significant association. 
\item \emph{Post-pruning:}
First grow a very large tree and afterwards prune splits that do not improve the model fit, 
either via cross-validation (e.g.,~cost-complexity pruning as in CART) or 
based on information criteria (e.g.,~AIC or BIC). 
\end{itemize}
\end{frame}


\begin{frame}%[fragile]
\frametitle{Simulation}

\begin{table}
\begin{center}
\begin{tabular}{l l l}
\hline
Name                    & Notation                   & Specification \\
\hline
\textit{Variables:}     &                            & \\ 
Response                & $Y$                        & $=\beta_0(Z_1) + \beta_1(Z_1) \cdot X + \epsilon$ \\
Regressor               & $X$                        & $\mathcal{U}([-1,1])$ \\
Error                   & $\epsilon$                 & $\mathcal{N}(0,1)$ \\
True split variable     & $Z_1$                      & $\mathcal{U}([-1,1])$  or $\mathcal{N}(0, 1)$ \\
Noise split variables   & $Z_2, Z_3, \dots, Z_{10}$  & $\mathcal{U}([-1,1])$  or $\mathcal{N}(0, 1)$ \\
\hline
\textit{Parameters/functions:} &            & \\ 
Intercept                      & $\beta_0$  & $0$ or $\pm \delta$\\
Slope                          & $\beta_1$  & $1$ or $\pm \delta$\\
True split point               & $\xi$      & $\in \{0, 0.2, 0.5, 0.8\}$ \\
Effect size                    & $\delta$   & $\in \{0, 0.1, 0.2, \ldots , 1\}$ \\
\hline
\end{tabular}
\end{center}
\end{table}


\end{frame}


\begin{frame}%[fragile]
\frametitle{Simulation 1: True tree structure}

<<dgp, echo=FALSE>>=
set.seed(7)

nobs <- 250
sigma <- 0.5
xi <- 0
delta <- 1
beta0 <- 0
beta1 <- 1

d <- dgp_stump(nobs = nobs, changetype= "abrupt",
               variation = "beta0", beta1 = beta1, delta = delta, xi = xi, sigma = sigma)

sp_xi1 <- partysplit(3L, breaks = 0)

pn <- partynode(1L, split = sp_xi1, 
                kids = list(
                  partynode(2L, info = c("true parameters:", 
                                         expression(beta[0] ==~" "~0~" or "~-delta), 
                                         expression(beta[1] ==~" "~1~" or "~+delta))),
                  partynode(3L, info = c("true parameters:", 
                                         expression(beta[0] ==~" "~0~" or "~+delta), 
                                         expression(beta[1] ==~" "~1~" or "~-delta)))))

p <- party(pn, d)

ep <- function (obj, digits = 3, abbreviate = FALSE, justmin = Inf, 
                just = c("alternate", "increasing", "decreasing", "equal"), 
                fill = "white") 
{
  meta <- obj$data
  justfun <- function(i, split) {
    myjust <- if (mean(nchar(split)) > justmin) {
      match.arg(just, c("alternate", "increasing", "decreasing", "equal"))
    } else {
      "equal"
    }
    k <- length(split)
    rval <- switch(myjust, equal = rep.int(0, k), 
                   alternate = rep(c(0.5, -0.5), length.out = k), 
                   increasing = seq(from = -k/2, to = k/2, by = 1), 
                   decreasing = seq(from = k/2, to = -k/2, by = -1))
    unit(0.5, "npc") + unit(rval[i], "lines")
  }
  function(node, i) {
    split <-  c("<= xi", "> xi")
    y <- justfun(i, split)
    split <- split[i]
    if (any(grep(">", split) > 0) | any(grep("<", split) > 0)) {
      tr <- suppressWarnings(try(parse(text = paste("phantom(0)", split)), silent = TRUE))
      if (!inherits(tr, "try-error")) split <- tr
    }
    grid.rect(y = y, gp = gpar(fill = fill, col = 0), width = unit(1, "strwidth", split))
    grid.text(split, y = y, just = "center")
  }
}
class(ep) <- "grapcon_generator"


tp <- function (obj, digits = 3, abbreviate = FALSE, 
                fill = c("lightgray", "white"), id = TRUE, 
                just = c("center", "top"), top = 0.85, 
                align = c("center", "left", "right"), gp = NULL, FUN = NULL, 
                height = NULL, width = NULL) 
{
  nam <- names(obj)
  extract_label <- function(node) formatinfo_node(node, FUN = FUN, 
                                                  default = c("terminal", "node"))
  maxstr <- function(node) {
    lab <- extract_label(node)
    klab <- if (is.terminal(node)) 
      ""
    else unlist(lapply(kids_node(node), maxstr))
    lab <- c(lab, klab)
    lab <- try(unlist(lapply(lab, function(x) strsplit(x, 
                                                       "\n"))), silent = TRUE)
    if (inherits(lab, "try-error")) {
      paste(rep("a", 9L), collapse = "")
    }
    else {
      return(lab[which.max(nchar(lab))])
    }
  }
  nstr <- if (is.null(width)) 
    maxstr(node_party(obj))
  else paste(rep("a", width), collapse = "")
  just <- match.arg(just[1L], c("center", "centre", "top"))
  if (just == "centre") 
    just <- "center"
  align <- match.arg(align[1L], c("center", "centre", "left", 
                                  "right"))
  if (align == "centre") 
    align <- "center"
  rval <- function(node) {
    if(node$id == 2) fill <- c("white", pal_dgp_light[1])
    if(node$id == 3 | node$id == 4) fill <- c("white", pal_dgp_light[2])
    if(node$id == 5) fill <- c("white", pal_dgp_light[3])
    fill <- rep(fill, length.out = 2)
    lab <- extract_label(node)
    if (!is.null(gp)) {
      outer_vp <- viewport(gp = gp)
      pushViewport(outer_vp)
    }
    if (is.null(height)) 
      height <- length(lab) + 1L
    node_vp <- viewport(x = unit(0.5, "npc"), y = unit(if (just == 
                                                           "top") 
      top
      else 0.5, "npc"), just = c("center", just), width = unit(1, 
                                                               "strwidth", nstr) * 1.1, height = unit(height, "lines"), 
      name = paste("node_terminal", id_node(node), sep = ""), 
      gp = if (is.null(gp)) 
        gpar()
      else gp)
    pushViewport(node_vp)
    grid.rect(gp = gpar(fill = fill[1]))
    for (i in seq_along(lab)) grid.text(x = switch(align, 
                                                   center = unit(0.5, "npc"), left = unit(1, "strwidth", 
                                                                                          "a"), right = unit(1, "npc") - unit(1, "strwidth", 
                                                                                                                              "a")), y = unit(length(lab) - i + 1, "lines"), 
                                        lab[i], just = align)
    if (id) {
      nodeIDvp <- viewport(x = unit(0.5, "npc"), y = unit(1, 
                                                          "npc"), width = max(unit(1, "lines"), unit(1.3, 
                                                                                                     "strwidth", nam[id_node(node)])), height = max(unit(1, 
                                                                                                                                                         "lines"), unit(1.3, "strheight", nam[id_node(node)])))
      pushViewport(nodeIDvp)
      grid.rect(gp = gpar(fill = fill[2], lty = "solid"))
      grid.text(nam[id_node(node)])
      popViewport()
    }
    if (is.null(gp)) 
      upViewport()
    else upViewport(2)
  }
  return(rval)
}

class(tp) <- "grapcon_generator"

@

\vspace{-0.2cm}

\begin{figure}%[t]
\setkeys{Gin}{width=1\linewidth}
\minipage{0.36\textwidth}
<<dgp_stump1, fig=TRUE, echo=FALSE, width=4, height=3>>=
#par(mar=c(3,3,0,0), oma = c(3,3,0,0))
plot(p, 
     tp_args = list(FUN = identity, width = 15), 
     ip_args = list(fill = paltrees[c(1, 2)]),
     drop_terminal = TRUE, tnex = 1, edge_panel = ep, terminal_panel = tp)
@
\endminipage
\minipage{0.36\textwidth}
<<dgp_stump2, fig=TRUE, echo=FALSE, width=4, height=3>>=
par(mar = c(4,4,2,4))
plot(x = d$x[d$z1<=xi], y = d$y[d$z1<=xi], ylim = c(-5,5), ylab = "", xlab = "", 
     main = expression("varying"~beta[0]),
     pch = 19, col = alpha(pal_dgp[1], 0.4), cex = 0.7)
mtext("Y", side = 2, line = 2.5)
mtext("X", side = 1, line = 2.5)
abline(coef = c(-1,1), col = pal_dgp[1], lwd = 2)
abline(coef = c(1,1), col = pal_dgp[2], lwd = 2)
points(x = d$x[d$z1>xi], y = d$y[d$z1>xi], ylim = c(-5,5), pch = 19, col = alpha(pal_dgp[2], 0.4), cex = 0.7)
legend("top", c(expression(z[1]<=xi), expression(z[1]>xi)), col = c(pal_dgp[1], pal_dgp[2]), pch = c(19), bty = "n", cex = 0.9)
mtext(expression(beta[0]==+delta), side = 4, col = pal_dgp[2], las = 1, line = 0.5, at = 3.2)
mtext(expression(beta[1]== 1), side = 4, col = pal_dgp[2], las = 1, line = 0.5, at = 2.2)
mtext(expression(beta[0]==-delta), side = 4, col = pal_dgp[1], las = 1, line = 0.5, at = 0.2)
mtext(expression(beta[1]== 1), side = 4, col = pal_dgp[1], las = 1, line = 0.5, at = -0.8)
@
\endminipage

\minipage{0.36\textwidth}
<<dgp_stump3, fig=TRUE, echo=FALSE, width=4, height=3>>=
d <- dgp_stump(nobs = nobs, changetype= "abrupt",
               variation = "beta1", beta0 = beta0, delta = delta, xi = xi, sigma = sigma)

par(mar = c(4,4,2,4))
plot(x = d$x[d$z1<=xi], y = d$y[d$z1<=xi], ylim = c(-5,5), ylab = "", xlab = "", main = expression("varying"~beta[1]),
     pch = 19, col = alpha(pal_dgp[1], 0.4), cex = 0.7)
mtext("Y", side = 2, line = 2.5)
mtext("X", side = 1, line = 2.5)
abline(coef = c(0,1), col = pal_dgp[1], lwd = 2)
abline(coef = c(0,-1), col = pal_dgp[2], lwd = 2)
points(x = d$x[d$z1>xi], y = d$y[d$z1>xi], ylim = c(-5,5), pch = 19, col = alpha(pal_dgp[2], 0.4), cex = 0.7)
legend("top", c(expression(z[1]<=xi), expression(z[1]>xi)), col = c(pal_dgp[1], pal_dgp[2]), pch = c(19), bty = "n", cex = 0.9)
mtext(expression(beta[0]==0), side = 4, col = pal_dgp[2], las = 1, line = 0.5, at = -1)
mtext(expression(beta[1]==-delta), side = 4, col = pal_dgp[2], las = 1, line = 0.5, at = -2)
mtext(expression(beta[0]==0), side = 4, col = pal_dgp[1], las = 1, line = 0.5, at = 2)
mtext(expression(beta[1]==+delta), side = 4, col = pal_dgp[1], las = 1, line = 0.5, at = 1)
@
\endminipage
\minipage{0.36\textwidth}
<<dgp_stump4, fig=TRUE, echo=FALSE, width=4, height=3>>=
d <- dgp_stump(nobs = nobs, changetype= "abrupt", 
               variation = "all", delta = delta, xi = xi, sigma = sigma)
par(mar = c(4,4,2,4))
plot(x = d$x[d$z1<=xi], y = d$y[d$z1<=xi], ylim = c(-5,5), ylab = "", xlab = "",
     main = expression("varying"~beta[0]~"and"~beta[1]),
     xlim = c(-1,1.1),
     pch = 19, col = alpha(pal_dgp[1], 0.4), cex = 0.7)
mtext("Y", side = 2, line = 2.5)
mtext("X", side = 1, line = 2.5)
abline(coef = c(-1,1), col = pal_dgp[1], lwd = 2)
abline(coef = c(1,-1), col = pal_dgp[2], lwd = 2)
points(x = d$x[d$z1>xi], y = d$y[d$z1>=xi], pch = 19, col = alpha(pal_dgp[2], 0.4), cex = 0.7)
legend("top", c(expression(z[1]<=xi), expression(z[1]>xi)), col = c(pal_dgp[1], pal_dgp[2]), pch = c(19), bty = "n", cex = 0.9)
mtext(expression(beta[0]==-delta), side = 4, col = pal_dgp[1], las = 1, line = 0.5, at = 1.7)
mtext(expression(beta[1]==+delta), side = 4, col = pal_dgp[1], las = 1, line = 0.5, at = 0.7)
mtext(expression(beta[0]==+delta), side = 4, col = pal_dgp[2], las = 1, line = 0.5, at = -0.7)
mtext(expression(beta[1]==-delta), side = 4, col = pal_dgp[2], las = 1, line = 0.5, at = -1.7)
@
\endminipage
\end{figure}
\end{frame}



\begin{frame}%[fragile]
\frametitle{Simulation 1: Residuals vs.\ full model scores}

\begin{figure}
\setkeys{Gin}{width=0.8\linewidth}
<<scores_stump, fig=TRUE, echo=FALSE, height=5, width=8>>=
subdata <- subset(sim_stump, test %in% c("ctree","mfluc",
                                         "guide_sum_12", "guide_sum_1_cor"))
subdata$test <- factor(subdata$test, 
                       levels = c("ctree","mfluc",
                                  "guide_sum_12", "guide_sum_1_cor"),
                       labels = c("CTree","MOB",
                                  "GUIDE+scores", "GUIDE"))

subdataxi <- subset(subdata, xi %in% c(0,0.8))

par.settings <- list(superpose.symbol = list(col = pal[c("CTree", "MOB", "GUIDE+scores", "GUIDE")], 
                                             fill = pal[c("CTree", "MOB", "GUIDE+scores", "GUIDE")]), 
                     superpose.line = list(col = pal[c("CTree", "MOB", "GUIDE+scores", "GUIDE")]))

useOuterStrips(xyplot(prop_Tsplit ~ delta | variation + xi, groups = ~ test, 
                      data = subdataxi, 
                      subset = changetype == "abrupt", 
                      type = "l", lty = c(1,1,1,1),
                      lwd = 2,
                      scales=list(x=list(at=seq(1,11,2))),
                      key =  list(text = list(c(levels(subdataxi$test)," ")),
                                  lines = list(lty = c(1,1,1,1,0), type = "l", pch = 1, cex = 0.8,
                                               col = pal[c("CTree", "MOB", "GUIDE+scores", "GUIDE")], lwd = 2)),
                      layout= c(3,2),
                      index.cond = list(c(2,3,1),c(2,1)),
                      xlab = expression(delta),
                      ylab = expression("Selection probability of"~Z[1]),
                      par.settings = par.settings),
               
               strip =strip.custom(factor.levels = c(expression("varying"~beta[0]~"and"~beta[1]), 
                                                     expression("varying"~beta[0]), 
                                                     expression("varying"~beta[1])),
                                   bg = "gray90"),
               strip.left=strip.custom(factor.levels = c(expression(xi == 0~" (50%)"),                                                              expression(xi == 0.8~" (90%)" )),
                                       bg = "gray90"))


@
\end{figure}
\end{frame}


\begin{frame}%[fragile]
\frametitle{Simulation 1: Maximum vs.\ linear selection}

\begin{figure}
\setkeys{Gin}{width=0.8\linewidth}
<<scores_stump_max, fig=TRUE, echo=FALSE, height=5, width=8>>=
subdata <- subset(sim_stump, test %in% c("ctree","ctree_max","mfluc","guide_sum_12","guide_sum_1_cor"))
subdata$test <- factor(subdata$test, 
                       levels = c("ctree", "ctree_max","mfluc","guide_sum_12","guide_sum_1_cor"),
                       labels = c("CTree","CTree+max","MOB","GUIDE+scores","GUIDE"))

subdataxi <- subset(subdata, xi %in% c(0,0.8))

par.settings <- list(superpose.symbol = list(col = pal[c("CTree","CTree+max","MOB","GUIDE+scores","GUIDE")], 
                                             fill = pal[c("CTree","CTree+max","MOB","GUIDE+scores","GUIDE")]), 
                     superpose.line = list(col = pal[c("CTree","CTree+max","MOB","GUIDE+scores","GUIDE")]))

useOuterStrips(xyplot(prop_Tsplit ~ delta | variation + xi, groups = ~ test, 
                      data = subdataxi, 
                      subset = changetype == "abrupt", 
                      type = "l", lty = c(1,1,1,1,1),
                      lwd = 2,
                      scales=list(x=list(at=seq(1,11,2))),
                      key =  list(text = list(levels(subdataxi$test)),
                                  lines = list(lty = c(1,1,1,1,1), type = "l", pch = 1, cex = 0.8,
                                               col = pal[c("CTree", "CTree+max", "MOB", "GUIDE+scores", "GUIDE")], lwd = 2)),
                      layout= c(3,2),
                      index.cond = list(c(2,3,1),c(2,1)),
                      xlab = expression(delta),
                      ylab = expression("Selection probability of"~Z[1]),
                      par.settings = par.settings),
               
               strip =strip.custom(factor.levels = c(expression("varying"~beta[0]~"and"~beta[1]), 
                                                     expression("varying"~beta[0]), 
                                                     expression("varying"~beta[1])),
                                   bg = "gray90"),
               strip.left=strip.custom(factor.levels = c(expression(xi == 0~" (50%)"),                                                              expression(xi == 0.8~" (90%)" )),
                                       bg = "gray90"))


@
\end{figure}
\end{frame}



\begin{frame}%[fragile]
\frametitle{Simulation 1: Continuously changing parameters}

\begin{figure}%[t]
\setkeys{Gin}{width=0.8\linewidth}
<<contbeta, fig=TRUE, echo=FALSE, height=4, width=8>>=
subdata <- subset(sim_stump, test %in% c("ctree", "ctree_max","mfluc","guide_sum_12","guide_sum_1_cor"))
subdata$test <- factor(subdata$test, 
                       levels = c("ctree", "ctree_max","mfluc","guide_sum_12","guide_sum_1_cor"),
                       labels = c("CTree","CTree+max","MOB","GUIDE+scores","GUIDE"))
par.settings <- list(superpose.symbol = list(col = pal[c("CTree","CTree+max","MOB","GUIDE+scores","GUIDE")], 
                                             fill = pal[c("CTree","CTree+max","MOB","GUIDE+scores","GUIDE")]), 
                     superpose.line = list(col = pal[c("CTree","CTree+max","MOB","GUIDE+scores","GUIDE")]))

xyplot(prop_Tsplit ~ delta | variation, groups = ~ test, 
       data = subdata, 
       subset = changetype == "continuous", 
       scales=list(x=list(at=seq(1,11,2))),
       type = "l", 
       layout= c(3,1),
       lwd = 2,
       key =  list(text = list(levels(subdata$test)),
                   lines = list(lty = c(1,1,1,1,1), 
                                type = "l", 
                                col = pal[c("CTree","CTree+max","MOB","GUIDE+scores","GUIDE")], 
                                lwd = 2)), 
       xlab = expression(delta),
       ylab = expression("Selection probability of"~Z[1]),
       index.cond = list(c(2,3,1)),
       strip = strip.custom(factor.levels = c(expression("varying"~beta[0]~" and "~beta[1]),
                                              expression("varying"~beta[0]), 
                                              expression("varying"~beta[1])),
                            bg = "gray90"),
       par.settings = par.settings)
@
\end{figure}
\end{frame}



\begin{frame}%[fragile]
\frametitle{Simulation 1: Full factorial analysis of building blocks}

\begin{figure}%[t]
\setkeys{Gin}{width=0.8\linewidth}
<<3way, fig=TRUE, echo=FALSE, height=4, width=8>>=
xyplot(pval_z1 ~ cat | xi, 
       groups = interaction(bin, res_scores), 
       data = sim_3way, 
       ylim = c(0,0.5),
       type = c("a"), 
       col = c(pal[1], pal[3], pal[1], pal[3]),
       lty = c(1,1,2,2),
       lwd = 2,
       layout = c(2,1),
       scales = list(alternating=FALSE),
       xlab = "Categorization",
       ylab = expression("p-value of"~Z[1]),
       strip = strip.custom(factor.levels = c(expression(xi==0~"(50%)"~" and"~delta==0.3), 
                                              expression(xi==0.8~"(90%)"~" and"~delta==1)),
                            bg = "gray90"),
       key = list(text = list(c("Goodness of fit:", "  residuals", "  scores", "Dichotomization:", "  yes", "  no")),
                  columns = 2,
                  lines = list(lty = c(0,1,2,0,1,1), 
                               type = "l", lwd = 2,
                               col = c("black","black","black","black", pal[1], pal[3]))))

@
\end{figure}
\end{frame}



\begin{frame}%[fragile]
\frametitle{Simulation 2: True tree structure}

<<true_tree_dgp, eval = TRUE, echo=FALSE, results=hide>>=
# d <- dgp_tree(nobs = 250)
set.seed(7)
d <- dgp_tree(nobs = 250, xi = 0, variation = "all", 
              changetype= "abrupt", sigma = 0.5)

sp_xi2 <- partysplit(4L, breaks = 0)
sp_xi1 <- partysplit(3L, breaks = 0)

pn <- partynode(1L, split = sp_xi2, 
                kids = list(
                  partynode(2L, info = c("true parameters:", 
                                         expression(beta[0] == 0), 
                                         expression(beta[1] == +delta))),
                  partynode(3L, split = sp_xi1, 
                            kids = list(
                              partynode(4L, info = c("true parameters:", 
                                                     expression(beta[0] == -delta), 
                                                     expression(beta[1] == -delta))),
                              partynode(5L, info = c("true parameters:", 
                                                     expression(beta[0] == +delta), 
                                                     expression(beta[1] == -delta)))))))

p <- party(pn, d)

#ctest <- evaltests(y~x|z1+z2+z3+z4+z5+z6+z7+z8+z9+z10, data = d, testfun = "ctree", stump = FALSE)
@


\begin{figure}%[t]
\begin{center}
\setkeys{Gin}{width=1\linewidth}
\minipage{0.45\textwidth}
<<dgp_tree1, fig=TRUE, echo=FALSE, width=5, height=4.5>>=
par(mar=c(0,0,0,0), oma=c(0,0,0.4,0))
#paltrees <- rgb(c(0.97, 0.64, 1), c(0.70, 0.83, 1), c(0.30, 0.99, 1))
paltrees <- rgb(c(1, 0.9, 1), c(1, 0.9, 1), c(1, 0.9, 1))
plot(p, edge_panel = ep, terminal_panel = tp,
     tp_args = list(FUN = identity, width = 15), 
     ip_args = list(fill = paltrees[c(2, 3)]),
     drop_terminal = TRUE, tnex = 1)
@
\endminipage
\minipage{0.4\textwidth}
<<dgp_tree2, fig=TRUE, echo=FALSE, width=4, height=4>>=
par(mar = c(4,4,2,4))
plot(x = d$x[d$z2<=xi], y = d$y[d$z2<=xi], ylim = c(-5,5), ylab = "Y", xlab = "X",
     pch = 19, col = alpha(pal_dgp[1], 0.4), cex = 0.7)
abline(coef = c(0,1), col = pal_dgp[1], lwd = 2)
abline(coef = c(-1,-1), col = pal_dgp[2], lwd = 2)
abline(coef = c(1,-1), col = pal_dgp[3], lwd = 2)
points(x = d$x[d$z2>xi & d$z1<=xi], y = d$y[d$z2>xi & d$z1<=xi], pch = 19, 
       col = alpha(pal_dgp[2], 0.4), cex = 0.7)
points(x = d$x[d$z2>xi & d$z1>xi], y = d$y[d$z2>xi & d$z1>xi], pch = 19, 
       col = alpha(pal_dgp[3], 0.4), cex = 0.7)
legend("top", c(expression(z[2]<=xi), 
                expression(z[2]>xi~" and "~z[1]<=xi),
                expression(z[2]>xi~" and "~z[1]>xi)), 
       col = c(pal_dgp[1], pal_dgp[2], pal_dgp[3]), pch = c(19), bty = "n", cex = 0.9)
mtext(expression(beta[0]==0), side = 4, col = pal_dgp[1], las = 1, line = 0.5, at = 2)
mtext(expression(beta[1]==+delta), side = 4, col = pal_dgp[1], las = 1, line = 0.5, at = 1.3)
mtext(expression(beta[0]==+delta), side = 4, col = pal_dgp[3], las = 1, line = 0.5, at = 0.1)
mtext(expression(beta[1]==-delta), side = 4, col = pal_dgp[3], las = 1, line = 0.5, at = -0.6)
mtext(expression(beta[0]==-delta), side = 4, col = pal_dgp[2], las = 1, line = 0.5, at = -2)
mtext(expression(beta[1]==-delta), side = 4, col = pal_dgp[2], las = 1, line = 0.5, at = -2.7)
@
\endminipage
\end{center}
\end{figure}
\end{frame}


\begin{frame}%[fragile]
\frametitle{Simulation 2: Residuals vs.\ full model scores}

\begin{figure}%[t]
\setkeys{Gin}{width=0.8\linewidth}
<<tree_ari, fig=TRUE, echo=FALSE, height=4, width=8>>=
par.settings <- list(superpose.symbol = list(col = pal[c("CTree", "MOB", "GUIDE+scores", "GUIDE")], 
                                             fill = pal[c("CTree", "MOB", "GUIDE+scores", "GUIDE")]), 
                     superpose.line = list(col = pal[c("CTree", "MOB", "GUIDE+scores", "GUIDE")]))
xyplot(ari ~ delta | xi, groups = ~ test, 
       data = sim_tree[sim_tree$pruning == "pre",], 
       type = "l", lty = c(1,1,1,1),
       lwd = 2,
       key =  list(text = list(levels(sim_tree$test)[1:4]),
                   lines = list(lty = c(1,1,1,1), type = "l", pch = 1, cex = 0.8,
                                col = pal[c("CTree", "MOB", "GUIDE+scores", "GUIDE")], lwd = 2)),
       layout= c(4,1),
       scales=list(x=list(at=seq(1,11,2))),
       xlab = expression(delta),
       ylab = "Adjusted Rand Index",
       #index.cond = list(c(2,3,1),c(2,1)),
       par.settings = par.settings,
       strip = strip.custom(factor.levels = c(expression(xi==0), 
                                              expression(xi==0.2), 
                                              expression(xi==0.5), 
                                              expression(xi==0.8)),
                            bg = "gray90"))
       


@

\end{figure}
\end{frame}

\begin{frame}%[fragile]
\frametitle{Simulation 2: Pre-pruning vs.\ post-pruning}

\begin{figure}%[t]
\setkeys{Gin}{width=0.8\linewidth}
<<prune_ari, fig=TRUE, echo=FALSE, height=4, width=8>>=
par.settings <- list(superpose.symbol = list(col = pal[c("CTree", "MOB", "GUIDE+scores", "GUIDE")], 
                                             fill = pal[c("CTree", "MOB", "GUIDE+scores", "GUIDE")]), 
                     superpose.line = list(col = pal[c("CTree", "MOB", "GUIDE+scores", "GUIDE")]))

# add new variable that combines test and pruning strategy
sim_tree$test_pruning <- paste(sim_tree$test, sim_tree$pruning, sep="_")
sim_tree$test_pruning <- factor(sim_tree$test_pruning,
                                levels = c("CTree_pre", "MOB_pre", "GUIDE+scores_pre", "GUIDE_pre",
                                           "CTree_post", "MOB_post", "GUIDE+scores_post", "GUIDE_post"),
                                labels = c("CTree_pre", "MOB_pre", "GUIDE+scores_pre", "GUIDE_pre",
                                           "CTree_post", "MOB_post", "GUIDE+scores_post", "GUIDE_post"))

xyplot(ari ~ delta | xi, groups = ~ test_pruning, 
       data = sim_tree, 
       type = "l", lty = c(1,1,1,1,2,2,2,2),
       lwd = 2,
       key =  list(text = list(levels(sim_tree$test)[1:4]),
                   lines = list(lty = c(1,1,1,1), type = "l", pch = 1, cex = 0.8,
                                col = pal[c("CTree", "MOB", "GUIDE+scores", "GUIDE")], lwd = 2)),
       layout= c(4,1),
       scales=list(x=list(at=seq(1,11,2))),
       xlab = expression(delta),
       ylab = "Adjusted Rand Index",
       strip = strip.custom(factor.levels = c(expression(xi==0), 
                                              expression(xi==0.2), 
                                              expression(xi==0.5), 
                                              expression(xi==0.8)),
                            bg = "gray90"),
       par.settings = par.settings
)
@
\end{figure}
\end{frame}


\begin{frame}%[fragile]
\frametitle{Recommendations}

\textbf{In this setting:}

\begin{itemize}
\setlength\itemsep{0.2cm}
\item Full model scores better than residuals only.
\item Original values of scores/residuals better than binarized values.
\item Categorization is simpler, but less powerful in margins.
\item If the significance test performs well pre-pruning works well,\\
otherwise post-pruning might be needed.
%\item Including full model scores is advantageous over using only residuals.
%\item Binarizing scores/residuals does not improve the performance.
%\item Categorizing split variables can simplify calculations, however,
%effects might be missed unless a binning point is close to the true split point.
%\item If the applied test performs well as significance test pre-pruning
%is preferable, otherwise post-pruning.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Software}

\textbf{Package:} Available on CRAN at

\url{https://CRAN.R-project.org/package=partykit}\\

\bigskip

\textbf{Replication code:} Available on R-Forge at

\url{https://R-Forge.R-project.org/projects/partykit/}\\

%\bigskip

%\textbf{Applied functions:}

%\begin{itemize}
%\item \texttt{ctree}
%\item \texttt{mob}
%\item Adaptions of both of them
%\item Reimplementation of GUIDE
%\end{itemize}
\end{frame}





\subsection{References}

\begin{frame}
\frametitle{References}

\vspace{-0.2cm}

\footnotesize
Schlosser L, Hothorn T, Zeileis A (2019).
 \dquote{The Power of Unbiased Recursive Partitioning: A Unifying View of CTree, MOB, and GUIDE.}
 \emph{Working paper in preparation}.
  
\medskip

Breiman L, Friedman J H, Olshen R A, Stone C J (1984).
 \textit{Classification and Regression Trees.}
 Wadsworth,
 California.

\medskip

Hothorn T, Hornik K, Zeileis A (2006).
 \dquote{Unbiased Recursive Partitioning: A Conditional Inference Framework.}
 \textit{Journal of Computational and Graphical Statistics},
 \textbf{15}(3), 651--674.
 \doi{10.1198/106186006X133933}
 
\medskip

Hothorn T, Zeileis A (2015).
 \dquote{\textbf{partykit}: A Modular Toolkit for Recursive Partytioning in \textsf{R}.}
 \textit{Journal of Machine Learning Research},
 \textbf{16}, 3905--3909.
 \url{http://www.jmlr.org/papers/v16/hothorn15a.html}

\medskip

Loh W-Y (2002).
 \dquote{Regression Trees with Unbiased Variable Selection and Interaction Detection.}
 \textit{Statistica Sinica},
 \textbf{12}(2), 361--386.
 \url{http://www.jstor.org/stable/24306967}

\medskip

Zeileis A, Hothorn T, Hornik K (2008).
 \dquote{Model-Based Recursive Partitioning.}
  \textit{Journal of Computational and Graphical Statistics},
  \textbf{17}(2), 492--514.
  \doi{10.1198/106186008X319331}

\end{frame}


%\begin{frame}
%\frametitle{References (2)}

%\footnotesize

%\end{frame}

\end{document}




