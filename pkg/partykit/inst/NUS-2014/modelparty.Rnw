\documentclass[11pt,compress,t]{beamer}
\usetheme{Z}
\usepackage{amsfonts,amstext,amsmath}
%% need no \usepackage{Sweave}
\definecolor{InputColor}{rgb}{0,0,0.3}
\definecolor{OutputColor}{rgb}{0.2,0.2,0.2}

\graphicspath{{../Psychoco-2014/images/}}

%% commands
\newcommand{\ui}{\underline{i}}
\newcommand{\oi}{\overline{\imath}}
\newcommand{\darrow}{\stackrel{\mbox{\tiny \textnormal{d}}}{\longrightarrow}}
\newcommand{\parrow}{\stackrel{\mbox{\tiny \textnormal{p}}}{\longrightarrow}}
\newcommand{\dotequals}{\stackrel{\cdot}{=}}
\newcommand{\efp}{\mathrm{\it efp}}
\newcommand{\given}{\, | \,}
\newcommand{\ltime}{\lambda_\mathrm{time}}
\newcommand{\lcomp}{\lambda_\mathrm{comp}}
\newcommand{\argmin}{\operatorname{argmin}\displaylimits}

\SweaveOpts{engine=R, eps=FALSE, keep.source=TRUE}

<<packages, echo=FALSE, results=hide>>=
library("partykit")
library("psychotools")
options(digits = 4)
@

<<data, echo=FALSE, results=hide>>=
data("Topmodel2007", package = "psychotree")
data("Titanic", package = "datasets")
ttnc <- as.data.frame(Titanic)
ttnc <- ttnc[rep(1:nrow(ttnc), ttnc$Freq), 1:4]
names(ttnc)[2] <- "Gender"
ttnc <- transform(ttnc, Treatment = factor(
  Gender == "Female" | Age == "Child",
  levels = c(FALSE, TRUE),
  labels = c("Normal\n(Male&Adult)", "Preferential\n(Female|Child)")
))
@

<<btfit2, echo=FALSE, results=hide>>=
btfit2 <- function(y, x = NULL, start = NULL, weights = NULL,
  offset = NULL, ..., estfun = FALSE, object = FALSE) {
  rval <- btReg.fit(y, weights = weights, ...,
    estfun = estfun, vcov = object)
  list(
    coefficients = rval$coefficients,
    objfun = -rval$loglik,
    estfun = if(estfun) rval$estfun else NULL,
    object = if(object) rval else NULL
  )
}
@

<<estfun.btReg, echo=FALSE, results=hide>>=
estfun.btReg <- function(x, ...) x$estfun
@

<<panelfunction, echo=FALSE, results=hide>>=
## visualization function
node_btplot <- function(mobobj, id = TRUE,
  worth = TRUE, names = TRUE, abbreviate = TRUE, index = TRUE, ref = TRUE,
  col = "black", linecol = "lightgray", cex = 0.5, pch = 19, xscale = NULL, yscale = NULL, ylines = 1.5)
{
    ## node ids
    node <- nodeids(mobobj, terminal = FALSE)
    
    ## get all coefficients 
    cf <- partykit:::apply_to_models(mobobj, node, FUN = function(z)        
      if(worth) worth(z) else coef(z, all = FALSE, ref = TRUE))
    cf <- do.call("rbind", cf)
    rownames(cf) <- node

    ## get one full model
    mod <- partykit:::apply_to_models(mobobj, node = 1L, FUN = NULL)

    if(!worth) {
      if(is.character(ref) | is.numeric(ref)) {
        reflab <- ref
        ref <- TRUE
      } else {
        reflab <- mod$ref
      }
      if(is.character(reflab)) reflab <- match(reflab, mod$labels)
      cf <- cf - cf[,reflab]
    }

    ## reference
    if(worth) {
      cf_ref <- 1/ncol(cf)
    } else {
      cf_ref <- 0
    }

    ## labeling
    if(is.character(names)) {
      colnames(cf) <- names
      names <- TRUE
    }

    ## abbreviation
    if(is.logical(abbreviate)) {
      nlab <- max(nchar(colnames(cf)))
      abbreviate <- if(abbreviate) as.numeric(cut(nlab, c(-Inf, 1.5, 4.5, 7.5, Inf))) else nlab
    }
    colnames(cf) <- abbreviate(colnames(cf), abbreviate)
    
    if(index) {
      x <- 1:NCOL(cf)
      if(is.null(xscale)) xscale <- range(x) + c(-0.1, 0.1) * diff(range(x))
    } else {
      x <- rep(0, length(cf))
      if(is.null(xscale)) xscale <- c(-1, 1)      
    }
    if(is.null(yscale)) yscale <- range(cf) + c(-0.1, 0.1) * diff(range(cf))
         
    ## panel function for bt plots in nodes
    rval <- function(node) {

      ## node index
      id <- id_node(node)
    
      ## dependent variable setup
      cfi <- cf[id,]

      ## viewport setup
      top_vp <- viewport(layout = grid.layout(nrow = 2, ncol = 3,
    			 widths = unit(c(ylines, 1, 1), c("lines", "null", "lines")),  
        		 heights = unit(c(1, 1), c("lines", "null"))),
    			 width = unit(1, "npc"), 
    			 height = unit(1, "npc") - unit(2, "lines"),
        		 name = paste("node_btplot", id, sep = ""))
      pushViewport(top_vp)
      grid.rect(gp = gpar(fill = "white", col = 0))

      ## main title
      top <- viewport(layout.pos.col = 2, layout.pos.row = 1)
      pushViewport(top)
      mainlab <- paste(ifelse(id, paste("Node", id, "(n = "), ""),
        	       info_node(node)$nobs, ifelse(id, ")", ""), sep = "")
      grid.text(mainlab)
      popViewport()

      ## actual plot  
      plot_vpi <- viewport(layout.pos.col = 2, layout.pos.row = 2,
        xscale = xscale, yscale = yscale, 
        name = paste("node_btplot", id, "plot", sep = ""))
      pushViewport(plot_vpi)

      grid.lines(xscale, c(cf_ref, cf_ref), gp = gpar(col = linecol), default.units = "native")
      if(index) {
        grid.lines(x, cfi, gp = gpar(col = col, lty = 2), default.units = "native")
        grid.points(x, cfi, gp = gpar(col = col, cex = cex), pch = pch, default.units = "native")
        grid.xaxis(at = x, label = if(names) names(cfi) else x)
      } else {  	
        if(names) grid.text(names(cfi), x = x, y = cfi, default.units = "native")
          else grid.points(x, cfi, gp = gpar(col = col, cex = cex), pch = pch, default.units = "native")
      }
      grid.yaxis(at = c(ceiling(yscale[1] * 100)/100, floor(yscale[2] * 100)/100))
      grid.rect(gp = gpar(fill = "transparent"))

      upViewport(2)
    }
	    
    return(rval)
}
class(node_btplot) <- "grapcon_generator"
@


\begin{document}

\title{Parties, Models, Mobsters\newline
{\fontsize{11}{15}\selectfont A New Implementation of Model-Based Recursive Partitioning in R}}
\author{Achim Zeileis, Torsten Hothorn}
\URL{http://eeecon.uibk.ac.at/~zeileis/}
\lecture{Parties, Models, Mobsters: A New Implementation of Model-Based Recursive Partitioning in R}

\subsection{Overview}

\begin{frame}
\frametitle{Overview}

\begin{itemize}
  \item Motivation: Trees and leaves
  \item Model-based (MOB) recursive partitioning
  \begin{itemize}
    \item Model estimation
    \item Tests for parameter instability
    \item Segmentation
    \item Pruning
    \item Local models
  \end{itemize}
  \item Implementation in R
  \begin{itemize}
    \item Building blocks: Parties, models, mobsters
    \item Old implementation in \emph{party}
    \item All new implementation in \emph{partykit}
  \end{itemize}
  \item Application
  \begin{itemize}
    \item Paired comparisons for Germany's Topmodel finalists
    \item Bradley-Terry trees
    \item Implementation from scratch
  \end{itemize}
\end{itemize}

\end{frame}


\subsection{Motivation}

\begin{frame}
\frametitle{Motivation: Trees}

Breiman (2001, \textit{Statistical Science}) distinguishes two cultures
of statistical modeling.

\medskip

\begin{itemize}
  \item \textbf{Data models:} Stochastic models, typically parametric.\\
        $\rightarrow$ Classical strategy in statistics.
	Regression models are still the workhorse for many empirical analyses.
  \item \textbf{Algorithmic models:} Flexible models, data-generating process unknown.
        $\rightarrow$ Still few applications in many fields, e.g., social sciences or economics.
\end{itemize}

\medskip

\textbf{Classical example}: Trees, i.e., modeling of dependent variable $y$ by ``learning''
a recursive partition w.r.t\ explanatory variables $z_1, \dots, z_l$.

\end{frame}

\begin{frame}
\frametitle{Motivation: Leaves}

\textbf{Key features}:
\begin{enumerate}
  \item Predictive power in nonlinear regression relationships.
  \item Interpretability (enhanced by visualization), i.e., no ``black box'' methods.
\end{enumerate}

\bigskip

\textbf{Typically:} Simple models for univeriate $y$, e.g., mean.

\bigskip

\textbf{Idea:} More complex models for more complex $y$, e.g.,
regression models, multivariate normal model, item responses, etc.

\bigskip

\textbf{Here:} Synthesis of parametric data models and algorithmic tree models.

\bigskip

\textbf{Goal:} Fitting local models by partitioning of the sample space.

\end{frame}


\subsection{Recursive partitioning}

\begin{frame}
\frametitle{Recursive partitioning}

\textbf{Model-based (MOB) algorithm}:

\begin{enumerate}
  \item Fit the parametric model in the current subsample.
  \item Assess the stability of the parameters across each partitioning variable $z_j$.
  \item Split sample along the $z_{j^*}$ with strongest instability:
        Choose breakpoint with highest improvement of the model fit.
  \item Repeat steps 1--3 recursively in the subsamples
        until some stopping criterion is met.
\end{enumerate}

\end{frame}

\begin{frame}[fragile]
\frametitle{Recursive partitioning}

\textbf{Example:} Logistic regression, assessing differences in
the effect of ``preferential treatment'' (``women and children first''?)
in the Titanic survival data.

\bigskip

\textbf{In R:} Generalized linear model tree with binomial family (and default logit link).

<<ttnc, results=hide>>=
mb <- glmtree(Survived ~ Treatment | Age + Gender + Class,
  data = ttnc, family = binomial, alpha = 0.05, prune = "BIC")
plot(mb)
print(mb)
@

\bigskip

\textbf{Result:} Log-odds ratio of survival given treatment differs across
classes (slope), as does the survival probability of male adults (intercept).


\end{frame}

\begin{frame}
\frametitle{Recursive partitioning}

\vspace*{-0.3cm}

\setkeys{Gin}{width=1.1\textwidth}
\hspace*{-0.4cm}%
<<ttnc-plot, fig=TRUE, height=7, width=12, echo=FALSE, results=hide>>=
plot(mb, tp_args = list(margins = c(1.5, 2, 1.5, 2.5)))
@

\end{frame}

\begin{frame}[fragile]
\frametitle{Recursive partitioning}

<<ttnc-print, echo=FALSE>>=
ttnc$Treatment <- factor(ttnc$Treatment, labels = c("Normal", "Preferential"))
mb0 <- glmtree(Survived ~ Treatment | Age + Gender + Class,
  data = ttnc, family = binomial, alpha = 0.05, prune = "BIC")
mb0
@

\end{frame}


\subsection{Model estimation}

\begin{frame}
\frametitle{1.~Model estimation}

\textbf{Models:} $\mathcal{M}(y, x, \theta)$ with (potentially multivariate)
observations $y$, optionally regressors $x$, and $k$-dimensional parameter vector
$\theta \in \Theta$.

\bigskip

\textbf{Parameter estimation:} $\widehat \theta$ by optimization of
additive objective function $\Psi(y, x, \theta)$ for $n$ observations $y_i$ ($i = 1, \dots, n$):

\[ \widehat \theta \quad = \quad \argmin_{\theta \in \Theta}
   \sum_{i = 1}^n \Psi(y_i, x_i, \theta). \]

\bigskip

\textbf{Special cases:} Maximum likelihood (ML), weighted and ordinary
least squares (OLS and WLS), quasi-ML, and other M-estimators.

\end{frame}


\begin{frame}
\frametitle{1.~Model estimation}

\textbf{Estimating function:} $\widehat{\theta}$ can also be defined in terms of
  \[ \sum_{i = 1}^n \psi(y_i, x_i, \widehat{\theta}) = 0, \]
where $\psi(y, x, \theta) = {\partial \Psi(y, x, \theta)}/{\partial \theta}$.

\bigskip

\textbf{Central limit theorem:} If there is a true parameter $\theta_0$ and given
certain weak regularity conditions:
  \[ \sqrt{n} (\widehat{\theta} - \theta_0) \quad \darrow \quad \mathcal{N}(0, V(\theta_0)), \]
where $V(\theta_0) = \{A(\theta_0)\}^{-1} B(\theta_0) \{A(\theta_0)\}^{-1}$.
$A$ and $B$ are the expectation of the derivative of $\psi$ and
the variance of $\psi$, respectively.

\end{frame}


\begin{frame}
\frametitle{1.~Model estimation}

\textbf{Idea:}
In many situations, a single global model 
$\mathcal{M}(y, x, \theta)$ that fits \textbf{all} $n$~observations cannot be found.
But it might be possible to find a partition w.r.t.\ the variables
$z_1, \dots, z_l$ so that a well-fitting model can be found locally
in each cell of the partition.

\bigskip

\textbf{Tools:}
\begin{itemize}
  \item Assess parameter instability w.r.t\ to partitioning variables
        $z_j \; (j = 1, \dots, l)$.
  \item A general measure of deviation from the model is the estimating function $\psi(y, x, \theta)$.
\end{itemize}

\end{frame}


\subsection{Parameter instability tests}

\begin{frame}
\frametitle{2.~Tests for parameter instability}

Generalized M-fluctuation tests
capture instabilities in $\widehat \theta$ for an ordering w.r.t\ $z_j$.

\bigskip

\textbf{Basis:} Empirical fluctuation process of cumulative deviations
w.r.t.\ to an ordering $\sigma(z_{ij})$.
\[
W_j(t, \widehat \theta)
  \quad = \quad {\widehat B}^{-1/2} n^{-1/2}
                \sum_{i = 1}^{\lfloor nt \rfloor}
		\psi(y_{\sigma(z_{ij})}, x_{\sigma(z_{ij})}, \widehat \theta)
\qquad (0 \le t \le 1)
\]

\bigskip

\textbf{Functional central limit theorem:} Under parameter stability
$W_j(\cdot) \, \darrow \, W^0(\cdot)$, where
$W^0$ is a $k$-dimensional Brownian bridge.

\end{frame}

\begin{frame}
\frametitle{2.~Tests for parameter instability}

\setkeys{Gin}{width=1.08\textwidth}
\hspace*{-0.7cm}%
<<fig=TRUE, height=5, width=10, echo=FALSE, results=hide>>=
## artificial data
set.seed(42)
a <- rnorm(41, mean = 0, sd = 0.12)
b <- rnorm(41, mean = 1, sd = 0.12)
b[40] <- b[40]-0.1
b[41] <- b[41]-0.2
a[1] <- a[1]+0.2
a[2] <- a[2]+0.1
x <- c(b, a)

par(mar = c(5, 5, 3, 1), mfrow = c(1, 2))

## data, model, and residuals
plot(0:81, x, xlab = expression(z[j]), ylab = "y", ylim = c(-2, 3),
  type = "l", xlim = c(1, 80), axes=FALSE)
axis(1,at=seq(0,80,20),labels=seq(2004,2012,2),cex.axis=1.2, cex.lab=1.2)
axis(2,at=seq(-2,3,1),labels=seq(200,1200,200),cex.axis=1.2, cex.lab=1.2)
box()
abline(h = mean(x), lty = 3)
for(i in c(5, 15, 25, 35)) {
  segments(x0 = i, x1 = i, y0 = x[i+1], y1 = mean(x), lty = 2)
  i <- i + 40
  segments(x0 = i, x1 = i, y0 = mean(x), y1 = x[i+1], lty = 2)
}

## CUSUM graph
x_efp <- c(0, cumsum(x - mean(x)))/sqrt(length(x) * var(x))
plot(-1:81, x_efp, xlab = expression(z[j]), ylab = "Fluctuation Process", 
  type = "l", xlim = c(1, 80), ylim = c(-5, 5), axes=FALSE)
axis(1,at=seq(0,80,20),labels=seq(2004,2012,2),cex.axis=1.2, cex.lab=1.2)
axis(2, cex.axis=1.2, cex.lab=1.2)
box()
abline(h = 0, lty = 3)
for(i in seq(5, 75, by = 10)) segments(x0 = i, x1 = i, y0 = 0, y1 = x_efp[i+2], lty = 2)
@

\end{frame}

\begin{frame}
\frametitle{2.~Tests for parameter instability}

\textbf{Test statistics:} Scalar functional $\lambda(W_j)$
that captures deviations from zero.

\bigskip

\textbf{Null distribution:} Asymptotic distribution of $\lambda(W^0)$.

\bigskip

\textbf{Special cases:} Class of test encompasses many well-known
tests for different classes of models.
Certain functionals~$\lambda$ are particularly intuitive for numeric and
categorical $z_j$, respectively.

\bigskip

\textbf{Advantage:} Model $\mathcal{M}(y, x, \widehat \theta)$ just has to be
estimated once. Empirical estimating functions $\psi(y_i, x_i, \widehat \theta)$
just have to be re-ordered and aggregated for each $z_j$.

\end{frame}


\begin{frame}
\frametitle{2.~Tests for parameter instability}

\textbf{Splitting numeric variables:} Assess instability using
sup$\mathit{LM}$ statistics.

\[
\lambda_{\mathit{supLM}} (W_j) \quad = \quad
\max_{i = \ui, \dots, \oi}  \, \left(\frac{i}{n} \cdot \frac{n-i}{n} \right)^{-1}
\left| \left| W_j \left( \frac{i}{n} \right) \right| \right|^2_2.
\]

\bigskip

\textbf{Interpretation:} Maximization of single shift $\mathit{LM}$ statistics
for all conceivable breakpoints in $[\ui, \oi]$.

\bigskip

\textbf{Limiting distribution:} Supremum of a squared, $k$-dimensional tied-down
Bessel process.

\bigskip

\textbf{Potential alternatives:} Many other parameter instability tests from the same
class of tests, e.g., a Cram\'{e}r-von Mises test (or Nyblom-Hansen test),
MOSUM tests, etc.

\end{frame}


\begin{frame}
\frametitle{2.~Tests for parameter instability}

\textbf{Splitting categorical variables:} Assess instability using
$\chi^2$ statistics.

\[ \lambda_{\chi^2} (W_j) \quad = \quad
\sum_{c = 1}^C 
\frac{n}{|I_c|}  \left| \left| \Delta_{I_c} W_j \left( \frac{i}{n} \right) \right| \right|^2_2.
\]

\bigskip

\textbf{Feature:} Invariant for re-ordering of the $C$ categories and the
observations within each category.

\bigskip

\textbf{Interpretation:} Capture instability for split-up into $C$ categories.

\bigskip

\textbf{Limiting distribution:} $\chi^2$ with $k \cdot (C-1)$ degrees of freedom.

\end{frame}

\begin{frame}
\frametitle{2.~Tests for parameter instability}

\textbf{Splitting ordinal variables:} Several strategies conceivable.
Assess instability either as for categorical variables (if $C$ is low),
or as for numeric variables (if $C$ is high), or via a specialized test.
\begin{eqnarray*}
  \lambda_\mathit{maxLMo}(W_j) & = & \max_{i \in \{i_1, \dots, i_{C-1} \}} ~
    \left(\frac{i}{n} \cdot \frac{n-i}{n} \right)^{-1}
    \left| \left| W_j \left( \frac{i}{n} \right) \right| \right|^2_2,\\
  \lambda_\mathit{WDMo}(W_j) & = & \max_{i \in \{i_1, \dots, i_{C-1} \}} ~
    \left(\frac{i}{n} \cdot \frac{n-i}{n} \right)^{-1/2}      
    \left| \left| W_j\left( \frac{i}{n} \right) \right| \right|_{\infty}.
\end{eqnarray*}

\bigskip

\textbf{Interpretation:} Assess only the possible splitpoints $i_1, \dots, i_{C-1}$,
based on $L_2$ or $L_\infty$ norm.

\bigskip

\textbf{Limiting distribution:} Maximum from selected points in a squared
Bessel process or multivariate normal distribution, respectively.


\end{frame}


\subsection{Segmentation}

\begin{frame}
\frametitle{3.~Segmentation}

\textbf{Goal:} Split model into $b = 1, \dots, B$ segments along the
partitioning variable $z_j$ associated with the highest parameter instability.
Local optimization of

\[ \sum_b \sum_{i \in I_b} \Psi(y_i, x_i, \theta_b). \]

\bigskip

$B = 2$: Exhaustive search of order $O(n)$.

\bigskip

$B > 2$: Exhaustive search is of order $O(n^{B-1})$, but can be replaced
by dynamic programming of order $O(n^2)$. Different methods (e.g., information
criteria) can choose $B$ adaptively.

\bigskip

\textbf{Here:} Binary partitioning. Optionally, $B = C$ can be chosen (without search)
for categorical variables.

\end{frame}


\subsection{Pruning}

\begin{frame}
\frametitle{4.~Pruning}

\textbf{Goal:} Avoid overfitting.

\bigskip

\textbf{Pre-pruning:}
\begin{itemize}
  \item Internal stopping criterium.
  \item Stop splitting when there is no significant parameter instability.
  \item Based on Bonferroni-corrected $p$~values of the fluctuation tests.
\end{itemize}

\bigskip

\textbf{Post-pruning:}
\begin{itemize}
  \item Grow large tree (e.g., with high significance level).
  \item Prune splits that do not improve the model fit
        based on information criteria (e.g., AIC or BIC).
\end{itemize}

\bigskip

\textbf{Hyperparameters:} Significance level and information criterion penalty
can be chosen manually (or possibly through cross-validation etc.).

\end{frame}


\subsection{Local models}

\begin{frame}
\frametitle{Local models}

\textbf{Goals:}
\begin{itemize}
  \item Detection of interactions and nonlinearities in regressions.
  \item Add explanatory variables for models without regressors.
  \item Detect violations of parameter stability (measurement invariance)
    across several variables adaptively.
\end{itemize}

\bigskip

\textbf{Mobsters:}
\begin{itemize}
  \item Linear and generalized linear model trees (Zeileis \emph{et al.} 2008).
  \item Censored survival regression trees: parametric proportional hazard
    and accelerated failure time models (Zeileis \emph{et al.}~2008).
  \item Beta regression trees (Gr\"un \emph{et al.}~2012).
  \item Bradley-Terry trees for paired comparisons (Strobl \emph{et al.}~2011).
  \item Item response theory (IRT) trees: Rasch, rating scale and partial credit model
    (Strobl \emph{et al.}~2014, Abou El-Komboz \emph{et al.}~2014).
\end{itemize}

\end{frame}


\subsection{Implementation in R}

\begin{frame}
\frametitle{Implementation: Building blocks}

\textbf{Workhorse function:} \code{mob()} for
\begin{itemize}
  \item data handling,
  \item calling model fitters,
  \item carrying out parameter instability tests and
  \item recursive partitioning algorithm.
\end{itemize}

\bigskip

\textbf{Required functionality:}
\begin{itemize}
  \item \emph{Parties:} Class and methods for recursive partytions.
  \item \emph{Models:} Fitting functions for statistical models (optimizing suitable objective function).
  \item \emph{Mobsters:} High-level interfaces (\code{lmtree()}, \code{bttree()}, \dots)
    that call lower-level \code{mob()} with suitable options and methods.
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Implementation: Old \texttt{mob()} in \emph{party}}

\textbf{Parties:} S4 class `\code{BinaryTree}'.
\begin{itemize}
  \item Originally developed only for \code{ctree()} and somewhat ``abused''.
  \item Rather rigid and hard to extend.
\end{itemize}

\medskip

\textbf{Models:} S4 `\code{StatModel}' objects.
\begin{itemize}
  \item Intended to conceptualize unfitted model objects.
  \item Required some ``glue code'' to accomodate non-standard
    interface for data handling and model fitting.
\end{itemize}

\medskip

\textbf{Mobsters:}
\begin{itemize}
  \item \code{mob()} already geared towards (generalized) linear models.
  \item Other interfaces in \emph{psychotree} and \emph{betareg}.
  \item Hard to do fine control due to adopted S4 classes: Many unnecessary
    computations and copies of data.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Implementation: New \texttt{mob()} in \emph{partykit}}

\textbf{Parties:} S3 class `\code{modelparty}' built on `\code{party}'.
\begin{itemize}
  \item Separates data and tree structure.
  \item Inherits generic infrastructure for printing, predicting,
    plotting, \dots
\end{itemize}

\medskip

\textbf{Models:} Plain functions with input/output convention.
\begin{itemize}
  \item Basic and extended interface for rapid prototyping and for
    speeding up computings, respectively.
  \item Only minimial glue code required if models are well-designed.
\end{itemize}

\medskip

\textbf{Mobsters:}
\begin{itemize}
  \item \code{mob()} completely agnostic regarding models employed.
  \item Separate interfaces \code{lmtree()}, \code{glmtree()}, \dots
  \item New interfaces typically need to bring their model fitter and adapt the
    main methods \code{print()}, \code{plot()}, \code{predict()} etc.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Implementation: New \texttt{mob()} in \emph{partykit}}

\textbf{New inference options:} Not used by default by optionally available.
\begin{itemize}
  \item New parameter instability tests for ordinal partitioning variables.
    Alternative to unordered $\chi^2$ test but computationally intensive.
  \item Post-pruning based on information criteria (e.g., AIC or BIC), especially
    for very large datasets where traditional significance levels are not useful.
  \item Multiway splits for categorical partitioning variables.
  \item Treat weights as proportionality weights and not as case weights.
\end{itemize}

\end{frame}

\begin{frame}[fragile]
\frametitle{Implementation: Models}

\textbf{Input:} Basic interface.
\begin{Soutput}
  fit(y, x = NULL, start = NULL, weights = NULL,
    offset = NULL, ...)
\end{Soutput}
\code{y}, \code{x}, \code{weights}, \code{offset} are (the subset of)
the preprocessed data.

Starting values and further fitting arguments are in \code{start} and \code{...}.

\bigskip

\textbf{Output:} Fitted model object of class with suitable methods.
\begin{itemize}
  \item \code{coef()}: Estimated parameters $\hat \theta$.
  \item \code{logLik()}: Maximized log-likelihood function $-\sum_i \Psi(y_i, x_, \hat \theta)$.
  \item \code{estfun()}: Empirical estimating functions
    $\Psi'(y_i, x_i, \hat \theta)$.
\end{itemize}

\end{frame}

\begin{frame}[fragile]
\frametitle{Implementation: Models}

\textbf{Input:} Extended interface.
\begin{Soutput}
  fit(y, x = NULL, start = NULL, weights = NULL,
    offset = NULL, ..., estfun = FALSE, object = FALSE)
\end{Soutput}

\medskip

\textbf{Output:} List.
\begin{itemize}
  \item \code{coefficients}: Estimated parameters $\hat \theta$.
  \item \code{objfun}: Minimized objective function $\sum_i \Psi(y_i, x_, \hat \theta)$.
  \item \code{estfun}: Empirical estimating functions
    $\Psi'(y_i, x_i, \hat \theta)$. Only needed if \code{estfun = TRUE}, otherwise optionally \code{NULL}.
  \item \code{object}: A model object for which further methods could
    be available (e.g., \code{predict()}, or \code{fitted()}, etc.).
    Only needed if \code{object = TRUE}, otherwise optionally \code{NULL}.
\end{itemize}


\medskip

\textbf{Internally:} Extended interface constructed from basic interface if supplied.
Efficiency can be gained through extended approach.

\end{frame}

\begin{frame}[fragile]
\frametitle{Implementation: Parties}

\textbf{Class:} `\code{modelparty}' inheriting from `\code{party}'.

\medskip

\textbf{Main addition:} Data handling for regressor and partitioning variables.
\begin{itemize}
  \item The \emph{Formula} package is used for two-part formulas, e.g.,\\
    \code{y ~ x1 + x2 | z1 + z2 + z3}.
  \item The corresponding terms are stored for the combined
    model and only for the partitioning variables.
\end{itemize}

\medskip

\textbf{Additional information:} In \code{info} slots of `\code{party}' and `\code{partynode}'.
\begin{itemize}
  \item \code{call}, \code{formula}, \code{Formula}, \code{terms} (partitioning variables only),
    \code{fit}, \code{control}, \code{dots}, \code{nreg}.
  \item \code{coefficients}, \code{objfun}, \code{object}, \code{nobs}, \code{p.value}, \code{test}.
\end{itemize}

\medskip

\textbf{Reusability:} Could in principle be used for other model trees as well (inferred
by other algorithms than MOB).

\end{frame}


\subsection{Bradley-Terry trees}

\begin{frame}
\frametitle{Bradley-Terry trees}

\hspace*{0.2cm}%
\includegraphics[width=0.23\textwidth]{Barbara.jpg} \hspace*{0.1cm}
\includegraphics[width=0.23\textwidth]{Anni.jpg} \hspace*{0.1cm}
\includegraphics[width=0.23\textwidth]{Hana.jpg} \hspace*{0.1cm}
\includegraphics[width=0.23\textwidth]{Fiona.jpg}

\vspace*{0.1cm}
\hspace*{0.2cm}%
\includegraphics[width=0.23\textwidth]{Mandy.jpg} \hspace*{0.1cm}
\includegraphics[width=0.23\textwidth]{Anja.jpg} \hspace*{0.2cm}%
\begin{tabular}{p{0.48\textwidth}}
\textbf{Questions:} Which of these

women is more attractive?

How does the answer depend on

age, gender, and the familiarity

with the associated TV show

Germany's Next Topmodel?
\vspace*{3.4cm}
\end{tabular}

\end{frame}

\begin{frame}
\frametitle{Bradley-Terry trees}

\textbf{Task:} Preference scaling of attractiveness.

\bigskip

\textbf{Data:} Paired comparisons of attractiveness.
\begin{itemize}
  \item \emph{Germany's Next Topmodel~2007} finalists:
    Barbara, Anni, Hana, Fiona, Mandy, Anja.
  \item Survey with 192~respondents at Universit{\"a}t T{\"u}bingen.
  \item Available covariates: Gender, age, familiarty with the TV~show.
  \item Familiarity assessed by yes/no questions:
    (1)~Do you recognize the women?/Do you know the show?
    (2)~Did you watch it regularly?
    (3)~Did you watch the final show?/Do you know who won?
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Bradley-Terry trees}

\textbf{Model:} Bradley-Terry (or Bradley-Terry-Luce) model.
\begin{itemize}
  \item Standard model for paired comparisons in social sciences.
  \item Parametrizes probability $\pi_{ij}$ for preferring object $i$ over $j$
    in terms of corresponding ``ability'' or ``worth'' parameters $\theta_i$.
    \begin{eqnarray*}
          \pi_{ij}\phantom{)}  & = & \frac{\theta_i}{\theta_i + \theta_j} \\
      \mathsf{logit}(\pi_{ij}) & = & \log(\theta_i) - \log(\theta_j)
    \end{eqnarray*}
  \item Maximum likelihood as a logistic or log-linear GLM.
\end{itemize}

\bigskip

\textbf{Mobster:} \code{bttree()} in \emph{psychotree} (Strobl \emph{et al.}~2011).

\bigskip

\textbf{Here:} Use \code{mob()} directly to build model from scratch using \code{btReg.fit()} from \emph{psychotree}.

\end{frame}

\begin{frame}
\frametitle{Bradley-Terry trees}

<<topmodel-tree, echo=FALSE, results=hide>>=
bt0 <- mob(preference ~ 1 | gender + age + q1 + q2 + q3,
  data = Topmodel2007, fit = btfit2)
@

\vspace*{-0.5cm}

\setkeys{Gin}{width=1.15\textwidth}
\hspace*{-0.7cm}%
<<topmodel-plot, echo=FALSE, results=hide, fig=TRUE, height=6.2, width=10>>=
plot(bt0, drop = TRUE, tnex = 2,
  terminal_panel = node_btplot(bt0, abbreviate = 1, yscale = c(0, 0.5)))
@

\end{frame}


\begin{frame}[fragile]
\frametitle{Bradley-Terry trees}

Data, packages, and \code{estfun()} method:
<<packages, eval=FALSE>>=
data("Topmodel2007", package = "psychotree")
library("partykit")
library("psychotools")
<<estfun.btReg>>
@

\medskip

Basic model fitting function:

<<btfit1>>=
btfit1 <- function(y, x = NULL, start = NULL, weights = NULL,
  offset = NULL, ...) btReg.fit(y, weights = weights, ...)
@


\medskip

Fit Bradley-Terry tree:

<<bt1>>=
system.time(bt1 <- mob(
  preference ~ 1 | gender + age + q1 + q2 + q3,
  data = Topmodel2007, fit = btfit1))
@

\end{frame}

\begin{frame}[fragile]
\frametitle{Bradley-Terry trees}

Extended model fitting function:

<<btfit2-2>>=
<<btfit2>>
@

\medskip

Fit Bradley-Terry tree again:

<<bt2>>=
system.time(bt2 <- mob(
  preference ~ 1 | gender + age + q1 + q2 + q3,
  data = Topmodel2007, fit = btfit2))
@

\end{frame}

\begin{frame}[fragile]
\frametitle{Bradley-Terry trees}

<<bt2-print, echo=FALSE>>=
out <- capture.output(bt2)
writeLines(out[1:21])
@

\end{frame}

\begin{frame}[fragile]
\frametitle{Bradley-Terry trees}

<<bt2-print, echo=FALSE>>=
out <- capture.output(bt2)
writeLines(out[-(1:22)])
@

\medskip

Standard methods readily available:
<<bt2-coef>>=
plot(bt2)
coef(bt2)
@

\medskip

Customization:
<<bt2-worthf, eval=FALSE>>=
worthf <- function(info) paste(info$object$labels,
  format(round(worth(info$object), digits = 2)), sep = ": ")
plot(bt2, FUN = worthf)
@

\code{}

\end{frame}

\begin{frame}
\frametitle{Bradley-Terry trees}

\vspace*{-0.5cm}

\setkeys{Gin}{width=1.15\textwidth}
\hspace*{-0.7cm}%
<<bt2-plot, fig=TRUE, height=7.4, width=12, echo=FALSE>>=
plot(bt2)
@

\end{frame}

\begin{frame}[fragile]
\frametitle{Bradley-Terry trees}

\vspace*{-0.5cm}

\setkeys{Gin}{width=1.15\textwidth}
\hspace*{-0.7cm}%
<<bt2-plot2, fig=TRUE, height=7.4, width=12, echo=FALSE>>=
<<bt2-worthf>>
@

\end{frame}

\begin{frame}[fragile]
\frametitle{Bradley-Terry trees}

\vspace*{-0.5cm}

\setkeys{Gin}{width=\textwidth}
<<bt2-nodeapply, fig=TRUE, height=7, width=9, echo=FALSE, results=hide>>=
par(mfrow = c(2, 2))
nodeapply(bt2, ids = c(3, 5, 6, 7), FUN = function(n)
  plot(n$info$object, main = n$id, ylim = c(0, 0.4)))
@

\end{frame}

\begin{frame}[fragile]
\frametitle{Bradley-Terry trees}

Apply plotting in all terminal nodes:
<<nodeapply, eval=FALSE>>=
<<bt2-nodeapply>>
@

\medskip

<<tm, echo=FALSE>>=
tm <- data.frame(age = c(60, 25, 35), gender = c("male", "female", "female"),
  q1 = "no", q2 = c("no", "no", "yes"), q3 = "no")
@

\medskip

Predicted nodes and ranking:
<<predict>>=
tm
predict(bt2, tm, type = "node")
predict(bt2, tm, type = function(object) t(rank(-worth(object))))
@
%predict(bt2, tm, type = function(object) t(worth(object)))

\end{frame}

\subsection{Summary}

\begin{frame}
\frametitle{Summary}

\begin{itemize}
  \item Synthesis of parametric data models and algorithmic tree models.
  \item Based on modern class of parameter instability tests.
  \item Aims to minimize clearly defined objective function by greedy forward search.
  \item Can be applied general class of parametric models.
  \item Alternative to traditional means of model specification,
        especially for variables with unknown association.
  \item All new implementation in \emph{partykit}.
  \item Enables more efficient computations, rapid prototyping, flexible customization.
\end{itemize}

\end{frame}

\subsection{References}

\begin{frame}
\frametitle{References}

\scriptsize

\textbf{Software:}

\medskip

Hothorn T, Zeileis A (2014).
 \textit{partykit: A Toolkit for Recursive Partytioning.}
 R~package version~0.8-0.
 URL~\url{http://R-Forge.R-project.org/projects/partykit/}

\medskip

Zeileis A, Hothorn T (2014).
 \textit{Parties, Models, Mobsters: A New Implementation of Model-Based Recursive Partitioning in R.}
 \texttt{vignette("mob",~package~=~"partykit")}

\medskip

Zeileis A, Croissant Y (2010).
  \dquote{Extended Model Formulas in R: Multiple Parts and Multiple Responses.}
  \textit{Journal of Statistical Software}, \textbf{34}(1), 1--13. 
  URL~\url{http://www.jstatsoft.org/v34/i01/}

\bigskip

\textbf{Inference:}

\medskip

Zeileis A, Hornik K (2007).
 \dquote{Generalized {M}-Fluctuation Tests for Parameter Instability.}
 \textit{Statistica Neerlandica},
 \textbf{61}(4), 488--508.
 \doi{10.1111/j.1467-9574.2007.00371.x}

\medskip

Merkle EC, Zeileis A (2013).
 \dquote{Tests of Measurement Invariance without Subgroups: A Generalization of Classical Methods.}
 \textit{Psychometrika}, \textbf{78}(1), 59--82.
 \doi{10.1007/S11336-012-9302-4}

\medskip

Merkle EC, Fan J, Zeileis A (2014).
 \dquote{Testing for Measurement Invariance with Respect to an Ordinal Variable.}
 \textit{Psychometrika}. Forthcoming.

\end{frame}

\begin{frame}
\frametitle{References}

\scriptsize

\textbf{Trees:}

\medskip

Zeileis A, Hothorn T, Hornik K (2008).
 \dquote{Model-Based Recursive Partitioning.}
  \textit{Journal of Computational and Graphical Statistics},
  \textbf{17}(2), 492--514.
  \doi{10.1198/106186008X319331}

\medskip

Strobl C, Wickelmaier F, Zeileis A (2011).
  \dquote{Accounting for Individual Differences in Bradley-Terry Models by Means of Recursive Partitioning.}
  \textit{Journal of Educational and Behavioral Statistics}, \textbf{36}(2), 135--153.
  \doi{10.3102/1076998609359791}

\medskip

Gr\"un B, Kosmidis I, Zeileis A (2012).
  \dquote{Extended Beta Regression in R: Shaken, Stirred, Mixed, and Partitioned.}
  \textit{Journal of Statistical Software}, \textbf{48}(11), 1--25.
  URL~\url{http://www.jstatsoft.org/v48/i11/}

\medskip

Rusch T, Lee I, Hornik K, Jank W, Zeileis A (2013).
  \dquote{Influencing Elections with Statistics: Targeting Voters with Logistic Regression Trees.}
  \textit{The Annals of Applied Statistics}, \textbf{7}(3), 1612--1639.
  \doi{10.1214/13-AOAS648}

\medskip

Strobl C, Kopf J, Zeileis A (2013).
  \dquote{A New Method for Detecting Differential Item Functioning in the Rasch Model.}
  \textit{Psychometrika}. Forthcoming.
  \doi{10.1007/s11336-013-9388-3}

\medskip

Abou El-Komboz B, Zeileis A, Strobl C (2014).
  \dquote{Detecting Differential Item and Step Functioning with Rating Scale and Partial Credit Trees.}
  \textit{Technical Report 152}, Department of Statistics, Ludwig-Maximilians-Universit\"at M\"unchen.
  URL~\url{http://epub.ub.uni-muenchen.de/17984/}

\end{frame}

\end{document}
