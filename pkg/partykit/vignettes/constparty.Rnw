\documentclass[nojss]{jss}

%\VignetteIndexEntry{partykit: A Toolkit for Recursive Partytioning}
%\VignetteDepends{partykit, rpart, RWeka, pmml, datasets}
%\VignetteKeywords{recursive partitioning, regression trees, classification trees, decision trees}
%\VignettePackage{partykit}

%% packages
\usepackage{amstext}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{thumbpdf}
\usepackage{rotating}
%% need no \usepackage{Sweave}

%% additional commands
\newcommand{\squote}[1]{`{#1}'}
\newcommand{\dquote}[1]{``{#1}''}
\newcommand{\fct}[1]{{\texttt{#1()}\index{#1@\texttt{#1()}}}}
\newcommand{\class}[1]{\dquote{\texttt{#1}}}
\newcommand{\fixme}[1]{\emph{\marginpar{FIXME} (#1)}}

%% further commands
\renewcommand{\Prob}{\mathbb{P} }
\renewcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\Var}{\mathbb{V}}
\newcommand{\R}{\mathbb{R} }
\newcommand{\N}{\mathbb{N} }
\newcommand{\C}{\mathbb{C} }
\newcommand{\argmin}{\operatorname{argmin}\displaylimits}
\newcommand{\argmax}{\operatorname{argmax}\displaylimits}
\newcommand{\LS}{\mathcal{L}_n}
\newcommand{\TS}{\mathcal{T}_n}
\newcommand{\LSc}{\mathcal{L}_{\text{comb},n}}
\newcommand{\LSbc}{\mathcal{L}^*_{\text{comb},n}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\yn}{y_{\text{new}}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\sX}{\mathcal{X}}
\newcommand{\sY}{\mathcal{Y}}
\newcommand{\T}{\mathbf{T}}
\newcommand{\x}{\mathbf{x}}
\renewcommand{\a}{\mathbf{a}}
\newcommand{\xn}{\mathbf{x}_{\text{new}}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\ws}{\mathbf{w}_\cdot}
\renewcommand{\t}{\mathbf{t}}
\newcommand{\M}{\mathbf{M}}
\renewcommand{\vec}{\text{vec}}
\newcommand{\B}{\mathbf{B}}
\newcommand{\K}{\mathbf{K}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\cellx}{\pi_n[\x]}
\newcommand{\partn}{\pi_n(\mathcal{L}_n)}
\newcommand{\err}{\text{Err}}
\newcommand{\ea}{\widehat{\text{Err}}^{(a)}}
\newcommand{\ecv}{\widehat{\text{Err}}^{(cv1)}}
\newcommand{\ecvten}{\widehat{\text{Err}}^{(cv10)}}
\newcommand{\eone}{\widehat{\text{Err}}^{(1)}}
\newcommand{\eplus}{\widehat{\text{Err}}^{(.632+)}}
\newcommand{\eoob}{\widehat{\text{Err}}^{(oob)}}
\newcommand{\bft}{\mathbf{t}}

\hyphenation{Qua-dra-tic}

\title{Growing and Handling Trees with Constant Fits}
\Plaintitle{Growing and Handling Trees with Constant Fits}

\author{Torsten Hothorn\\Universit\"at Z\"urich
   \And Achim Zeileis\\Universit\"at Innsbruck}
\Plainauthor{Torsten Hothorn, Achim Zeileis}

\Abstract{
We describe how one can coerse objects representing trees to objects represented by 
\pkg{partykit} classes and how one can grow trees using the \pkg{partykit}.
}
\Keywords{recursive partitioning, regression trees, classification trees, decision trees}

\Address{
  Torsten Hothorn\\
  Institut f\"ur Sozial- und Pr\"aventivmedizin, Abteilung Biostatistik \\
  Universit\"at Z\"urich \\
  Hirschengraben 84\\
  CH-8001 Z\"urich, Switzerland \\
  E-mail: \email{Torsten.Hothorn@R-project.org}\\
  URL: \url{http://user.math.uzh.ch/hothorn/}\\

  Achim Zeileis\\
  Department of Statistics \\
  Universit\"at Innsbruck \\
  Universit\"atsstr.~15 \\
  6020 Innsbruck, Austria \\
  E-mail: \email{Achim.Zeileis@R-project.org}\\
  URL: \url{http://eeecon.uibk.ac.at/~zeileis/}
}


\begin{document}


\SweaveOpts{eps=FALSE, keep.source=TRUE, eval = TRUE}

<<setup, echo = FALSE, results = hide>>=
options(width = 70)
library("partykit")
set.seed(290875)
@

This vignette describes the handling of trees with constant fits in the
terminal nodes.  This class of regression models includes most classical
tree algorithms like AID, CHAID, CART, FACT, QUEST, C4.5, etc.  This
document consists of two parts.  In the first part we assume that the trees
were fitted using external software and we describe how these models can be
coerced into objects that can be dealt with using infrastructure implemented
in the \pkg{partykit} package.  We focus on displaying such trees in textual
and graphical ways and on computing predictions.

In the second part we show how one can implement a simple regression tree
algorithm using the \pkg{partykit} tools.

\section{Coercing  tree objects}

For the illustrations, we use the Titanic data set from package \pkg{datasets}, consisting of
four variables on each of the $2201$ Titanic passengers: gender (male, female), age (child, adult), 
and class (1st, 2nd, 3rd, or crew) set-up as follows:
<<Titanic>>=
data("Titanic", package = "datasets")
ttnc <- as.data.frame(Titanic)
ttnc <- ttnc[rep(1:nrow(ttnc), ttnc$Freq), 1:4]
names(ttnc)[2] <- "Gender"
@
The response variable describes wheather or not the passenger survived the sinking of the ship.

\subsection{Coersing rpart objects}

We first fit a classification tree by means of the the \code{rpart} function 
from package \pkg{rpart} \citep{rpart} to this data set:
<<rpart>>=
library("rpart")
(rp <- rpart(Survived ~ ., data = ttnc))
@
The \code{rp} object of class \code{\Sexpr{class(rp)}} can be coerced to an
object of a certain class inheriting from class \code{party} by the
\code{as.party} function
<<rpart_party>>=
(party_rp <- as.party(rp))
@
and, instead of the print method for \code{rpart} objects, the print method
for \code{\Sexpr{class(party_rp)}} objects creates a textual display of the tree
structure.  In a similar way, the corresponding \code{plot} method 
produces a graphical representation of this tree, see Figure~\ref{party_plot}.

\begin{figure}
\begin{center}
<<rpart-plot, fig = TRUE, width = 10, height = 6>>=
plot(party_rp)
@
\caption{\code{rpart} tree of Titanic data plotted using \pkg{partykit}
         infrastructure. \label{party_plot}}
\end{center}
\end{figure}

By default, the \code{predict} method for \code{rpart} object computes conditional
class probabilities. The same numbers are returned by the \code{predict} method
for \Sexpr{class(party_rp)} objects with \code{type = "prob"} argument:
<<rpart-pred>>=
all.equal(predict(rp), predict(party_rp, type = "prob"), 
          check.attributes = FALSE)
@

\subsection{Coercing J48 objects}

The \pkg{RWeka} package \citep{RWeka} provides an interface to the
\proglang{WEKA} machine learning library and we can use the \code{J48} function to
fit a J4.8 tree to the Titanic data
<<J48>>=
library("RWeka")
(j48 <- J48(Survived ~ ., data = ttnc))
@
This object can be coerced to a \code{party} object using
<<J48_party>>=
(party_j48 <- as.party(j48))
@
and, again, the print method from the \pkg{partykit} package creates a textual display. Note
that, unlike the \code{rpart} trees, this tree includes multiway splits. The \code{plot} method
draws this tree, see Figure~\ref{J48_plot}.

\begin{sidewaysfigure}
\begin{center}
<<J48-plot, fig = TRUE, width = 15, height = 9>>=
plot(party_j48)
@
\caption{\code{J48} tree of Titanic data plotted using \pkg{partykit}
         infrastructure. \label{J48_plot}}
\end{center}
\end{sidewaysfigure}

The conditional class probabilities computed by the \code{predict} methods
implemented in packages \pkg{RWeka} and \pkg{partykit} are equivalent:
<<J48-pred>>=
all.equal(predict(j48, type = "prob"), 
          predict(party_j48, type = "prob"), 
          check.attributes = FALSE)
@

\subsection{Importing trees from PMML files}

Some software packages can output a \proglang{PMML} description of a tree,
we use the \pkg{pmml} package to create a \proglang{PMML} file containing
the \code{rpart} tree for the Titanic data
<<PMML-write>>=
library("pmml")
tfile <- tempfile()
write(toString(pmml(rp)), file = tfile)
@
\fixme{does not work}
<<PMML-read>>=
try(party_pmml <- pmmlTreeModel(tfile))
#isTRUE(all.equal(party_rp, party_pmml))
#all.equal(predict(party_rp, newdata = Titanic, type = "prob"), 
#          predict(party_pmml, newdata = Titanic, type = "prob"),
#          check.attributes = FALSE)
@
\fixme{does not work}
<<PMML-Audit>>=
if (!file.exists("AuditTree.xml"))
    download.file("http://www.dmg.org/pmml_examples/rattle_pmml_examples/AuditTree.xml",
                  destfile = "AuditTree.xml")
try(pmmlTreeModel("AuditTree.xml"))
@

\fixme{Seems to work}
<<PMML-Iris>>=
if (!file.exists("IrisTree.xml"))
    download.file("http://www.dmg.org/pmml_examples/rattle_pmml_examples/IrisTree.xml",
                  destfile = "IrisTree.xml")
pmmlTreeModel("IrisTree.xml")
@

<<PMML-Titantic>>=
(ttnc_quest <- pmmlTreeModel(
  file.path(system.file("pmml", package = "partykit"), "ttnc.pmml")))
@

\begin{figure}
<<PMML-Titanic-plot, fig = TRUE>>=
ttnc_cp <- ttnc_quest
ttnc_cp$data <- ttnc[, names(ttnc_quest$data)]
for(n in names(ttnc_cp$data)) {
  if(is.factor(ttnc_cp$data[[n]])) ttnc_cp$data[[n]] <- factor(
    ttnc_cp$data[[n]], levels = levels(ttnc_quest$data[[n]]))
}
ttnc_cp$fitted <- data.frame(
  "(fitted)" = predict(ttnc_quest, ttnc_cp$data, type = "node"),
  "(response)" = ttnc$Survived,
  check.names = FALSE)
ttnc_cp <- as.constparty(ttnc_cp)
plot(ttnc_cp)
@
\end{figure}


\section{Growing a simple regression tree} 

Package \pkg{partykit} does not offer unified infrastructure for growing
trees.  However, once you know how to estimate splits from data, it is
fairly straightforward to implement trees.  Consider a very simple tree
algorithm.  We assume that both response and explantory variables are
numeric.  We search for the binary best split by means of $t$-test
$p$-values, i.e., we cycle through all variables and potential split points
and assess the quality of the split by comparing the distributions of the
response in the so-defined two groups.  We select the feature/split point
combination with lowest two-sided $p$-value, however, only if this result is
significant at level $\alpha = 0.05$.

This strategy can be implemented based on the data (response and explanatory
variables) and some case weights as follows (\code{response} is just the
name of the response and \code{data} is a data frame with all variables):
<<mytree-1, echo = TRUE>>= 
findsplit <- function(response, data, weights) {

  ### extract response values from data
  y <- data[[response]]

  ### cycle through all features
  logpmin <- 0; xselect <- NULL
  for (i in which(names(data) != response)) {

    ### expand data by case weights 
    x <- data[[i]]
    xt <- rep(x, weights)
    yt <- rep(y, weights)

    ### obtain potential split points (not too many)
    qx <- unique(quantile(xt, 
        	 prob = seq(from = 0.1, to = 0.9, by = 0.05)))

    ### assess all potential splits by their two-sided t-test log p-value
    logp <- sapply(qx, function(q) {
      tt <- t.test(yt[xt <= q], yt[xt > q])
      pt(-abs(tt$statistic), tt$parameter, log = TRUE) + log(2)
    })

    ### if the best split in variable i significant AND
    ### better than what we already had store this information
    if (min(logp) < logpmin & min(logp) < log(0.05)) {
      logpmin <- min(logp)
      xselect <- i
      splitpoint <- qx[which.min(logp)]
    }
  }

  ### no significant split found, give up
  if (is.null(xselect)) return(NULL)

  ### return split as partysplit object
  return(partysplit(
      varid = as.integer(xselect),       ### which variable?
      breaks = as.numeric(splitpoint),   ### which split point?
      info = list(pvalue = exp(logpmin)  ### save p-value in addition
  )))
}
@

In order to actually grow a tree on data, 
we have to set-up the recursion for growing a recursive 
\class{partynode} structure:
<<mytree-2, echo = TRUE>>=
growtree <- function(id = 1L, response, data, weights) {

  ### for less than 30 observations stop here
  if (sum(weights) < 30) return(partynode(id = id))

  ### find best split
  sp <- findsplit(response, data, weights)
  ### no split found, stop here
  if (is.null(sp)) return(partynode(id = id))

  ### actually split the data
  kidids <- kidids_split(sp, data = data)

  ### set-up all daugther nodes
  kids <- vector(mode = "list", length = max(kidids))
  for (kidid in 1:max(kidids)) {
  ### select observations for current node
  w <- weights
  w[kidids != kidid] <- 0
  ### get next node id
  if (kidid > 1) {
    myid <- max(nodeids(kids[[kidid - 1]]))
  } else {
    myid <- id
  }
  ### start recursion on this daugther node
  kids[[kidid]] <- growtree(id = as.integer(myid + 1), response, data, w)
  }

  ### return nodes
  return(partynode(id = as.integer(id), split = sp, kids = kids))
}
@

A very rough sketch of formula-based user-interface 
sets-up the data and calls \fct{growtree}:
<<mytree-3, echo = TRUE>>=
mytree <- function(formula, data, weights = NULL) {

  ### name of the response variable
  response <- all.vars(formula)[1]
  ### data without missing values, response comes last
  data <- data[complete.cases(data), c(all.vars(formula)[-1], response)]
  ### data is numeric
  stopifnot(all(sapply(data, is.numeric)))

  if (is.null(weights)) weights <- rep(1L, nrow(data))
  ### weights are case weights, i.e., integers
  stopifnot(length(weights) == nrow(data) &
    max(abs(weights - floor(weights))) < .Machine$double.eps)

  ### grow tree
  nodes <- growtree(id = 1L, response, data, weights)

  ### compute terminal node number for each observation
  fitted <- fitted_node(nodes, data = data)
  ### return rich constparty object
  ret <- party(nodes, 
    data = data,
    fitted = data.frame(
      "(fitted)" = fitted,
      "(response)" = data[[response]],
      "(weights)" = weights,
      check.names = FALSE),
    terms = terms(formula))
  as.constparty(ret)
}
@

We now can fit this tree, for example to the airquality data; the
\fct{print} method provides us with a first overview on the 
resulting model
<<mytree-4, echo = TRUE>>=
(aqt <- mytree(Ozone ~ Solar.R + Wind + Temp, data = airquality))
@

Of course, we can immediately use the \fct{plot} function to obtain a
graphical representation of this tree, the result is given in
Figure~\ref{plottree}.  The default behaviour for trees with numeric
responses is to depict the conditional distribution of the response in each
terminal node by means of a box plot.  Using the \fct{node\_ecdf} terminal
panel function draws the same tree but with the empirical cumulative
distribution functions (Figure~\ref{plottree_ecdf}).

\begin{figure}
\centering
<<mytree-5, echo = TRUE, fig = TRUE, width = 10, height = 6>>=
plot(aqt)
@
\caption{Regression tree fitted by the \fct{mytree} function to the
         \code{airquality} data. \label{plottree}}
\end{figure}

\begin{figure}
\centering
<<mytree-5-ecdf, echo = TRUE, fig = TRUE, width = 10, height = 6>>=
plot(aqt, terminal_panel = node_ecdf)
@
\caption{Regression tree fitted by the \fct{mytree} function to the
         \code{airquality} data with empiricial cumulative distribution
         functions given for each terminal node.  \label{plottree_ecdf}}
\end{figure}

An interesting feature of \code{party} tree is the ability to extract
subtrees:
<<mytree-7, echo = TRUE>>=
aqt4 <- aqt[4]
aqt4
@
which again are objects inheriting from \class{party} and thus
can be plotted easily (see Figure~\ref{subtree}).

\begin{figure}[t!]
\centering
<<mytree-8, echo = TRUE, fig = TRUE, width = 10, height = 6>>=
plot(aqt4)
@
\caption{A subtree of the \code{airquality} regression tree. \label{subtree}}
\end{figure}

We also might be interested in extracting the $p$-values in the
inner nodes in a nicely formatted way:
<<mytree-10, echo = TRUE>>=
fun <- function(x) format.pval(info_split(split_node(x))$pvalue,
  digits = 3, eps = 0.001)
nid <- nodeids(aqt)
iid <- nid[!(nid %in% nodeids(aqt, terminal = TRUE))]
unlist(nodeapply(aqt, ids = iid, FUN = fun))
@

\section{Predictions}

Computing predictions for new observations (here the first ten observations from
the \code{airquality} data frame) is done using the \fct{predict} method:
<<mytree-predict, echo = TRUE>>=
predict(aqt, newdata = airquality[1:10,])
@
The default for numeric responses is to compute conditional means for each
of the rows in \code{newdata}.  One can also simply compute the
corresponding terminal node numbers via
<<mytree-node, echo = TRUE>>=
predict(aqt, newdata = airquality[1:10,], type = "node")
@
or estimates of the conditional distribution function (objects of class \code{ecdf}):
<<mytree-prob, echo = TRUE>>=
predict(aqt, newdata = airquality[1:10,], type = "prob")
@
In principle, any functional of these distribution functions can be computed
as a prediction, for example the median
<<mytree-median, echo = TRUE>>=
predict(aqt, newdata = airquality[1:10,], 
        FUN = function(y, w) median(rep(y, w)))
@
which is a more convenient way compared to direct computation on
the \code{ecdf} objects:
<<mytree-median-2, echo = TRUE>>=
sapply(predict(aqt, newdata = airquality[1:10,], type = "prob"),
       function(x) quantile(x, .5))

@

The \code{type} argument to the \fct{predict} method has several meanings,
depending on the class of the response variable.  Table~\ref{predict-type}
gives an overview on the type of predictions returned in the difference
settings of response and \code{type} argument.

\begin{table}
\begin{center}
\begin{tabular}{l|lll}
               & \multicolumn{3}{l}{\code{predict(<party>, }} \\
Response class & \code{type = "node"} & \code{type = "response"} & \code{type = "prob"} \\ \hline
\code{factor}  & terminal node number & majority class & class probabilities \\
\code{numeric} & terminal node number & mean  & ECDF \\
\code{Surv} & terminal node number & median survival time  & Kaplan-Meier \\
\end{tabular}
\caption{Overview on type of predictions computed by the \code{predict}
         method for \code{constparty} objects. For multivariate responses,
         combinations thereof are returned. \label{predict-type}}
\end{center}
\end{table}

\bibliography{party}
    
\end{document}
