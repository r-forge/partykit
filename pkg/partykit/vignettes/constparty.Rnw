\documentclass[nojss]{jss}

%\VignetteIndexEntry{partykit: A Toolkit for Recursive Partytioning}
%\VignetteDepends{partykit, rpart, RWeka, pmml, datasets}
%\VignetteKeywords{recursive partitioning, regression trees, classification trees, decision trees}
%\VignettePackage{partykit}

%% packages
\usepackage{amstext}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{thumbpdf}
\usepackage{rotating}
%% need no \usepackage{Sweave}

%% additional commands
\newcommand{\squote}[1]{`{#1}'}
\newcommand{\dquote}[1]{``{#1}''}
\newcommand{\fct}[1]{{\texttt{#1()}\index{#1@\texttt{#1()}}}}
\newcommand{\class}[1]{\dquote{\texttt{#1}}}
\newcommand{\fixme}[1]{\emph{\marginpar{FIXME} (#1)}}

%% further commands
\renewcommand{\Prob}{\mathbb{P} }
\renewcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\Var}{\mathbb{V}}
\newcommand{\R}{\mathbb{R} }
\newcommand{\N}{\mathbb{N} }
\newcommand{\C}{\mathbb{C} }
\newcommand{\argmin}{\operatorname{argmin}\displaylimits}
\newcommand{\argmax}{\operatorname{argmax}\displaylimits}
\newcommand{\LS}{\mathcal{L}_n}
\newcommand{\TS}{\mathcal{T}_n}
\newcommand{\LSc}{\mathcal{L}_{\text{comb},n}}
\newcommand{\LSbc}{\mathcal{L}^*_{\text{comb},n}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\yn}{y_{\text{new}}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\sX}{\mathcal{X}}
\newcommand{\sY}{\mathcal{Y}}
\newcommand{\T}{\mathbf{T}}
\newcommand{\x}{\mathbf{x}}
\renewcommand{\a}{\mathbf{a}}
\newcommand{\xn}{\mathbf{x}_{\text{new}}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\ws}{\mathbf{w}_\cdot}
\renewcommand{\t}{\mathbf{t}}
\newcommand{\M}{\mathbf{M}}
\renewcommand{\vec}{\text{vec}}
\newcommand{\B}{\mathbf{B}}
\newcommand{\K}{\mathbf{K}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\cellx}{\pi_n[\x]}
\newcommand{\partn}{\pi_n(\mathcal{L}_n)}
\newcommand{\err}{\text{Err}}
\newcommand{\ea}{\widehat{\text{Err}}^{(a)}}
\newcommand{\ecv}{\widehat{\text{Err}}^{(cv1)}}
\newcommand{\ecvten}{\widehat{\text{Err}}^{(cv10)}}
\newcommand{\eone}{\widehat{\text{Err}}^{(1)}}
\newcommand{\eplus}{\widehat{\text{Err}}^{(.632+)}}
\newcommand{\eoob}{\widehat{\text{Err}}^{(oob)}}
\newcommand{\bft}{\mathbf{t}}

\hyphenation{Qua-dra-tic}

\title{Growing and Handling Trees with Constant Fits}
\Plaintitle{Growing and Handling Trees with Constant Fits}

\author{Torsten Hothorn\\Universit\"at Z\"urich
   \And Achim Zeileis\\Universit\"at Innsbruck}
\Plainauthor{Torsten Hothorn, Achim Zeileis}

\Abstract{

This document describes tree objects with very simple one-parameter models
(constant fits) in each of the terminal nodes.  This class of trees is very
common and includes all traditional tree variants (AID, CHAID, CART, FACT,
QUEST, C4.5) and also more recent approaches like CTree.  We describe how
one can coerce objects representing such trees to objects represented by
\pkg{partykit} classes and how one can grow trees using the \pkg{partykit}
infrastructure.

}
\Keywords{recursive partitioning, regression trees, classification trees, decision trees}

\Address{
  Torsten Hothorn\\
  Institut f\"ur Sozial- und Pr\"aventivmedizin, Abteilung Biostatistik \\
  Universit\"at Z\"urich \\
  Hirschengraben 84\\
  CH-8001 Z\"urich, Switzerland \\
  E-mail: \email{Torsten.Hothorn@R-project.org}\\
  URL: \url{http://user.math.uzh.ch/hothorn/}\\

  Achim Zeileis\\
  Department of Statistics \\
  Universit\"at Innsbruck \\
  Universit\"atsstr.~15 \\
  6020 Innsbruck, Austria \\
  E-mail: \email{Achim.Zeileis@R-project.org}\\
  URL: \url{http://eeecon.uibk.ac.at/~zeileis/}
}


\begin{document}

\setkeys{Gin}{width=\textwidth}

\SweaveOpts{engine=R, eps=FALSE, keep.source=TRUE, eval=TRUE}

<<setup, echo = FALSE, results = hide>>=
options(width = 70)
library("partykit")
set.seed(290875)
@

This vignette describes the handling of trees with constant fits in the
terminal nodes.  This class of regression models includes most classical
tree algorithms like AID, CHAID, CART, FACT, QUEST, C4.5, etc.  In this
class of tree models, one can compute simple predictions, such as the
conditional mean in a regression setup, for a new observation from the
response values of those learning sample in the same terminal node.  Thus,
it is sufficient to store the terminal node number and the response (and
potentially weights) for the observations used for fitting the tree in order
to compute predictions. This
document consists of two parts.  In the first part we assume that the trees
were fitted using external software and we describe how these models can be
coerced into objects that can be dealt with using infrastructure implemented
in the \pkg{partykit} package.  We focus on displaying such trees in textual
and graphical ways and on computing predictions.  In the second part we show
how one can implement a simple classification tree algorithm using the
\pkg{partykit} tools.

\section{Coercing  tree objects}

For the illustrations, we use the Titanic data set from package \pkg{datasets}, consisting of
four variables on each of the $2201$ Titanic passengers: gender (male, female), age (child, adult), 
and class (1st, 2nd, 3rd, or crew) set-up as follows:
<<Titanic>>=
data("Titanic", package = "datasets")
ttnc <- as.data.frame(Titanic)
ttnc <- ttnc[rep(1:nrow(ttnc), ttnc$Freq), 1:4]
names(ttnc)[2] <- "Gender"
@
The response variable describes whether or not the passenger survived the sinking of the ship.

\subsection{Coercing rpart objects}

We first fit a classification tree by means of the the \fct{rpart} function 
from package \pkg{rpart} \citep{rpart} to this data set:
<<rpart>>=
library("rpart")
(rp <- rpart(Survived ~ ., data = ttnc))
@
The \code{rp} object of class \class{\Sexpr{class(rp)[1L]}} can be coerced to an
object of a certain class inheriting from class \class{party} by the
\fct{as.party} function
<<rpart-party>>=
(party_rp <- as.party(rp))
@
and, instead of the print method for \class{rpart} objects, the print method
for \code{\Sexpr{class(party_rp)[1L]}} objects creates a textual display of the tree
structure.  In a similar way, the corresponding \fct{plot} method 
produces a graphical representation of this tree, see Figure~\ref{party_plot}.

\begin{figure}
\centering
<<rpart-plot, fig = TRUE, width = 10, height = 6>>=
plot(party_rp)
@
\caption{\class{rpart} tree of Titanic data plotted using \pkg{partykit}
         infrastructure. \label{party_plot}}
\end{figure}

By default, the \fct{predict} method for \class{rpart} object computes conditional
class probabilities. The same numbers are returned by the \fct{predict} method
for \Sexpr{class(party_rp)[1L]} objects with \code{type = "prob"} argument:
<<rpart-pred>>=
all.equal(predict(rp), predict(party_rp, type = "prob"), 
          check.attributes = FALSE)
@
Predictions are computed based on the \code{fitted} slot of a
\class{constparty} object
<<rpart-fitted>>=
str(fitted(party_rp))
@
which contains the terminal node numbers and the response for each
of the training samples. So, the conditional class probabilities for
each terminal node can be computed via
<<rpart-prob>>=
prop.table(do.call("table", fitted(party_rp)), 1)
@
Optionally, weights can be stored in the \code{fitted} slot as well.

\subsection{Coercing J48 objects}

The \pkg{RWeka} package \citep{RWeka} provides an interface to the
\proglang{WEKA} machine learning library and we can use the \fct{J48} function to
fit a J4.8 tree to the Titanic data
<<J48>>=
library("RWeka")
(j48 <- J48(Survived ~ ., data = ttnc))
@
This object can be coerced to a \class{party} object using
<<J48-party>>=
(party_j48 <- as.party(j48))
@
and, again, the print method from the \pkg{partykit} package creates a
textual display.  Note that, unlike the \class{rpart} trees, this tree
includes multiway splits.  The \fct{plot} method draws this tree, see
Figure~\ref{J48_plot}.

\begin{sidewaysfigure}
\centering
<<J48-plot, fig = TRUE, width = 15, height = 9>>=
plot(party_j48)
@
\caption{\class{J48} tree of Titanic data plotted using \pkg{partykit}
         infrastructure. \label{J48_plot}}
\end{sidewaysfigure}

The conditional class probabilities computed by the \fct{predict} methods
implemented in packages \pkg{RWeka} and \pkg{partykit} are equivalent:
<<J48-pred>>=
all.equal(predict(j48, type = "prob"), 
          predict(party_j48, type = "prob"), 
          check.attributes = FALSE)
@

\subsection{Importing trees from PMML files}

Some software packages can output a \proglang{PMML} description of a tree,
we use the \pkg{pmml} package to create a \proglang{PMML} file containing
the \class{rpart} tree for the Titanic data
<<PMML-write>>=
library("pmml")
tfile <- tempfile()
write(toString(pmml(rp)), file = tfile)
@
Now, we can simply read this file and inspect the resulting tree
<<PMML-read>>=
(party_pmml <- pmmlTreeModel(tfile))
all.equal(predict(party_rp, newdata = ttnc, type = "prob"), 
	  predict(party_pmml, newdata = ttnc, type = "prob"),
	  check.attributes = FALSE)
@
We used \proglang{SPSS} to fit a QUEST tree to the Titanic data and exported this
tree in \proglang{PMML} format. This file is part of the \pkg{partykit} package and we
read and print the QUEST tree as follows
<<PMML-Titantic>>=
(ttnc_quest <- pmmlTreeModel(
  file.path(system.file("pmml", package = "partykit"), "ttnc.pmml")))
@

One problem with the resulting object is that the data are not part of the \proglang{PMML}
file and thus also missing from this representation of the QUEST tree. We can, however,
bring the data into the same shape as it is assumed to be by \code{ttnc_quest}.
Then a new \class{party} is set up and coerced to \class{constparty}.
<<PMML-Titanic-augmentation>>=
ttnc2 <- ttnc[, names(ttnc_quest$data)]
for(n in names(ttnc2)) {
  if(is.factor(ttnc2[[n]])) ttnc2[[n]] <- factor(
    ttnc2[[n]], levels = levels(ttnc_quest$data[[n]]))
}
ttnc_cp <- party(ttnc_quest$node,
  data = ttnc2,
  fitted = data.frame(
    "(fitted)" = predict(ttnc_quest, ttnc2, type = "node"),
    "(response)" = ttnc2$Survived,
    check.names = FALSE)
)
ttnc_cp <- as.constparty(ttnc_cp)
@
This object is plotted in Figure~\ref{PMML-Titanic-plot}.

\begin{figure}[t]
\centering
<<PMML-Titanic-plot, fig=TRUE, height=8.5, width=14>>=
plot(ttnc_cp)
@
\caption{QUEST tree for Titanic data, fitted using \proglang{SPSS}, exported
         to \proglang{PMML}, and read into \proglang{R} as a \class{constparty} object.
         \label{PMML-Titanic-plot}}
\end{figure}

\section{Growing a simple classification tree} 

Package \pkg{partykit} does not offer unified infrastructure for growing
trees.  However, once you know how to estimate splits from data, it is
fairly straightforward to implement trees.  Consider a very simple tree
algorithm.  We assume that both response and explanatory variables are
factors, as for the Titanic data set.  We search for the binary best split by means of $t$-test
$p$-values, i.e., we cycle through all variables and potential split points
and assess the quality of the split by comparing the distributions of the
response in the so-defined two groups.  We select the feature/split point
combination with lowest $p$-value of a $\chi^2$ test, however, only if this result is
significant at level $\alpha = 0.01$.

This strategy can be implemented based on the data (response and explanatory
variables) and some case weights as follows (\code{response} is just the
name of the response and \code{data} is a data frame with all variables):
<<mytree-1, echo = TRUE>>= 
findsplit <- function(response, data, weights) {

  ### extract response values from data
  y <- data[[response]]

  ### cycle through all features
  logpmin <- 0; xselect <- NULL
  for (i in which(names(data) != response)) {

    ### expand data by case weights
    x <- data[[i]]
    xt <- rep(x, weights)
    yt <- rep(y, weights)

    ### set-up all possible splits in two daughter nodes
    lev <- levels(xt[drop = TRUE])
    if (length(lev) == 1) next()
    if (length(lev) == 2) comb <- list(lev[1])
    if (length(lev) > 2) 
        comb <- do.call("c", lapply(1:(length(lev) - 2),
            function(x) combn(lev,x, simplify = FALSE)))

    ### assess all potential splits by their chi^2 test log p-value
    logp <- sapply(comb, function(q) {
      ct <- chisq.test(table(yt, xt %in% q))
      pchisq(ct$statistic, ct$parameter, log = TRUE, lower.tail = FALSE)
    })

    ### if the best split in variable i significant AND
    ### better than what we already had store this information
    if (min(logp) < logpmin & min(logp) < log(0.01)) {
      logpmin <- min(logp)
      xselect <- i
      splitpoint <- comb[[which.min(logp)]]
    }
  }

  ### no significant split found, give up
  if (is.null(xselect)) return(NULL)

  ### return split as partysplit object
  return(partysplit(
      varid = as.integer(xselect),         ### which variable?
      index = levels(data[[xselect]]) %in% 
              splitpoint + 1L,             ### which split point?
      info = list(p.value = exp(logpmin)   ### save p-value in addition
  )))
}
@

In order to actually grow a tree on data, 
we have to set-up the recursion for growing a recursive 
\class{partynode} structure:
<<mytree-2, echo = TRUE>>=
growtree <- function(id = 1L, response, data, weights) {

  ### for less than 30 observations stop here
  if (sum(weights) < 30) return(partynode(id = id))

  ### find best split
  sp <- findsplit(response, data, weights)
  ### no split found, stop here
  if (is.null(sp)) return(partynode(id = id))

  ### actually split the data
  kidids <- kidids_split(sp, data = data)

  ### set-up all daugther nodes
  kids <- vector(mode = "list", length = max(kidids))
  for (kidid in 1:max(kidids)) {
  ### select observations for current node
  w <- weights
  w[kidids != kidid] <- 0
  ### get next node id
  if (kidid > 1) {
    myid <- max(nodeids(kids[[kidid - 1]]))
  } else {
    myid <- id
  }
  ### start recursion on this daugther node
  kids[[kidid]] <- growtree(id = as.integer(myid + 1), response, data, w)
  }

  ### return nodes
  return(partynode(id = as.integer(id), split = sp, kids = kids))
}
@

A very rough sketch of a formula-based user-interface 
sets-up the data and calls \fct{growtree}:
<<mytree-3, echo = TRUE>>=
mytree <- function(formula, data, weights = NULL) {

  ### name of the response variable
  response <- all.vars(formula)[1]
  ### data without missing values, response comes last
  data <- data[complete.cases(data), c(all.vars(formula)[-1], response)]
  ### data is factors only
  stopifnot(all(sapply(data, is.factor)))

  if (is.null(weights)) weights <- rep(1L, nrow(data))
  ### weights are case weights, i.e., integers
  stopifnot(length(weights) == nrow(data) &
    max(abs(weights - floor(weights))) < .Machine$double.eps)

  ### grow tree
  nodes <- growtree(id = 1L, response, data, weights)

  ### compute terminal node number for each observation
  fitted <- fitted_node(nodes, data = data)
  ### return rich constparty object
  ret <- party(nodes, data = data,
    fitted = data.frame("(fitted)" = fitted,
                        "(response)" = data[[response]],
                        "(weights)" = weights,
                        check.names = FALSE),
    terms = terms(formula))
  as.constparty(ret)
}
@
The call to the constructor \fct{party} sets-up a \class{party}
object with the tree structure contained in \code{nodes}, the
training samples in \code{data} and the corresponding \code{terms}
object. For trees with constant fits, we specify the \code{fitted}
slot with the terminal node numbers for each sample in the
training data \code{fitted}, the response and weights.

We now can fit this tree to the Titanic data; the
\fct{print} method provides us with a first overview on the 
resulting model
<<mytree-4, echo = TRUE>>=
(myttnc <- mytree(Survived ~ Class + Age + Gender, data = ttnc))
@

Of course, we can immediately use the \fct{plot} function to obtain a
graphical representation of this tree, the result is given in
Figure~\ref{plottree}.  The default behavior for trees with numeric
responses is to depict the conditional distribution of the response in each
terminal node by means of a bar plot.

\begin{figure}
\centering
<<mytree-5, echo = TRUE, fig = TRUE, height=8.5, width=14>>=
plot(myttnc)
@
\caption{Classification tree fitted by the \fct{mytree} function to the
         \code{ttnc} data. \label{plottree}}
\end{figure}

An interesting feature of \class{party} trees is the ability to extract
subtrees:
<<mytree-7, echo = TRUE>>=
(myttnc7 <- myttnc[7])
@
which again are objects inheriting from \class{party} and thus
can be plotted easily (see Figure~\ref{subtree}). Also, it is possible
to prune-off nodes based on their node identifiers or their names
using \fct{nodeprune}; see Figure~\ref{prunetree}.

\begin{figure}
\centering
<<mytree-5-nodeprune, echo = TRUE, fig = TRUE, height=8.5, width=14>>=
plot(nodeprune(myttnc, 10))
@
\caption{Pruned classification tree fitted by the \fct{mytree} function to the
         \code{ttnc} data. \label{prunetree}}
\end{figure}


\begin{figure}[t!]
\centering
<<mytree-8, echo = TRUE, fig = TRUE, width = 10, height = 6>>=
plot(myttnc7)
@
\caption{A subtree of the \code{ttnc} classification tree. \label{subtree}}
\end{figure}

We also might be interested in extracting the $p$-values in the
inner nodes in a nicely formatted way. The \fct{nodeapply} function
can be used to iterate over a set of nodes, in our case the 
inner nodes:
<<mytree-10, echo = TRUE>>=
fun <- function(x) format.pval(info_split(split_node(x))$p.value,
  digits = 3, eps = 0.001)
nid <- nodeids(myttnc)
iid <- nid[!(nid %in% nodeids(myttnc, terminal = TRUE))]
unlist(nodeapply(myttnc, ids = iid, FUN = fun))
@

\section{Predictions}

Computing predictions for new observations is done using the \fct{predict} method.
For example, we compute the predicted classes from the QUEST tree and
compare these results with the predicted classes from our very simple
classification tree fitted above
<<mytree-predict, echo = TRUE>>=
table(predict(ttnc_quest, newdata = ttnc), 
      predict(myttnc, newdata = ttnc))
@
The default for factors is to compute the predicted class for each
of the rows in \code{newdata}.  One can also simply compute the
corresponding terminal node numbers via
<<mytree-node, echo = TRUE>>=
predict(myttnc, newdata = ttnc[1:10,], type = "node")
@
or estimates of the conditional class probabilities 
<<mytree-prob, echo = TRUE>>=
predict(myttnc, newdata = ttnc[1:10,], type = "prob")
@

The \code{type} argument to the \fct{predict} method has several meanings,
depending on the class of the response variable.  Table~\ref{predict-type}
gives an overview on the type of predictions returned in the difference
settings of response and \code{type} argument.

\begin{table}
\centering
\begin{tabular}{l|lll}
               & \multicolumn{3}{l}{\code{predict(<party>, }} \\
Response class & \code{type = "node"} & \code{type = "response"} & \code{type = "prob"} \\ \hline
\code{factor}  & terminal node number & majority class & class probabilities \\
\code{numeric} & terminal node number & mean  & ECDF \\
\code{Surv} & terminal node number & median survival time  & Kaplan-Meier \\
\end{tabular}
\caption{Overview on type of predictions computed by the \fct{predict}
         method for \class{constparty} objects. For multivariate responses,
         combinations thereof are returned. \label{predict-type}}
\end{table}

\section{Conclusion}

The class \class{constparty} can be used to represent trees with constant
fits in the terminal nodes, including most of the traditional tree variants. 
For a number of implementations it is possible to convert the resulting
trees to a \class{constparty} object.  Thus, \pkg{partykit} offers unified
methods for representing trees fitted by different algorithms or
implementations.  Also, computing non-standard predictions, such as the
median or empirical cumulative distribution functions, is easily possible
within this framework.  It is rather straightforward to implement a new tree
algorithm using the \pkg{partykit} infrastructure, at least for prototyping
purposes.

\bibliography{party}
    
\end{document}
