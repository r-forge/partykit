\documentclass[nojss]{jss}

%\VignetteIndexEntry{ctree: Conditional Inference Trees}
%\VignetteDepends{coin, TH.data, survival, strucchange}
%\VignetteKeywords{conditional inference, non-parametric models, recursive partitioning}
%\VignettePackage{partykit}

%% packages
\usepackage{amstext}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{thumbpdf}
\usepackage{rotating}
\usepackage{caption}
\captionsetup{format=hang}
\usepackage{subcaption}
\usepackage{Sweave}
\usepackage{enumitem}
\usepackage{array, makecell}
\usepackage{tikz}
\usetikzlibrary{positioning,shapes,arrows,decorations.pathreplacing,calc,automata,mindmap}
%\usepackage[pagebackref=true]{hyperref}
%\usepackage{doi}

%% commands
\newcommand{\fixme}[1]{\emph{\marginpar{FIXME} (#1)}}
\renewcommand{\Prob}{\mathbb{P} }
\renewcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\Var}{\mathbb{V}}
\newcommand{\R}{\mathbb{R} }
\newcommand{\N}{\mathbb{N} }
\newcommand{\C}{\mathbb{C} }
\newcommand{\argmin}{\operatorname{argmin}\displaylimits}
\newcommand{\argmax}{\operatorname{argmax}\displaylimits}
\newcommand{\LS}{\mathcal{L}_n}
\newcommand{\TS}{\mathcal{T}_n}
\newcommand{\LSc}{\mathcal{L}_{\text{comb},n}}
\newcommand{\LSbc}{\mathcal{L}^*_{\text{comb},n}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\yn}{y_{\text{new}}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\sX}{\mathcal{X}}
\newcommand{\sY}{\mathcal{Y}}
\newcommand{\T}{\mathbf{T}}
\newcommand{\x}{\mathbf{x}}
\renewcommand{\a}{\mathbf{a}}
\newcommand{\xn}{\mathbf{x}_{\text{new}}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\ws}{\mathbf{w}_\cdot}
\renewcommand{\t}{\mathbf{t}}
\newcommand{\M}{\mathbf{M}}
\renewcommand{\vec}{\text{vec}}
\newcommand{\B}{\mathbf{B}}
\newcommand{\K}{\mathbf{K}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\cellx}{\pi_n[\x]}
\newcommand{\partn}{\pi_n(\mathcal{L}_n)}
\newcommand{\err}{\text{Err}}
\newcommand{\ea}{\widehat{\text{Err}}^{(a)}}
\newcommand{\ecv}{\widehat{\text{Err}}^{(cv1)}}
\newcommand{\ecvten}{\widehat{\text{Err}}^{(cv10)}}
\newcommand{\eone}{\widehat{\text{Err}}^{(1)}}
\newcommand{\eplus}{\widehat{\text{Err}}^{(.632+)}}
\newcommand{\eoob}{\widehat{\text{Err}}^{(oob)}}
\newcommand{\bft}{\mathbf{t}}

\hyphenation{Qua-dra-tic}

%% colors
\definecolor{HighlightOrange}{rgb}{0.9490196,0.5725490,0.0000000}
\definecolor{HighlightBlue}{rgb}{0.4784314,0.7490196,0.9803922}



\SweaveOpts{engine=R, eps=FALSE, keep.source=TRUE}
<<preliminaries, echo=FALSE, results=hide>>=
options(prompt = "R> ", continue = "+  ", useFancyQuotes = FALSE, width = 70)
library("ggplot2")
theme_set(theme_bw(base_size = 18))
library("disttree")
library("Formula")  # FIX ME: should be imported in disttree
library("gamlss")
library("randomForest")
library("lattice")
library("crch")
library("latex2exp")
library("parallel")
library("gamlss.cens")
gen.cens(NO, type = "left")
library("mgcv")
library("MASS")
library("countreg")

## HCL palette
pal <- hcl(c(10, 128, 260, 290, 30, 90, 180), 100, 50)
names(pal) <- c("forest", "tree", "gamlss", "randomForest", "bamlss", "gamboostLSS", "cforest")

pallight <- hcl(c(10, 128, 260, 290, 30, 90, 180), 100, 50, alpha = 0.25)
names(pallight) <- c("forest", "tree", "gamlss", "randomForest", "bamlss", "gamboostLSS", "cforest")

transpgrey <- rgb(0.190,0.190,0.190, alpha = 0.2)

## define distribution list:
# dist_list_normal
{
  
  dist_list_normal <- list()
  
  parnames <- c("mu", "sigma")
  etanames <- c("mu", "log(sigma)")
  
  
  ddist <-  function(y, eta, log = TRUE, weights = NULL, sum = FALSE) {     
    
    val <- -1/2 * (log(2*pi) + 2*eta[2] + exp(log((y-eta[1])^2) - 2*eta[2]))
    if(!log) val <- exp(val)
    
    # par <- c(eta[1], exp(eta[2]))
    # val <- dnorm(y, mean = par[1], sd = par[2], log = log)
    
    if(sum) {
      if(is.null(weights)) weights <- rep.int(1, length(y))
      val <- sum(weights * val, na.rm = TRUE)
    }
    return(val)
  }
  
  
  sdist <- function(y, eta, weights = NULL, sum = FALSE) {   
    
    score <- cbind(exp(-2*eta[2]) * (y-eta[1]), 
                   -1 + exp(-2*eta[2] + log((y-eta[1])^2)))
    
    # par <- c(eta[1], exp(eta[2])) 
    # score <- cbind(1/par[2]^2 * (y-par[1]), 
    #                (-1/par[2] + ((y - par[1])^2)/(par[2]^3)) * exp(eta[2]))
    
    score <- as.matrix(score)
    colnames(score) <- etanames
    if(sum) {
      if(is.null(weights)) weights <- rep.int(1, length(y))
      # if score == Inf replace score with 1.7e308 because Inf*0 would lead to NaN -> gradient is NaN
      score[score==Inf] = 1.7e308
      score <- colSums(weights * score, na.rm = TRUE)
    }
    return(score)
  }
  
  
  hdist <- function(y, eta, weights = NULL) {    
    ny <- length(y)
    if(is.null(weights)) weights <- rep.int(1, ny)
    
    d2ld.etamu2 <- sum(weights * rep.int(-exp(-2*eta[2]), ny))
    d2ld.etamu.d.etasigma <- sum(weights * (-2)*(y-eta[1]) * exp(-2*eta[2]), na.rm = TRUE)          # should be 0 for exact parameters (here: observed hess)
    d2ld.etasigma2 <- sum(weights * (-2)*exp(log((y-eta[1])^2) - 2*eta[2]), na.rm = TRUE)    
    
    # par <- c(eta[1], exp(eta[2]))                           
    # d2ld.etamu2 <- sum(weights * rep.int(-1/par[2]^2, ny))
    # d2ld.etamu.d.etasigma <- sum(weights * (-2)*(y-par[1])/par[2]^2), na.rm = TRUE)          # should be 0 for exact parameters (here: observed hess)
    # d2ld.etasigma2 <- sum(weights * (-2)*(y-par[1])^2/par[2]^2, na.rm = TRUE)         
    
    hess <- matrix(c(d2ld.etamu2, d2ld.etamu.d.etasigma, d2ld.etamu.d.etasigma, d2ld.etasigma2), nrow = 2)
    colnames(hess) <- rownames(hess) <-  etanames
    
    return(hess)
  }
  
  
  ## additional functions pdist, qdist, rdist
  pdist <- pnorm
  qdist <- qnorm
  rdist <- rnorm  
  
  
  link <- c("identity", "log")
  
  linkfun <- function(par) {
    eta <- c(par[1], log(par[2]))
    names(eta) <- etanames
    return(eta)
  }
  
  
  linkinv <- function(eta) {
    par <- c(eta[1], exp(eta[2]))
    names(par) <- parnames
    return(par)
  }
  
  
  linkinvdr <- function(eta) {
    dpardeta <- c(1, exp(eta[2]))
    names(dpardeta) <- parnames
    return(dpardeta)
  }
  
  
  startfun <- function(y, weights = NULL){
    if(is.null(weights)) {
      mu <- mean(y)
      sigma <- sqrt(1/length(y) * sum((y - mu)^2))
    } else {
      mu <- weighted.mean(y, weights)
      sigma <- sqrt(1/sum(weights) * sum(weights * (y - mu)^2))
    }
    starteta <- c(mu, log(sigma))
    names(starteta) <- etanames
    return(starteta)
  }
  
  mle <- TRUE
  
  dist_list_normal <- list(family.name = "Normal Distribution",
                           ddist = ddist, 
                           sdist = sdist, 
                           hdist = hdist,
                           pdist = pdist,
                           qdist = qdist,
                           rdist = rdist,
                           link = link, 
                           linkfun = linkfun, 
                           linkinv = linkinv, 
                           linkinvdr = linkinvdr,
                           startfun = startfun,
                           mle = mle,
                           gamlssobj = FALSE,
                           censored = FALSE
  )
}


# dist_list_cens_normal
{
  
  dist_list_cens_normal <- list()
  
  parnames <- c("mu", "sigma")
  etanames <- c("mu", "log(sigma)")
  
  ddist <-  function(y, eta, log = TRUE, weights = NULL, sum = FALSE, left = 0, right = Inf) {     
    par <- c(eta[1], exp(eta[2]))
    val <- crch::dcnorm(x = y, mean = par[1], sd = par[2], left = left, right = right, log = log)
    if(sum) {
      if(is.null(weights)) weights <- if(is.matrix(y)) rep.int(1, dim(y)[1]) else rep.int(1, length(y))
      val <- sum(weights * val, na.rm = TRUE)
    }
    return(val)
  }
  
  
  sdist <- function(y, eta, weights = NULL, sum = FALSE, left = 0, right = Inf) {   
    par <- c(eta[1], exp(eta[2]))
    # y[y==0] <- 1e-323
    
    score_m <- crch:::scnorm(x = y, mean = par[1], sd = par[2], which = "mu", left = left, right = right)
    score_s <- crch:::scnorm(x = y, mean = par[1], sd = par[2], which = "sigma", left = left, right = right) * exp(eta[2]) # inner derivation exp(eta[2])
    score <- cbind(score_m, score_s)
    score <- as.matrix(score)
    colnames(score) <- etanames
    if(sum) {
      if(is.null(weights)) weights <- rep.int(1, length(y)[1])
      # if score == Inf replace score with 1.7e308 because Inf*0 would lead to NaN (0 in weights)
      score[score==Inf] = 1.7e308
      score <- colSums(weights * score, na.rm = TRUE)
      #if(any(is.nan(score))) print(c(eta, "y", y))
    }
    return(score)
  }
  
  
  hdist <- function(y, eta, weights = NULL, left = 0, right = Inf) {    
    ny <- length(y)
    if(is.null(weights)) weights <- rep.int(1, ny)
    
    par <- c(eta[1], exp(eta[2]))                           
    # y[y==0] <- 1e-323
    
    d2mu <- crch:::hcnorm(x = y, mean = par[1], sd = par[2], which = "mu", left = left, right = right)
    d2sigma <- crch:::hcnorm(x = y, mean = par[1], sd = par[2], which = "sigma", left = left, right = right)
    dmudsigma <- crch:::hcnorm(x = y, mean = par[1], sd = par[2], which = "mu.sigma", left = left, right = right) # FIX: order?
    dsigmadmu <- crch:::hcnorm(x = y, mean = par[1], sd = par[2], which = "sigma.mu", left = left, right = right) # FIX: order?
    dsigma <- crch:::scnorm(x = y, mean = par[1], sd = par[2], which = "sigma", left = left, right = right)
    
    d2ld.etamu2 <- sum(weights * d2mu, na.rm = TRUE)
    d2ld.etamu.d.etasigma <- sum(weights * dmudsigma * par[2], na.rm = TRUE)
    d2ld.etasigma.d.etamu <- sum(weights * dsigmadmu * par[2], na.rm = TRUE)
    d2ld.etasigma2 <- sum(weights * (d2sigma * exp(2*eta[2]) + dsigma * par[2]), na.rm = TRUE)         
    
    hess <- matrix(c(d2ld.etamu2, d2ld.etamu.d.etasigma, d2ld.etasigma.d.etamu, d2ld.etasigma2), nrow = 2)
    colnames(hess) <- rownames(hess) <-  etanames
    
    return(hess)
  }
  
  
  ## additional functions pdist, qdist, rdist
  pdist <- function(q, eta, lower.tail = TRUE, log.p = FALSE) crch:::pcnorm(q, mean = eta[1], sd = eta[2], 
                                                                            lower.tail = lower.tail, log.p = log.p, 
                                                                            left = left, right = right)
  qdist <- function(p, eta, lower.tail = TRUE, log.p = FALSE) crch:::qcnorm(p, mean = eta[1], sd = eta[2], 
                                                                            lower.tail = lower.tail, log.p = log.p, 
                                                                            left = left, right = right)
  rdist <- function(n, eta) crch:::rcnorm(n, mean = eta[1], sd = eta[2], left = left, right = right)

  
  link <- c("identity", "log")
  
  linkfun <- function(par) {
    eta <- c(par[1], log(par[2]))
    names(eta) <- etanames
    return(eta)
  }
  
  
  linkinv <- function(eta) {
    par <- c(eta[1], exp(eta[2]))
    names(par) <- parnames
    return(par)
  }
  
  
  linkinvdr <- function(eta) {
    dpardeta <- c(1, exp(eta[2]))
    names(dpardeta) <- parnames
    return(dpardeta)
  }
  
  
  startfun <- function(y, weights = NULL){
    yc <- pmax(0,y)  # optional ?
    if(is.null(weights)) {
      mu <- mean(yc)
      sigma <- sqrt(1/length(yc) * sum((yc - mu)^2))
    } else {
      mu <- weighted.mean(yc, weights)
      sigma <- sqrt(1/sum(weights) * sum(weights * (yc - mu)^2))
    }
    starteta <- c(mu, log(sigma))
    names(starteta) <- etanames
    return(starteta)
  }
  
  mle <- FALSE
  
  dist_list_cens_normal <- list(family.name = "censored Normal Distribution",
                                ddist = ddist, 
                                sdist = sdist, 
                                hdist = hdist,
                                pdist = pdist,
                                qdist = qdist,
                                rdist = rdist,
                                link = link, 
                                linkfun = linkfun, 
                                linkinv = linkinv, 
                                linkinvdr = linkinvdr,
                                startfun = startfun,
                                mle = mle,
                                gamlssobj = FALSE,
                                censored = TRUE
  )
}





  
  
  
## function to estimate standard deviation of randomForest for a new observation
# (in randomForest the argument 'keep.inbag' must be set to TRUE)
rf_getsd <- function(rf, newdata = NULL, rfdata){
  
  if(is.null(newdata)) newdata <- rfdata
  
  rf_sd <- numeric(length = NROW(newdata))
  
  for(k in 1:NROW(newdata)){
    newobs <- newdata[k,]
    # get predictions for the new observations from all trees
    pred.newobs <- predict(rf, predict.all = TRUE, newdata = newobs)
    
    # vector where the standard deviations from all trees are stored
    sd_trees <- numeric(length = rf$ntree)
    
    # loop over all trees of the forest
    for(i in 1:rf$ntree){
      
      # get data used to build this tree
      obsid <- rep.int(c(1:NROW(rfdata)), as.vector(rf$inbag[,i]))
      obs_tree <- rfdata[obsid,]
      rownames(obs_tree) <- c(1:NROW(obs_tree))
      # get predictions for this data from this tree
      pred.obs_tree <- predict(rf, newdata = obs_tree, predict.all = TRUE)$individual[,i]
      
      # get prediction for the new observation from this tree
      pred.newobs_tree <- pred.newobs$individual[,i]
      
      # get part of the data that ends up in the same terminal node (has the same prediction)
      obs_node <- obs_tree[pred.obs_tree == pred.newobs_tree,]
      
      sd_trees[i] <- sd(obs_node$y)
    }
    
    # average of sd over all trees
    sd_newobs <- mean(sd_trees, na.rm = TRUE)
    rf_sd[k] <- sd_newobs
  }
  return(rf_sd)   
}



## function to estimate standard deviation of cforest for a new observation
cf_getsd <- function(cf, newdata = NULL){
  
  # get IDs of predicted nodes for the learning data cfdata (does not have to be handed over as cf was learned on cfdata)
  pred.node.learn <- predict(cf, type = "node")
  
  # get IDs of predicted nodes for the new observations
  if(is.null(newdata)) {
    pred.node.new <- pred.node.learn
    newdata <- cf$data
  } else {
    pred.node.new <- predict(cf, newdata = newdata, type = "node")
  }
  
  sdnew <- numeric(length = NROW(newdata))
  
  for(i in 1:NROW(newdata)){
    for(t in 1:cf$info$call$ntree){
      nodedata <- cf$data[(pred.node.learn[[t]] == pred.node.new[[t]][i]),]
      sdnew[i] <- sd(nodedata[,paste(cf$terms[[2]])])
    }
  }
  
  return(sdnew)   
}



## simulation plot functions
# plot RMSE
plot_rmse <- function(simres, type = c("exp", "par"), ylim = NULL, legend = TRUE){
  
  if(type == "exp"){
    rmse <- NULL
    colnames <- NULL
    col <- NULL
    legendnames <- NULL
    if("av.rmse.exp.true.dt" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.dt"], simres$resmat[,"av.rmse.exp.obs.dt"])
      colnames <- c(colnames, "dt.true", "dt.obs")
      col <- c(col, pal["tree"])
      legendnames <- c(legendnames, "disttree")
    }
    if("av.rmse.exp.true.df" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.df"], simres$resmat[,"av.rmse.exp.obs.df"])
      colnames <- c(colnames, "df.true", "df.obs")
      col <- c(col, pal["forest"])
      legendnames <- c(legendnames, "distforest")
    }
    if("av.rmse.exp.true.g" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.g"], simres$resmat[,"av.rmse.exp.obs.g"])
      colnames <- c(colnames, "g.true", "g.obs")
      col <- c(col, pal["gamlss"])
      legendnames <- c(legendnames, "gamlss")
    }
    if("av.rmse.exp.true.b" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.b"], simres$resmat[,"av.rmse.exp.obs.b"])
      colnames <- c(colnames, "b.true", "b.obs")
      col <- c(col, pal["bamlss"])
      legendnames <- c(legendnames, "bamlss")
    }
    if("av.rmse.exp.true.gb" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.gb"], simres$resmat[,"av.rmse.exp.obs.gb"])
      colnames <- c(colnames, "gb.true", "gb.obs")
      col <- c(col, pal["gamboostLSS"])
      legendnames <- c(legendnames, "gamboostLSS")
    }
    if("av.rmse.exp.true.rf" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.rf"], simres$resmat[,"av.rmse.exp.obs.rf"])
      colnames <- c(colnames, "rf.true", "rf.obs")
      col <- c(col, pal["randomForest"])
      legendnames <- c(legendnames, "randomForest")
    }
    if("av.rmse.exp.true.cf" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.cf"], simres$resmat[,"av.rmse.exp.obs.cf"])
      colnames <- c(colnames, "cf.true","cf.obs")
      col <- c(col, pal["cforest"])
      legendnames <- c(legendnames, "cforest")
    }

    colnames(rmse) <- colnames
    if(is.null(ylim)) ylim <- c(min(na.omit(rmse)), max(na.omit(rmse)))
    
    plot(x = simres$x.axis, y = rmse[,1], type = "l", col = col[1], ylim = ylim,
         xlab = "kappa", ylab = "RMSE")
    lines(x = simres$x.axis, y = rmse[,2], type = "l", lty = 2, col = col[1])
    
    for(i in 1:(length(col)-1)){
      lines(x = simres$x.axis, y = rmse[,(2*i+1)], type = "l", col = col[i+1])
      lines(x = simres$x.axis, y = rmse[,(2*i+2)], type = "l", lty = 2, col = col[i+1])
    }
    if(legend) legend('topleft', legendnames, 
           col = col, lty = 1, cex = 0.7)
  } 
  
  if(type == "par"){
    
    rmse <- NULL
    colnames <- NULL
    col <- NULL
    legendnames <- NULL
    if("av.rmse.exp.true.dt" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.dt"], simres$resmat[,"av.rmse.sigma.dt"])
      colnames <- c(colnames, "dt.true", "dt.obs")
      col <- c(col, pal["tree"])
      legendnames <- c(legendnames, "disttree")
    }
    if("av.rmse.exp.true.df" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.df"], simres$resmat[,"av.rmse.sigma.df"])
      colnames <- c(colnames, "df.true", "df.obs")
      col <- c(col, pal["forest"])
      legendnames <- c(legendnames, "distforest")
    }
    if("av.rmse.exp.true.g" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.g"], simres$resmat[,"av.rmse.sigma.g"])
      colnames <- c(colnames, "g.true", "g.obs")
      col <- c(col, pal["gamlss"])
      legendnames <- c(legendnames, "gamlss")
    }
    if("av.rmse.exp.true.b" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.b"], simres$resmat[,"av.rmse.sigma.b"])
      colnames <- c(colnames, "b.true", "b.obs")
      col <- c(col, pal["bamlss"])
      legendnames <- c(legendnames, "bamlss")
    }
    if("av.rmse.exp.true.gb" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.gb"], simres$resmat[,"av.rmse.sigma.gb"])
      colnames <- c(colnames, "gb.true", "gb.obs")
      col <- c(col, pal["gamboostLSS"])
      legendnames <- c(legendnames, "gamboostLSS")
    }
    if("av.rmse.exp.true.rf" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.rf"], simres$resmat[,"av.rmse.sigma.rf"])
      colnames <- c(colnames, "rf.true", "rf.obs")
      col <- c(col, pal["randomForest"])
      legendnames <- c(legendnames, "randomForest")
    }
    if("av.rmse.exp.true.cf" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.cf"], simres$resmat[,"av.rmse.sigma.cf"])
      colnames <- c(colnames, "cf.true","cf.obs")
      col <- c(col, pal["cforest"])
      legendnames <- c(legendnames, "cforest")
    }

    colnames(rmse) <- colnames
    if(is.null(ylim)) ylim <- c(min(na.omit(rmse)), max(na.omit(rmse)))
    
    plot(x = simres$x.axis, y = rmse[,1], type = "l", col = col[1], ylim = ylim,
         xlab = "kappa", ylab = "RMSE")
    lines(x = simres$x.axis, y = rmse[,2], type = "l", lty = 2, col = col[1])
    
    for(i in 1:(length(col)-1)){
      lines(x = simres$x.axis, y = rmse[,(2*i+1)], type = "l", col = col[i+1])
      lines(x = simres$x.axis, y = rmse[,(2*i+2)], type = "l", lty = 2, col = col[i+1])
    }
    if(legend) legend('topleft', legendnames, 
           col = col, lty = 1, cex = 0.7)
  }
}



# plot loglikelihood
plot_ll <- function(simres, ylim = NULL, legend = TRUE){
  
    ll <- NULL
    colnames <- NULL
    col <- NULL
    legendnames <- NULL
    if("av.rmse.exp.true.dt" %in% colnames(simres$resmat)) {
      ll <- cbind(ll, simres$resmat[,"av.loglik.dt"])
      colnames <- c(colnames, "dt.ll")
      col <- c(col, pal["tree"])
      legendnames <- c(legendnames, "disttree")
    }
    if("av.rmse.exp.true.df" %in% colnames(simres$resmat)) {
      ll <- cbind(ll, simres$resmat[,"av.loglik.df"])
      colnames <- c(colnames, "df.ll")
      col <- c(col, pal["forest"])
      legendnames <- c(legendnames, "distforest")
    }
    if("av.rmse.exp.true.g" %in% colnames(simres$resmat)) {
      ll <- cbind(ll, simres$resmat[,"av.loglik.g"])
      colnames <- c(colnames, "g.true")
      col <- c(col, pal["gamlss"])
      legendnames <- c(legendnames, "gamlss")
    }
    if("av.rmse.exp.true.b" %in% colnames(simres$resmat)) {
      ll <- cbind(ll, simres$resmat[,"av.loglik.b"])
      colnames <- c(colnames, "b.true")
      col <- c(col, pal["bamlss"])
      legendnames <- c(legendnames, "bamlss")
    }
    if("av.rmse.exp.true.gb" %in% colnames(simres$resmat)) {
      ll <- cbind(ll, simres$resmat[,"av.loglik.gb"])
      colnames <- c(colnames, "gb.true")
      col <- c(col, pal["gamboostLSS"])
      legendnames <- c(legendnames, "gamboostLSS")
    }
    if("av.rmse.exp.true.rf" %in% colnames(simres$resmat)) {
      ll <- cbind(ll, simres$resmat[,"av.loglik.rf"])
      colnames <- c(colnames, "rf.true")
      col <- c(col, pal["randomForest"])
      legendnames <- c(legendnames, "randomForest")
    }
    if("av.rmse.exp.true.cf" %in% colnames(simres$resmat)) {
      ll <- cbind(ll, simres$resmat[,"av.loglik.cf"])
      colnames <- c(colnames, "cf.true")
      col <- c(col, pal["cforest"])
      legendnames <- c(legendnames, "cforest")
    }

    colnames(ll) <- colnames
    if(is.null(ylim)) ylim <- c(min(na.omit(ll)), max(na.omit(ll)))
    
    plot(x = simres$x.axis, y = ll[,1], type = "l", col = col[1], ylim = ylim,
         xlab = "kappa", ylab = "log-likelihood")
    
    for(i in 2:length(col)){
      lines(x = simres$x.axis, y = ll[,i], type = "l", col = col[i])
    }
    if(legend) legend('topleft', legendnames, 
           col = col, lty = 1, cex = 0.7)
}
@



\title{Distributional Trees and Forests}
\Plaintitle{disttree: Distributional Trees and Forests} 

\author{Lisa Schlosser\\Universit\"at Innsbruck
   \And Torsten Hothorn\\Universit\"at Z\"urich
   \And Reto Stauffer\\Universit\"at Innsbruck
   \And Achim Zeileis\\Universit\"at Innsbruck}
\Plainauthor{Lisa Schlosser, Torsten Hothorn, Reto Stauffer, Achim Zeileis}

\Abstract{
In regression analysis one is interested in the relationship between a dependent variable and one or more explanatory variables. Various methods to fit statistical models to the data set have been developed, starting from linear models considering only the mean of the response variable and ranging to probabilistic models where all parameters of a distribution are fit to the given data set.\\
If there is a strong variation within the data it might be advantageous to split the data first into more homogeneous subgroups based on given covariates and then fit a local model in each subgroup rather than fitting one global model to the whole data set. This can be done by applying regression trees and forests.\\
Both of these two concepts, parametric modeling and algorithmic trees, have been investigated and developed further, however, mostly separated from each other. Therefore, our goal is to embed the progress made in the field of probabilistic modeling in the idea of algorithmic tree and forest models. In particular, more flexible models such as GAMLSS (\cite{stasinopoulos2005}) should be fitted in the nodes of a tree in order to capture location, scale and shape as well as censoring, tail behavior etc. while non-additive effects of the explanatory variables can be detected automatically by the splitting algorithm used to build the tree. In distributional forests an ensemble of distributional trees is built and used to calculate weights which are then included in the fitting process of a distributional model. These forest models can detect smooth effects as well as abrupt changes and interactions in a reasonable way without needing any kind of variable selection or information a!
 bout the expected effects advance and therefore offers a good compromise particularly in complex settings. 
}
\Keywords{parametric models, trees and forests, recursive partitioning, maximum likelihood}

\Address{
  Lisa Schlosser \\
  Department of Statistics \\
  Faculty of Economics and Statistics \\
  Universit\"at Innsbruck \\
  Universit\"atsstr.~15 \\
  6020 Innsbruck, Austria \\
  E-mail: \email{Lisa.Schlosser@uibk.ac.at} \\
  URL: \url{https://www.uibk.ac.at/statistics/personal/schlosser-lisa/lisa-schlosser.html}\\
  
  Torsten Hothorn\\
  Institut f\"ur Sozial- und Pr\"aventivmedizin, Abteilung Biostatistik \\
  Universit\"at Z\"urich \\
  Hirschengraben 84\\
  CH-8001 Z\"urich, Switzerland \\
  E-mail: \email{Torsten.Hothorn@R-project.org}\\
  URL: \url{http://user.math.uzh.ch/hothorn/}\\
  
  Reto Stauffer \\
  Department of Statistics \\
  Faculty of Economics and Statistics \\
  Universit\"at Innsbruck \\
  Universit\"atsstr.~15 \\
  6020 Innsbruck, Austria \\
  E-mail: \email{Reto.Stauffer@uibk.ac.at} \\
  URL: \url{https://www.uibk.ac.at/statistics/personal/stauffer/reto-stauffer.html}\\
  
  Achim Zeileis \\
  Department of Statistics \\
  Faculty of Economics and Statistics \\
  Universit\"at Innsbruck \\
  Universit\"atsstr.~15 \\
  6020 Innsbruck, Austria \\
  E-mail: \email{Achim.Zeileis@R-project.org} \\
  URL: \url{http://eeecon.uibk.ac.at/~zeileis/}

}

\begin{document}
\SweaveOpts{concordance=TRUE}


\SweaveOpts{eps=FALSE, keep.source=TRUE, eval = TRUE}

\SweaveOpts{eval = TRUE}
\section{Introduction}

In the field of regression analysis many statistical models have already been developed to model the relationship between a response variable and one or more explanatory variables.\\
Among the first were linear models (LM) which predict the (conditional) expected value of the response variable as a linear combination of the explanatory variables. This idea was extended to generalized linear models (GLM) which broadened the range of possible distributions of the response variable to the exponential family. (In R these models can be built using the functions \code{lm()} and \code{glm()} provided by the package \pkg{stats}).\\
Then, in order to also allow for non-linear effects, generalized additive models (GAM) were introduced where smooth functions of the explanatory variables are summed up instead of the linear combination. (In the fitting function \code{gam} types of smoothing terms can be selected.)\\
However, with these methods only the mean of the distribution of the response variable can be modeled. Generalized additive models for location, scale and shape
(GAMLSS) (\cite{stasinopoulos2005}) made it possible to model each of the parameters of a distribution separately by its own GAM. In that way a whole distribution can be specified. The R-packages \pkg{gamlss} and \pkg{gamlss.dist} offer the corresponding software with a wide variety of distributions.\\
This development in the field of parametric modeling is illustrated in Figure \ref{Devel_parmod}.\\


\begin{figure}[t!]
\minipage{0.29\textwidth}
\begin{center}
<<motivation_GLM, echo=FALSE, results=hide>>=
nobs <- 200
## GLM
set.seed(7)
x <- c(1:nobs)/nobs
ytrue <- 1+x*1.5
y <- ytrue + rnorm(nobs,0,0.3)
@

<<plot_motivation_GLM, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(y=y , x=x, xaxt="n", yaxt="n", ann=FALSE, col = "slategray", pch = 19)
box(lwd=5)
lines(x = x, y = ytrue, col = pal["forest"], lwd = 7, main = "")
@
\end{center}
\endminipage
{\LARGE$\rightarrow$}
\minipage{0.29\textwidth}
\begin{center}
<<motivation_GAM, echo=FALSE, results=hide>>=
## GAM
set.seed(7)
x <- c(1:nobs)/nobs
x <- 2*(x-0.5)
ytrue <- x^3 
y <- ytrue + rnorm(nobs,0,0.3)
@

<<plot_motivation_GAM, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(y=y , x=x, xaxt="n", yaxt="n", ann=FALSE, col = "slategray", pch = 19)
box(lwd=5)
lines(x = x, y = ytrue, col = pal["forest"], lwd = 7, main = "")
@
\end{center}
\endminipage
{{\LARGE$\rightarrow$}}
\minipage{0.29\textwidth}
\begin{center}
<<motivation_GAMLSS, echo=FALSE, results=hide>>=
## GAMLSS
set.seed(7)
x <- c(1:nobs)/nobs
x <- 2*(x-0.5)
ytrue <- x^3
var <- exp(-(2*x)^2)/2
y <- ytrue + rnorm(nobs, 0, 0.1 + var)
@

<<plot_motivation_GAMLSS, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(x, y, xaxt = "n", yaxt = "n", ann = FALSE, type = "n")
polygon(c(x, rev(x)), c(ytrue + 0.1 + var, rev(ytrue - 0.1 - var)),
  col = pallight["forest"], border = "transparent")
lines(x, ytrue, col = pal["forest"], lwd=7)
points(x, y, col = "slategray", pch = 19)
box(lwd = 5)
@
\end{center}
\endminipage
\vspace{0.0cm}
\minipage{0.25\textwidth}
\begin{center}
\hspace{0.4cm}
LM, GLM 
\end{center}
\endminipage
\hspace{1.4cm}
\minipage{0.25\textwidth}
\begin{center}
GAM
\end{center}
\endminipage
\hspace{1.4cm}
\minipage{0.25\textwidth}
\begin{center}
GAMLSS
\end{center}
\endminipage
\caption{\label{Devel_parmod}Development in the field of parametric modeling}
\end{figure}




\begin{figure}[t!]
\minipage{0.29\textwidth}
\begin{center}
<<motivation_regtree, echo=FALSE, results=hide>>=
## Reg. Tree
set.seed(7)
kappa <- 12
x <- c(1:nobs)/nobs
ytrue <- ytree <- yforest <- numeric(length = length(x))
for(i in 1:nobs) ytrue[i] <- if(x[i]<1/3) 0.5 else 1+(1-plogis(kappa*(2*(x[i]-0.2)-1)))
y <- ytrue + rnorm(nobs,0,0.3)
for(i in 1:nobs) ytree[i] <- if(x[i]<1/3) 0.5 else {if(x[i]<2/3) 2 else 1}
@

<<plot_motivation_regtree, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(x = x, y = y, xaxt="n", yaxt="n", ann=FALSE, col = "slategray", pch = 19)
box(lwd=5)
#lines(x = x, y = ytrue, col = "grey", lwd=5, main = "")
lines(x = x, y = ytree, col = pal["forest"], lwd=7)
@
\end{center}
\endminipage
{\LARGE$\rightarrow$}
\minipage{0.29\textwidth}
\begin{center}
<<motivation_randforest, echo=FALSE, results=hide>>=
## Random Forest
for(i in 1:nobs) yforest[i] <- if(x[i]<0.27) 0.5 else { if(x[i]<0.39) 0.5 + 1.5*(plogis((x[i]-0.33)/6*700)) else 1+(1-plogis(kappa*(2*(x[i]-0.2)-1)))}
@

<<plot_motivation_randforest, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(x = x, y = y, xaxt="n", yaxt="n", ann=FALSE, col = "slategray", pch = 19)
box(lwd=5)
#lines(x = x, y = ytrue, col = "grey", lwd=5, main = "")
lines(x = x, y = yforest, col = pal["forest"], lwd=7, main = "")
@
\end{center}
\endminipage
{\LARGE$\rightarrow$}
\minipage{0.29\textwidth}
\begin{center}
<<motivation_distforest, echo=FALSE, results=hide>>=
## distforest
set.seed(4723)
for(i in 1:nobs) yforest[i] <- if(x[i]<0.27) 0.5 else { if(x[i]<0.39) 0.5 + 1.5*(plogis((x[i]-0.33)/6*700)) else 1+(1-plogis(kappa*(2*(x[i]-0.2)-1)))}
xt <- 2*(x-0.5)
var <- exp(-(2*xt)^4)/2
y <- ytrue + rnorm(nobs, 0, 0.1 + var)
x <- c(1:nobs)/nobs
@

<<plot_motivation_distforest, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(x, y, xaxt = "n", yaxt = "n", ann = FALSE, type = "n")
polygon(c(x, rev(x)), c(ytrue + 0.1 + var, rev(ytrue - 0.1 - var)),
  col = pallight["forest"], border = "transparent")
lines(x = x, y = yforest, col = pal["forest"], lwd=7, main = "")
points(x, y, col = "slategray", pch = 19)
box(lwd=5)
@
\end{center}
\endminipage


\minipage{0.29\textwidth}
\begin{center}
\vspace{0.0cm}
Regression Tree 
\end{center}
\endminipage
\hspace{0.65cm}
\minipage{0.29\textwidth}
\begin{center}
\vspace{0.0cm}
Random Forest 
\end{center}
\endminipage
\hspace{0.65cm}
\minipage{0.28\textwidth}
\begin{center}
\vspace{0.0cm}
Distributional Forest 
\end{center}
\endminipage\\
\minipage{0.29\textwidth}
\vspace{0.2cm}
\center {\LARGE$\downarrow$}\\
\vspace{-0.4cm}
%\hspace{0.0cm}
\begin{center}
<<motivation_disttree, echo=FALSE, results=hide>>=
## disttree
set.seed(74)
x <- c(1:nobs)/nobs
var <- ytree/3
y <- ytrue + rnorm(nobs,0,var)
@

<<plot_motivation_disttree, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(x = x, y = y, xaxt="n", yaxt="n", ann=FALSE, col = "slategray", pch = 19)
lines(x = x, y = ytree, col = pal["forest"], lwd=7, main = "")
polygon(c(x, rev(x)), c(ytree + var, rev(ytree - var)),
  col = pallight["forest"], border = "transparent")
box(lwd=5)
@
\vspace{-0.2cm}\\
Distributional Tree
\vspace{0.2cm}
\end{center}
\endminipage
\caption{\label{Devel_treeforest}Development in the field of algorithmic trees and forests}
\end{figure}


All these parametric models work very well as long as only additive effects appear in the considered data. A different type of model that can capture non-additive effects as well are regression trees. Here the basic idea is to split the data set into more homogeneous subgroups based on information provided by the explanatory variables and to fit a model/response to each terminal node. One problem that might appear when applying tree algorithms is that they impose steps and abrupt changes on the model, even in situations where effects are rather smooth, as it can be seen in the top left plot of Figure \ref{Devel_treeforest}. One way how to tackle this problem is to not just consider one tree but an ensemble of trees. This is the idea of random forests. A set of trees is built, each on a slightly different data set which can either be a bootstrap or a subsample of the original data set. Then the results of all trees are combined to get a final model which unlike a tree model c!
 an also deal with smooth effects in a reasonable way.\\ 
In distributional trees and forests we now want to combine all these ideas from different fields of statistical modeling in order to benefit from all their advantages.\\
\\
In the past the combination of parametric data models and algorithmic tree models has gained more and more interest since it allows for a more flexible and clear structure. \\
Algorithms that only fit constants to each final node and therefore only model the mean of the response variable (such as CART, Breiman 1984) might lead to very large trees which complicates analyzing and interpreting. Algorithms such as GUIDE (Loh 2002), CRUISE (Kim and Loh 2001) and LOTUS (Chan and Loh 2004) were among the first to provide trees with parametric models in each terminal node. The MOB algorithm (Zeileis, Hothorn and Hornik 2008), an algorithm for model-based recursive partitioning, extended this idea by fitting parametric models to each node and using the gained information of the fitted model to create further splits in inner nodes. Using this algorithm as a framework to build trees LMs and GLMs can be fitted in the nodes of the trees. However, up to now trees which only model the mean of the response variable in its nodes are still more common and widely used.\\
Random forests (\cite{breiman2001random}) applying the CART algorithm to build the trees and cforest applying the ctree algorithm (\cite{hothorn2015partykit}, \cite{hothorn2006unbiased}) have used this idea of combining an ensemble of trees to get smoother effects of the covariates after combining or averaging over the parameters of the trees.\\
\\
But as mentioned before, until now the above listed development in the field of probabilistic modeling has not yet been fully integrated in the idea of algorithmic tree and forest models. Probabilistic models are rarely used in combination with tree algorithms and more flexible models such as GAMLSSs have not yet been applied as models in the nodes of a tree.\\
\\
Therefore, our main goal is to embed the progress made in the field of probabilistic modeling in the idea of regression trees and forests. In particular, more flexible models such as GAMLSSs should be fitted in the nodes of a tree in order to capture location, scale and shape as well as censoring, tail behavior etc. while non-additive effects or interactions of the explanatory variables can be detected automatically by the splitting algorithm used to build the tree. In that way, no variable selection or predefining of variable interactions is necessary and a whole distribution is specified in each node which leads to a wide range of inference methods that can be applied.\\
These (future) steps are illustrated in Figure \ref{Devel_treeforest}, embedded in the development made in the field of tree and forest algorithms.\\







\newpage
\section{Methodology}
\label{Methodology}
\subsection{Distributional Fit}
Fitting a distributional model to a set of observations without considering any covariates is a well known procedure which can for example be done by applying the maximum likelihood method. This is also the chosen approach in the methodology described in this section.\\
The goal is to fit a distributional model $D(Y, \theta)$ to a response variable $Y \in \mathcal{Y}$ for a \textit{k}-dimensional vector of distribution parameters $\theta \in \Theta$ in case of a \textit{k}-parametric distribution. The distribution family has to be specified in advance such that a log-likelihood function $\ell(\theta; Y)$ is provided. Therefore, the task of fitting a distributional model turns into the task of determining the distribution parameter $\theta$. This is done by the following maximization.
$$
\hat{\theta} = \max_{\theta \in \Theta} \sum_{i=1}^n \ell(\theta; y_i)
$$
where $\{y_i\}_{i=1,\ldots,n}$ are the observations of the response variable $Y$.\\
In that way, a whole distribution is specified with all its features, including location, scale and shape. However, fitting one global model to the whole data set might be too much of a generalization and therefore not represent the data and its features in a reasonable way. Moreover, if covariates are available it is desirable to include them into the model as they might provide important information. In particular, they can be used to separate the data into more homogeneous subgroups. Doing so after fitting a global model to the whole data set and then fitting a local model to each subgroup should improve the model remarkably. This procedure of splitting a data set into subgroups can be done by applying a tree algorithm.

\subsection{Distributional Tree}
After having fit a global distributional model $D(Y, \bold{\hat{\theta}})$ the idea is now to use the information provided by this model together with a set of covariates $\bold{Z} = (Z_1, \ldots, Z_m) \in \mathcal{Z}$ to decide if the data set should be split into subgroups. In particular, the information provided by the fitted model are the scores 
$$
s(\hat{\theta}, y_i) = \frac{\partial \ell}{\partial \theta}(\hat{\theta}; y_i).
$$ 
These values express how well the model fits the data regarding each parameter. Ideally they should fluctuate randomly around zero, similar to residuals in ordinary least squares estimations, but in this case there is a score value for each estimated parameter $\hat{\theta}_j \in (\hat{\theta}_1, \ldots, \hat{\theta}_k) = \bold{\hat{\theta}}$ and each observation.\\
Statistical tests are applied to find out whether any significant connection/dependency between the scores and each of the partitioning variables $Z_l \in \bold{Z} = (Z_1, \ldots, Z_m)$ is given. If so, the data set is split into subgroups based on the values of the partitioning variable that shows the highest connection/dependency.\\
In a next step a local distributional model is fit in each of the subgroups. Repeating this procedure of fitting a model, applying statistical tests to evaluate the goodness of fit and splitting the data set depending on the test results leads to a tree structure. In that way the learning data set is separated into \textit{B} disjoint segments such that
$$
\mathcal{Z} = \dot{\bigcup\limits_{b = 1, \ldots, \textit{B}}} \mathcal{B}_b
$$
where each segment is represented by a terminal node of the tree.\\

The exact way of determining how and where to split varies over different tree algorithms. For distributional trees, one of two algorithms can be chosen as a framework to build the tree: either the MOB algorithm or the ctree algorithm. Summed up, the steps of building a distributional tree using one of these algorithms are
\begin{enumerate}
\item Specify a distribution with log-likelihood function $\ell(\theta; y)$.
\item Estimate $\bold{\hat{\theta}}$ via maximum likelihood.
\item Test for associations or instabilities of the scores $s(\bold{\hat{\theta}}, y_i)$ and each partitioning variable $Z_l$.
%\item Assess whether the \emph{model scores} are associated with
%    (or change along) any of the available covariates -- e.g.,
%    using parameter instability tests (\emph{strucchange}) or
%    conditional inference (\emph{coin}).
\item Split the sample along the partitioning variable with the strongest association or instability.
    Choose the breakpoint with the highest improvement in log-likelihood.
\item Repeat steps 2--4 recursively in the subgroups
    until some stopping criterion is met.
    %-- e.g., for significance or sample size.
%\item Choose the variable with the strongest association
%\item Choose the split point which leads to the highest improvement of the model
%\item Split and repeat 2-6 in each node until a stopping criterion is met
\end{enumerate}
The stopping criterion in step 5 can for example be a minimum number of observations in a node or a p-value for the statistical tests applied in step 3.\\
Executing these steps results in a stepwise parametric model where a complete distribution is fit in each node of the tree.\\
\\
For distributional trees the strategy of how to make a prediction for a new observation $\bold{z}^{\ast} = (z_1^{\ast}, \ldots, z_m^{\ast})$ is very simple and straight forward. All observations of the learning data set $\{(y_i,\bold{z}_i)\}_{i=1\ldots,n}$ which end up in the same terminal node of the tree as the new observation are selected and a distributional model is fit on the subset. To select this subset a weight vector $\bold{\omega^{tree}} = (\omega^{tree}_1, \ldots, \omega^{tree}_n)$ is included in the fitting process.
$$
\omega^{tree}_i(\bold{z}^{\ast}) = \sum_{b=1}^B I((\bold{z}_i \in \mathcal{B}_b) \land (\bold{z}^{\ast} \in \mathcal{B}_b))
$$
The predicted distribution of the new observation is then fully specified by the estimated parameter $\hat{\theta}(\bold{z}^{\ast})$ where
$$
\hat{\theta}(\bold{z}^{\ast}) = \max_{\theta \in \Theta} \sum_{i=1}^n \omega^{tree}_i(\bold{z}^{\ast}) \cdot \ell(\theta; y_i).
$$

While the clear structure of such a model is advantageous in analyzing it, the abrupt changes are often too rough and impose steps on the model even in case of smooth effects. In these situations bagging can solve the problem by combining an ensemble of trees such that wrongly detected abrupt changes of the model are turned into almost smooth transitions. In that way, all kinds of effects can be modeled, which is is also one the of the main advantages of forest models (taking bagging one step further as the variance is reduced by sampling the covariates considered for the splitting process in each node).

\subsection{Distributional Forest}
A distributional forest consists of an ensemble of distributional trees $\{t_s\}_{s=1,\ldots,T}$. Each of these trees is built on a different data set which can either be a bootstrap or a subset of the original data set. Moreover, in each node a subset of the partitioning variables is chosen randomly as candidates for splitting variables. In that way the correlation among the resulting trees is reduced and therefores also the variance of the model.\\
\\
Forest models differ mainly in the way how the trees are combined to make predictions. In this case the forest itself is only used to evaluate neighborhood relationships among the observations which can then be included in the fitting process.\\
Contrary to distributional trees, in distributional forests the whole learning data set can be used to fit the distributional model and specify the predicted distribution for a new observation. This difference is represented in the weight vector, as the tree weights $\bold{\omega}^{tree}$ containing only zeros and ones are now replaced by integer valued forest weights \\
$\bold{\omega}^{forest} = (\omega^{forest}_1, \ldots, \omega^{forest}_n)$.\\
For a new observation $\bold{z}^{\ast} = (z^{\ast}_1, \ldots, z^{\ast}_m) $ this set of weights is obtained by the following procedure: For each observation $(y_j,\bold{z}_j)$ in the learning data set $\{(y_i,\bold{z}_i)\}_{i=1\ldots,n}$ a weight is calculated by counting the number of trees in which it ends up in the same terminal node as the new observation $\bold{z}^{\ast}$. 
$$
\omega^{forest}_i(\bold{z}^{\ast}) = \sum_{s=1}^T \sum_{b=1}^{B^s} I((\bold{z}_i \in \mathcal{B}^s_b) \land (\bold{z}^{\ast} \in \mathcal{B}^s_b))
$$
where $T$ is the number of trees of the forest, $B^s$ is the number of terminal nodes of the \textit{s}-th tree and $\mathcal{B}^s_b$ represents the \textit{b}-th terminal node of the \textit{s}-th tree.\\
This individual set of weights $\bold{\omega}^{forest}(\bold{z}^{\ast}) = (\omega^{forest}_1(\bold{z}^{\ast}), \ldots, \omega^{forest}_n(\bold{z}^{\ast}))$ for the new observation $\bold{z}^{\ast}$ can now be included in the estimation process of the distribution parameters and in that way, leads to its individual parameter vector $\hat{\theta}(\bold{z}^{\ast})$.
$$
\hat{\theta}(\bold{z}^{\ast}) =  \max_{\theta \in \Theta} \sum_{i=1}^n \omega^{forest}_i(\bold{z}^{\ast}) \cdot \ell(\theta; y_i)
$$
While in a distributional tree the same distribution is specified for all observations being in the same node a distributional forest predicts and fits a different distribution for each different observation which is due to the individual weights calculated separately for each observation.











\newpage
\section{Simulation Study}
Within this simulation study the performance of distributional trees and forests is tested and compared to other models, showing situations where one or the other method is more advantageous than others. By doing so the following hypotheses are investigated:
\begin{itemize}%[leftmargin=0.5cm]
\item When applying a tree algorithm the use of a fitting function which returns scores for each distribution parameter is advantageous to find correct splits. In particular it is crucial to find splits in parameters other than the location parameter. Hence a distributional forest consisting of distributional trees leads to better results than a simple forest considering only the mean.
\item In the case of smooth parameter functions without any abrupt changes / non-additive effects a smooth parameteric model such as GAMLSS is clearly the first choice while a stepfunction can be modelled best by applying a tree-structured model. Even though for both of these extreme situations a specificly advandageous method is provided, a forest model can be a good compromise and performs well in a wide range of settings including the two mentioned before. (Therefore, a forest is a recommendable choice particularly when no information about the type of parameter function is provided.)
\item While specifying interactions and selecting covariates is necessary to achieve reasonable results in parameteric models such as GAMLSS, this is done automatically within trees and forests. For that reason, distributional trees and forests provide a framework where no additional effort is required for the model specification. This feature plays to its strength especially in case of a high number of covariates and no information about their effects on the response.
\end{itemize}

%Generally, a smooth parametric model such as GAMLSS should be the most successful method if the parameter function is smooth with no rapid changes. However, if there are any jumps or non-additive effects or interactions, the trees should be more effective in detecting these steps and therefore also lead to a better fitting forest. But this situation might also appear for smooth functions which contain sections with very steep slopes. Therefore, it is now of great interest, whether the concept of smooth parametric models or of trees and forests covers a broader range of functions where it is superior to the other.\\


The following methods are applied and compared to distributional trees and forests:
\begin{itemize}
\item gamlss \cite{stasinopoulos2005} fits a separate generalized additive model to each of the distribution models. In that way it specifies a whole distribution and is a very efficient option for smooth effects of the covariates on the parameters.
\item bamlss \cite{bamlss2017} also provides an infrastructure for estimating probabilistic distributional regression but within a Bayesian Framework. Again each parameter is modelled by (possibly) complex additive terms.
\item gamboostLSS \cite{gamboostLSS2017} offers boosting models for fitting  generalized additive models for location, shape and scale
\item randomForest \cite{randomForest2002} combines an ensemble of classification and regression trees applying bagging and reducing the correlation among trees through a randomized selection of possible split variables within each node.
\item cforest \cite{hothorn2015partykit} builds a forest in a similar way but consisting of ctrees applying the ctree algorithm mentioned in section \ref{Methodology}. In this simulation study no transformation function is included.
\end{itemize}

The different methods are tested on generated data sets consisting of the values of a response variable $Y$ and ten covariates $X_1, \ldots, X_{10}$ which are all independent und identically distributed following a uniform distribution.
$$
X_i \sim \textit{Unif}([-1,1]) \qquad \forall i \in \{1,\ldots,10\}
$$

The distribution of the response variable is a left censored normal distribution with censoring point 0. The corresponding distribution parameters are defined by a parameter function $f$ depending on the covariates and an additional factor $\kappa$ which regulates the steepness of certain sections of the parameter function. For low values of $\kappa$ the resulting parameters change rather slowly over varying values of the covariates while high values of $\kappa$ lead to abrupt changes. \\ 
The location parameter $\mu$ depends on $X_1$ and $\kappa$ and the scale parameter $\sigma$ depends on $X_2$ and $\kappa$. The other covariates $X_3, \ldots, X_{10}$ are used as noise variables.\\
The latent variable $Y^{\ast}$ is generated based on the following setting.
\begin{center}
$$ 
Y^{\ast} \sim \mathcal{N} (f(X_1, X_2, \kappa))
$$
with
$$
f(X_1, X_2, \kappa) = \left(\mu(X_1,\kappa) , \sigma(X_2,\kappa)\right)
$$ 
and
$$
\mu(X_1,\kappa) = \mu_{\text{base}} + 8 \cdot \exp{(-(3 \cdot X_1-1)^{(2\cdot \kappa)})}
$$

$$
\sigma(X_2,\kappa) = 2 + 2 \cdot (1-\Lambda(\kappa^{1.8} \cdot 5 \cdot (X_2+0.5)))
$$
where $\Lambda$ is the cumulative distribution function of the logistic distribution
$$
\Lambda(\omega) = \frac{1}{1 + \exp(-\omega)}
$$
The response variable $Y$ is defined as 
$$
Y = 
\begin{cases}
    Y^{\ast},& \text{if } Y^{\ast} \geq 0\\
    0,       & \text{if } else
\end{cases}
$$
\end{center}


To illustrate the parameter functions and the impact of $\kappa$ together with the results of the considered methods they are first applied to one generated data set only where $\kappa$ is set to 1 (top plots) and then to a second data set where $\kappa$ is set to 10 (bottom plots).\\
In the left plots the location parameter function $\mu(X_1, \kappa)$ is plotted along the covariate $X_1$ together with the estimated values of the different models for fixed covariates $(x_2, x_3, \ldots, x_{10}) = (0,0, \ldots, 0)$. \\
In the right plots the scale parameter function $\sigma(X_2, \kappa)$ is plotted along the covariate $X_2$ together with the estimated values of the different models for fixed covariates $(x_1, x_3, \ldots, x_{10}) = (0.4, 0, \ldots, 0)$. \\


\SweaveOpts{eval = FALSE}

<<echo=FALSE, results=hide>>=
source("oneset.R")
if(file.exists("oneset1.rda")){
  load("oneset1.rda")
} else {
  oneset1 <- sim_oneset(kappa = 7, nobs = 400,
                       seedconst = 7, ntree = 100,
                       formula = y~x1+x2+x3+x4+x5+x6, 
                       tree_minsplit = 25, tree_minbucket = 10, tree_mincrit = 0.95, 
                       forest_minsplit = 25, forest_minbucket = 10, forest_mincrit = 0, 
                       forest_mtry = 3,
                       type.tree = "ctree",
                       censNO = TRUE,
                       fix.mu = FALSE,
                       fix.sigma = FALSE,
                       mu.sigma.interaction = FALSE,
                       pred_fix = TRUE,
                       pred_fix_x1 = -0.5,
                       pred_fix_x4 = 0,
                       gamboost_cvr = FALSE,
                       eval_disttree = TRUE,
                       eval_distforest = TRUE,
                       eval_gamlss = TRUE,
                       eval_gamboostLSS = FALSE,
                       eval_randomForest = FALSE,
                       eval_cforest = FALSE,
                       mubase = 3,
                       sigmabase = 3)
  save(oneset1, file = "oneset1.rda")
}

if(file.exists("oneset10.rda")){
  load("oneset10.rda")
} else {
  oneset10 <- sim_oneset(kappa = 10, nobs = 400,
                       seedconst = 7, ntree = 100,
                       formula = y~x1+x2+x3+x4+x5+x6, 
                       tree_minsplit = 25, tree_minbucket = 10, tree_mincrit = 0.95, 
                       forest_minsplit = 25, forest_minbucket = 10, forest_mincrit = 0, 
                       forest_mtry = 3,
                       type.tree = "ctree",
                       censNO = TRUE,
                       fix.mu = FALSE,
                       fix.sigma = FALSE,
                       mu.sigma.interaction = FALSE,
                       pred_fix = TRUE,
                       pred_fix_x1 = -0.5,
                       pred_fix_x4 = 0,
                       gamboost_cvr = FALSE,
                       eval_disttree = TRUE,
                       eval_distforest = TRUE,
                       eval_gamlss = TRUE,
                       eval_gamboostLSS = FALSE,
                       eval_randomForest = FALSE,
                       eval_cforest = FALSE,
                       mubase = 3,
                       sigmabase = 3)
  save(oneset10, file = "oneset10.rda")
}
@


\begin{figure}[t!]
\begin{subfigure}{0.6\textwidth}
<<sim_oneset1_mu, fig=TRUE, echo=FALSE>>=
plot_oneset(oneset1, 
            compare_mu = TRUE, 
            add_dt = TRUE, add_df = TRUE, add_g = TRUE #, 
            #add_gb = TRUE, add_rf = TRUE, add_cf = TRUE
            )
@
\end{subfigure}
\hspace{-1cm}
\begin{subfigure}{0.6\textwidth}
<<sim_oneset1_sigma, fig=TRUE, echo=FALSE>>=
plot_oneset(oneset1, 
            compare_sigma_line = TRUE, 
            add_dt = TRUE, add_df = TRUE, add_g = TRUE #, 
            #add_gb = TRUE, add_rf = TRUE, add_cf = TRUE
            )
@
\end{subfigure}
\caption{\label{plot_sim_oneset1} ... }
\end{figure}


\begin{figure}[t!]
\begin{subfigure}{0.6\textwidth}
<<sim_oneset10_mu, fig=TRUE, echo=FALSE>>=
plot_oneset(oneset10,
            compare_mu = TRUE,
            add_dt = TRUE, add_df = TRUE, add_g = TRUE #, 
            #add_gb = TRUE, add_rf = TRUE, add_cf = TRUE
            )
@
\end{subfigure}
\hspace{-1cm}
\begin{subfigure}{0.6\textwidth}
<<sim_oneset10_sigma, fig=TRUE, echo=FALSE>>=
plot_oneset(oneset10, 
            compare_sigma_line = TRUE, 
            add_dt = TRUE, add_df = TRUE, add_g = TRUE #, 
            #add_gb = TRUE, add_rf = TRUE, add_cf = TRUE
            )
@
\end{subfigure}
\caption{\label{plot_sim_oneset10} ... }
\end{figure}









Now, to get a more profound impression the methods are evaluated on various data sets. The value of $\kappa$ is increasing from 1 to 10 by a stepsize of 1 and for each step ... data sets are generated. The methods are then compared by the out-of-sample RMSE for the location parameter $\mu$ and the shape parameter $\sigma$ as well as by the out-of-sample log-likelihood of the fitted models, averaged within each step.
(In that way it can be investigated how the results change for a changing value of $\kappa$.)

<<echo=FALSE, results=hide>>=
if(file.exists("simres.rda")){
  load("simres.rda")
} else {
  #source("~/svn/partykit/pkg/disttree/inst/draft/.R")
  source("gensim.R")
  simres <- gensim(seedconst = 7, nrep = 20, ntree = 100,
                   nsteps = 5, stepsize = 2,
                   formula = y~x1+x2+x3+x4+x5+x6,
                   nobs = 400, testnobs = 200L,
                   tree_minsplit = 25, tree_minbucket = 10, tree_mincrit = 0.95,
                   forest_minsplit = 25, forest_minbucket = 10, forest_mincrit = 0, 
                   forest_mtry = 3,
                   fix.mu = FALSE,
                   fix.sigma = FALSE,
                   mu.sigma.interaction = FALSE,
                   mubase = 1,
                   sigmabase = 3,
                   censNO = TRUE,
                   gamboost_cvr = FALSE,
                   eval_disttree = TRUE,
                   eval_distforest = TRUE,
                   eval_gamlss = TRUE,
                   eval_bamlss = FALSE,
                   eval_gamboostLSS = FALSE,
                   eval_randomForest = FALSE,
                   eval_cforest = FALSE)
  
  save(simres, file = "simres.rda")
}
@



\begin{figure}[t!]
\begin{subfigure}{0.6\textwidth}
<<sim_rmse_exp, fig=TRUE, echo=FALSE>>=
plot_rmse(simres, type = "exp")
@
\end{subfigure}
\hspace{-1cm}
\begin{subfigure}{0.6\textwidth}
<<sim_rmse_par, fig=TRUE, echo=FALSE>>=
plot_rmse(simres, type = "par")
@
\end{subfigure}
\caption{\label{plot_sim_rmse}Results of the simulation study (RMSE)\\
left: continuous lines represent the difference between the true and the estimated expected value, dotted lines represent the difference between the observations and the estimated expected value\\
right: continuous lines represent the location parameter, dotted lines the scale parameter}
\end{figure}



\begin{figure}[t!]
\begin{subfigure}{0.6\textwidth}
<<sim_ll, fig=TRUE, echo=FALSE>>=
plot_ll(simres)
@
\end{subfigure}
\hspace{-1cm}
\begin{subfigure}{0.6\textwidth}
<<sim_levelplot, fig=TRUE, echo=FALSE>>=
levelfun <- function(x) simres$fun(x,5)

ncuts <- 100
colR <- colorRampPalette(c("blue","green"))                                       
cols <- colR(ncuts+1)
griddata <- expand.grid(list(x1 = seq(-1, 1, 0.01), x4 = seq(0, 1, 0.2)))
griddata$mu <- levelfun(griddata)
levelplot(mu ~ x1 * x4, data = griddata, xlab = "x1", ylab = "x4", 
          region = TRUE, cuts = ncuts, col.regions = cols)

@
\end{subfigure}
\caption{\label{plot_sim_ll}left: results of the simulation study: log-likelihood\\ %\code{disttree} vs. \code{distforest} vs. \code{gamlss} vs. \code{randomForest} vs. %\code{bamlss} vs. \code{gamboostLSS}\\
right: levelplot of the parameter function for $\kappa=5$}
\end{figure}





%Even though \code{gamlss} can easily deal with each of the smooth functions it struggles with the separation in subgroups. This is the point where the tree structure provides a better solution. However, a single tree can not keep up with the performance of \code{gamlss} when it comes to modeling the smooth functions within the subgroups. Offering a good compromise the forest takes advantages of the tree structure to find subgroups but can also model smooth effects if enough trees are included.\\
%However, once the increasing value of $\kappa$ leads to very steep sections within the parameter functions of each of the subgroups the tree turns out to be the most successful method in this specific situation.\\
%\\
%Looking at the resulting values of the RMSE it can be observed that trees and forests cleary outperform gamlss in this situation. For $\kappa=1$ the performance of all three methods is similar. However, as $\kappa$ increases the RMSE of gamlss increases while the RMSE of trees and forests stays at almost the same level or even decreases sligthly as soon as $\kappa$ is greater or equal 2. The same conclusion can be drawn from looking at the log-likelihood where the resulting values of gamlss decrease rapidly while the values of trees and forests only decrease a little at the beginning but then increase.\\
%This is the result that was expected as trees and forests can deal with rapid changes of the distribution functions more easily.\\
%For low values of $\kappa$ the forests are a little bit more efficient than the trees, but the higher $\kappa$ and therefore also the steeper the parameter function is the better the trees become. Especially regarding the scale parameter the trees are clearly advantageous over the forests.   


%(As it has already been demonstrated in the first examples on single data sets the performances of \code{distforest} and \code{disttree} are not as successful as the one of \code{gamlss} for low values of $\kappa$ but remarkably improve as this values increases and are superior to \code{gamlss} for high values of $\kappa$. This is illustrated by an almost constant RMSE of \code{distforest} and \code{disttree} while the RMSE of \code{gamlss} increases. Comparing the tree and the forest model it can be observed that the forest seems to perform better for lower $\kappa$ while very steep sections lead to a better performance of trees.













\SweaveOpts{eval = TRUE}
\newpage
\newpage
\section{Rain}
In this application we analyze precipitation data meassured over 28 years at 95 (?) stations all over Tyrol and Vorarlberg. The considered data is provided by ..... and consists of the meassured total precipiation within 24 hours of each day in July from 1985 until 2012, both included. Additionally, numerical forecasts from the ECMWF are used as covariates. These forecasts are ensemble predictions consisting of ... individual forecasts based on perturbed initiall conditions.\\
In particular, 14 basic forecast variables are considered with up to 12 variations of these variables being included as covariates. Since each forecast variable consists of an ensemble of individual predictions the possible variations include the mean, the standarad deviation, the minimum and the maximum over all these individual predictions, noted as ensemble mean, ensemble std dev, ensemble min and ensemble max. All of these values can be calculated on the whole day (6--30 UTC) or a 6h time window of the day.\\
Another possible option is to consider the mean, minimum or maximum over 24 hours for each individual prediction and then take the ensemble mean or ensemble standarad deviation, noted as ensemble mean of mean/min/max or ensemble std dev of mean/min/max respectively.\\
The following table shows all variables together with the number of variations and the provided variations listed in the last column.\\
\\
The response variable total precicipitation is power transformed by a factor of 1.6. This is done due to experience and knowledge about possible distributions of precipitation meassurements. The resulting transformed variable is expected to be normally distributed and left censored at censoring point 0.


\newpage
The ensemble forecast for the total precipitation is represented by 12 covariates: 
\begin{itemize}[noitemsep]
\item ensemble mean of the sum of the total precipitation from 6--30 UTC
\item ensemble standard deviation of the sum of the total precipitation from 6--30 UTC
\item ensemble minimum of the sum of the total precipitation from 6--30 UTC
\item ensemble maximum of the sum of the total precipitation from 6--30 UTC
\item ensemble mean of the sum of the total precipitation from 6--12 UTC, from 12--18 UTC, from 18--24 UTC and from 24--30 UTC
\item ensemble standard deviation of the sum of the total precipitation from 6--12 UTC, from 12--18 UTC, from 18--24 UTC and from 24--30 UTC
\end{itemize}

The preciptable water is represented by 6 covariates:
\begin{itemize}[noitemsep]
\item the ensemble mean of the mean of preciptable water over the time period 6--30 UTC 
\item the ensemble mean of the minimum of preciptable water over the time period 6--30 UTC 
\item the ensemble mean of the maximum of preciptable water over the time period 6--30 UTC 
\item the ensemble standard deviation of the mean of preciptable water over the time period 6--30 UTC 
\item the ensemble standard deviation of the minimum of preciptable water over the time period 6--30 UTC 
\item the ensemble standard deviation of the maximum of preciptable water over the time period 6--30 UTC 
\end{itemize}






\newpage
\begin{table}[h]
%\resizebox{1.4\textwidth}{!}{
\begin{minipage}{\textwidth}
\begin{tabular}{| l  c  l |}
\hline
Basic covariates & {\#} & Variations\\
\hline
\textbf{tp}: total precipitation            & 12 variations:  & ensemble mean of sum, \\
\hspace*{\fill} power transformed (by 1.6) &    & ensemble std dev of sum, \\ 
\textbf{cape}: convective                  &    & ensemble minimum of sum, \\
\hspace*{0.4cm} available potential energy &    & ensemble maximum of sum\\
\hspace*{\fill}  power transformed (by 1.6)&    & \qquad all over 6--30 UTC\\
\hspace*{\fill}                            &    & ensemble mean of sum over\\
\hspace*{\fill}                            &    & \qquad 6--12, 12--18, 18--24, 24--30 UTC\\
\hspace*{\fill}                            &    & ensemble std dev of sum over\\
\hspace*{\fill}                            &    & \qquad 6--12, 12--18, 18--24, 24--30 UTC\\
\hline
\textbf{dswrf}: downwards short ware      & 6 variations: & ensemble mean of mean values, \\
\hspace*{\fill} radiation flux (sunshine) &   & ensemble mean of minimal values,\\
\textbf{msl}: mean sea level pressure     &   &  ensemble mean of maximal values,\\
\textbf{pwat}: preciptable water          &   & ensemble std dev of mean values,\\
\textbf{tmax}: 2m maximum                 &   & ensemble std dev of minimal values,\\
\hspace*{1.15cm} temperature              &   & ensemble std dev of maximal values,\\
\textbf{tcolc}: total column-integrated   &   & \qquad all over 6--30 UTC \\
\hspace*{1.05cm} condensate  &   & \\
\textbf{t500}: temperature   &   & \\
\hspace*{1cm} on 500 hPA     &   & \\
\textbf{t700}: temperature   &   & \\
\hspace*{1cm} on 700 hPA     &   & \\
\textbf{t850}: temperature   &   & \\
\hspace{1cm} on 850 hPA      &   & \\
\hline
\textbf{tdiff500850}: temperature         & 3 variations: & ensemble mean of difference in mean,\\
\hspace*{\fill} difference 500 to 850 hPA &   & ensemble minimum of difference in mean,\\
\textbf{tdiff500700}: temperature         &   & ensemble maximum of difference in mean\\
\hspace*{\fill} difference 500 to 700 hPA &   & \qquad all over 6--30 UTC\\
\textbf{tdiff700850}: temperature         &   & \\
\hspace*{\fill} difference 700 to 850 hPA &   & \\
\hline
\textbf{msl{\_}diff}: mean sea level  & 1 variation: & msl{\_}mean{\_}max - msl{\_}mean{\_}min\\
\hspace{1.65cm} difference            &   & \qquad over 6--30 UTC\\
\hline
\end{tabular}
\caption[Table caption text]{Basic covariates together with the number and the type of variations}
\label{covariates}
\end{minipage} 
%}
\end{table}




\newpage


The following methods are applied on this data set:

\begin{table}[h]
%\resizebox{1.4\textwidth}{!}{
\begin{minipage}{\textwidth}
\begin{tabular}{| l | l   l   l |}
\hline
 & \textbf{Type} & \textbf{Location ($\mu$)} & \textbf{Scale ($\sigma$)} \\
\hline
\textbf{Forest} & recursive    & all & all \\ 
                & partitioning &     &     \\ 
\hline
\textbf{GAM (prespecified)} & spline  & tp{\_}mean, & tppow{\_}sprd,\\
                            & in each & tp{\_}max,  & dswrf{\_}sprd{\_}mean,\\
 & & tp{\_}mean1218 $\cdot$ & tp{\_}sprd1218 $\cdot$\\ 
 & & \quad cape{\_}mean1218, & \quad  cape{\_}mean1218,\\
 & & dswrf{\_}mean{\_}mean, & tcolc{\_}sprd{\_}mean,\\
 & & tcolc{\_}mean{\_}mean, & tdiff500850{\_}mean\\
 & & pwat{\_}mean{\_}mean, & \\
 & & tdiff500850{\_}mean, & \\
 & & msl{\_}diff & \\
\hline
\textbf{GAM (boosted)} & spline  & all & all \\
                       & in each &     &     \\
\hline
\textbf{EMOS} & linear & tp{\_}mean & tp{\_}spread \\
\hline
\end{tabular}
\caption[Table caption text]{Table of applied methods together with their type (way of including the covariates) and the included covariates for each distribution parameter}
\label{model_specification}
\end{minipage} 
%}
\end{table}

For the gamlss model the covariates were selected manually based on expert knowledge. Additionally, interactions are specified in advance, once between tppow{\_}mean1218 and capepow{\_}mean1218 for the location parameter and once between tppow{\_}sprd1218 and capepow{\_}mean1218 for the l ocatioin parameter. By putting extra effort in the specification of the formulas the gamlss model is provided with additional information and is therefore expected to profit from this advantage.\\


\SweaveOpts{eval = TRUE}
\newpage
\subsection{Station Axams}
First of all we focus on one observation station, Axams in Tyrol. This station is selected due to its geographical closeness to Innsbruck, the capital of Tyrol.\\
As for all other stations, daily observations of the month of July of 28 years are available, starting in 1985 and ending in 2012.\\


\subsubsection{Predictions}
In a first step one model is fit on the first 25 years (1985--2009, both included) and then predictions for the 26th, 27th and 28th year (2010, 2011 and 2012) are made.\\
To illustrate the results, one day of July is chosen and the estimated distributions for this day of all three prediction years are plotted together with the corresponding obersvations.  
<<echo=FALSE, results=hide>>=
#### Axams prediction 25 - 3
if(file.exists("rain_Axams_pred.rda")){
  #load("~/svn/partykit/pkg/disttree/inst/draft/rain_Axams_pred.rda")
  load("rain_Axams_pred.rda")
} else {
  #source("~/svn/partykit/pkg/disttree/inst/draft/rain_axams_pred.R")
  source("rain_axams_pred.R")
  library("gamlss.cens")
  gen.cens("NO", type = "left")
  res <- rain_axams_pred(seedconst = 7, ntree = 100,
                         forest_mtry = 27,   # if frac == TRUE: nvar/3 = 30
                         gamboost_cvr = TRUE,
                         frac = FALSE)
  
  save(res, file = "rain_Axams_pred.rda")
}

# prepare data for plot of estimated density functions
pday <- 24  # 2 (hohe Beobachtung zu niedrig geschaetzt), 4, 15, evtl. auch 7, 8, 23 (eine 0-Beobachtung und 2 sehr aehnliche), 

pdays <- if(pday<19) c(pday, pday + 31, pday + 61) else c(pday, pday + 30, pday + 61)
pdf <- predict(res$df, newdata = res$testdata[pdays,], type = "parameter")
df_mu <- pdf$mu
df_sigma <- pdf$sigma
df_exp <- pnorm(df_mu/df_sigma) * (df_mu + df_sigma * (dnorm(df_mu/df_sigma) / pnorm(df_mu/df_sigma)))
cbind(df_exp, res$testdata[pdays,"robs"])


# plot predicted distributions together with observations
#set.seed(res$call$seedconst)
set.seed(7)
x <- c(0.01, sort(runif(500,0.01,8)))
y1 <- crch::dcnorm(x, mean = df_mu[1], sd = df_sigma[1], left = 0)
y2 <- crch::dcnorm(x, mean = df_mu[2], sd = df_sigma[2], left = 0)
y3 <- crch::dcnorm(x, mean = df_mu[3], sd = df_sigma[3], left = 0)
dayending <- if(pday > 3) "th" else switch(pday, "1" = {dayending <- "st"}, "2" = {dayending <- "nd"}, "3" = {dayending <- "rd"})

# point mass (slightly shifted)
pm1 <- c(0.04, crch::dcnorm(-1, mean = df_mu[1], sd = df_sigma[1], left = 0))
pm2 <- c(-0.03, crch::dcnorm(-1, mean = df_mu[2], sd = df_sigma[2], left = 0))
pm3 <- c(-0.1, crch::dcnorm(-1, mean = df_mu[3], sd = df_sigma[3], left = 0))

# predictions
pred1 <- c(res$testdata[pdays,"robs"][1], crch::dcnorm(res$testdata[pdays,"robs"][1], mean = df_mu[1], sd = df_sigma[1], left = 0))
pred2 <- c(res$testdata[pdays,"robs"][2], crch::dcnorm(res$testdata[pdays,"robs"][2], mean = df_mu[2], sd = df_sigma[2], left = 0))
pred3 <- c(res$testdata[pdays,"robs"][3], crch::dcnorm(res$testdata[pdays,"robs"][3], mean = df_mu[3], sd = df_sigma[3], left = 0))

#legendheight
lh1 <- crch::dcnorm(0.01, mean = df_mu[1], sd = df_sigma[1], left = 0)
lh2 <- crch::dcnorm(0.01, mean = df_mu[2], sd = df_sigma[2], left = 0)
lh3 <- crch::dcnorm(0.01, mean = df_mu[3], sd = df_sigma[3], left = 0)
@



\begin{figure}[t!]
<<rain_Axams_pred, fig=TRUE, echo=FALSE>>=
plot(x = x, y = y1, type = "l", col = "red", 
     main = paste0("July ", pday, dayending), ylab = "density", 
     ylim = c(0,max(y1, y2, y3, pm1, pm2, pm3) + 0.01),
     xlim = c(-1.5,8))
    
lines(x = x, y = y2, type = "l", col = "blue")
    lines(x = x, y = y3, type = "l", col = "darkgreen")
    #legend('topright', c("2010", "2011", "2012"), col = c("red", "blue", "darkgreen"), lty = 1, cex = 1)
    
    # plot point mass
    lines(x = c(pm1[1], pm1[1]), y = c(pm1[2], 0), col = "red", type = "l", lwd = 1)
    lines(x = c(pm2[1], pm2[1]), y = c(pm2[2], 0), col = "blue", type = "l", lwd = 1)
    lines(x = c(pm3[1], pm3[1]), y = c(pm3[2], 0), col = "darkgreen", type = "l", lwd = 1)
    points(x = pm1[1], y = pm1[2], col = "red", pch = 19)
    points(x = pm2[1], y = pm2[2], col = "blue", pch = 19)
    points(x = pm3[1], y = pm3[2], col = "darkgreen", pch = 19)
    
    
    # plot predictions
    points(x = pred1[1], y = pred1[2], col = "red", pch = 4)
    points(x = pred2[1], y = pred2[2], col = "blue", pch = 4)
    points(x = pred3[1], y = pred3[2], col = "darkgreen", pch = 4)
    
    lines(x = c(pred1[1], pred1[1]), y = c(pred1[2], 0), col = "darkgrey", type = "l", lty = 2)
    lines(x = c(pred2[1], pred2[1]), y = c(pred2[2], 0), col = "darkgrey", type = "l", lty = 2)
    lines(x = c(pred3[1], pred3[1]), y = c(pred3[2], 0), col = "darkgrey", type = "l", lty = 2)
    
    # add labels
    text(x = -0.8, y = lh1, labels = "2010", col = "red", cex = 0.8)
    text(x = -0.8, y = lh2, labels = "2011", col = "blue", cex = 0.8)
    text(x = -0.8, y = lh3, labels = "2012", col = "darkgreen", cex = 0.8)
@
\caption{\label{}Predictions at station Axams for one day in July of 2010, 2011 and 2012}
\end{figure}

Analyzing variable importance of the distforest model leads to the following list of the covariates which show the highest values in variable importance.\\
In the following plot the marginal effect of these variables on the distribution parameters are plotted. (All other covariates are kept at their mean.)
<<variabble_importance, echo=FALSE, eval=TRUE>>=
if(file.exists("vimp_crps.rda")){
  #load("~/svn/partykit/pkg/disttree/inst/draft/vimp_crps.rda")
  load("vimp_crps.rda")
} else {
  
  # locally redefine logLik.distforest to use crps in varimp()
  logLik.distforest <- function(object, newdata = NULL, ...) {
    if(is.null(newdata)) {
      newdata <- object$data
    } 
    
    # predict parameter
    pdf <- predict(object, newdata = newdata, type = "parameter")
    df_mu <- pdf$mu
    df_sigma <- pdf$sigma
    
    # calculate CRPS
    crps <- mean(crps_cnorm(newdata$robs, location = df_mu, scale = df_sigma, lower = 0, upper = Inf), na.rm = TRUE)
  
    return(structure(crps, df = NA, class = "logLik"))
  }
  
  vimp_crps <- varimp(res$df, nperm = 1L)
  #vimp10_crps <- varimp(res$df, nperm = 10L)
  #vimp100_crps <- varimp(res$df, nperm = 100L)
  #vimp1000_crps <- varimp(res$df, nperm = 1000L)
  
  save(vimp_crps, file = "~/svn/partykit/pkg/disttree/inst/draft/vimp_crps.rda")
  #save(vimp10_crps, file = "~/svn/partykit/pkg/disttree/inst/draft/vimp10_crps.rda")
  #save(vimp100_crps, file = "~/svn/partykit/pkg/disttree/inst/draft/vimp100_crps.rda")
  #save(vimp1000_crps, file = "~/svn/partykit/pkg/disttree/inst/draft/vimp1000_crps.rda")
  
  rm(logLik.distforest) 
}
    
@
  

\begin{figure}[t!]
%\begin{subfigure}{0.5\textwidth}
<<rain_axams_varim_margeffects1_mu, fig=TRUE, echo=FALSE>>=
par(mar = c(2,10,2,2))
barplot(sort(vimp_crps, decreasing = TRUE)[1:10], horiz = TRUE, las = 1, cex.names = 0.8, axes = FALSE)
@
%\end{subfigure}
%\hspace{-1cm}
%\begin{subfigure}{0.5\textwidth}
%<<rain_axams_varim_margeffects1_sigma, fig=TRUE, echo=FALSE>>=
%plot(x = sp1[,id_mi[1]], y = sp1[, "sigma"], type = "l", xlab = names_mi[1], ylab = %"sigma")
%@
%\end{subfigure}
\caption{\label{varimp} Variable imortance: top 10 list for the forest model}
\end{figure}





<<pit_qqr, echo=FALSE, eval=TRUE>>=

## pit histograms and qqr-plots

## get predicted parameter
# for learndata and testdata
{
  
  
  ## disttree
  # in sample
  pdt <- predict(res$dt, type = "parameter")
  dt_mu_l <- pdt$mu 
  dt_sigma_l <- pdt$sigma 
  pit_dt_l <- cbind(0, pnorm(res$learndata[,"robs"], mean = dt_mu_l, sd = dt_sigma_l))
  pit_dt_l[res$learndata[,"robs"]>0, 1] <- pit_dt_l[res$learndata[,"robs"]>0, 2]
  
  # out of sample
  pdt <- predict(res$dt, type = "parameter", newdata = res$testdata)
  dt_mu_t <- pdt$mu 
  dt_sigma_t <- pdt$sigma 
  pit_dt_t <- cbind(0, pnorm(res$testdata[,"robs"], mean = dt_mu_t, sd = dt_sigma_t))
  pit_dt_t[res$testdata[,"robs"]>0, 1] <- pit_dt_t[res$testdata[,"robs"]>0, 2]
  
  
  ## distforest
  # in sample
  pdf <- predict(res$df, type = "parameter")
  df_mu_l <- pdf$mu
  df_sigma_l <- pdf$sigma
  pit_df_l <- cbind(0, pnorm(res$learndata[,"robs"], mean = df_mu_l, sd = df_sigma_l))
  pit_df_l[res$learndata[,"robs"]>0, 1] <- pit_df_l[res$learndata[,"robs"]>0, 2]
  
  # out of sample
  pdf <- predict(res$df, type = "parameter", newdata = res$testdata)
  df_mu_t <- pdf$mu
  df_sigma_t <- pdf$sigma
  pit_df_t <- cbind(0, pnorm(res$testdata[,"robs"], mean = df_mu_t, sd = df_sigma_t))
  pit_df_t[res$testdata[,"robs"]>0, 1] <- pit_df_t[res$testdata[,"robs"]>0, 2]
  
  
  ## gamlss
  # in sample
  g_learndata <- res$learndata
  g_learndata$robs <- Surv(g_learndata$robs, g_learndata$robs>0, type="left")
  g_mu_l <- predict(res$g, what = "mu", type = "response")
  g_sigma_l <- predict(res$g, what = "sigma", type = "response")
  pit_g_l <- cbind(0, pnorm(res$learndata[,"robs"], mean = g_mu_l, sd = g_sigma_l))
  pit_g_l[res$learndata[,"robs"]>0, 1] <- pit_g_l[res$learndata[,"robs"]>0, 2]
  
  # out of sample
  g_mu_t <- predict(res$g, what = "mu", type = "response", newdata = res$testdata)
  g_sigma_t <- predict(res$g, what = "sigma", type = "response", newdata = res$testdata)
  pit_g_t <- cbind(0, pnorm(res$testdata[,"robs"], mean = g_mu_t, sd = g_sigma_t))
  pit_g_t[res$testdata[,"robs"]>0, 1] <- pit_g_t[res$testdata[,"robs"]>0, 2]
  
  
  #gamboostLSS
  pgb <- predict(res$gb, parameter = c("mu","sigma"), type = "response")
  gb_mu_l <- pgb$mu
  gb_sigma_l <- pgb$sigma
  pit_gb_l <- cbind(0, pnorm(res$learndata[,"robs"], mean = gb_mu_l, sd = gb_sigma_l))
  pit_gb_l[res$learndata[,"robs"]>0, 1] <- pit_gb_l[res$learndata[,"robs"]>0, 2]
  
  # out of sample
  pgb <- predict(res$gb, parameter = c("mu","sigma"), type = "response", newdata = res$testdata)
  gb_mu_t <- pgb$mu
  gb_sigma_t <- pgb$sigma
  pit_gb_t <- cbind(0, pnorm(res$testdata[,"robs"], mean = gb_mu_t, sd = gb_sigma_t))
  pit_gb_t[res$testdata[,"robs"]>0, 1] <- pit_gb_t[res$testdata[,"robs"]>0, 2]
  
  
  #EMOS
  ml_mu_l <- predict(res$ml, type = "location")     # returns parameter on response scale
  ml_sigma_l <- predict(res$ml, type = "scale")
  pit_ml_l <- cbind(0, pnorm(res$learndata[,"robs"], mean = ml_mu_l, sd = ml_sigma_l))
  pit_ml_l[res$learndata[,"robs"]>0, 1] <- pit_ml_l[res$learndata[,"robs"]>0, 2]
  
  # out of sample
  ml_mu_t <- predict(res$ml, type = "location", newdata = res$testdata)     # returns parameter on response scale
  ml_sigma_t <- predict(res$ml, type = "scale", newdata = res$testdata)
  pit_ml_t <- cbind(0, pnorm(res$testdata[,"robs"], mean = ml_mu_t, sd = ml_sigma_t))
  pit_ml_t[res$testdata[,"robs"]>0, 1] <- pit_ml_t[res$testdata[,"robs"]>0, 2]
  
}
    
@
  

\begin{figure}[t!]
%\begin{subfigure}{0.5\textwidth}
<<rain_axams_pit_insample, fig=TRUE, echo=FALSE>>=

# in sample
set.seed(7)
par(mfrow = c(2,2))
pithist(pit_df_l, nsim = 1000, breaks = seq(0, 1, length.out = 9))
pithist(pit_g_l, nsim = 1000, breaks = seq(0, 1, length.out = 9))
pithist(pit_gb_l, nsim = 1000, breaks = seq(0, 1, length.out = 9))
pithist(pit_ml_l, nsim = 1000, breaks = seq(0, 1, length.out = 9))

@
\caption{\label{pit_insample} PIT-Histograms for station Axams (in sample)}
\end{figure}

\begin{figure}[t!]
%\begin{subfigure}{0.5\textwidth}
<<rain_axams_pit_oosample, fig=TRUE, echo=FALSE>>=
    
# out of sample
set.seed(7)

par(mfrow = c(2,2))
pithist(pit_df_t, nsim = 1000, breaks = seq(0, 1, length.out = 9))
pithist(pit_g_t, nsim = 1000, breaks = seq(0, 1, length.out = 9))
pithist(pit_gb_t, nsim = 1000, breaks = seq(0, 1, length.out = 9))
pithist(pit_ml_t, nsim = 1000, breaks = seq(0, 1, length.out = 9))
      
@
\caption{\label{pit_oosample} PIT-Histograms for station Axams (out of sample)}
\end{figure}

\begin{figure}[t!]
%\begin{subfigure}{0.5\textwidth}
<<rain_axams_qqr_insample, fig=TRUE, echo=FALSE>>=
# in sample
set.seed(7)
par(mfrow = c(2,2))
set.seed(1)
qqrplot(pit_df_l, nsim = 1, main = "distforest (in bag)")
qqrplot(pit_g_l, nsim = 1, main = "gamlss (in bag)")
qqrplot(pit_gb_l, nsim = 1, main = "gamboostLSS (in bag)")
qqrplot(pit_ml_l, nsim = 1, main = "EMOS (in bag)")

@
\caption{\label{qqr_insample} QQR-Plot for station Axams (in sample)}
\end{figure}

\begin{figure}[t!]
%\begin{subfigure}{0.5\textwidth}
<<rain_axams_qqr_oosample, fig=TRUE, echo=FALSE>>=
    
# out of sample
set.seed(7)
par(mfrow = c(2,2))
set.seed(1)
qqrplot(pit_df_t, nsim = 1, main = "distforest (out of bag)")
qqrplot(pit_g_t, nsim = 1, main = "gamlss (out of bag)")
qqrplot(pit_gb_t, nsim = 1, main = "gamboostLSS (out of bag)")
qqrplot(pit_ml_t, nsim = 1, main = "EMOS (out of bag)")

@
\caption{\label{qqr_oosample} QQR-Plot for station Axams (out of sample)}
\end{figure}


  






\SweaveOpts{eval = TRUE}

\newpage
\subsubsection{Cross-Validation}
randomly split into 10 (roughly) equally sized data sets, learn on 9, evaluate on the 10th\\
average over 10 variations\\
repeat 10 times\\
compare rmse, ll, crps score skills with reference model EMOS log\\

<<echo=FALSE, results=hide>>=
#### cross validation rain
if(file.exists("rain_Axams.rda")){
  #load("~/svn/partykit/pkg/disttree/inst/draft/rain_Axams.rda")
  load("rain_Axams.rda")
} else {
  #source("~/svn/partykit/pkg/disttree/inst/draft/rain_cross.R")
  source("rain_cross.R")
  library("gamlss.cens")
  gen.cens("NO", type = "left")
  rainres <- rain_cross(stationname = "Axams", 
                        seedconst = 7, ntree = 100,
                        forest_mtry = 27,
                        gamboost_cvr = TRUE,
                        frac = FALSE)
  
  save(rainres, file = "rain_Axams.rda")
}

rain_rmse <- matrix(0, ncol = ncol(rainres[[1]]$rmse), length(rainres)-1)
rain_ll <- matrix(0, ncol = ncol(rainres[[1]]$ll), length(rainres)-1)
rain_crps <- matrix(0, ncol = ncol(rainres[[1]]$crps), length(rainres)-1)

for(i in 1:(length(rainres)-1)){
  rain_rmse[i,] <- colMeans(rainres[[i]]$rmse, na.rm = TRUE)
  rain_ll[i,] <- colMeans(rainres[[i]]$ll, na.rm = TRUE)
  rain_crps[i,] <- colMeans(rainres[[i]]$crps, na.rm = TRUE)
}

colnames(rain_rmse) <- colnames(rain_ll) <- colnames(rain_crps) <-
  colnames(rainres[[1]]$rmse)

@



\begin{figure}[t!]
\begin{subfigure}{0.5\textwidth}
<<rain_cross_axams_rmse, fig=TRUE, echo=FALSE>>=
boxplot(rain_rmse)
@
\end{subfigure}
\hspace{-1cm}
\begin{subfigure}{0.5\textwidth}
<<rain_cross_axams_ll, fig=TRUE, echo=FALSE>>=
boxplot(rain_ll, ylim = c(-2.8,-1.3))
#boxplot(rain_ll)
@
\end{subfigure}
\caption{\label{boxplots_rain}Results 10x10 cross validation at the station Axams\\
left: RMSE, right: log-likelihood}
\end{figure}

\begin{figure}[t!]
<<rain_cross_axams_crps_skills_score, fig=TRUE, echo=FALSE>>=
boxplot(1 - rain_crps / rain_crps[,6]) 
abline(h = 0, col = 2, lwd = 2)
@
\caption{\label{boxplots_rain}Results 10x10 cross validation at the station Axams \\
CRPS skills score with reference model EMOS log}
\end{figure}




\newpage
\subsection{Predictions over all stations}

NAs in gamlss in 13 stations!!!\\
for example in 13th station in complete{\_}stations\\
\text{Error in glim.fit(f = mu.object, X = mu.X, y = y, w = w, fv = mu, os = mu.offset,  :}\\ 
\text{NA's in the working vector or weights for parameter mu}
\\
% sum(is.na(crps[,3]))
<<eval = TRUE, echo = FALSE>>=
#### prediction over all stations 25 - 3
if(file.exists("rain_pred.rda")){
  #load("~/svn/partykit/pkg/disttree/inst/draft/rain_pred.rda")
  load("rain_pred.rda")
} else {
  
  source("rain_pred.R")
  library("gamlss.cens")
  gen.cens("NO", type = "left")
  
  res <- rain_pred(seedconst = 7,
                   gamboost_cvr = TRUE,
                   frac = FALSE)
  
  save(res, file = "rain_pred.rda")
}
 
ll <- res[[1]]$results["ll",]
for(i in 2:(length(res)-1)) ll <- rbind(ll, res[[i]]$results["ll",])

rmse <- res[[1]]$results["rmse",]
for(i in 2:(length(res)-1)) rmse <- rbind(rmse, res[[i]]$results["rmse",])

crps <- res[[1]]$results["crps",]
for(i in 2:(length(res)-1))  crps <- rbind(crps, res[[i]]$results["crps",])

colnames(ll) <- colnames(rmse) <- colnames(crps) <- colnames(res[[1]]$results)

# skills score
s <- 1 - crps[, 2:4]/crps[,6]
@



\begin{figure}[t!]
<<rain_pred_skills_score, fig=TRUE, echo=FALSE>>=
  matplot(t(s), type = "l", lwd = 1, col = gray(0.5, alpha = 0.5), lty = 1, axes = FALSE, xlab = "", ylab = "", xlim = c(0.5, 3.5))
  boxplot(s, add = TRUE)
  abline(h = 0, col = "gold", lwd = 2)
@
\caption{\label{boxplots_rain}Results of predictions over all stations: CRPS skills score with reference model EMOS log}
\end{figure}









%\newpage
%\section{References}

%Hothorn T, Hornik K, Zeileis A (2006). 
%``Unbiased Recursive Partitioning: A Conditional Inference Framework.''
%\textit{Journal of Computational and Graphical Statistics}, 
%\textbf{15}(3), 651--674. 
%\doi{10.1198/106186006X133933}\\
%\\
%Zeileis A, Hothorn T, Hornik K (2008). 
%``Model-Based Recursive Partitioning.''
%\textit{Journal of Computational and Graphical Statistics}, 
%\textbf{17}(2), 492--514. 
%\doi{10.1198/106186008X319331}\\
%\\
%Hothorn T, Zeileis A (2015). 
%``partykit: A Modular Toolkit for Recursive Partytioning in R.''
%\textit{Journal of Machine Learning Research}, 
%\textbf{16}, 3905--3909. 
%\url{http://www.jmlr.org/papers/v16/hothorn15a.html}\\
%\\
%Stasinopoulos DM, Rigby RA (2007).
%``Generalized Additive Models for Location Scale and Shape (GAMLSS) in R.''
%\textit{Journal of Statistical Software}, 
%\textbf{23}(7), 1--46.
%\doi{10.18637/jss.v023.i07}\\
%\\
%Stasinopoulos DM, Rigby RA (2005).
%``Generalized Additive Models for Location Scale and Shape (with discussion)''
%\textit{Applied Statistics},
%\textbf{54}(3), 507--554.
%\doi{10.1111/j.1467-9876.2005.00510.x}\\
%\\
%Seibold H, Zeileis A, Hothorn T (2017).
%``Individual Treatment Effect Prediction for Amyotrophic Lateral Sclerosis Patients.''
%\textit{Statistical Methods in Medical Research}, 
%\textbf{12}(1), 45--63.
%\doi{10.1177/0962280217693034}\\
%\\
%Hothorn T, Zeileis A (2017).
%``Transformation Forests.''
%\emph{arXiv 1701.02110}, arXiv.org E-Print Archive.
%\url{http://arxiv.org/abs/1701.02110}\\
%\\
%Messner JW, Mayr GJ, Zeileis A (2016).
%``Heteroscedastic Censored and Truncated Regression with {crch}''
%\textit{The R Journal},
%\textbf{8}(1), 173--181.
%\url{https://journal.R-project.org/archive/2016-1/messner-mayr-zeileis.pdf}\\
%\\
%Zeileis A, Wiel MA, Hornik K, Hothorn T (2008).
%``Implementing a {Class} of {Permutation} {Tests}: {The} {Coin} {Package}''
%\textit{Journal of Statistical Software},
%\textbf{28}(8), 1--23.
%\doi{10.18637/jss.v028.i08}\\
%%publisher={American Statistical Association}
%\\
%Zeileis A, Hothorn T, Hornik K (2010).
%``Party with the {MOB}: {Model-Based} {Recursive} {Partitioning} in {R}''
%\textit{R package version 0.9-9999}.
%\url{https://cran.r-project.org/web/packages/party/vignettes/MOB.pdf}\\
%\\
%Breiman L (2001).
%``Random {Forests}''
%\textit{Machine Learning},
%\textbf{45}(1), 5--32
%%\doi{10.1023/A:1010933404324}
%%publisher={Springer}\\
%\\
%Loh WY (2011).
%``Classification and {Regression} {Trees}''
%\textit{Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
%\textbf{1}(1), 14--23.
%\doi{10.1002/widm.8}
%%publisher={Wiley Online Library}\\
%\\
%Hothorn T (Maintainer), Hornik K, Strobl C, Zeileis A (2015).
%``Package 'party' ''
%\textit{Package Reference Manual for Party Version 0.9--0.998},
%\textbf{16}, 37.\\
%%  \doi{}
%\\
%Yee TW (2010).
%``The {VGAM} {Package} for {Categorical} {Data} {Analysis}''
%\textit{Journal of Statistical Software},
%\textbf{32}(10), 1--34.
%\url{http://www.jstatsoft.org/v32/i10/}\\
%\\
%Umlauf N, Klein N, Zeileis A (2017).
%``{BAMLSS}: {B}ayesian Additive Models for Location, Scale
%  and Shape (and Beyond)''
%\textit{Working Papers in Economics and Statistics, Research
%  Platform Empirical and Experimental Economics, Universit\"at
%  Innsbruck},
%\textbf{2017-05},
%\url{http://EconPapers.RePEc.org/RePEc:inn:wpaper:2017-05}\\
%%type = {Working Paper},
%%number = {2017-05},
%%month = {February},
%\\
%Benjamin Hofner and Andreas Mayr and Nora Fenske and
%  Matthias Schmid (2017). 
%``{gamboostLSS}: Boosting Methods for {GAMLSS} Models''
%\textit{{R} package version 2.0-0}.
%\url{https://CRAN.R-project.org/package=gamboostLSS}\\
%\\
%Liaw A, Wiener M (2002).
%``Classification and Regression by randomForest''
%\textit{R News},
%\textbf{2}(3), 18--22.
%\url{http://CRAN.R-project.org/doc/Rnews/}







%\newpage
\nocite{hothorn2006unbiased, zeileis2008model, hothorn2015partykit, Messner2016, loh2011classification, stasinopoulos2007generalized, Seibold2016, Seibold2017, breiman2001random, stasinopoulos2005, yee2010vgam}
% zeileis2008implementing, 
% zeileis2010party, 
% hothorn2015package,

\newpage
\bibliography{ref.bib}

\end{document}