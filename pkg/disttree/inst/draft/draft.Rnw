\documentclass[nojss]{jss}

%\VignetteIndexEntry{disttree: Distributional Regression Forests for Probabilistic Precipitation Forecasting in Complex Terrain}
%\VignetteDepends{partykit, gamlss.dist}
%\VignetteKeywords{regression trees, random forests, distributional regression, recursive partitioning}
%\VignettePackage{disttree}

%% packages
\usepackage{amstext,amsfonts,amsmath,bm,thumbpdf,lmodern}

%% tikz
\usepackage{array,makecell,tikz}
\usetikzlibrary{arrows.meta,positioning,shapes,arrows,decorations.pathreplacing,calc,automata,mindmap}

%% need no \usepackage{Sweave}
\SweaveOpts{concordance=FALSE, eps=FALSE, keep.source=TRUE, eval = TRUE}

%% commands
\newcommand{\fixme}[1]{\emph{\marginpar{FIXME} (#1)}}
\renewcommand{\Prob}{\mathbb{P} }
\renewcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\Var}{\mathbb{V}}
\newcommand{\R}{\mathbb{R} }
\newcommand{\N}{\mathbb{N} }
\newcommand{\C}{\mathbb{C} }
\newcommand{\argmin}{\operatorname{argmin}\displaylimits}
\newcommand{\argmax}{\operatorname{argmax}\displaylimits}
\newcommand{\LS}{\mathcal{L}_n}
\newcommand{\TS}{\mathcal{T}_n}
\newcommand{\LSc}{\mathcal{L}_{\text{comb},n}}
\newcommand{\LSbc}{\mathcal{L}^*_{\text{comb},n}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\yn}{y_{\text{new}}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\sX}{\mathcal{X}}
\newcommand{\sY}{\mathcal{Y}}
\newcommand{\T}{\mathbf{T}}
\newcommand{\x}{\mathbf{x}}
\renewcommand{\a}{\mathbf{a}}
\newcommand{\xn}{\mathbf{x}_{\text{new}}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\ws}{\mathbf{w}_\cdot}
\renewcommand{\t}{\mathbf{t}}
\newcommand{\M}{\mathbf{M}}
\renewcommand{\vec}{\text{vec}}
\newcommand{\B}{\mathbf{B}}
\newcommand{\K}{\mathbf{K}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\cellx}{\pi_n[\x]}
\newcommand{\partn}{\pi_n(\mathcal{L}_n)}
\newcommand{\err}{\text{Err}}
\newcommand{\ea}{\widehat{\text{Err}}^{(a)}}
\newcommand{\ecv}{\widehat{\text{Err}}^{(cv1)}}
\newcommand{\ecvten}{\widehat{\text{Err}}^{(cv10)}}
\newcommand{\eone}{\widehat{\text{Err}}^{(1)}}
\newcommand{\eplus}{\widehat{\text{Err}}^{(.632+)}}
\newcommand{\eoob}{\widehat{\text{Err}}^{(oob)}}
\newcommand{\bft}{\mathbf{t}}

\hyphenation{Qua-dra-tic}

%% colors
\definecolor{HighlightOrange}{rgb}{0.9490196,0.5725490,0.0000000}
\definecolor{HighlightBlue}{rgb}{0.4784314,0.7490196,0.9803922}



\SweaveOpts{engine=R, eps=FALSE, keep.source=TRUE}
<<preliminaries, echo=FALSE, results=hide>>=
options(prompt = "R> ", continue = "+  ", useFancyQuotes = FALSE, width = 70)
library("disttree")
library("Formula")  # FIX ME: should be imported in disttree
library("gamlss")
library("crch")
library("gamlss.cens")
gen.cens(NO, type = "left")
library("mgcv")
library("countreg")

## HCL palette
pal <- hcl(c(10, 128, 260, 290, 50), 100, 50)
names(pal) <- c("forest", "tree", "gamlss", "gamboostLSS", "EMOS")

pallight <- hcl(c(10, 128, 260, 290, 70), 100, 50, alpha = 0.25)
names(pallight) <- c("forest", "tree", "gamlss", "gamboostLSS", "EMOS")

transpgrey <- rgb(0.190,0.190,0.190, alpha = 0.2)




# plot RMSE
plot_rmse <- function(simres, type = c("exp", "par"), ylim = NULL, legend = TRUE){
  
  if(type == "exp"){
    rmse <- NULL
    colnames <- NULL
    col <- NULL
    legendnames <- NULL
    if("av.rmse.exp.true.dt" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.dt"], simres$resmat[,"av.rmse.exp.obs.dt"])
      colnames <- c(colnames, "dt.true", "dt.obs")
      col <- c(col, pal["tree"])
      legendnames <- c(legendnames, "tree")
    }
    if("av.rmse.exp.true.df" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.df"], simres$resmat[,"av.rmse.exp.obs.df"])
      colnames <- c(colnames, "df.true", "df.obs")
      col <- c(col, pal["forest"])
      legendnames <- c(legendnames, "distributional forest")
    }
    if("av.rmse.exp.true.g" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.g"], simres$resmat[,"av.rmse.exp.obs.g"])
      colnames <- c(colnames, "g.true", "g.obs")
      col <- c(col, pal["gamlss"])
      legendnames <- c(legendnames, "gamlss")
    }
    if("av.rmse.exp.true.b" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.b"], simres$resmat[,"av.rmse.exp.obs.b"])
      colnames <- c(colnames, "b.true", "b.obs")
      col <- c(col, pal["bamlss"])
      legendnames <- c(legendnames, "bamlss")
    }
    if("av.rmse.exp.true.gb" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.gb"], simres$resmat[,"av.rmse.exp.obs.gb"])
      colnames <- c(colnames, "gb.true", "gb.obs")
      col <- c(col, pal["gamboostLSS"])
      legendnames <- c(legendnames, "gamboostLSS")
    }
    if("av.rmse.exp.true.rf" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.rf"], simres$resmat[,"av.rmse.exp.obs.rf"])
      colnames <- c(colnames, "rf.true", "rf.obs")
      col <- c(col, pal["randomForest"])
      legendnames <- c(legendnames, "randomForest")
    }
    if("av.rmse.exp.true.cf" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.cf"], simres$resmat[,"av.rmse.exp.obs.cf"])
      colnames <- c(colnames, "cf.true","cf.obs")
      col <- c(col, pal["cforest"])
      legendnames <- c(legendnames, "cforest")
    }
    
    colnames(rmse) <- colnames
    if(is.null(ylim)) ylim <- c(min(na.omit(rmse)), max(na.omit(rmse)))
    
    plot(x = simres$x.axis, y = rmse[,1], type = "l", col = col[1], ylim = ylim,
         xlab = expression(kappa), ylab = "RMSE")
    lines(x = simres$x.axis, y = rmse[,2], type = "l", lty = 2, col = col[1])
    
    for(i in 1:(length(col)-1)){
      lines(x = simres$x.axis, y = rmse[,(2*i+1)], type = "l", col = col[i+1])
      lines(x = simres$x.axis, y = rmse[,(2*i+2)], type = "l", lty = 2, col = col[i+1])
    }
    if(legend) legend('topleft', legendnames, 
                      col = col, lty = 1, cex = 0.7)
  } 
  
  if(type == "par"){
    
    rmse <- NULL
    colnames <- NULL
    col <- NULL
    legendnames <- NULL
    if("av.rmse.exp.true.dt" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.dt"], simres$resmat[,"av.rmse.sigma.dt"])
      colnames <- c(colnames, "dt.true", "dt.obs")
      col <- c(col, pal["tree"])
      legendnames <- c(legendnames, "tree")
    }
    if("av.rmse.exp.true.df" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.df"], simres$resmat[,"av.rmse.sigma.df"])
      colnames <- c(colnames, "df.true", "df.obs")
      col <- c(col, pal["forest"])
      legendnames <- c(legendnames, "distributional forest")
    }
    if("av.rmse.exp.true.g" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.g"], simres$resmat[,"av.rmse.sigma.g"])
      colnames <- c(colnames, "g.true", "g.obs")
      col <- c(col, pal["gamlss"])
      legendnames <- c(legendnames, "gamlss")
    }
    if("av.rmse.exp.true.b" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.b"], simres$resmat[,"av.rmse.sigma.b"])
      colnames <- c(colnames, "b.true", "b.obs")
      col <- c(col, pal["bamlss"])
      legendnames <- c(legendnames, "bamlss")
    }
    if("av.rmse.exp.true.gb" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.gb"], simres$resmat[,"av.rmse.sigma.gb"])
      colnames <- c(colnames, "gb.true", "gb.obs")
      col <- c(col, pal["gamboostLSS"])
      legendnames <- c(legendnames, "gamboostLSS")
    }
    if("av.rmse.exp.true.rf" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.rf"], simres$resmat[,"av.rmse.sigma.rf"])
      colnames <- c(colnames, "rf.true", "rf.obs")
      col <- c(col, pal["randomForest"])
      legendnames <- c(legendnames, "randomForest")
    }
    if("av.rmse.exp.true.cf" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.cf"], simres$resmat[,"av.rmse.sigma.cf"])
      colnames <- c(colnames, "cf.true","cf.obs")
      col <- c(col, pal["cforest"])
      legendnames <- c(legendnames, "cforest")
    }
    
    colnames(rmse) <- colnames
    if(is.null(ylim)) ylim <- c(min(na.omit(rmse)), max(na.omit(rmse)))
    
    plot(x = simres$x.axis, y = rmse[,1], type = "l", col = col[1], ylim = ylim,
         xlab = expression(kappa), ylab = "RMSE")
    lines(x = simres$x.axis, y = rmse[,2], type = "l", lty = 2, col = col[1])
    
    for(i in 1:(length(col)-1)){
      lines(x = simres$x.axis, y = rmse[,(2*i+1)], type = "l", col = col[i+1])
      lines(x = simres$x.axis, y = rmse[,(2*i+2)], type = "l", lty = 2, col = col[i+1])
    }
    if(legend) legend('topleft', legendnames, 
                      col = col, lty = 1, cex = 0.7)
  }
}

# plot loglikelihood
plot_ll <- function(simres, ylim = NULL, legend = TRUE){
  
  ll <- NULL
  colnames <- NULL
  col <- NULL
  legendnames <- NULL
  if("av.rmse.exp.true.dt" %in% colnames(simres$resmat)) {
    ll <- cbind(ll, simres$resmat[,"av.loglik.dt"])
    colnames <- c(colnames, "dt.ll")
    col <- c(col, pal["tree"])
    legendnames <- c(legendnames, "tree")
  }
  if("av.rmse.exp.true.df" %in% colnames(simres$resmat)) {
    ll <- cbind(ll, simres$resmat[,"av.loglik.df"])
    colnames <- c(colnames, "df.ll")
    col <- c(col, pal["forest"])
    legendnames <- c(legendnames, "distributional forest")
  }
  if("av.rmse.exp.true.g" %in% colnames(simres$resmat)) {
    ll <- cbind(ll, simres$resmat[,"av.loglik.g"])
    colnames <- c(colnames, "g.true")
    col <- c(col, pal["gamlss"])
    legendnames <- c(legendnames, "gamlss")
  }
  if("av.rmse.exp.true.b" %in% colnames(simres$resmat)) {
    ll <- cbind(ll, simres$resmat[,"av.loglik.b"])
    colnames <- c(colnames, "b.true")
    col <- c(col, pal["bamlss"])
    legendnames <- c(legendnames, "bamlss")
  }
  if("av.rmse.exp.true.gb" %in% colnames(simres$resmat)) {
    ll <- cbind(ll, simres$resmat[,"av.loglik.gb"])
    colnames <- c(colnames, "gb.true")
    col <- c(col, pal["gamboostLSS"])
    legendnames <- c(legendnames, "gamboostLSS")
  }
  if("av.rmse.exp.true.rf" %in% colnames(simres$resmat)) {
    ll <- cbind(ll, simres$resmat[,"av.loglik.rf"])
    colnames <- c(colnames, "rf.true")
    col <- c(col, pal["randomForest"])
    legendnames <- c(legendnames, "randomForest")
  }
  if("av.rmse.exp.true.cf" %in% colnames(simres$resmat)) {
    ll <- cbind(ll, simres$resmat[,"av.loglik.cf"])
    colnames <- c(colnames, "cf.true")
    col <- c(col, pal["cforest"])
    legendnames <- c(legendnames, "cforest")
  }
  
  colnames(ll) <- colnames
  if(is.null(ylim)) ylim <- c(min(na.omit(ll)), max(na.omit(ll)))
  
  plot(x = simres$x.axis, y = ll[,1], type = "l", col = col[1], ylim = ylim,
       xlab = expression(kappa), ylab = "log-likelihood")
  
  for(i in 2:length(col)){
    lines(x = simres$x.axis, y = ll[,i], type = "l", col = col[i])
  }
  if(legend) legend('topleft', legendnames, 
                    col = col, lty = 1, cex = 0.7)
}

# plot crpse
plot_crps <- function(simres, ylim = NULL, legend = TRUE){
  
  crps <- NULL
  colnames <- NULL
  col <- NULL
  legendnames <- NULL
  if("av.rmse.exp.true.dt" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.dt"])
    colnames <- c(colnames, "dt.crps")
    col <- c(col, pal["tree"])
    legendnames <- c(legendnames, "tree")
  }
  if("av.rmse.exp.true.df" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.df"])
    colnames <- c(colnames, "df.crps")
    col <- c(col, pal["forest"])
    legendnames <- c(legendnames, "distributional forest")
  }
  if("av.rmse.exp.true.g" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.g"])
    colnames <- c(colnames, "g.true")
    col <- c(col, pal["gamlss"])
    legendnames <- c(legendnames, "gamlss")
  }
  if("av.rmse.exp.true.b" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.b"])
    colnames <- c(colnames, "b.true")
    col <- c(col, pal["bamlss"])
    legendnames <- c(legendnames, "bamlss")
  }
  if("av.rmse.exp.true.gb" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.gb"])
    colnames <- c(colnames, "gb.true")
    col <- c(col, pal["gamboostLSS"])
    legendnames <- c(legendnames, "gamboostLSS")
  }
  if("av.rmse.exp.true.rf" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.rf"])
    colnames <- c(colnames, "rf.true")
    col <- c(col, pal["randomForest"])
    legendnames <- c(legendnames, "randomForest")
  }
  if("av.rmse.exp.true.cf" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.cf"])
    colnames <- c(colnames, "cf.true")
    col <- c(col, pal["cforest"])
    legendnames <- c(legendnames, "cforest")
  }
  
  colnames(crps) <- colnames
  if(is.null(ylim)) ylim <- c(min(na.omit(crps)), max(na.omit(crps)))
  
  plot(x = simres$x.axis, y = crps[,1], type = "l", col = col[1], ylim = ylim,
       xlab = expression(kappa), ylab = "CRPS")
  
  for(i in 2:length(col)){
    lines(x = simres$x.axis, y = crps[,i], type = "l", col = col[i])
  }
  if(legend) legend('topright', legendnames, 
                    col = col, lty = 1, cex = 1, bty = "n")
}
@



\title{Distributional Regression Forests for Probabilistic Precipitation Forecasting in Complex Terrain}
\Shorttitle{Distributional Regression Forests for Probabilistic Precipitation Forecasting}

\author{Lisa Schlosser\\Universit\"at Innsbruck
   \And Torsten Hothorn\\Universit\"at Z\"urich
   \And Reto Stauffer\\Universit\"at Innsbruck
   \And Achim Zeileis\\Universit\"at Innsbruck}
\Plainauthor{Lisa Schlosser, Torsten Hothorn, Reto Stauffer, Achim Zeileis}

\Abstract{
To obtain a probabilistic model for a dependent variable based on some set of
explanatory variables, a distributional approach is often adopted where the
parameter(s) of the distribution are linked to regressors. In many classical
models this only captures the location/expectation of the distribution but
over the last decade there has been increasing interest in distributional
regression approaches modeling all parameters including location, scale, and
shape. Notably, so-called non-homogenous Gaussian regression (NGR) models both
mean and variance of a Gaussian response and is particularly
popular in weather forecasting and, more generally, the GAMLSS framework
allows to establish generalized additive models for location, scale, and shape
with smooth linear or nonlinear effects.
%
However, when variable selection is required and/or there are
non-smooth dependencies or interactions (especially unknown or of high-order),
it is challenging to establish a good GAMLSS. A natural alternative in these
situations would be the application of regression trees or random forests but,
so far, no general distributional framework is available for these. Therefore,
a framework for distributional trees and forests is proposed that blends
regression trees and random forests with classical distributions from the GAMLSS
framework as well as their censored or truncated counterparts.
%
To illustrate these novel approaches in practice, they are employed to obtain
probabilistic precipitation forecasts at numerous locations in a mountainous
region (Tyrol, Austria) based on a very large number of numerical weather
prediction quantities. It is shown that distributional random forests
automatically select variables and interactions, performing on par or often even
better than GAMLSS specified either through prior meteorological knowledge or a
computationally more demanding boosting approach.
}
\Keywords{parametric models, regression trees, random forests, recursive partitioning, probabilistic forecasting, distributional regression}

\Address{
  Lisa Schlosser, Reto Stauffer, Achim Zeileis \\
  Universit\"at Innsbruck \\
  Department of Statistics \\
  Faculty of Economics and Statistics \\
  Universit\"atsstr.~15 \\
  6020 Innsbruck, Austria \\
  E-mail: \email{Lisa.Schlosser@uibk.ac.at}, \email{Reto.Stauffer@uibk.ac.at},\\
  \email{Achim.Zeileis@R-project.org} \\
  URL: \url{https://www.uibk.ac.at/statistics/personal/schlosser-lisa/},\\
  \phantom{URL: }\url{https://retostauffer.org/},\\
  \phantom{URL: }\url{https://eeecon.uibk.ac.at/~zeileis/}\\
  
  Torsten Hothorn\\
  Universit\"at Z\"urich \\
  Institut f\"ur Epidemiologie, Biostatistik und Pr\"avention \\
  Hirschengraben 84\\
  CH-8001 Z\"urich, Switzerland \\
  E-mail: \email{Torsten.Hothorn@R-project.org}\\
  URL: \url{http://user.math.uzh.ch/hothorn/}\\
  
}

\begin{document}

\section{Introduction}

In regression analysis a wide range of models has been developed to describe the
relationship between a response variable and a set of covariates. The classical
model is the linear model (LM) where the conditional mean of the response
is modeled through a linear function of the covariates (see the left panel of
Figure~\ref{Devel_parmod} for a schematic illustration). Over the last decades
this has been extended in various directions including:

\pagebreak

\begin{itemize}
  \item \emph{Generalized linear models} (GLMs, \citealp{Nelder+Wedderburn:1972})
    encompassing an additional nonlinear link function for the conditional mean.
  \item \emph{Generalized additive models} (GAMs, \citealp{Hastie+Tibshirani:1986})
    allowing for smooth nonlinear effects in the covariates
    (Figure~\ref{Devel_parmod}, middle).
  \item \emph{Generalized additive models for location, scale, and shape}
    (GAMLSS, \citealt{Rigby+Stasinopoulos:2005}) adopting a probabilistic modeling
    approach. In GAMLSS, each parameter of a statistical distribution can depend
    on an additive predictor of the covariates comprising linear and/or
    smooth nonlinear terms (Figure~\ref{Devel_parmod}, right).
\end{itemize}
Thus, the above-mentioned models provide a broad toolbox for capturing different
aspects of the response (mean only vs.\ full distribution) and different types
of dependencies on the covariates (linear vs.\ nonlinear additive terms).

\begin{figure}[t!]
\begin{tikzpicture}
\node (a) at (0,0){
\minipage{0.29\textwidth}
\centering
<<motivation_GLM, echo=FALSE, results=hide>>=
nobs <- 200
## GLM
set.seed(7)
x <- c(1:nobs)/nobs
ytrue <- 1+x*1.5
y <- ytrue + rnorm(nobs,0,0.3)
@

<<plot_motivation_GLM, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(y=y , x=x, xaxt="n", yaxt="n", ann=FALSE, col = "slategray", pch = 19)
box(lwd=5)
lines(x = x, y = ytrue, col = pal["forest"], lwd = 7, main = "")
@
\endminipage};
\node (b) at (5,0) 
{
\minipage{0.29\textwidth}
\centering
<<motivation_GAM, echo=FALSE, results=hide>>=
## GAM
set.seed(7)
x <- c(1:nobs)/nobs
x <- 2*(x-0.5)
ytrue <- x^3 
y <- ytrue + rnorm(nobs,0,0.3)
@

<<plot_motivation_GAM, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(y=y , x=x, xaxt="n", yaxt="n", ann=FALSE, col = "slategray", pch = 19)
box(lwd=5)
lines(x = x, y = ytrue, col = pal["forest"], lwd = 7, main = "")
@
\endminipage};
\node (c) at (10,0) 
{
\minipage{0.29\textwidth}
\centering
<<motivation_GAMLSS, echo=FALSE, results=hide>>=
## GAMLSS
set.seed(7)
x <- c(1:nobs)/nobs
x <- 2*(x-0.5)
ytrue <- x^3
var <- exp(-(2*x)^2)/2
y <- ytrue + rnorm(nobs, 0, 0.1 + var)
@

<<plot_motivation_GAMLSS, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(x, y, xaxt = "n", yaxt = "n", ann = FALSE, type = "n")
polygon(c(x, rev(x)), c(ytrue + 0.1 + var, rev(ytrue - 0.1 - var)),
  col = pallight["forest"], border = "transparent")
lines(x, ytrue, col = pal["forest"], lwd=7)
points(x, y, col = "slategray", pch = 19)
box(lwd = 5)
@
\endminipage};
\node (d) at (0,-2) 
{
\minipage{0.29\textwidth}
\centering
\vspace{0.0cm}
LM, GLM  
\endminipage};
\node (e) at (5,-2) 
{
\minipage{0.29\textwidth}
\centering
\vspace{0.0cm}
GAM 
\endminipage};
\node at (10,-2)
{
\minipage{0.28\textwidth}
\centering
\vspace{0.0cm}
GAMLSS 
\endminipage};
\draw[-{>[scale=4, length=2, width=3]},line width=0.4pt](2.1,0)--(2.9,0);
\draw[-{>[scale=4, length=2, width=3]},line width=0.4pt](7.1,0)--(7.9,0);
\end{tikzpicture}
\caption{\label{Devel_parmod}Parametric modeling developments. (Generalized) linear models (left), generalized additive models (middle), generalized additive models for location, scale, and shape (right).}
\end{figure}


While in many applications conditional mean regression models have been receiving
the most attention, there has been a paradigm shift over the last decade towards
distributional regression models. An important reason for this is that in many
fields forecasts of the mean are not the only (or not even the main) concern but
instead there is an increasing interest in probabilistic forecasts. Quantities of
interest typically include exceedence probabilities for certain thresholds of the
response or quantiles of the response distribution. As an example, consider
weather forecasting where there is less interest in the mean amount of
precipitation on the next day. Instead, the probability of rain vs.\ no rain
is typically more relevant or, in some situations, a prediction interval of
expected precipitation (say from the expected 10\% to 90\% quantile). Similar
considerations apply for other meteorological quantities and hence attention
in the weather forecasting literature has been shifting from classical linear
models \citep{Glahn+Lowry:1972} towards probabilistic models such as the
non-homogeneous Gaussian regression (NGR) of \cite{Gneiting+Raftery+Westveld:2005}.
The NGR typically describes the mean of some meteorological response variable through
the average of the corresponding quantity from an ensemble of physically-based
numerical weather predictions (NWPs). Similarly, the variance of the response
is captured through the variance of the ensemble of NWP outputs. Thus, the NGR
considers both the mean as well as the uncertainty of the NWP model outputs
to obtain probabilistic forecasts calibrated to a particular site.

%% late: introduce MOS and EMOS? more details about precipitation forecasting
%In ensemble model output statistics (EMOS, \cite{Gneiting+Raftery+Westveld:2005}) this idea is taken one step further. Usually acting on the assumption of a normally distributed response variable this model allows for both parameters, the mean and the variance, to depend linearly on covariates provided by NWPs. However, the restriction to model only linear effects has risen the interest in applying more flexible models such as offered by the GAMLSS framework.

\begin{figure}[t!]
\begin{tikzpicture}
\node (a) at (0,0){
\minipage{0.29\textwidth}
\centering
<<motivation_regtree, echo=FALSE, results=hide>>=
## Reg. Tree
set.seed(7)
kappa <- 12
x <- c(1:nobs)/nobs
ytrue <- ytree <- yforest <- numeric(length = length(x))
for(i in 1:nobs) ytrue[i] <- if(x[i]<1/3) 0.5 else 1+(1-plogis(kappa*(2*(x[i]-0.2)-1)))
y <- ytrue + rnorm(nobs,0,0.3)
for(i in 1:nobs) ytree[i] <- if(x[i]<1/3) 0.5 else {if(x[i]<2/3) 2 else 1}
@

<<plot_motivation_regtree, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(x = x, y = y, xaxt="n", yaxt="n", ann=FALSE, col = "slategray", pch = 19)
box(lwd=5)
#lines(x = x, y = ytrue, col = "grey", lwd=5, main = "")
lines(x = x, y = ytree, col = pal["forest"], lwd=7)
@
\endminipage};
\node (b) at (5,0) 
{
\minipage{0.29\textwidth}
\centering
<<motivation_randforest, echo=FALSE, results=hide>>=
## Random Forest
for(i in 1:nobs) yforest[i] <- if(x[i]<0.27) 0.5 else { if(x[i]<0.39) 0.5 + 1.5*(plogis((x[i]-0.33)/6*700)) else 1+(1-plogis(kappa*(2*(x[i]-0.2)-1)))}
@

<<plot_motivation_randforest, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(x = x, y = y, xaxt="n", yaxt="n", ann=FALSE, col = "slategray", pch = 19)
box(lwd=5)
#lines(x = x, y = ytrue, col = "grey", lwd=5, main = "")
lines(x = x, y = yforest, col = pal["forest"], lwd=7, main = "")
@
\endminipage};
\node (c) at (10,0) 
{
\minipage{0.29\textwidth}
\centering
<<motivation_distforest, echo=FALSE, results=hide>>=
## distforest
set.seed(4723)
for(i in 1:nobs) yforest[i] <- if(x[i]<0.27) 0.5 else { if(x[i]<0.39) 0.5 + 1.5*(plogis((x[i]-0.33)/6*700)) else 1+(1-plogis(kappa*(2*(x[i]-0.2)-1)))}
xt <- 2*(x-0.5)
var <- exp(-(2*xt)^4)/2
y <- ytrue + rnorm(nobs, 0, 0.1 + var)
x <- c(1:nobs)/nobs
@

<<plot_motivation_distforest, fig=TRUE, echo=FALSE>>=
par(mar = c(2, 0, 2, 0))
plot(x, y, xaxt = "n", yaxt = "n", ann = FALSE, type = "n")
usr <- par("usr")
rect(usr[1], usr[3], usr[2], usr[4], col = gray(0.94), border = "transparent")
polygon(c(x, rev(x)), c(ytrue + 0.1 + var, rev(ytrue - 0.1 - var)),
  col = pallight["forest"], border = "transparent")
lines(x = x, y = yforest, col = pal["forest"], lwd=7, main = "")
points(x, y, col = "slategray", pch = 19)
box(lwd=8)
@
\endminipage};
\node (d) at (0,-2) 
{
\minipage{0.29\textwidth}
\centering
\vspace{0.0cm}
Regression Tree 
\endminipage};
\node (e) at (5,-2) 
{
\minipage{0.29\textwidth}
\centering
\vspace{0.0cm}
Random Forest 
\endminipage};
\node at (10,-2)
{
\minipage{0.28\textwidth}
\centering
\vspace{0.0cm}
Distributional Forest 
\endminipage};
\node (g) at (0,-5.4) 
{
\minipage{0.29\textwidth}
\vspace{0.2cm}
%\hspace{0.0cm}
\centering
<<motivation_disttree, echo=FALSE, results=hide>>=
## disttree
set.seed(74)
x <- c(1:nobs)/nobs
var <- ytree/3
y <- ytrue + rnorm(nobs,0,var)
@

<<plot_motivation_disttree, fig=TRUE, echo=FALSE>>=
par(mar = c(2, 0, 2, 0))
plot(x, y, xaxt="n", yaxt="n", ann=FALSE, type = "n")
usr <- par("usr")
rect(usr[1], usr[3], usr[2], usr[4], col = gray(0.94), border = "transparent")
lines(x = x, y = ytree, col = pal["forest"], lwd=7, main = "")
polygon(c(x, rev(x)), c(ytree + var, rev(ytree - var)),
  col = pallight["forest"], border = "transparent")
points(x, y, col = "slategray", pch = 19)
box(lwd=8)
@
\vspace{-0.2cm}%\\
Distributional Tree
\vspace{0.2cm}
\endminipage};
\draw[-{>[scale=4, length=2, width=3]},line width=0.4pt](2.1,0)--(2.9,0);
\draw[-{>[scale=4, length=2, width=3]},line width=0.9pt](7.1,0)--(7.9,0);
\draw[-{>[scale=4, length=2, width=3]},line width=0.9pt](0,-2.4)--(0,-3.2);
\draw[-{>[scale=4, length=2, width=3]},line width=0.9pt](2.2,-5.4)--(7.9,-2.5);
\end{tikzpicture}
\caption{\label{Devel_treeforest}Tree and forest developments.
Regression tree (top left), distributional tree (bottom left), random forest
(top middle), and distributional forest (top right).}
\end{figure}

In summary, the models discussed so far provide a broad and powerful toolset for
parametric distributional fits depending on a specified set of additive linear or
smooth nonlinear terms. A rather different approach to capturing the dependence on
covariates are tree-based models.
\begin{itemize}
  \item \emph{Regression trees} (\citealt{Breiman+Friedman+Stone:1984}) recursively
    split the data into more homogeneous subgroups and can thus capture abrupt shifts
    (Figure~\ref{Devel_treeforest}, top left) and approximate nonlinear functions.
    Furthermore, trees automatically carry out a forward selection of covariates and
    their interactions. 
  \item \emph{Random forests} (\citealt{Breiman:2001}) average the predictions of
    an ensemble of trees fitted to resampled versions of the learning data. This
    stabilizes the recursive partitions from individual trees and hence better
    approximates smooth functions (Figure~\ref{Devel_treeforest}, top middle)
\end{itemize}
While classical regression trees and random forests only model the mean of the response
we propose to follow the ideas from GAMLSS modeling -- as outlined
in Figure~\ref{Devel_parmod} -- and combine tree-based methods with parametric distributional models,
yielding two novel techniques:
\begin{itemize}
  \item \emph{Distributional trees} split the data into more homogeneous groups with
    respect to a parametric distribution, thus capturing changes in any distribution
    parameter like location, scale, or shape (Figure~\ref{Devel_treeforest}, bottom left)
  \item \emph{Distributional forests} utilize an ensemble of distributional trees
    for obtaining stabilized and smoothed parametric predictions (Figure~\ref{Devel_treeforest},
    top right).
\end{itemize}
In the following, particular focus is given to distributional forests as a method for obtaining
probabilistic forecasts by leveraging the strengths of random forests: the ability to
capture both smooth and abruptly changing functions along with simultaneous selection of
variables and possibly complex interactions. Thus, these properties make the method particularly
appealing in case of many covariates with unknown effects where it would be challenging
to specify a distributional regression model like GAMLSS.


<<echo=FALSE, results=hide>>=
#### Axams prediction 24 - 4
if(file.exists("rain_Axams_pred_24to4.rda")){
  #load("~/svn/partykit/pkg/disttree/inst/draft/rain_Axams_pred_24to4.rda")
  load("rain_Axams_pred_24to4.rda")
} else {
  #source("~/svn/partykit/pkg/disttree/inst/draft/rain_axams_pred_24to4.R")
  source("rain_axams_pred_24to4.R")
  library("gamlss.cens")
  gen.cens("NO", type = "left")
  res <- rain_axams_pred(seedconst = 7, ntree = 100,
                         forest_mtry = 27,   # if frac == TRUE: nvar/3 = 30
                         gamboost_cvr = TRUE,
                         frac = FALSE)
  
  save(res, file = "rain_Axams_pred_24to4.rda")
}

#### prepare data for plot of estimated density functions
# predictions for one day (in each of the four years) 
# (19th of July 2011 is missing)
pday <- 24  # 2 (hohe Beobachtung zu niedrig geschaetzt), 4, 15, evtl. auch 7, 8, 23 (eine 0-Beobachtung und 2 sehr aehnliche), 

pdays <- if(pday<19) c(pday, pday + 31, pday + 62, pday + 92) else c(pday, pday + 31, pday + 61, pday + 92)
pdf <- predict(res$df, newdata = res$testdata[pdays,], type = "parameter")
df_mu <- pdf$mu
df_sigma <- pdf$sigma
df_exp <- pnorm(df_mu/df_sigma) * (df_mu + df_sigma * (dnorm(df_mu/df_sigma) / pnorm(df_mu/df_sigma)))
cbind(df_exp, res$testdata[pdays,"robs"])


# plot predicted distributions together with observations
#set.seed(res$call$seedconst)
set.seed(7)
x <- c(0.01, sort(runif(500,0.01,8)))
y1 <- crch::dcnorm(x, mean = df_mu[1], sd = df_sigma[1], left = 0)
y2 <- crch::dcnorm(x, mean = df_mu[2], sd = df_sigma[2], left = 0)
y3 <- crch::dcnorm(x, mean = df_mu[3], sd = df_sigma[3], left = 0)
y4 <- crch::dcnorm(x, mean = df_mu[4], sd = df_sigma[4], left = 0)
dayending <- if(pday > 3) "th" else switch(pday, "1" = {dayending <- "st"}, "2" = {dayending <- "nd"}, "3" = {dayending <- "rd"})

# point mass (slightly shifted)
pm1 <- c(0.05, crch::dcnorm(-1, mean = df_mu[1], sd = df_sigma[1], left = 0))
pm2 <- c(0.01, crch::dcnorm(-1, mean = df_mu[2], sd = df_sigma[2], left = 0))
pm3 <- c(-0.03, crch::dcnorm(-1, mean = df_mu[3], sd = df_sigma[3], left = 0))
pm4 <- c(-0.07, crch::dcnorm(-1, mean = df_mu[4], sd = df_sigma[4], left = 0))

# predictions
pred1 <- c(res$testdata[pdays,"robs"][1], crch::dcnorm(res$testdata[pdays,"robs"][1], mean = df_mu[1], sd = df_sigma[1], left = 0))
pred2 <- c(res$testdata[pdays,"robs"][2], crch::dcnorm(res$testdata[pdays,"robs"][2], mean = df_mu[2], sd = df_sigma[2], left = 0))
pred3 <- c(res$testdata[pdays,"robs"][3], crch::dcnorm(res$testdata[pdays,"robs"][3], mean = df_mu[3], sd = df_sigma[3], left = 0))
pred4 <- c(res$testdata[pdays,"robs"][4], crch::dcnorm(res$testdata[pdays,"robs"][4], mean = df_mu[4], sd = df_sigma[4], left = 0))

#legendheight
lh1 <- crch::dcnorm(0.01, mean = df_mu[1], sd = df_sigma[1], left = 0)
lh2 <- crch::dcnorm(0.01, mean = df_mu[2], sd = df_sigma[2], left = 0)
lh3 <- crch::dcnorm(0.01, mean = df_mu[3], sd = df_sigma[3], left = 0)
lh4 <- crch::dcnorm(0.01, mean = df_mu[4], sd = df_sigma[4], left = 0)



## PIT histograms and qqr-plots

## get predicted parameter
# for learndata and testdata
{
  set.seed(7)
  ## disttree
  if(FALSE){
    # in sample
    pdt <- predict(res$dt, type = "parameter")
    dt_mu_l <- pdt$mu 
    dt_sigma_l <- pdt$sigma 
    pit_dt_l <- cbind(0, pnorm(res$learndata[,"robs"], mean = dt_mu_l, sd = dt_sigma_l))
    pit_dt_l[res$learndata[,"robs"]>0, 1] <- pit_dt_l[res$learndata[,"robs"]>0, 2]
  }  
  # out of sample
  pdt <- predict(res$dt, type = "parameter", newdata = res$testdata)
  dt_mu_t <- pdt$mu 
  dt_sigma_t <- pdt$sigma 
  pit_dt_t <- cbind(0, pnorm(res$testdata[,"robs"], mean = dt_mu_t, sd = dt_sigma_t))
  pit_dt_t[res$testdata[,"robs"]>0, 1] <- pit_dt_t[res$testdata[,"robs"]>0, 2]
  
  
  ## distforest
  if(FALSE){
    # in sample
    pdf <- predict(res$df, type = "parameter")
    df_mu_l <- pdf$mu
    df_sigma_l <- pdf$sigma
    pit_df_l <- cbind(0, pnorm(res$learndata[,"robs"], mean = df_mu_l, sd = df_sigma_l))
    pit_df_l[res$learndata[,"robs"]>0, 1] <- pit_df_l[res$learndata[,"robs"]>0, 2]
  }
  # out of sample
  pdf <- predict(res$df, type = "parameter", newdata = res$testdata)
  df_mu_t <- pdf$mu
  df_sigma_t <- pdf$sigma
  pit_df_t <- cbind(0, pnorm(res$testdata[,"robs"], mean = df_mu_t, sd = df_sigma_t))
  pit_df_t[res$testdata[,"robs"]>0, 1] <- pit_df_t[res$testdata[,"robs"]>0, 2]
  
  
  ## gamlss
  g_learndata <- res$learndata
  g_learndata$robs <- Surv(g_learndata$robs, g_learndata$robs>0, type="left")
  if(FALSE){
    # in sample
    g_mu_l <- predict(res$g, what = "mu", type = "response")
    g_sigma_l <- predict(res$g, what = "sigma", type = "response")
    pit_g_l <- cbind(0, pnorm(res$learndata[,"robs"], mean = g_mu_l, sd = g_sigma_l))
    pit_g_l[res$learndata[,"robs"]>0, 1] <- pit_g_l[res$learndata[,"robs"]>0, 2]
  }  
  # out of sample
  g_mu_t <- predict(res$g, what = "mu", type = "response", newdata = res$testdata)
  g_sigma_t <- predict(res$g, what = "sigma", type = "response", newdata = res$testdata)
  pit_g_t <- cbind(0, pnorm(res$testdata[,"robs"], mean = g_mu_t, sd = g_sigma_t))
  pit_g_t[res$testdata[,"robs"]>0, 1] <- pit_g_t[res$testdata[,"robs"]>0, 2]
  
  
  #gamboostLSS
  if(FALSE){
    #in sample
    pgb <- predict(res$gb, parameter = c("mu","sigma"), type = "response")
    gb_mu_l <- pgb$mu
    gb_sigma_l <- pgb$sigma
    pit_gb_l <- cbind(0, pnorm(res$learndata[,"robs"], mean = gb_mu_l, sd = gb_sigma_l))
    pit_gb_l[res$learndata[,"robs"]>0, 1] <- pit_gb_l[res$learndata[,"robs"]>0, 2]
  }  
  # out of sample
  pgb <- predict(res$gb, parameter = c("mu","sigma"), type = "response", newdata = res$testdata)
  gb_mu_t <- pgb$mu
  gb_sigma_t <- pgb$sigma
  pit_gb_t <- cbind(0, pnorm(res$testdata[,"robs"], mean = gb_mu_t, sd = gb_sigma_t))
  pit_gb_t[res$testdata[,"robs"]>0, 1] <- pit_gb_t[res$testdata[,"robs"]>0, 2]
  
  
  #EMOS
  if(FALSE){
    # in sample
    ml_mu_l <- predict(res$ml, type = "location")     # returns parameter on response scale
    ml_sigma_l <- predict(res$ml, type = "scale")
    pit_ml_l <- cbind(0, pnorm(res$learndata[,"robs"], mean = ml_mu_l, sd = ml_sigma_l))
    pit_ml_l[res$learndata[,"robs"]>0, 1] <- pit_ml_l[res$learndata[,"robs"]>0, 2]
  }  
  # out of sample
  ml_mu_t <- predict(res$ml, type = "location", newdata = res$testdata)     # returns parameter on response scale
  ml_sigma_t <- predict(res$ml, type = "scale", newdata = res$testdata)
  pit_ml_t <- cbind(0, pnorm(res$testdata[,"robs"], mean = ml_mu_t, sd = ml_sigma_t))
  pit_ml_t[res$testdata[,"robs"]>0, 1] <- pit_ml_t[res$testdata[,"robs"]>0, 2]
  
}


## Variable importance
if(file.exists("vimp_crps.rda")){
  #load("~/svn/partykit/pkg/disttree/inst/draft/vimp_crps.rda")
  load("vimp_crps.rda")
} else {
  
  # locally redefine logLik.distforest to use crps in varimp()
  logLik.distforest <- function(object, newdata = NULL, ...) {
    if(is.null(newdata)) {
      newdata <- object$data
    } 
    
    # predict parameter
    pdf <- predict(object, newdata = newdata, type = "parameter")
    df_mu <- pdf$mu
    df_sigma <- pdf$sigma
    
    # calculate CRPS
    crps <- mean(crps_cnorm(newdata$robs, location = df_mu, scale = df_sigma, lower = 0, upper = Inf), na.rm = TRUE)
  
    return(structure(crps, df = NA, class = "logLik"))
  }
  
  vimp_crps <- varimp(res$df, nperm = 1L)
  #vimp10_crps <- varimp(res$df, nperm = 10L)
  #vimp100_crps <- varimp(res$df, nperm = 100L)
  #vimp1000_crps <- varimp(res$df, nperm = 1000L)
  
  save(vimp_crps, file = "~/svn/partykit/pkg/disttree/inst/draft/vimp_crps.rda")
  #save(vimp10_crps, file = "~/svn/partykit/pkg/disttree/inst/draft/vimp10_crps.rda")
  #save(vimp100_crps, file = "~/svn/partykit/pkg/disttree/inst/draft/vimp100_crps.rda")
  #save(vimp1000_crps, file = "~/svn/partykit/pkg/disttree/inst/draft/vimp1000_crps.rda")
  
  rm(logLik.distforest) 
}
    
@


\begin{figure}[t!]
\centering
\setkeys{Gin}{width=0.8\textwidth}
<<rain_Axams_pred, fig=TRUE, echo=FALSE, width=6.5, height=4>>=
par(mar = c(4, 4, 3, 1.5))
plot(x = x, y = y1, type = "l", col = "red", 
     main = paste0("July ", pday), ylab = "Density", 
     xlab = expression(Total~precipitation~"["~mm^(1/1.6)~"/"~"24h"~"]"),
     ylim = c(0,max(y1, y2, y3, y4, pm1, pm2, pm3, pm4) + 0.01),
     xlim = c(-1.5,8))
    
lines(x = x, y = y2, type = "l", col = "blue")
lines(x = x, y = y3, type = "l", col = "darkgreen")
lines(x = x, y = y4, type = "l", col = "purple")
legend("topright", c("Predicted distribution", "Point mass at censoring point", "Observation"),
       bty = "n", col = "black", lty = c(1, NA, NA), pch = c(NA, 19, 4), cex = 0.8)

# plot point mass
lines(x = c(pm1[1], pm1[1]), y = c(pm1[2], 0), col = "red", type = "l", lwd = 1)
lines(x = c(pm2[1], pm2[1]), y = c(pm2[2], 0), col = "blue", type = "l", lwd = 1)
lines(x = c(pm3[1], pm3[1]), y = c(pm3[2], 0), col = "darkgreen", type = "l", lwd = 1)
lines(x = c(pm4[1], pm4[1]), y = c(pm4[2], 0), col = "purple", type = "l", lwd = 1)

points(x = pm1[1], y = pm1[2], col = "red", pch = 19)
points(x = pm2[1], y = pm2[2], col = "blue", pch = 19)
points(x = pm3[1], y = pm3[2], col = "darkgreen", pch = 19)
points(x = pm4[1], y = pm4[2], col = "purple", pch = 19)


# plot predictions
points(x = pred1[1], y = pred1[2], col = "red", pch = 4)
points(x = pred2[1], y = pred2[2], col = "blue", pch = 4)
points(x = pred3[1], y = pred3[2], col = "darkgreen", pch = 4)
points(x = pred4[1], y = pred4[2], col = "purple", pch = 4)

lines(x = c(pred1[1], pred1[1]), y = c(pred1[2], 0), col = "darkgrey", type = "l", lty = 2)
lines(x = c(pred2[1], pred2[1]), y = c(pred2[2], 0), col = "darkgrey", type = "l", lty = 2)
lines(x = c(pred3[1], pred3[1]), y = c(pred3[2], 0), col = "darkgrey", type = "l", lty = 2)
lines(x = c(pred4[1], pred4[1]), y = c(pred4[2], 0), col = "darkgrey", type = "l", lty = 2)

# add labels
text(x = -0.8, y = lh1, labels = "2009", col = "red", cex = 0.8)
text(x = -0.8, y = lh2, labels = "2010", col = "blue", cex = 0.8)
text(x = -0.8, y = lh3, labels = "2011", col = "darkgreen", cex = 0.8)
text(x = -0.8, y = lh4, labels = "2012", col = "purple", cex = 0.8)
@
\caption{\label{axams_onestation}Predictions of total precipitation at station Axams for July 24 in 2009, 2010, 2011 and 2012 by a distributional forest learned on data from 1985--2008. Observations are left-censored at 0. The corresponding predicted point mass is shown at the censoring point (0).}
\end{figure}

In weather forecasting, these properties are appealing in mountainous regions
and complex terrain where small-scale effects are not well-resolved in the NWP
outputs. As these are computed at a coarser scale, there may be abrupt local
effects within a single NWP grid cell. To illustrate this in practice,
precipitation forecasts are obtained with distributional forests at 95
meteorological stations in a mountainous region in the Alps, covering mainly
Tyrol, Austria, and adjacent areas (see the map in Figure~\ref{map}).
More specifically, a zero-censored Gaussian
distribution is employed to model 24-hour total precipitation so that the
zero-censored point mass describes the probability of no precipitation on a
given day (see Figure~\ref{axams_onestation}). Forecasts for July are
established based on data from the same month in the years 1985--2012 along with
80~covariates derived from a wide range of NWP ensemble outputs. As
Figure~\ref{axams_onestation} shows, the station-wise models yield a full
distributional forecast for each day -- here for the same day (July~24) at one
station (Axams) over four years (2009--2012) -- based on the previous 24~years
as training data. The corresponding observations conform reasonably well with
the predictions. In Section~\ref{ProbabilisticForecasting} we investigate the
performance of distributional forests in this forecasting task in more detail.
It is shown that they perform at least on par and sometimes clearly better than
three alternative zero-censored Gaussian models: a standard ensemble model
output statistics approach (EMOS, \citealp{Gneiting+Raftery+Westveld:2005}), a GAMLSS
with regressors prespecified based on meteorological expertise following
\cite{Stauffer+Umlauf+Messner:2017}, and a boosted GAMLSS
\citep{Hofner+Mayr+Schmid:2016} using non-homogeneous boosting
\citep{Messner+Mayr+Zeileis:2017} as a technique for variable selection from all
80~available regressors.



\section{Methodology}
\label{Methodology}

To embed the distributional approach from GAMLSS into regression trees and random
forests, we proceed in three steps. (1)~To fix notation, we briefly review fitting
distributions using standard maximum likelihood in in Section~\ref{distfit}.
(2)~A recursive partitioning strategy based on the corresponding scores (or
gradients) is introduced in Section~\ref{disttree}, leading to distributional
trees. (3)~Ensembles of distributional trees fitted to randomized subsamples are
employed to establish distributional forests in Section~\ref{distforest}.

The general distributional notation is exemplified in all three steps for the
zero-censored Gaussian distribution. The latter is employed in the empirical
in the empirical case study in Section~\ref{ProbabilisticForecasting} to model
power-transformed daily precipitation amounts.


\subsection{Distributional fit}
\label{distfit}

A distributional model $\mathcal{D}(Y, \bm{\theta})$ is considered for the response
variable $Y \in \mathcal{Y}$ using the distributional family $\mathcal{D}$ with
\textit{k}-dimensional parameter vector $\bm{\theta} \in \bold{\Theta}$. Based on the
corresponding probability density function $f(Y; \bm{\theta})$ the log-likelihood
function is defined $\ell(\bm{\theta}; Y) = \log\{f(Y; \bm{\theta})\}$. The GAMLSS
framework \citep{Rigby+Stasinopoulos:2005} provides a wide range of such
distributional families with parameterizations corresponding to location, scale,
and shape. Furthermore, censoring and/or truncation of these distributions
can be incorporated in the usual straightforward way \fixme{reference needed}.

To capture both location and scale of the probabilistic precipitation forecasts
while accounting for a point mass at zero precipitation (i.e., dry days without
rain), zero-censored Gaussian distribution with with mean/location parameter
$\mu$ and standard deviation/scale parameter $\sigma$ is employed. Therefore,
the corresponding log-likelihood function with parameter vector 
$\bm{\theta} = (\mu, \sigma)$ is
$$
\ell(\mu, \sigma; Y) = 
\begin{cases}
    \log\left\{\frac{1}{\sigma} \cdot \phi\left(\frac{Y - \mu}{\sigma}\right) \right\}, & \text{if } Y > 0\\[0.2cm]
    \log\left\{\Phi\left(\frac{-\mu}{\sigma}\right)\right\}, & \text{if } Y = 0
\end{cases}
$$
where $\phi$ and $\Phi$ are the probability density function and the 
distribution function of the standard normal distribution $\mathcal{N}(0,1)$. 
Other distributions $\mathcal{D}$ and corresponding log-likelihoods
$\ell(\mu, \sigma; Y)$ could be set up in the same way, e.g., for
censored shifted gamma distributions \citep{Scheuerer+Hamill:2015} or
zero-censored logistic distributions \citep{Gebetsberger+Messner+Mayr:2017}.

With the specification of the distribution family and its log-likelihood 
function the task of fitting a distributional model turns into the task 
of estimating the distribution parameter~$\bm{\theta}$. This is commonly done by
maximum likelihood (ML) based on the learning sample
$\{y_i\}_{i = 1, \dots, n}$ of the response variable $Y$. The maximum
likelihood estimator (MLE) $\bm{\hat \theta}$ is given by
$$ \label{eq:mle}
\bm{\hat \theta} = \argmax_{\bm{\theta} \in \bold{\Theta}} \sum_{i=1}^n \ell(\bm{\theta}; y_i).
$$
Equivalently, this can be defined based on the corresponding first-order
conditions
$$ \label{eq:foc}
\sum_{i = 1}^n s(\bm{\hat \theta}, y_i) = 0,
$$
where $s(\bm{\theta}, y_i)$ is the associated score function
$$ \label{eq:score}
s(\bm{\theta}, y_i) = \frac{\partial \ell}{\partial \bm{\theta}}(\bm{\theta}; y_i).
$$ 
The latter is subsequently employed as a general goodness-of-fit measure
to assess how well the distribution with parameters $\bm{\theta}$ fits
one individual observation $y_i$.


\subsection{Distributional tree}
\label{disttree}

Typically, a single global model $\mathcal{D}(Y, \bm{\theta})$ is not sufficient
for reasonably representing the response distribution and covariates
$\bold{Z} = Z_1, \dots, Z_m \in \mathcal{Z}$ are
employed to capture differences in the distribution parameters $\bm{\theta}$.
In weather forecasting, these covariates typically include the output
from numerical weather prediction systems and/or lagged weather observations.

To incorporate the covariates into the distributional model, GAMLSS considers
them as regressors in additive predictors
$g_j(\theta_j) = f_{j,1}(\bold{Z}) + f_{j,2}(\bold{Z}) + \dots$.
Link functions $g_j(\cdot)$ are used for every parameter $\theta_j$
($j = 1, \dots, k$) based on smooth terms $f_{j,k}$ such as
nonlinear effects, spatial effects, random coefficients, or interaction surfaces
\citep{Klein+Kneib+Lang:2015}. However, this requires specifying the
additive terms and their functional forms in advance which can be challenging in
practice, especially if the number of covariates $m$ is large.

Regression trees generally take a different approach for automatically
including covariates in a data-driven way and allowing for abrupt
changes, non-additive effects, and interactions. In the context of
distributional models the goal is to partition the covariate space
$\mathcal{Z}$ recursively into disjoint segments so that a homogenous
distributional model for the response $Y$ can be found with segment-specific
parameters. More specifically, the $B$ disjoint segments $\mathcal{B}_b$
($b = 1, \dots, B$) partition the covariate space
$$ \label{eq:partition}
\mathcal{Z} = \dot{\bigcup\limits_{b = 1, \ldots, \textit{B}}} \mathcal{B}_b,
$$
and a local distributional model $\mathcal{D}(Y, \bm{\theta}^{(b)})$
(i.e., with segment-specific parameters $\bm{\theta}^{(b)}$) is fitted to the
response $Y$ in that segment.

To find the segments $\mathcal{B}_b$ that are (approximately) homogenous
with respect to the distributional model with given parameters, the idea
is to use a gradient-based recursive-partitioning approach. In a given
subsample of the learning data this fits the model by ML (see
Equation~\ref{eq:mle}) and then assesses the goodness of fit by assessing the
corresponding scores $s(\bm{\hat \theta}; y_i)$ (see Equation~\ref{eq:score}).

In summary, distributional trees are fitted recursively via:
\begin{enumerate}
\item Estimate $\bm{\hat \theta}$ via maximum likelihood for the observations
  in the current subsample.
\item Test for associations or instabilities of the scores $s(\bm{\hat \theta}, y_i)$ 
  and $Z_{l,i}$ for each partitioning variable~$Z_l$ ($l = 1, \dots, m$).
\item Split the sample along the partitioning variable $Z_{l,i}^*$ with the
  strongest association or instability. Choose the breakpoint with the highest
  improvement in the log-likelihood or the highest discrepancy .
\item Repeat steps 1--3 recursively in the subsamples until these become too
  small or there is no significant association/instability (or some other
  stopping criterion is reached).
\end{enumerate}
%
Different inference techniques can be used for assessing the association between
scores and covariates in step~3. In the following we use the general class of
permutation tests introduced by \cite{Hothorn+Hornik+VanDeWiel:2006} which is
also the basis of conditional inference trees \citep[CTree,][]{Hothorn+Hornik+Zeileis:2006}.
Alternatively, one could use asymptotic M-fluctuation tests for parameter
instability \citep{Zeileis+Hornik:2007} as in model-based recursive partitioning
\citep[MOB,][]{Zeileis+Hothorn+Hornik:2008}. More details are provided in
Appendix~\ref{app:tree}.

For obtaining probabilistic predictions from the tree for a (possibly new) set
of covariates $\bold{z} = (z_1, \ldots, z_m)$, the observation simply has to
be ``sent down'' the tree and the corresponding segment-specific MLE has to be
obtained. This can also be understood as a weighted MLE where the weights
select those observations from the learning sample that fall into the same
segment:
$$
w^{\text{tree}}_i(\bold{z}) = \sum_{b=1}^B \mathbf{1}((\bold{z}_i \in \mathcal{B}_b) \land (\bold{z} \in \mathcal{B}_b)),
$$
where $\mathbf{1}(\cdot)$ is the indicator function. The predicted distribution
for a given $\bold{z}$ is then fully specified by the estimated parameter
$\bm{\hat \theta}(\bold{z})$ where
$$
\bm{\hat \theta}(\bold{z}) = \argmax_{\bm{\theta} \in \bm{\Theta}} \sum_{i=1}^n w^{\text{tree}}_i(\bold{z}) \cdot \ell(\bm{\theta}; y_i).
$$

\subsection{Distributional forest}
\label{distforest}

While the simple recursive structure of a tree model is easy to visualize and
interpret, the abrupt changes are often too rough, instable, and impose steps
on the model even if the true underlying effect is smooth. Hence, ensemble
methods such as bagging or random forests \citep{Breiman:2001} typically smooth
the effects, stabilize the model, and improve predictive performance.

While classical random forests \citep{Breiman:2001} grow ensembles of trees
that pick up changes in the location of the response across the covariates,
\emph{distributional forests} employ an ensemble of $T$~\emph{distributional trees}. These
pick up changes in the ``direction'' of any distribution parameter by
considering the full score vector for choosing splitting variables and split
points. Each of the distributional trees is grown on a different data set
obtained through bootstrap sampling (or subsampling) and in each node only a
random subset of the covariates $\bold{Z}$ is considered. As usual in random
forests, this reduces the correlation among the trees and stabilizes the
variance of the model.


To obtain probabilistic predictions from a distributional forest, it still
needs to be specified how to compute the parameter estimates
$\bm{\hat \theta}(\bold{z})$ for a (potentially new) set of covariates $\bold{z}$.
Following \cite{Hothorn+Zeileis:2017} we interpret random forests as adaptive
local likelihood estimators using the averaged ``nearest neighbor weights''
\citep{Lin+Jeon:2006} from the $T$~trees in the forest
$$
w^{\text{forest}}_i(\bold{z}) = \frac{1}{T} \sum_{t=1}^T \sum_{b=1}^{B^t} \mathbf{1}((\bold{z}_i \in \mathcal{B}^t_b) \land (\bold{z} \in \mathcal{B}^t_b))
$$
Thus, these $w^{\text{forest}}_i(\bold{z}) \in [0, 1]$ whereas
$w^{\text{tree}}_i(\bold{z}) \in \{0, 1\}$. Hence, weights cannot only be $0$
or $1$ but change more smoothly, giving high weight to those observations $i$
from the learning sample that co-occur in the same segment $\mathcal{B}_b^t$
as the new observation $\bold{z}$ for many of the trees $t = 1, \dots, T$.
Consequently, the parameter estimates may, in principle, change for every
observation and can be obtained by
$$
\bm{\hat \theta}(\bold{z}) =  \argmax_{\bm{\theta} \in \bm{\Theta}} \sum_{i=1}^n w^{\text{forest}}_i(\bold{z}) \cdot \ell(\bm{\theta}; y_i).
$$
In summary, this yields a parametric distributional regression model 
(through the score-based approach) that can capture both abrupt effects and
high-order interactions (through the trees) and smooth effects (through
the forest).


\SweaveOpts{eval = TRUE}

\section{Probabilistic precipitation forecasting in complex terrain}
\label{ProbabilisticForecasting}

Many weather forecasting models leverage the strengths of modern
numerical weather prediction (NWP) systems \citep[see][]{Bauer+Thorpe+Brunet:2015}
and produce high-quality forecasts adapted to specific sites/locations
through distributional regression models like EMOS
(ensemble model output statistics, \citealp{Gneiting+Raftery+Westveld:2005}).
In the case of precipiation forecasting, EMOS explains past precipitation
sums by the NWP output ``total precipitation'' (\emph{tp}), using its
ensemble mean as the predictor variable for the location parameter and
the ensemble standard deviation for the scale parameter.

While this approach alone is already highly effective in the plains,
it typically does not perform as well in complex terrain due to unresolved
effects in the NWP system. For example, in the Tyrolean Alps -- considered
in the following application case study -- the NWP grid cells of $50 \times 50$
km$^2$ are too coarse to capture mountains, valleys, etc. Therefore,
it is often possible to substantially improve the EMOS by including
further predictor variables, either from local observations or from
other NWP outputs. Unfortunately, it is typically unclear which
variables are relevant for improving the predictions. Simply including
all available variables may be computationally burdensome and
can lead to overfitting but, on the other hand, excluding too many variables
may result in a loss of valuable information. Therefore, selecting
the relevant variables and interactions is crucial for improving the
forecasting model.

In the following, it is illustrated how distributional regression forests
can solve this problem of automatically selecting relevant variables,
interactions, and potentially nonlinear effects. For fitting the forest
only the response distribution and the list of potential predictor variables
need to be specified (along with a few algorithmic details) and then
the model fit is determined by the forest in a data-driven way. Here, we
employ a zero-censored Gaussian distribution and 80~predictor variables
computed from ensemble means and spreads of various NWP outputs.
The predictive performance of the forest is compared to three other
zero-censored Gaussian models: (a) a standard basic EMOS, (b) a GAMLSS
with prespecified effects and interactions based on meteorological
knowedge/experience, and (c) a boosted GAMLSS with automatic
selection of smooth additive terms based on all 80~predictor variables.

\subsection{Data}
\label{data}

\begin{table}[t!]
%\resizebox{1.4\textwidth}{!}{
\begin{minipage}{\textwidth}
\begin{tabular}{ l  c  l }
\hline
Basic covariates & {\#} & Variations\\
\hline
\emph{tp}: total precipitation            & 12  & ensemble mean of sums over 24h, \\
\hspace*{0.5cm} power transformed (by $\frac{1}{1.6}$) &    & ensemble std. deviation of sums over 24h, \\ 
\emph{cape}: convective available     &    & ensemble minimum of sums over 24h, \\
\hspace*{0.9cm} potential energy &    & ensemble maximum of sums over 24h\\
\hspace*{0.9cm} power transformed (by $\frac{1}{1.6}$)&    & \qquad all for 6--30 UTC\\
\hspace*{\fill}                            &    & ensemble mean of sums over 6h\\
\hspace*{\fill}                            &    & \qquad for 6--12, 12--18, 18--24, 24--30 UTC\\
\hspace*{\fill}                            &    & ensemble std. deviation of sums over 6h\\
\hspace*{\fill}                            &    & \qquad for 6--12, 12--18, 18--24, 24--30 UTC\\
\hline
\emph{dswrf}: downwards short wave      & 6 & ensemble mean of mean values, \\
\hspace*{1.13cm} radiation flux (``sunshine'') &   & ensemble mean of minimal values,\\
\emph{msl}: mean sea level pressure     &   &  ensemble mean of maximal values,\\
\emph{pwat}: preciptable water          &   & ensemble std. deviation of mean values,\\
\emph{tmax}: 2m maximum temperature     &   & ensemble std. deviation of minimal values,\\
\hspace*{1.15cm}                          &   & ensemble std. deviation of maximal values,\\
\emph{tcolc}: total column-integrated   &   & \qquad all over 6--30 UTC \\
\hspace*{0.95cm} condensate               &   & \\
\emph{t500}: temperature on 500 hPa     &   & \\
\hspace*{1cm}                             &   & \\
\emph{t700}: temperature on 700 hPa     &   & \\
\hspace*{1cm}                             &   & \\
\emph{t850}: temperature on 850 hPa     &   & \\
\hspace{1cm}                              &   & \\
\hline
\emph{tdiff500850}: temperature         & 3 & ensemble mean of difference in mean,\\
\hspace*{\fill} difference 500 to 850 hPa &   & ensemble minimum of difference in mean,\\
\emph{tdiff500700}: temperature         &   & ensemble maximum of difference in mean\\
\hspace*{\fill} difference 500 to 700 hPa &   & \qquad all over 6--30 UTC\\
\emph{tdiff700850}: temperature         &   & \\
\hspace*{\fill} difference 700 to 850 hPa &   & \\
\hline
\emph{msl{\_}diff}: mean sea level pressure & 1 & \emph{msl{\_}mean{\_}max} $-$ \emph{msl{\_}mean{\_}min}\\
\hspace{1.6cm} difference            &   & \qquad over 6--30 UTC\\
\hline
\end{tabular}
\caption[Table caption text]{Basic covariates together with the number ({\#}) and the type of variations.}
\label{covariates}
\end{minipage} 
%}
\end{table}

Training and validation data consist of observed 
daily precipitation sums provided by the National Hydrographical Service 
(\citealt{ehyd}) and numerical weather forecasts from the U.S.~National
Oceanic and Atmospheric Administration (NOAA).
Both, observations and forecasts are available for 1985--2012 and
the analysis is exemplified using July as the month with the most precipitation
in Tyrol.

Observations are obtained for 95~stations all over Tyrol and surroundings,
providing 24~hour precipitation sums measured at 6~UTC and rigorously
quality-checked by the National Hydrographical Service. NWP outputs
are obtained from the second generation reforecast data set of the
global ensemble forecast system \citep[GEFS,][]{Hamill+Bates+Whitaker:2013}. 
This data set consists of an 11-member ensemble based on a fixed version of the 
numerical model and a horizontal grid spacing of about $50 \times 50$ 
km$^2$ initialized daily at 0~UTC from December 1984 to present 
providing forecasts on 26 hourly temporal resolution. Each of the 
11 ensemble members uses slightly different perturbed initial 
conditions trying to predict the situation specific uncertainty 
of the atmospheric state. 

From the GEFS outputs 14 basic forecast variables are considered with up 
to 12 variations of each of these variables such as mean/maximum/minimum
or different aggregation time ranges, etc. A detailed overview is
provided in Table~\ref{covariates}, yielding 80~predictor variables in total.
(Since the minimum of the ``variable downwards short wave radiation flux'',
\emph{dswrf}, over 24 hours is always 0, the ensemble mean and standard
deviation of these minimal values are not considered.)

To remove large parts of the skewness of precipitation data, 
a power transformation \citep{Box+Cox:1964} is often applied, e.g., using
cubic \citep{Stidd:1973} or square root \citep{Hutchinson:1998} transformations.
However, the power parameter may vary for different climatic zones or temporal 
aggregation periods and hence we follow \cite{Stauffer+Mayr+Messner:2017} 
in their choice of $\frac{1}{1.6}$ as a suitable power parameter for the
region of Tyrol. The same power transformation is applied to both the
observed precipitation sums and the NWP outputs ``total precipitation'' (\emph{tp}) and
``convective available potential energy'' (\emph{cape}).


\subsection{Models and evaluation}

\begin{table}[t!]
\centering
\begin{tabular}{ l l l l }
\hline
Model & Type & Location ($\mu$) & Scale ($\log(\sigma)$)                                  \\ \hline
Distributional forest & recursive    & all                & all                           \\ 
                      & partitioning &                    &                               \\ \hline
EMOS                  & linear       & \emph{tp{\_}mean}  & \emph{tp{\_}sprd}             \\ \hline
Prespecified GAMLSS   & spline       & \emph{tp{\_}mean}, & \emph{tp{\_}sprd},            \\
                      & in each      & \emph{tp{\_}max},  & \emph{dswrf{\_}sprd{\_}mean}, \\
 & & \emph{tp{\_}mean1218} $\ast$ & \emph{tp{\_}sprd1218} $\ast$\\ 
 & & \quad \emph{cape{\_}mean1218}, & \quad \emph{cape{\_}mean1218},\\
 & & \emph{dswrf{\_}mean{\_}mean}, & \emph{tcolc{\_}sprd{\_}mean},\\
 & & \emph{tcolc{\_}mean{\_}mean}, & \emph{tdiff500850{\_}mean}\\
 & & \emph{pwat{\_}mean{\_}mean}, & \\
 & & \emph{tdiff500850{\_}mean}, & \\
 & & \emph{msl{\_}diff} & \\ \hline
Boosted GAMLSS        & spline  & all & all \\
                      & in each &     &     \\ \hline
\end{tabular}
\caption[Table caption text]{Overview of models with type of 
covariate dependency and included covariates for each distribution 
parameter.}
\label{model_specification}
\end{table}

The following zero-censored Gaussian regression models are employed
in the empirical case study, see also Table~\ref{model_specification} for
further details:
%
\begin{itemize}

\item \emph{Distributional forest:} All 80~predictor variables are
  considered for learning a forest of 100 trees. Bootstrap
  sampling is employed for each tree using a third of the predictors
  in each split of the tree (``mtry''). Parameters are estimated by
  adaptive local likelihood based on the forest weights, as described
  in Section~\ref{Methodology}.

\item \emph{EMOS:} Standard ensemble model output statistics are used
  with the total precipitation ensemble mean as regressor in the location
  submodel and the ensemble standard deviation in the scale submodel.
  The parameters are estimated by maximum likelihood, using an identity
  link for the location and a log link for the scale
  \citep[following the advice of][]{Gebetsberger+Messner+Mayr:2017}.

\item \emph{Prespecified GAMLSS:} Smooth additive splines are selected for
  the most relevant predictors, based on meteorological expert knowledge
  following \cite{Stauffer+Umlauf+Messner:2017}. More specifically, based
  on the 80~available variables, 8 terms are included in the location
  submodel and 6 in the scale submodel. Both involve an interaction of
  \emph{tp} and \emph{cape} in the afternoon (between 12 and 18 UTC)
  to capture the potential for thunderstorms that frequently occur in
  summer afternoons in the Alps. The model is estimated by maximum
  penalized likelihood using a backfitting algorithm \citep{Stasinopoulos+Rigby:2007}.

\item \emph{Boosted GAMLSS:} Smooth additive splines are selected
  automatically from all 80~available variables, using non-cyclic boosting
  for parameter estimation \citep{Hofner+Mayr+Schmid:2016,Messner+Mayr+Zeileis:2017}.
  This updates the predictor terms for the location or scale submodels iteratively
  by maximizing the log-likelihood only for the variable yielding the
  highest improvement. The iteration stops early -- before fully maximizing
  the in-sample likelihood -- based on a (computationally intensive)
  out-of-bag bootstrap estimate of the log-likelihood. The grid considered for
  the number of boosting iterations (``mstop'') is: $50, 75, \dots, 975, 1000$.

\end{itemize}

The predictive performance in terms of full probabilistic forecasts is
assessed using the continuous ranked probability score (CRPS, \fixme{reference}).
For each of the models this assesses the discrepancy of the
predicted distribution function $F$ from the observation $y$ by
$$
\text{CRPS}(y, F) = \int_{-\infty}^\infty (F(z) - \mathbf{1}(y \leq z))^2 dz
$$
where \(\mathbf{1}\) is the indicator function. In the subsequent
applications, the mean CRPS is always evaluated out of sample, 
either using cross-validation or a hold-out data set (2009--2012)
that was not used for learning (1985--2008). CRPS is a proper
scoring rule \citep{Gneiting+Raftery:2007} and lower values indicate a better fit.

To assess differences in the improvement of the forests and GAMLSS
models over the basic EMOS, a CRPS-based skill score with EMOS as
the reference method is computed as well:
$$
\text{SkillScore}_{\text{method}} = 1 - \frac{\text{CRPS}_{\text{method}}}{\text{CRPS}_{\text{EMOS}}}.
$$




\subsection{Application for one station}
\label{Application for one station}

In a first step, we show a detailed comparison of the competing models
for one observation station, Axams in Tyrol (selected somewhat arbitrarily
as the closest in the data set to Innsbruck, capital of Tyrol). As for all
other stations, daily observations and  numerical weather predictions are available for the month of July of 
28~years from 1985 through the year 2012. In Figure~\ref{axams_onestation} 
the results of fitting a distributional forest to the first 24 years of the 
data set and predicting the total amount of precipitation for one specific 
day of each of the four successive years have been shown as a motivational 
example for a probabilistic forecast. Staying with this fitted model,
variable importance of the distributional forest model will be analyzed and its
performance will be compared to those of the prespecified and the boosted GAMLSS 
and the EMOS model based on residual QQ-plots.

%%%<TH> always us the term "distributional forest"
As shown in Table~\ref{model_specification} the distributional forest 
includes all of the available covariates. A single tree model would allow 
to easily analyze which covariates are used as split variables in the nodes 
and hence have a significant impact on the response variable. In a forest 
model though, for each node in each of the trees a subset of the covariates 
is chosen randomly as possible split variables to select one from. Moreover, 
each tree is built on a resampled version of the learning data. For these 
reasons the choices of split variables vary over the trees of a forest making 
it difficult to analyze the impact of covariates just by investigating the 
different trees. To get an overview of which covariates have the most influential 
effect on the response variable a standard measurement of variable importance 
for forest models has been applied. Based on the mean decrease in accuracy 
regarding the CRPS the 10 leading covariates of the above described distributional 
forest for station Axams learned on the first 24 years are listed in 
Figure~\ref{varimp}. As expected, the numerical forecast for total precipitation (\emph{tp}) is the most influential variable. In particular, five variations of 
this variable are among the top ten of the list of covariates ordered by 
variable importance.  
  

\begin{figure}[t!]
\centering
\setkeys{Gin}{width=0.7\textwidth}
<<rain_axams_varim, fig=TRUE, echo=FALSE, width=6.5, height=4.5>>=
par(mar = c(3,10,2,2))
barplot(sort(vimp_crps, decreasing = FALSE)[(length(vimp_crps)-9):length(vimp_crps)], 
        horiz = TRUE, las = 1, axes = FALSE,
        xlab = "Variable importance: mean decrease in CRPS",
        font.axis = 3, #list(family="HersheySerif", face=3),
        names.arg = gsub("pow", "", names(sort(vimp_crps, decreasing = FALSE)[(length(vimp_crps)-9):length(vimp_crps)])))
axis(1, at = seq(0,1.6,0.2), las = 1, mgp=c(0,1,0))
@
\caption{\label{varimp} CRPS-based variable importance for the top 10 covariates 
in the distributional forest. Based on data for station Axams, learning period 
1985--2008 and assessed in 2009--2012.}
\end{figure}

In Figure~\ref{qqr_oosample} an impression of the quality of predictions made 
by the tested methods is given by residual QQ-plots for out-of-sample predictions 
of each of the models. For these plots 100 randomized quantile residuals are 
simulated per observation. The closer the points are to the diagonal line the 
better the estimated distributional model fits the data. Overall, it can be 
stated that all four models provide well calibrated forecasts. This is also 
supported by the corresponding probability integral transform (PIT) histograms 
(\citealt{Gneiting+Balabdaoui+Raftery:2007}) in the appendix.



\begin{figure}[t!]
\centering
\setkeys{Gin}{width=0.99\textwidth}
<<rain_axams_qqr_oosample, fig=TRUE, echo=FALSE, width = 6.5, pdf = FALSE, png = TRUE, resolution = 150>>=
# out of sample
set.seed(7)
par(mfrow = c(2,2))
qqrplot(pit_df_t, nsim = 100, main = "Distributional forest", ylim = c(-5,5), col = gray(0.04, alpha = 0.01), pch = 19)
qqrplot(pit_ml_t, nsim = 100, main = "EMOS", ylim = c(-5,5), col = gray(0.04, alpha = 0.01), pch = 19)
qqrplot(pit_g_t, nsim = 100, main = "Prespecified GAMLSS", ylim = c(-5,5), col = gray(0.04, alpha = 0.01), pch = 19)
qqrplot(pit_gb_t, nsim = 100, main = "Boosted GAMLSS", ylim = c(-5,5), col = gray(0.04, alpha = 0.01), pch = 19)
@
\caption{\label{qqr_oosample} Out-of-sample residual QQ-plots (2009--2012) 
for station Axams based on models learned on data from 1985--2008.}
\end{figure}

  
After analyzing the results of the models on only one data set where 
visualization is easier, the methods are now going to be evaluated
in a cross validation framework for the station Axams. 
The data set over all 28 years is split into 7 subsets each consisting of 
4 randomly selected years. While 6 of these subsets are used as learning 
data the 7th is the testing data set for which predictions are made and 
evaluated. This is done by calculating the CRPS value for each of the 
observations in the testing data set and then averaging them. Each of the 
7 parts is selected once as testing data which leads to 7 different settings 
of which again the average values are stored. This whole procedure is repeated 
10 times. The resulting skill scores are illustrated in 
Figure~\ref{boxplots_crps_axams} using the EMOS model as the reference model 
represented by the horizontal line at height 0. This CRPS skill score boxplot 
shows that, in this setting, both GAMLSS models and the distributional forest 
perform better than the EMOS model. While the two GAMLSS lead to an improvement 
of around 4 percent with the boosted version being slightly ahead, the 
distributional forest even reaches an improvement rate close to 6 percent.
<<echo=FALSE, results=hide>>=
#### cross validation rain
if(file.exists("rain_Axams_7x10.rda")){
  load("rain_Axams_7x10.rda")
} else {
  source("rain_cross_7x10.R")
  library("gamlss.cens")
  gen.cens("NO", type = "left")
  rainres <- rain_cross(stationname = "Axams", 
                        seedconst = 7, ntree = 100,
                        forest_mtry = 27,
                        gamboost_cvr = TRUE,
                        frac = FALSE)
  
  save(rainres, file = "rain_Axams_7x10.rda")
}

rain_rmse <- matrix(0, ncol = ncol(rainres[[1]]$rmse), length(rainres)-1)
rain_ll <- matrix(0, ncol = ncol(rainres[[1]]$ll), length(rainres)-1)
rain_crps <- matrix(0, ncol = ncol(rainres[[1]]$crps), length(rainres)-1)

for(i in 1:(length(rainres)-1)){
  rain_rmse[i,] <- colMeans(rainres[[i]]$rmse, na.rm = TRUE)
  rain_ll[i,] <- colMeans(rainres[[i]]$ll, na.rm = TRUE)
  rain_crps[i,] <- colMeans(rainres[[i]]$crps, na.rm = TRUE)
}

colnames(rain_rmse) <- colnames(rain_ll) <- colnames(rain_crps) <- colnames(rainres[[1]]$rmse)
@


\begin{figure}[t!]
\centering
<<rain_cross_axams_crps_skill_score, fig=TRUE, echo=FALSE, height=4.5, width = 7>>=
boxplot(1 - rain_crps[,c(2,3,4)] / rain_crps[,6], ylim = c(-0.005, 0.065),
        names = c("Distributional forest", "Prespecified GAMLSS", "Boosted GAMLSS"),
        ylab = "CRPS skill score", col = "lightgray") 
abline(h = 0, col = pal["EMOS"], lwd = 2)
@
\caption{\label{boxplots_crps_axams}CRPS skill score from the 10 times 
7-fold cross validation at station Axams (1985--2012). The horizontal orange 
line pertains to the reference model EMOS.}
\end{figure}




\SweaveOpts{eval = TRUE}
%\newpage
\subsection{Application for all stations}
Until now the focus has been on one observation station only. To test and 
compare the four methods under different geographical circumstances they
are applied on all 95 stations. For each station, the first 
24 years are used as learning data set and predictions are made for the 
ensuing 4 years of available observations. For these out-of-sample predictions 
the CRPS skill score with the EMOS model as reference model is calculated 
for each method at each station. In Figure~\ref{boxplot_crps_all} the boxplots 
of the CRPS skill score values are plotted illustrating that the distributional 
forest leads to the highest improvement averaged over all stations.
The green line represents the results for the station Axams.
%NAs in gamlss in 13 stations!!!\\
%for example in 13th station in complete{\_}stations\\
%\text{Error in glim.fit(f = mu.object, X = mu.X, y = y, w = w, fv = mu, os = mu.offset,  :}\\ 
%\text{NA's in the working vector or weights for parameter mu}
%\\
% sum(is.na(crps[,3]))
%<<eval = TRUE, echo = FALSE>>=
<<echo = FALSE>>=
#### prediction over all stations 24 - 4
if(file.exists("rain_pred_24to4.rda")){
  #load("~/svn/partykit/pkg/disttree/inst/draft/rain_pred_24to4.rda")
  load("rain_pred_24to4.rda")
} else {
  
  source("rain_pred_24to4.R")
  library("gamlss.cens")
  gen.cens("NO", type = "left")
  
  res <- rain_pred(seedconst = 7,
                   gamboost_cvr = TRUE,
                   frac = FALSE)
  
  save(res, file = "rain_pred_24to4.rda")
}
 
#ll_all <- res[[1]]$results["ll",]
#for(i in 2:(length(res)-1)) ll_all <- rbind(ll_all, res[[i]]$results["ll",])

#rmse_all <- res[[1]]$results["rmse",]
#for(i in 2:(length(res)-1)) rmse_all <- rbind(rmse_all, res[[i]]$results["rmse",])

crps_all <- res[[1]]$results["crps",]
for(i in 2:(length(res)-1))  crps_all <- rbind(crps_all, res[[i]]$results["crps",])

#colnames(ll_all) <- colnames(rmse_all) <- 
  colnames(crps_all) <- colnames(res[[1]]$results)

# skill score
s <- 1 - crps_all[, 2:4]/crps_all[,6]
colnames(s) <- c("Distributional forest", "Prespecified GAMLSS", "Boosted GAMLSS")

## prepare data for map which shows where distforest performed better than gamlss or gamboostLSS based on the crps

crps <- crps_all[,c("distforest", "gamlss", "gamboostLSS", "EMOS log")]  

## best method
bst <- apply(crps, 1, which.min)

## distance of forest to best other method
dst <- crps[,1] - crps[cbind(1:nrow(crps), apply(crps[, -1], 1, which.min) + 1)]

## breaks/groups
brk <- c(-0.1, -0.05, -0.005, 0.005, 0.05, 0.1)
#brk <- c(-0.1, -0.05, -0.01, 0.01, 0.05, 0.1)
grp <- cut(dst, breaks = brk)

## HCL colors (relatively flashy, essentially CARTO Tropic)
clr <- colorspace::diverge_hcl(5, h = c(195, 325), c = 80, l = c(50, 90), power = 1.3)



library("raster") # dem (digital elevation model)
library("sp")     # gadm www.gadm.org/country

load("plot_map_rain/demo/tirol.gadm.rda")
load("plot_map_rain/demo/tirol.dem.rda")
load("plot_map_rain/demo/ehyd.statlist.rda")
  
# Create SpatialPointsDataFrame from station list
sp <- SpatialPointsDataFrame(subset(ehyd.statlist[res$complete_stations,],
                                    select=c(lon,lat)),
                             data = subset(ehyd.statlist[res$complete_stations,],
                                           select = -c(lon,lat)),
                             proj4string = crs(tirol.dem))

#library("colorspace")
@



\begin{figure}[t!]
\centering
<<rain_all_crps_skill_score, fig=TRUE, echo=FALSE, width = 7>>=
  matplot(t(s[,]), type = "l", lwd = 2, 
          col = gray(0.5, alpha = 0.2),
          lty = 1, axes = FALSE, 
          xlab = "", ylab = "CRPS skill score", xlim = c(0.5, 3.5))
lines(s[70,], col = "limegreen", type = "o", pch = 19, lwd = 2)  
# Station Axams is the 77th station which is the 70th in the list of complete stations
  boxplot(s, add = TRUE, col = "transparent")
  abline(h = 0, col = pal["EMOS"], lwd = 2)
    
@
\caption{\label{boxplot_crps_all}CRPS skill score for each station (grey lines) 
and aggregated over all stations (boxplots). Station Axams is highlighted in green 
and the horizontal orange line pertains to the reference model EMOS. The models are 
learned on data from 24 years (1985--2008) and validated on 4 years (2009--2012).}
\end{figure}

Figure~\ref{map} shows a map of Tyrol (based on the SRTM 90m Digital Elevation 
Data, \cite{srtm}) with the location of all observation stations included in the 
data set. Each station is illustrated by a symbol with the type of symbol 
representing the method that performed best in terms of CRPS at this station. 
The color of the symbol indicates the difference between the CRPS of the 
distributional forest and the CRPS of the best performing among the other 
three models (prespecified GAMLSS, boosted GAMLSS and EMOS).
Even though for some stations, especially in East Tyrol, one of the two GAMLSS
models and for a few others the EMOS model leads to the best results, for the 
majority of stations the distributional forest performs best.

\begin{figure}[t!]
\centering
\setkeys{Gin}{width=0.99\textwidth}
<<map, fig=TRUE, echo=FALSE, height=6.1, width=9>>=
  
  layout(cbind(1, 2), width = c(9, 1))
  par(mar = c(5,4,4,0.1))
  raster::image(tirol.dem, col = rev(gray.colors(100)),
                main="Stations in Tyrol", ylab = "Latitude", xlab = "Longitude", 
                xlim = c(9.8,13.2), 
                ylim = c(46.6, 47.87))
  plot(tirol.gadm, add = TRUE)
  points(sp, pch = c(21, 24, 25, 22)[bst], bg = clr[grp], col = "black", las = 1, cex = 1.5)
  legend(x = 9.8, y = 47.815, pch = c(21, 24, 25, 22), legend = c("Distributional forest", "Prespecified GAMLSS", "Boosted GAMLSS", "EMOS"), cex = 1, bty = "n")
  text(x = 10.3, y = 47.82, labels = "Models with lowest CRPS")
  mtext("CRPS\ndifference", side=4, las = TRUE, at = c(x = 13.5, y = 47.76), line = 0.3)
  par(mar = c(0.5,0.2,0.5,2.3))
  ## legend
  plot(0, 0, type = "n", axes = FALSE, xlab = "", ylab = "",
       xlim = c(0, 1), ylim = c(-0.2, 0.2), xaxs = "i", yaxs = "i")
  rect(0, brk[-6], 0.5, brk[-1], col = rev(clr))
  axis(4, at = brk, las = 1, mgp=c(0,-0.5,-1))
  
@
\caption{\label{map}Map of Tyrol showing which model performed best at 
which station (type of symbol). The color codes whether the distributional 
forest had lower or higher CRPS (red or blue) compared to the best of the 
other three models (prespecified GAMLSS, boosted GAMLSS and EMOS).}
\end{figure}





\section{Discussion}

From the results in Section~\ref{ProbabilisticForecasting} we can conclude 
that distributional forests provide a powerful alternative framework requiring 
only little effort to set up a model which is competitive to commonly used 
methods or even outperforms them. Even though a proper selection of covariates 
and specification of interactions in advance can lead to better fitting 
(and probably less complex) models, this presupposes profound knowledge of 
all influencing factors. Otherwise, important aspects might be missed or 
effects which do not correspond to the truth might be imposed on the model. 
This risk can be avoided by applying a boosting algorithm for an automatic 
variable selection. In this case the trade-off is often a remarkably higher 
computational effort. To build a distributional forest the user is not 
required to provide any expert knowledge or additional information about 
the setting as variable selection and detection of interactions are done 
automatically by the tree-structure of the model. This feature is 
particularly advantageous in case of a very high number of covariates. 
Therefore, the above discussed application illustrates the strengths of 
distributional forests as they offer a very flexible framework which 
enables an easy use of the method and at the same time lead to an overall 
improvement in the predictive performance in this setting. Especially 
in situations where no or only little information is available about 
possible influences, effects or interactions this novel methodology 
can be seen as a good and recommendable compromise being able to deal 
with a wide range of modeling situations in a reasonable way.

%Even though many situations demand for the use of specifically tailored 
%models providing the needed features and therefore being clearly advantageous 
%and the best choice, it is often hard to tell in advance which features 
%will be required. In these cases a distributional forest is a 
%recommendable compromise providing reasonable models and performing 
%well without any prespecification or additional knowledge needed in advance. 



\section*{Computational details}
The corresponding implementation to the novel methods presented 
in this article is available on \textsf{R}-Forge in an \textsf{R} 
package called \textbf{disttree} within the project \textbf{partykit} 
(\url{https://R-Forge.R-project.org/projects/partykit/}). The function
\code{distfit} fits a distributional model and is called within the
tree-building function \code{disttree} and the forest-building
function \code{distforest}. 
Within \code{disttree} either the \code{mob} or \code{ctree} function 
can be applied as a framework to build a tree. Similarly, the function 
\code{distforest} is based on the function \code{cforest}. All three 
functions, \code{mob}, \code{ctree} and \code{cforest}, are provided 
in the package \textbf{partykit} (\citealt{Hothorn+Zeileis:2015}). 
To specify a distribution family the user can hand over one of the 
following objects as input argument in \code{distfit}, \code{disttree} 
and \code{distforest}: 
\begin{itemize}
\item a GAMLSS family object from the \textsf{R} package \textbf{gamlss.dist} 
(\citealt{Stasinopoulos+Rigby:2007}).
\item a family list generating function from the package \textbf{disttree}.
\item a complete family list from the package \textbf{disttree}.
\item a custom list containing all required information about the 
distribution family.
\end{itemize}

In Section~\ref{ProbabilisticForecasting} the prespecified GAMLSS were 
built using the function \code{gamlss} from the \textsf{R} package 
\textbf{gamlss} (\citealt{Stasinopoulos+Rigby:2007}). For the 
boosted GAMLSS the function \code{gamboostLSS} from the \textsf{R} 
package \textbf{gamboostLSS} (\citealt{Hofner+Mayr+Schmid:2016}) 
was applied. The EMOS models were built using the function 
\code{crch} from the \textsf{R} package \textbf{crch} 
(\citealt{Messner+Mayr+Zeileis:2016})


\bibliography{ref.bib}


\newpage
\begin{appendix}

\section{Tree algorithm} \label{app:tree}
The tree algorithm used in the applications discussed in this paper 
is going to be explained following the ideas presented in 
\cite{Seibold+Zeileis+Hothorn:2017}. For notational simplicity, the testing 
and splitting procedure is described for the root node with observations 
$\{y_i\}_{i = 1,\ldots,n}$, $n \in \mathbb{N}$. In each child node the 
corresponding subset of observations depends on the assignment made 
by the foregoing split.

\begin{itemize}
\item A distributional model $\mathcal{D}(Y, \bm{\theta})$ is fit to the set of 
observations $\{y_i\}_{i = 1,\ldots,n}$ by estimating the distribution 
parameter $\bm{\hat \theta}$. This parameter can be a multidimensional 
vector $(\hat{\theta}_1, \ldots, \hat{\theta}_K)$, $K \in \mathbb{N}$, 
depending on the type of distribution. The estimation is done by 
maximizing the log-likelihood function $\ell(\bm{\theta}; Y)$, i.e.
$$
\hat{\theta} = \argmax_{\theta \in \Theta} \sum_{i=1}^n \ell(\theta; y_i)
$$
which is equal to solving
$$
\sum_{i=1}^n s(\theta, y_i) = \sum_{i=1}^n \frac{\partial \ell}{\partial \theta}(\theta; y_i) = 0
$$
for $\theta$.

\item Calculate score values 
$$
s(\hat{\theta}, y_i) = 
\begin{pmatrix} 
s(\hat{\theta}, y_1) \\
s(\hat{\theta}, y_2) \\
\vdots \\
s(\hat{\theta}, y_n)
\end{pmatrix} =
\begin{pmatrix} 
s(\hat{\theta}, y_1)_1 & s(\hat{\theta}, y_1)_2 & \ldots & s(\hat{\theta}, y_1)_K\\
s(\hat{\theta}, y_2)_1 & s(\hat{\theta}, y_2)_2 & \ldots & s(\hat{\theta}, y_2)_K\\
\vdots & \vdots & \ddots & \vdots \\
s(\hat{\theta}, y_n)_1 & s(\hat{\theta}, y_n)_2 & \ldots & s(\hat{\theta}, y_n)_K
\end{pmatrix}
$$

\item Test for independence between each column of scores $\left(s(\hat{\theta}, y_i)_k\right)_{i=1,\ldots,n}$ for $k\in\{1,\ldots,K\}$, and split variable $Z_j \in \{Z_1, \ldots, Z_m\}$.  
\begin{align*}
H_0^{1,j}:  \left(s(\hat{\theta}, y_i)_1\right)_{i=1,\ldots,n} \qquad \bot \qquad Z_j \\
H_0^{2,j}:  \left(s(\hat{\theta}, y_i)_2\right)_{i=1,\ldots,n} \qquad \bot \qquad Z_j \\
\vdots \hspace{3cm}\\
H_0^{K,j}:  \left(s(\hat{\theta}, y_i)_K\right)_{i=1,\ldots,n} \qquad \bot \qquad Z_j 
\end{align*}

To test these hypotheses permutation testing with the multivariate linear statistic
$$
T_{j} = \sum_{i=1}^n g_j(z_{ji}) \cdot s(\hat{\theta}, y_i)
$$
is applied. The type of the transformation function $g_j$ depends on 
the type of the split variable $Z_j$. If $Z_j$ is numeric then $g_j$ 
is simply the identity function $g_j(z_{ji}) = z_{ji}$. If $Z_j$ is 
a categorical variable with $L$ categories then 
$g_j(z_{ji}) = e_L(z_{ji}) = (\I(z_{ji} = 1), \ldots, \I(z_{ji} = L))$ 
such that $g_j$ is a unit vector where the element corresponding to 
the value of $z_{ji}$ is $1$. Observations with missing values are 
excluded from the sums.

With the conditional expectation $\mu_{j}$ and the covariance 
$\Sigma_{j}$ as derived by \cite{Strasser+Weber:1999} the test 
statistic can be standardized. For a numeric split variable this 
results in the Pearson correlation coefficient
$$
c(t_{j},\mu_{j},\Sigma_{j}) = \left|\frac{t_{j} - \mu_{j}}{\sqrt{\Sigma_{j}}}\right|.
$$
Otherwise the standardized form is
$$
c(t_{j},\mu_{jk},\Sigma_{j}) = \max_{l=1,\ldots,L}\left|\frac{(t_{j} - \mu_{j})_l}{\sqrt{(\Sigma_{j})_{ll}}}\right|.
$$
The smaller the p-value corresponding to the standardized test 
statistic $c(t_{j},\mu_{j},\Sigma_{j})$ is the stronger the discrepancy 
from the assumption of independence between the scores and the split 
variable $Z_j$.

\item After Bonferroni adjusting the p-values check whether any of 
the resulting p-values is beneath the selected significance level. 
If so, choose the partitioning variable $Z_{j^\ast}$ with the lowest 
p-value to the scores corresponding to the distribution parameters 
relevant for the split.

\item Choose the breakpoint that leads to the highest discrepancy 
between score functions in the two resulting subgroups. This 
difference can be measured by the linear statistic
$$
T_{j^{\ast}}^r = \sum_{i \in \mathcal{B}_{1r}} s(\hat{\theta}, y_i)
$$
where $\mathcal{B}_{1r}$ is the first of the two new subgroups that 
are defined by splitting in split point $r$ of variable $Z_{j^{\ast}}$. 
The split point is then chosen as follows:
$$
r^{\ast} = \argmin_{r} c(t_{j^{\ast}}^r,\mu_{j^{\ast}}^r,\Sigma_{j^{\ast}}^r).
$$

\item Repeat the testing and splitting procedure in each of the resulting 
subgroups until some stopping criterion is reached. This criterion 
can for example be a minimal number of observations in a node or a 
minimal p-value for the statistical tests. 

\end{itemize}

This permutation test based tree algorithm is presented in 
\cite{Hothorn+Hornik+Zeileis:2006} as the ctree algorithm. 
A different framework to build a tree is provided by the MOB algorithm 
which is based on M-fluctuation tests (\citealt{Zeileis+Hothorn+Hornik:2008}).


\section{Probability integral transform (PIT) histograms}
Probability integral transform (PIT) histograms 
\cite{Gneiting+Balabdaoui+Raftery:2007} are a commonly used tool to 
assess the predictive power of distributional forecasting methods, 
particularly in the field of meteorological forecasting.

Let $x$ be an observation with true distribution function $G$ and 
predicted distribution function $F$. Then the corresponding probability 
integral transform (PIT) value is $p = F(x)$. In the ideal case of a 
perfect prediction and $F$ being continuous $p$ follows a uniform distribution. 
This uniformity can be analysed applying a histogram. The closer the bars 
of a PIT histogram are to the value 1, the closer the distribution of $p$ 
is to a uniform distribution and therefore the better the predicted 
distribution models the true distribution.

The PIT histograms in Figure~\ref{pit_oosample} correspond to the forecasts 
presented in Section~\ref{Application for one station}. Daily observations 
of total precipitation from station Axams are considered and predictions 
are made for the 24th of July 2009, 2010, 2011 and 2012, based on data 
from the previous 24 years.


\begin{figure}[t!]
\centering
%\begin{subfigure}{0.8\textwidth}
<<rain_axams_pit_oosample, fig=TRUE, echo=FALSE, width = 6.5>>=
    
# out of sample
set.seed(4)

par(mfrow = c(2,2))
pithist(pit_df_t, nsim = 1000, breaks = seq(0, 1, length.out = 9), main = "Distributional forest", ylim = c(0,1.5))
pithist(pit_ml_t, nsim = 1000, breaks = seq(0, 1, length.out = 9), main = "EMOS", ylim = c(0,1.5))
pithist(pit_g_t, nsim = 1000, breaks = seq(0, 1, length.out = 9), main = "Prespecified GAMLSS", ylim = c(0,1.5))
pithist(pit_gb_t, nsim = 1000, breaks = seq(0, 1, length.out = 9), main = "Boosted GAMLSS", ylim = c(0,1.5))
      
@
%\end{subfigure}
\caption{\label{pit_oosample}Out-of-sample PIT histograms (2009--2012) for station Axams and models learned on data from 1985--2008.}
\end{figure}

\end{appendix}


\end{document}
