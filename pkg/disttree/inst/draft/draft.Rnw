\documentclass[nojss]{jss}

%\VignetteIndexEntry{disttree: Distributional Regression Forests for Probabilistic Precipitation Forecasting in Complex Terrain}
%\VignetteDepends{partykit, gamlss.dist}
%\VignetteKeywords{regression trees, random forests, distributional regression, recursive partitioning}
%\VignettePackage{disttree}

%% packages
\usepackage{amstext}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{thumbpdf,lmodern}
%\usepackage{rotating}
%\usepackage{caption}
%\captionsetup{format=hang}
%\usepackage{subcaption}
\usepackage{Sweave}
%\usepackage{enumitem}
%\usepackage{graphicx}
\usepackage{array, makecell}
\usepackage{tikz}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{positioning,shapes,arrows,decorations.pathreplacing,calc,automata,mindmap}
%\usepackage[pagebackref=true]{hyperref}
%\usepackage{doi}

%% commands
\newcommand{\fixme}[1]{\emph{\marginpar{FIXME} (#1)}}
\renewcommand{\Prob}{\mathbb{P} }
\renewcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\Var}{\mathbb{V}}
\newcommand{\R}{\mathbb{R} }
\newcommand{\N}{\mathbb{N} }
\newcommand{\C}{\mathbb{C} }
\newcommand{\argmin}{\operatorname{argmin}\displaylimits}
\newcommand{\argmax}{\operatorname{argmax}\displaylimits}
\newcommand{\LS}{\mathcal{L}_n}
\newcommand{\TS}{\mathcal{T}_n}
\newcommand{\LSc}{\mathcal{L}_{\text{comb},n}}
\newcommand{\LSbc}{\mathcal{L}^*_{\text{comb},n}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\yn}{y_{\text{new}}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\sX}{\mathcal{X}}
\newcommand{\sY}{\mathcal{Y}}
\newcommand{\T}{\mathbf{T}}
\newcommand{\x}{\mathbf{x}}
\renewcommand{\a}{\mathbf{a}}
\newcommand{\xn}{\mathbf{x}_{\text{new}}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\ws}{\mathbf{w}_\cdot}
\renewcommand{\t}{\mathbf{t}}
\newcommand{\M}{\mathbf{M}}
\renewcommand{\vec}{\text{vec}}
\newcommand{\B}{\mathbf{B}}
\newcommand{\K}{\mathbf{K}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\cellx}{\pi_n[\x]}
\newcommand{\partn}{\pi_n(\mathcal{L}_n)}
\newcommand{\err}{\text{Err}}
\newcommand{\ea}{\widehat{\text{Err}}^{(a)}}
\newcommand{\ecv}{\widehat{\text{Err}}^{(cv1)}}
\newcommand{\ecvten}{\widehat{\text{Err}}^{(cv10)}}
\newcommand{\eone}{\widehat{\text{Err}}^{(1)}}
\newcommand{\eplus}{\widehat{\text{Err}}^{(.632+)}}
\newcommand{\eoob}{\widehat{\text{Err}}^{(oob)}}
\newcommand{\bft}{\mathbf{t}}

\hyphenation{Qua-dra-tic}

%% colors
\definecolor{HighlightOrange}{rgb}{0.9490196,0.5725490,0.0000000}
\definecolor{HighlightBlue}{rgb}{0.4784314,0.7490196,0.9803922}



\SweaveOpts{engine=R, eps=FALSE, keep.source=TRUE}
<<preliminaries, echo=FALSE, results=hide>>=
options(prompt = "R> ", continue = "+  ", useFancyQuotes = FALSE, width = 70)
library("disttree")
library("Formula")  # FIX ME: should be imported in disttree
library("gamlss")
library("crch")
library("gamlss.cens")
gen.cens(NO, type = "left")
library("mgcv")
library("countreg")

## HCL palette
pal <- hcl(c(10, 128, 260, 290, 50), 100, 50)
names(pal) <- c("forest", "tree", "gamlss", "gamboostLSS", "EMOS")

pallight <- hcl(c(10, 128, 260, 290, 70), 100, 50, alpha = 0.25)
names(pallight) <- c("forest", "tree", "gamlss", "gamboostLSS", "EMOS")

transpgrey <- rgb(0.190,0.190,0.190, alpha = 0.2)




# plot RMSE
plot_rmse <- function(simres, type = c("exp", "par"), ylim = NULL, legend = TRUE){
  
  if(type == "exp"){
    rmse <- NULL
    colnames <- NULL
    col <- NULL
    legendnames <- NULL
    if("av.rmse.exp.true.dt" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.dt"], simres$resmat[,"av.rmse.exp.obs.dt"])
      colnames <- c(colnames, "dt.true", "dt.obs")
      col <- c(col, pal["tree"])
      legendnames <- c(legendnames, "tree")
    }
    if("av.rmse.exp.true.df" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.df"], simres$resmat[,"av.rmse.exp.obs.df"])
      colnames <- c(colnames, "df.true", "df.obs")
      col <- c(col, pal["forest"])
      legendnames <- c(legendnames, "distributional forest")
    }
    if("av.rmse.exp.true.g" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.g"], simres$resmat[,"av.rmse.exp.obs.g"])
      colnames <- c(colnames, "g.true", "g.obs")
      col <- c(col, pal["gamlss"])
      legendnames <- c(legendnames, "gamlss")
    }
    if("av.rmse.exp.true.b" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.b"], simres$resmat[,"av.rmse.exp.obs.b"])
      colnames <- c(colnames, "b.true", "b.obs")
      col <- c(col, pal["bamlss"])
      legendnames <- c(legendnames, "bamlss")
    }
    if("av.rmse.exp.true.gb" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.gb"], simres$resmat[,"av.rmse.exp.obs.gb"])
      colnames <- c(colnames, "gb.true", "gb.obs")
      col <- c(col, pal["gamboostLSS"])
      legendnames <- c(legendnames, "gamboostLSS")
    }
    if("av.rmse.exp.true.rf" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.rf"], simres$resmat[,"av.rmse.exp.obs.rf"])
      colnames <- c(colnames, "rf.true", "rf.obs")
      col <- c(col, pal["randomForest"])
      legendnames <- c(legendnames, "randomForest")
    }
    if("av.rmse.exp.true.cf" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.cf"], simres$resmat[,"av.rmse.exp.obs.cf"])
      colnames <- c(colnames, "cf.true","cf.obs")
      col <- c(col, pal["cforest"])
      legendnames <- c(legendnames, "cforest")
    }
    
    colnames(rmse) <- colnames
    if(is.null(ylim)) ylim <- c(min(na.omit(rmse)), max(na.omit(rmse)))
    
    plot(x = simres$x.axis, y = rmse[,1], type = "l", col = col[1], ylim = ylim,
         xlab = expression(kappa), ylab = "RMSE")
    lines(x = simres$x.axis, y = rmse[,2], type = "l", lty = 2, col = col[1])
    
    for(i in 1:(length(col)-1)){
      lines(x = simres$x.axis, y = rmse[,(2*i+1)], type = "l", col = col[i+1])
      lines(x = simres$x.axis, y = rmse[,(2*i+2)], type = "l", lty = 2, col = col[i+1])
    }
    if(legend) legend('topleft', legendnames, 
                      col = col, lty = 1, cex = 0.7)
  } 
  
  if(type == "par"){
    
    rmse <- NULL
    colnames <- NULL
    col <- NULL
    legendnames <- NULL
    if("av.rmse.exp.true.dt" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.dt"], simres$resmat[,"av.rmse.sigma.dt"])
      colnames <- c(colnames, "dt.true", "dt.obs")
      col <- c(col, pal["tree"])
      legendnames <- c(legendnames, "tree")
    }
    if("av.rmse.exp.true.df" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.df"], simres$resmat[,"av.rmse.sigma.df"])
      colnames <- c(colnames, "df.true", "df.obs")
      col <- c(col, pal["forest"])
      legendnames <- c(legendnames, "distributional forest")
    }
    if("av.rmse.exp.true.g" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.g"], simres$resmat[,"av.rmse.sigma.g"])
      colnames <- c(colnames, "g.true", "g.obs")
      col <- c(col, pal["gamlss"])
      legendnames <- c(legendnames, "gamlss")
    }
    if("av.rmse.exp.true.b" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.b"], simres$resmat[,"av.rmse.sigma.b"])
      colnames <- c(colnames, "b.true", "b.obs")
      col <- c(col, pal["bamlss"])
      legendnames <- c(legendnames, "bamlss")
    }
    if("av.rmse.exp.true.gb" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.gb"], simres$resmat[,"av.rmse.sigma.gb"])
      colnames <- c(colnames, "gb.true", "gb.obs")
      col <- c(col, pal["gamboostLSS"])
      legendnames <- c(legendnames, "gamboostLSS")
    }
    if("av.rmse.exp.true.rf" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.rf"], simres$resmat[,"av.rmse.sigma.rf"])
      colnames <- c(colnames, "rf.true", "rf.obs")
      col <- c(col, pal["randomForest"])
      legendnames <- c(legendnames, "randomForest")
    }
    if("av.rmse.exp.true.cf" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.cf"], simres$resmat[,"av.rmse.sigma.cf"])
      colnames <- c(colnames, "cf.true","cf.obs")
      col <- c(col, pal["cforest"])
      legendnames <- c(legendnames, "cforest")
    }
    
    colnames(rmse) <- colnames
    if(is.null(ylim)) ylim <- c(min(na.omit(rmse)), max(na.omit(rmse)))
    
    plot(x = simres$x.axis, y = rmse[,1], type = "l", col = col[1], ylim = ylim,
         xlab = expression(kappa), ylab = "RMSE")
    lines(x = simres$x.axis, y = rmse[,2], type = "l", lty = 2, col = col[1])
    
    for(i in 1:(length(col)-1)){
      lines(x = simres$x.axis, y = rmse[,(2*i+1)], type = "l", col = col[i+1])
      lines(x = simres$x.axis, y = rmse[,(2*i+2)], type = "l", lty = 2, col = col[i+1])
    }
    if(legend) legend('topleft', legendnames, 
                      col = col, lty = 1, cex = 0.7)
  }
}

# plot loglikelihood
plot_ll <- function(simres, ylim = NULL, legend = TRUE){
  
  ll <- NULL
  colnames <- NULL
  col <- NULL
  legendnames <- NULL
  if("av.rmse.exp.true.dt" %in% colnames(simres$resmat)) {
    ll <- cbind(ll, simres$resmat[,"av.loglik.dt"])
    colnames <- c(colnames, "dt.ll")
    col <- c(col, pal["tree"])
    legendnames <- c(legendnames, "tree")
  }
  if("av.rmse.exp.true.df" %in% colnames(simres$resmat)) {
    ll <- cbind(ll, simres$resmat[,"av.loglik.df"])
    colnames <- c(colnames, "df.ll")
    col <- c(col, pal["forest"])
    legendnames <- c(legendnames, "distributional forest")
  }
  if("av.rmse.exp.true.g" %in% colnames(simres$resmat)) {
    ll <- cbind(ll, simres$resmat[,"av.loglik.g"])
    colnames <- c(colnames, "g.true")
    col <- c(col, pal["gamlss"])
    legendnames <- c(legendnames, "gamlss")
  }
  if("av.rmse.exp.true.b" %in% colnames(simres$resmat)) {
    ll <- cbind(ll, simres$resmat[,"av.loglik.b"])
    colnames <- c(colnames, "b.true")
    col <- c(col, pal["bamlss"])
    legendnames <- c(legendnames, "bamlss")
  }
  if("av.rmse.exp.true.gb" %in% colnames(simres$resmat)) {
    ll <- cbind(ll, simres$resmat[,"av.loglik.gb"])
    colnames <- c(colnames, "gb.true")
    col <- c(col, pal["gamboostLSS"])
    legendnames <- c(legendnames, "gamboostLSS")
  }
  if("av.rmse.exp.true.rf" %in% colnames(simres$resmat)) {
    ll <- cbind(ll, simres$resmat[,"av.loglik.rf"])
    colnames <- c(colnames, "rf.true")
    col <- c(col, pal["randomForest"])
    legendnames <- c(legendnames, "randomForest")
  }
  if("av.rmse.exp.true.cf" %in% colnames(simres$resmat)) {
    ll <- cbind(ll, simres$resmat[,"av.loglik.cf"])
    colnames <- c(colnames, "cf.true")
    col <- c(col, pal["cforest"])
    legendnames <- c(legendnames, "cforest")
  }
  
  colnames(ll) <- colnames
  if(is.null(ylim)) ylim <- c(min(na.omit(ll)), max(na.omit(ll)))
  
  plot(x = simres$x.axis, y = ll[,1], type = "l", col = col[1], ylim = ylim,
       xlab = expression(kappa), ylab = "log-likelihood")
  
  for(i in 2:length(col)){
    lines(x = simres$x.axis, y = ll[,i], type = "l", col = col[i])
  }
  if(legend) legend('topleft', legendnames, 
                    col = col, lty = 1, cex = 0.7)
}

# plot crpse
plot_crps <- function(simres, ylim = NULL, legend = TRUE){
  
  crps <- NULL
  colnames <- NULL
  col <- NULL
  legendnames <- NULL
  if("av.rmse.exp.true.dt" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.dt"])
    colnames <- c(colnames, "dt.crps")
    col <- c(col, pal["tree"])
    legendnames <- c(legendnames, "tree")
  }
  if("av.rmse.exp.true.df" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.df"])
    colnames <- c(colnames, "df.crps")
    col <- c(col, pal["forest"])
    legendnames <- c(legendnames, "distributional forest")
  }
  if("av.rmse.exp.true.g" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.g"])
    colnames <- c(colnames, "g.true")
    col <- c(col, pal["gamlss"])
    legendnames <- c(legendnames, "gamlss")
  }
  if("av.rmse.exp.true.b" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.b"])
    colnames <- c(colnames, "b.true")
    col <- c(col, pal["bamlss"])
    legendnames <- c(legendnames, "bamlss")
  }
  if("av.rmse.exp.true.gb" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.gb"])
    colnames <- c(colnames, "gb.true")
    col <- c(col, pal["gamboostLSS"])
    legendnames <- c(legendnames, "gamboostLSS")
  }
  if("av.rmse.exp.true.rf" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.rf"])
    colnames <- c(colnames, "rf.true")
    col <- c(col, pal["randomForest"])
    legendnames <- c(legendnames, "randomForest")
  }
  if("av.rmse.exp.true.cf" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.cf"])
    colnames <- c(colnames, "cf.true")
    col <- c(col, pal["cforest"])
    legendnames <- c(legendnames, "cforest")
  }
  
  colnames(crps) <- colnames
  if(is.null(ylim)) ylim <- c(min(na.omit(crps)), max(na.omit(crps)))
  
  plot(x = simres$x.axis, y = crps[,1], type = "l", col = col[1], ylim = ylim,
       xlab = expression(kappa), ylab = "CRPS")
  
  for(i in 2:length(col)){
    lines(x = simres$x.axis, y = crps[,i], type = "l", col = col[i])
  }
  if(legend) legend('topright', legendnames, 
                    col = col, lty = 1, cex = 1, bty = "n")
}
@



\title{Distributional Regression Forests for Probabilistic Precipitation Forecasting in Complex Terrain}
\Shorttitle{Distributional Regression Forests for Probabilistic Precipitation Forecasting}

\author{Lisa Schlosser\\Universit\"at Innsbruck
   \And Torsten Hothorn\\Universit\"at Z\"urich
   \And Reto Stauffer\\Universit\"at Innsbruck
   \And Achim Zeileis\\Universit\"at Innsbruck}
\Plainauthor{Lisa Schlosser, Torsten Hothorn, Reto Stauffer, Achim Zeileis}

\Abstract{
To obtain a probabilistic model for a dependent variable based on some set of
explanatory variables, a distributional approach is often adopted where the
parameter(s) of the distribution are linked to regressors. In many classical
models this often only captures the location/expectation of the distribution but
over the last decade there has been increasing interest in distributional
regression approaches modeling all parameters including location, scale, and
shape. Notably, so-called non-homogenous Gaussian regression models both
mean and variance of a Gaussian response variable and is particularly
popular in weather forecasting and, more generally, the GAMLSS framework
allows to establish generalized additive models for location, scale, and shape
with smooth linear or nonlinear effects.
%
However, in situations where variable selection is required and/or there are
non-smooth dependencies or interactions (especially unknown or of high-order),
it is challenging to establish a good GAMLSS. A natural alternative in these
situations would be the application of regression trees or random forests but,
so far, no general distributional framework is available for these. Therefore,
a framework for distributional trees and forests is proposed that blends
regression trees and random forests with classical distributions from the GAMLSS
framework as well as their censored or truncated counterparts.
%
To illustrate these novel approaches in practice, they are employed to obtain
probabilistic precipitation forecasts at numerous locations in a mountainous
region (Tyrol, Austria) based on a very large number of numerical weather
prediction quantities. Comparisons with GAMLSS using either a specification
based on prior meteorological knowledge or a computationally more demanding
boosting approach show that distributional random forests automatically
select variables and interactions, performing on par or often even better than
the GAMLSS approaches.
}
\Keywords{parametric models, regression trees, random forests, recursive partitioning, probabilistic forecasting, distributional regression}

\Address{
  Lisa Schlosser, Reto Stauffer, Achim Zeileis \\
  Universit\"at Innsbruck \\
  Department of Statistics \\
  Faculty of Economics and Statistics \\
  Universit\"atsstr.~15 \\
  6020 Innsbruck, Austria \\
  E-mail: \email{Lisa.Schlosser@uibk.ac.at}, \email{Reto.Stauffer@uibk.ac.at},\\
  \email{Achim.Zeileis@R-project.org} \\
  URL: \url{https://www.uibk.ac.at/statistics/personal/schlosser-lisa/},\\
  \phantom{URL: }\url{https://retostauffer.org/},\\
  \phantom{URL: }\url{https://eeecon.uibk.ac.at/~zeileis/}\\
  
  Torsten Hothorn\\
  Universit\"at Z\"urich \\
  Institut f\"ur Epidemiologie, Biostatistik und Pr\"avention \\
  Hirschengraben 84\\
  CH-8001 Z\"urich, Switzerland \\
  E-mail: \email{Torsten.Hothorn@R-project.org}\\
  URL: \url{http://user.math.uzh.ch/hothorn/}\\
  
}

\begin{document}
\SweaveOpts{concordance=TRUE}


\SweaveOpts{eps=FALSE, keep.source=TRUE, eval = TRUE}


\SweaveOpts{eval = TRUE}
\section{Introduction}

In regression analysis a wide range of models has been developed to describe the
relationship between a response variable and a set of covariates. The classical
model is the linear model (LM) where the conditional mean of the response
is modeled through a linear function of the covariates (see the left panel of
Figure~\ref{Devel_parmod} for a schematic illustration). Over the last decades
this has been extended in various directions including:

\pagebreak

\begin{itemize}
  \item \emph{Generalized linear models} (GLMs, \citealp{Nelder+Wedderburn:1972})
    encompassing an additional nonlinear link function for the conditional mean.
  \item \emph{Generalized additive models} (GAMs, \citealp{Hastie+Tibshirani:1986})
    allowing for smooth nonlinear effects in the covariates
    (Figure~\ref{Devel_parmod}, middle).
  \item \emph{Generalized additive models for location, scale, and shape}
    (GAMLSS, \citealt{Rigby+Stasinopoulos:2005}) adopting a probabilistic modeling
    approach. In GAMLSS, each parameter of a statistical distribution can depend
    on an additive predictor of the covariates comprising linear and/or
    smooth nonlinear terms (Figure~\ref{Devel_parmod}, right).
\end{itemize}
Thus, the above-mentioned models provide a broad toolbox for capturing different
aspects of the response (mean only vs.\ full distribution) and different types
of dependencies on the covariates (linear vs.\ nonlinear additive terms).

\begin{figure}[t!]
\begin{tikzpicture}
\node (a) at (0,0){
\minipage{0.29\textwidth}
\centering
<<motivation_GLM, echo=FALSE, results=hide>>=
nobs <- 200
## GLM
set.seed(7)
x <- c(1:nobs)/nobs
ytrue <- 1+x*1.5
y <- ytrue + rnorm(nobs,0,0.3)
@

<<plot_motivation_GLM, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(y=y , x=x, xaxt="n", yaxt="n", ann=FALSE, col = "slategray", pch = 19)
box(lwd=5)
lines(x = x, y = ytrue, col = pal["forest"], lwd = 7, main = "")
@
\endminipage};
\node (b) at (5,0) 
{
\minipage{0.29\textwidth}
\centering
<<motivation_GAM, echo=FALSE, results=hide>>=
## GAM
set.seed(7)
x <- c(1:nobs)/nobs
x <- 2*(x-0.5)
ytrue <- x^3 
y <- ytrue + rnorm(nobs,0,0.3)
@

<<plot_motivation_GAM, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(y=y , x=x, xaxt="n", yaxt="n", ann=FALSE, col = "slategray", pch = 19)
box(lwd=5)
lines(x = x, y = ytrue, col = pal["forest"], lwd = 7, main = "")
@
\endminipage};
\node (c) at (10,0) 
{
\minipage{0.29\textwidth}
\centering
<<motivation_GAMLSS, echo=FALSE, results=hide>>=
## GAMLSS
set.seed(7)
x <- c(1:nobs)/nobs
x <- 2*(x-0.5)
ytrue <- x^3
var <- exp(-(2*x)^2)/2
y <- ytrue + rnorm(nobs, 0, 0.1 + var)
@

<<plot_motivation_GAMLSS, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(x, y, xaxt = "n", yaxt = "n", ann = FALSE, type = "n")
polygon(c(x, rev(x)), c(ytrue + 0.1 + var, rev(ytrue - 0.1 - var)),
  col = pallight["forest"], border = "transparent")
lines(x, ytrue, col = pal["forest"], lwd=7)
points(x, y, col = "slategray", pch = 19)
box(lwd = 5)
@
\endminipage};
\node (d) at (0,-2) 
{
\minipage{0.29\textwidth}
\centering
\vspace{0.0cm}
LM, GLM  
\endminipage};
\node (e) at (5,-2) 
{
\minipage{0.29\textwidth}
\centering
\vspace{0.0cm}
GAM 
\endminipage};
\node at (10,-2)
{
\minipage{0.28\textwidth}
\centering
\vspace{0.0cm}
GAMLSS 
\endminipage};
\draw[-{>[scale=4, length=2, width=3]},line width=0.4pt](2.1,0)--(2.9,0);
\draw[-{>[scale=4, length=2, width=3]},line width=0.4pt](7.1,0)--(7.9,0);
\end{tikzpicture}
\caption{\label{Devel_parmod}Parametric modeling developments. (Generalized) linear models (left), generalized additive models (middle), generalized additive models for location, scale, and shape (right).}
\end{figure}


While in many applications conditional mean regression models have been receiving
the most attention, there has been a paradigm shift over the last decade towards
distributional regression models. An important reason for this is that in many
fields forecasts of the mean are not the only (or not even the main) concern but
instead there is an increasing interest in probabilistic forecasts. Quantities of
interest typically include exceedence probabilities for certain thresholds of the
response or quantiles of the response distribution. As an example, consider
weather forecasting where there is less interest in the mean amount of
precipitation on the next day. Instead, the probability of rain vs.\ no rain
is typically more relevant or, in some situations, a prediction interval of
expected precipitation (say from the expected 10\% to 90\% quantile). Similar
considerations apply for other meteorological quantities and hence attention
in the weather forecasting literature has been shifting from classical linear
models \citep{Glahn+Lowry:1972} towards probabilistic models such as the
non-homogeneous Gaussian regression (NGR) of \cite{Gneiting+Raftery+Westveld:2005}.
The NGR typically describes the mean of some meteorological response variable through
the average of the corresponding quantity from an ensemble of physically-based
numerical weather predictions (NWPs). Similarly, the variance of the response
is captured through the variance of the ensemble of NWP outputs. Thus, the NGR
considers both the mean as well as the uncertainty of the NWP model outputs
to obtain probabilistic forecasts calibrated to a particular site.

%% late: introduce MOS and EMOS? more details about precipitation forecasting
%In ensemble model output statistics (EMOS, \cite{Gneiting+Raftery+Westveld:2005}) this idea is taken one step further. Usually acting on the assumption of a normally distributed response variable this model allows for both parameters, the mean and the variance, to depend linearly on covariates provided by NWPs. However, the restriction to model only linear effects has risen the interest in applying more flexible models such as offered by the GAMLSS framework.

\begin{figure}[t!]
\begin{tikzpicture}
\node (a) at (0,0){
\minipage{0.29\textwidth}
\centering
<<motivation_regtree, echo=FALSE, results=hide>>=
## Reg. Tree
set.seed(7)
kappa <- 12
x <- c(1:nobs)/nobs
ytrue <- ytree <- yforest <- numeric(length = length(x))
for(i in 1:nobs) ytrue[i] <- if(x[i]<1/3) 0.5 else 1+(1-plogis(kappa*(2*(x[i]-0.2)-1)))
y <- ytrue + rnorm(nobs,0,0.3)
for(i in 1:nobs) ytree[i] <- if(x[i]<1/3) 0.5 else {if(x[i]<2/3) 2 else 1}
@

<<plot_motivation_regtree, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(x = x, y = y, xaxt="n", yaxt="n", ann=FALSE, col = "slategray", pch = 19)
box(lwd=5)
#lines(x = x, y = ytrue, col = "grey", lwd=5, main = "")
lines(x = x, y = ytree, col = pal["forest"], lwd=7)
@
\endminipage};
\node (b) at (5,0) 
{
\minipage{0.29\textwidth}
\centering
<<motivation_randforest, echo=FALSE, results=hide>>=
## Random Forest
for(i in 1:nobs) yforest[i] <- if(x[i]<0.27) 0.5 else { if(x[i]<0.39) 0.5 + 1.5*(plogis((x[i]-0.33)/6*700)) else 1+(1-plogis(kappa*(2*(x[i]-0.2)-1)))}
@

<<plot_motivation_randforest, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(x = x, y = y, xaxt="n", yaxt="n", ann=FALSE, col = "slategray", pch = 19)
box(lwd=5)
#lines(x = x, y = ytrue, col = "grey", lwd=5, main = "")
lines(x = x, y = yforest, col = pal["forest"], lwd=7, main = "")
@
\endminipage};
\node (c) at (10,0) 
{
\minipage{0.29\textwidth}
\centering
<<motivation_distforest, echo=FALSE, results=hide>>=
## distforest
set.seed(4723)
for(i in 1:nobs) yforest[i] <- if(x[i]<0.27) 0.5 else { if(x[i]<0.39) 0.5 + 1.5*(plogis((x[i]-0.33)/6*700)) else 1+(1-plogis(kappa*(2*(x[i]-0.2)-1)))}
xt <- 2*(x-0.5)
var <- exp(-(2*xt)^4)/2
y <- ytrue + rnorm(nobs, 0, 0.1 + var)
x <- c(1:nobs)/nobs
@

<<plot_motivation_distforest, fig=TRUE, echo=FALSE>>=
par(mar = c(2, 0, 2, 0))
plot(x, y, xaxt = "n", yaxt = "n", ann = FALSE, type = "n")
usr <- par("usr")
rect(usr[1], usr[3], usr[2], usr[4], col = gray(0.94), border = "transparent")
polygon(c(x, rev(x)), c(ytrue + 0.1 + var, rev(ytrue - 0.1 - var)),
  col = pallight["forest"], border = "transparent")
lines(x = x, y = yforest, col = pal["forest"], lwd=7, main = "")
points(x, y, col = "slategray", pch = 19)
box(lwd=8)
@
\endminipage};
\node (d) at (0,-2) 
{
\minipage{0.29\textwidth}
\centering
\vspace{0.0cm}
Regression Tree 
\endminipage};
\node (e) at (5,-2) 
{
\minipage{0.29\textwidth}
\centering
\vspace{0.0cm}
Random Forest 
\endminipage};
\node at (10,-2)
{
\minipage{0.28\textwidth}
\centering
\vspace{0.0cm}
Distributional Forest 
\endminipage};
\node (g) at (0,-5.4) 
{
\minipage{0.29\textwidth}
\vspace{0.2cm}
%\hspace{0.0cm}
\centering
<<motivation_disttree, echo=FALSE, results=hide>>=
## disttree
set.seed(74)
x <- c(1:nobs)/nobs
var <- ytree/3
y <- ytrue + rnorm(nobs,0,var)
@

<<plot_motivation_disttree, fig=TRUE, echo=FALSE>>=
par(mar = c(2, 0, 2, 0))
plot(x, y, xaxt="n", yaxt="n", ann=FALSE, type = "n")
usr <- par("usr")
rect(usr[1], usr[3], usr[2], usr[4], col = gray(0.94), border = "transparent")
lines(x = x, y = ytree, col = pal["forest"], lwd=7, main = "")
polygon(c(x, rev(x)), c(ytree + var, rev(ytree - var)),
  col = pallight["forest"], border = "transparent")
points(x, y, col = "slategray", pch = 19)
box(lwd=8)
@
\vspace{-0.2cm}%\\
Distributional Tree
\vspace{0.2cm}
\endminipage};
\draw[-{>[scale=4, length=2, width=3]},line width=0.4pt](2.1,0)--(2.9,0);
\draw[-{>[scale=4, length=2, width=3]},line width=0.9pt](7.1,0)--(7.9,0);
\draw[-{>[scale=4, length=2, width=3]},line width=0.9pt](0,-2.4)--(0,-3.2);
\draw[-{>[scale=4, length=2, width=3]},line width=0.9pt](2.2,-5.4)--(7.9,-2.5);
\end{tikzpicture}
\caption{\label{Devel_treeforest}Tree and forest developments.
Regression tree (top left), distributional tree (bottom left), random forest
(top middle), and distributional forest (top right).}
\end{figure}

In summary, the models discussed so far provide a broad and powerful toolset for
parametric distributional fits depending on a specified set of additive linear or
smooth nonlinear terms. A rather different approach to capturing the dependence on
covariates are tree-based models.
\begin{itemize}
  \item \emph{Regression trees} (\citealt{Breiman+Friedman+Stone:1984}) recursively
    split the data into more homogeneous subgroups and can thus capture abrupt shifts
    (Figure~\ref{Devel_treeforest}, top left) and approximate nonlinear functions.
    Furthermore, trees automatically carry out a forward selection of covariates and
    their interactions. 
  \item \emph{Random forests} (\citealt{Breiman:2001}) average the predictions of
    an ensemble of trees fitted to resampled versions of the learning data. This
    stabilizes the recursive partitions from individual trees and hence better
    approximates smooth functions (Figure~\ref{Devel_treeforest}, top middle)
\end{itemize}
While classical regression trees and random forests only model the mean of the response
we propose to follow the ideas from GAMLSS modeling -- as outlined
in Figure~\ref{Devel_parmod} -- and combine tree-based methods with parametric distributional models,
yielding two novel techniques:
\begin{itemize}
  \item \emph{Distributional trees} split the data into more homogeneous groups with
    respect to a parametric distribution, thus capturing changes in any distribution
    parameter like location, scale, or shape (Figure~\ref{Devel_treeforest}, bottom left)
  \item \emph{Distributional forests} utilize an ensemble of distributional trees
    for obtaining stabilized and smoothed parametric predictions (Figure~\ref{Devel_treeforest},
    top right).
\end{itemize}
In the following, particular focus is given to distributional forests as a method for obtaining
probabilistic forecasts by leveraging the strengths of random forests: the ability to
capture both smooth and abruptly changing functions along with simultaneous selection of
variables and possibly complex interactions. Thus, these properties make the method particularly
appealing in case of many covariates with unknown effects where it would be challenging
to specify a distributional regression model like GAMLSS.


<<echo=FALSE, results=hide>>=
#### Axams prediction 24 - 4
if(file.exists("rain_Axams_pred_24to4.rda")){
  #load("~/svn/partykit/pkg/disttree/inst/draft/rain_Axams_pred_24to4.rda")
  load("rain_Axams_pred_24to4.rda")
} else {
  #source("~/svn/partykit/pkg/disttree/inst/draft/rain_axams_pred_24to4.R")
  source("rain_axams_pred_24to4.R")
  library("gamlss.cens")
  gen.cens("NO", type = "left")
  res <- rain_axams_pred(seedconst = 7, ntree = 100,
                         forest_mtry = 27,   # if frac == TRUE: nvar/3 = 30
                         gamboost_cvr = TRUE,
                         frac = FALSE)
  
  save(res, file = "rain_Axams_pred_24to4.rda")
}

#### prepare data for plot of estimated density functions
# predictions for one day (in each of the four years) 
# (19th of July 2011 is missing)
pday <- 24  # 2 (hohe Beobachtung zu niedrig geschaetzt), 4, 15, evtl. auch 7, 8, 23 (eine 0-Beobachtung und 2 sehr aehnliche), 

pdays <- if(pday<19) c(pday, pday + 31, pday + 62, pday + 92) else c(pday, pday + 31, pday + 61, pday + 92)
pdf <- predict(res$df, newdata = res$testdata[pdays,], type = "parameter")
df_mu <- pdf$mu
df_sigma <- pdf$sigma
df_exp <- pnorm(df_mu/df_sigma) * (df_mu + df_sigma * (dnorm(df_mu/df_sigma) / pnorm(df_mu/df_sigma)))
cbind(df_exp, res$testdata[pdays,"robs"])


# plot predicted distributions together with observations
#set.seed(res$call$seedconst)
set.seed(7)
x <- c(0.01, sort(runif(500,0.01,8)))
y1 <- crch::dcnorm(x, mean = df_mu[1], sd = df_sigma[1], left = 0)
y2 <- crch::dcnorm(x, mean = df_mu[2], sd = df_sigma[2], left = 0)
y3 <- crch::dcnorm(x, mean = df_mu[3], sd = df_sigma[3], left = 0)
y4 <- crch::dcnorm(x, mean = df_mu[4], sd = df_sigma[4], left = 0)
dayending <- if(pday > 3) "th" else switch(pday, "1" = {dayending <- "st"}, "2" = {dayending <- "nd"}, "3" = {dayending <- "rd"})

# point mass (slightly shifted)
pm1 <- c(0.05, crch::dcnorm(-1, mean = df_mu[1], sd = df_sigma[1], left = 0))
pm2 <- c(0.01, crch::dcnorm(-1, mean = df_mu[2], sd = df_sigma[2], left = 0))
pm3 <- c(-0.03, crch::dcnorm(-1, mean = df_mu[3], sd = df_sigma[3], left = 0))
pm4 <- c(-0.07, crch::dcnorm(-1, mean = df_mu[4], sd = df_sigma[4], left = 0))

# predictions
pred1 <- c(res$testdata[pdays,"robs"][1], crch::dcnorm(res$testdata[pdays,"robs"][1], mean = df_mu[1], sd = df_sigma[1], left = 0))
pred2 <- c(res$testdata[pdays,"robs"][2], crch::dcnorm(res$testdata[pdays,"robs"][2], mean = df_mu[2], sd = df_sigma[2], left = 0))
pred3 <- c(res$testdata[pdays,"robs"][3], crch::dcnorm(res$testdata[pdays,"robs"][3], mean = df_mu[3], sd = df_sigma[3], left = 0))
pred4 <- c(res$testdata[pdays,"robs"][4], crch::dcnorm(res$testdata[pdays,"robs"][4], mean = df_mu[4], sd = df_sigma[4], left = 0))

#legendheight
lh1 <- crch::dcnorm(0.01, mean = df_mu[1], sd = df_sigma[1], left = 0)
lh2 <- crch::dcnorm(0.01, mean = df_mu[2], sd = df_sigma[2], left = 0)
lh3 <- crch::dcnorm(0.01, mean = df_mu[3], sd = df_sigma[3], left = 0)
lh4 <- crch::dcnorm(0.01, mean = df_mu[4], sd = df_sigma[4], left = 0)



## PIT histograms and qqr-plots

## get predicted parameter
# for learndata and testdata
{
  set.seed(7)
  ## disttree
  if(FALSE){
    # in sample
    pdt <- predict(res$dt, type = "parameter")
    dt_mu_l <- pdt$mu 
    dt_sigma_l <- pdt$sigma 
    pit_dt_l <- cbind(0, pnorm(res$learndata[,"robs"], mean = dt_mu_l, sd = dt_sigma_l))
    pit_dt_l[res$learndata[,"robs"]>0, 1] <- pit_dt_l[res$learndata[,"robs"]>0, 2]
  }  
  # out of sample
  pdt <- predict(res$dt, type = "parameter", newdata = res$testdata)
  dt_mu_t <- pdt$mu 
  dt_sigma_t <- pdt$sigma 
  pit_dt_t <- cbind(0, pnorm(res$testdata[,"robs"], mean = dt_mu_t, sd = dt_sigma_t))
  pit_dt_t[res$testdata[,"robs"]>0, 1] <- pit_dt_t[res$testdata[,"robs"]>0, 2]
  
  
  ## distforest
  if(FALSE){
    # in sample
    pdf <- predict(res$df, type = "parameter")
    df_mu_l <- pdf$mu
    df_sigma_l <- pdf$sigma
    pit_df_l <- cbind(0, pnorm(res$learndata[,"robs"], mean = df_mu_l, sd = df_sigma_l))
    pit_df_l[res$learndata[,"robs"]>0, 1] <- pit_df_l[res$learndata[,"robs"]>0, 2]
  }
  # out of sample
  pdf <- predict(res$df, type = "parameter", newdata = res$testdata)
  df_mu_t <- pdf$mu
  df_sigma_t <- pdf$sigma
  pit_df_t <- cbind(0, pnorm(res$testdata[,"robs"], mean = df_mu_t, sd = df_sigma_t))
  pit_df_t[res$testdata[,"robs"]>0, 1] <- pit_df_t[res$testdata[,"robs"]>0, 2]
  
  
  ## gamlss
  g_learndata <- res$learndata
  g_learndata$robs <- Surv(g_learndata$robs, g_learndata$robs>0, type="left")
  if(FALSE){
    # in sample
    g_mu_l <- predict(res$g, what = "mu", type = "response")
    g_sigma_l <- predict(res$g, what = "sigma", type = "response")
    pit_g_l <- cbind(0, pnorm(res$learndata[,"robs"], mean = g_mu_l, sd = g_sigma_l))
    pit_g_l[res$learndata[,"robs"]>0, 1] <- pit_g_l[res$learndata[,"robs"]>0, 2]
  }  
  # out of sample
  g_mu_t <- predict(res$g, what = "mu", type = "response", newdata = res$testdata)
  g_sigma_t <- predict(res$g, what = "sigma", type = "response", newdata = res$testdata)
  pit_g_t <- cbind(0, pnorm(res$testdata[,"robs"], mean = g_mu_t, sd = g_sigma_t))
  pit_g_t[res$testdata[,"robs"]>0, 1] <- pit_g_t[res$testdata[,"robs"]>0, 2]
  
  
  #gamboostLSS
  if(FALSE){
    #in sample
    pgb <- predict(res$gb, parameter = c("mu","sigma"), type = "response")
    gb_mu_l <- pgb$mu
    gb_sigma_l <- pgb$sigma
    pit_gb_l <- cbind(0, pnorm(res$learndata[,"robs"], mean = gb_mu_l, sd = gb_sigma_l))
    pit_gb_l[res$learndata[,"robs"]>0, 1] <- pit_gb_l[res$learndata[,"robs"]>0, 2]
  }  
  # out of sample
  pgb <- predict(res$gb, parameter = c("mu","sigma"), type = "response", newdata = res$testdata)
  gb_mu_t <- pgb$mu
  gb_sigma_t <- pgb$sigma
  pit_gb_t <- cbind(0, pnorm(res$testdata[,"robs"], mean = gb_mu_t, sd = gb_sigma_t))
  pit_gb_t[res$testdata[,"robs"]>0, 1] <- pit_gb_t[res$testdata[,"robs"]>0, 2]
  
  
  #EMOS
  if(FALSE){
    # in sample
    ml_mu_l <- predict(res$ml, type = "location")     # returns parameter on response scale
    ml_sigma_l <- predict(res$ml, type = "scale")
    pit_ml_l <- cbind(0, pnorm(res$learndata[,"robs"], mean = ml_mu_l, sd = ml_sigma_l))
    pit_ml_l[res$learndata[,"robs"]>0, 1] <- pit_ml_l[res$learndata[,"robs"]>0, 2]
  }  
  # out of sample
  ml_mu_t <- predict(res$ml, type = "location", newdata = res$testdata)     # returns parameter on response scale
  ml_sigma_t <- predict(res$ml, type = "scale", newdata = res$testdata)
  pit_ml_t <- cbind(0, pnorm(res$testdata[,"robs"], mean = ml_mu_t, sd = ml_sigma_t))
  pit_ml_t[res$testdata[,"robs"]>0, 1] <- pit_ml_t[res$testdata[,"robs"]>0, 2]
  
}


## Variable importance
if(file.exists("vimp_crps.rda")){
  #load("~/svn/partykit/pkg/disttree/inst/draft/vimp_crps.rda")
  load("vimp_crps.rda")
} else {
  
  # locally redefine logLik.distforest to use crps in varimp()
  logLik.distforest <- function(object, newdata = NULL, ...) {
    if(is.null(newdata)) {
      newdata <- object$data
    } 
    
    # predict parameter
    pdf <- predict(object, newdata = newdata, type = "parameter")
    df_mu <- pdf$mu
    df_sigma <- pdf$sigma
    
    # calculate CRPS
    crps <- mean(crps_cnorm(newdata$robs, location = df_mu, scale = df_sigma, lower = 0, upper = Inf), na.rm = TRUE)
  
    return(structure(crps, df = NA, class = "logLik"))
  }
  
  vimp_crps <- varimp(res$df, nperm = 1L)
  #vimp10_crps <- varimp(res$df, nperm = 10L)
  #vimp100_crps <- varimp(res$df, nperm = 100L)
  #vimp1000_crps <- varimp(res$df, nperm = 1000L)
  
  save(vimp_crps, file = "~/svn/partykit/pkg/disttree/inst/draft/vimp_crps.rda")
  #save(vimp10_crps, file = "~/svn/partykit/pkg/disttree/inst/draft/vimp10_crps.rda")
  #save(vimp100_crps, file = "~/svn/partykit/pkg/disttree/inst/draft/vimp100_crps.rda")
  #save(vimp1000_crps, file = "~/svn/partykit/pkg/disttree/inst/draft/vimp1000_crps.rda")
  
  rm(logLik.distforest) 
}
    
@


\begin{figure}[t!]
\centering
\setkeys{Gin}{width=0.8\textwidth}
<<rain_Axams_pred, fig=TRUE, echo=FALSE, width=6.5, height=4.5>>=
plot(x = x, y = y1, type = "l", col = "red", 
     main = paste0("July ", pday), ylab = "Density", 
     xlab = expression(Total~precipitation~"["~mm^(1/1.6)~"/"~"24h"~"]"),
     ylim = c(0,max(y1, y2, y3, y4, pm1, pm2, pm3, pm4) + 0.01),
     xlim = c(-1.5,8))
    
lines(x = x, y = y2, type = "l", col = "blue")
lines(x = x, y = y3, type = "l", col = "darkgreen")
lines(x = x, y = y4, type = "l", col = "purple")
legend("topright", c("Predicted distribution", "Point mass at censoring point", "Observation"),
       bty = "n", col = "black", lty = c(1, NA, NA), pch = c(NA, 19, 4), cex = 0.8)

# plot point mass
lines(x = c(pm1[1], pm1[1]), y = c(pm1[2], 0), col = "red", type = "l", lwd = 1)
lines(x = c(pm2[1], pm2[1]), y = c(pm2[2], 0), col = "blue", type = "l", lwd = 1)
lines(x = c(pm3[1], pm3[1]), y = c(pm3[2], 0), col = "darkgreen", type = "l", lwd = 1)
lines(x = c(pm4[1], pm4[1]), y = c(pm4[2], 0), col = "purple", type = "l", lwd = 1)

points(x = pm1[1], y = pm1[2], col = "red", pch = 19)
points(x = pm2[1], y = pm2[2], col = "blue", pch = 19)
points(x = pm3[1], y = pm3[2], col = "darkgreen", pch = 19)
points(x = pm4[1], y = pm4[2], col = "purple", pch = 19)


# plot predictions
points(x = pred1[1], y = pred1[2], col = "red", pch = 4)
points(x = pred2[1], y = pred2[2], col = "blue", pch = 4)
points(x = pred3[1], y = pred3[2], col = "darkgreen", pch = 4)
points(x = pred4[1], y = pred4[2], col = "purple", pch = 4)

lines(x = c(pred1[1], pred1[1]), y = c(pred1[2], 0), col = "darkgrey", type = "l", lty = 2)
lines(x = c(pred2[1], pred2[1]), y = c(pred2[2], 0), col = "darkgrey", type = "l", lty = 2)
lines(x = c(pred3[1], pred3[1]), y = c(pred3[2], 0), col = "darkgrey", type = "l", lty = 2)
lines(x = c(pred4[1], pred4[1]), y = c(pred4[2], 0), col = "darkgrey", type = "l", lty = 2)

# add labels
text(x = -0.8, y = lh1, labels = "2009", col = "red", cex = 0.8)
text(x = -0.8, y = lh2, labels = "2010", col = "blue", cex = 0.8)
text(x = -0.8, y = lh3, labels = "2011", col = "darkgreen", cex = 0.8)
text(x = -0.8, y = lh4, labels = "2012", col = "purple", cex = 0.8)
@
\caption{\label{axams_onestation}Predictions of total precipitation at station Axams for July 24 in 2009, 2010, 2011 and 2012 by a distributional forest learned on data from 1985--2008. Observations are left-censored at 0. The corresponding predicted point mass is shown at the censoring point (0).}
\end{figure}

In weather forecasting, these properties are appealing in mountainous regions
and complex terrain where small-scale effects are not well-resolved in the NWP
outputs. As these are computed at a coarser scale, there may be abrupt local
effects within a single NWP grid cell. To illustrate this in practice,
precipitation forecasts are obtained with distributional forests at 95
meteorological stations in a mountainous region in the Alps, covering mainly
Tyrol, Austria, and adjacent areas (see the map in Figure~\ref{map}).
More specifically, a zero-censored Gaussian
distribution is employed to model 24-hour total precipitation so that the
zero-censored point mass describes the probability of no precipitation on a
given day (see Figure~\ref{axams_onestation}). Forecasts for July are
established based on data from the same month in the years 1985--2012 along with
80~covariates derived from a wide range of NWP ensemble outputs. As
Figure~\ref{axams_onestation} shows, the station-wise models yield a full
distributional forecast for each day -- here for the same day (July~24) at one
station (Axams) over four years (2009--2012) -- based on the previous 24~years
as training data. The corresponding observations conform reasonably well with
the predictions. In Section~\ref{ProbabilisticForecasting} we investigate the
performance of distributional forests in this forecasting task in more detail.
It is shown that they perform at least on par and sometimes clearly better than
three alternative zero-censored Gaussian models: a standard ensemble model
output statistics approach (EMOS, \citealp{Gneiting+Raftery+Westveld:2005}), a GAMLSS
with regressors prespecified based on meteorological expertise following
\cite{Stauffer+Umlauf+Messner:2017}, and a boosted GAMLSS
\citep{Hofner+Mayr+Schmid:2016} using non-homogeneous boosting
\citep{Messner+Mayr+Zeileis:2017} as a technique for variable selection from all
80~available regressors.



\section{Methodology}
\label{Methodology}

As illustrated in Figure~\ref{Devel_parmod} a wide range of tools has been developed 
in the field of parametric modeling. Especially the shift from conditional mean 
regression models to distributional regression models has increased the application 
spectrum by allowing for much broader inference. In some cases further features 
such as the ability to capture non-additive effects or interactions are desirable 
which demands for more flexibility. Since tree-structured models provide these 
required features, combining them with parametric models can offer a framework 
that is able to deal with a wide variety of different modeling situations. 
In this section the novel methodology which embeds parametric modeling into 
the idea of tree-structured models will be introduced step by step. Starting 
with a simple distributional fit in Section~\ref{distfit} will be followed by 
building a distributional tree in Section~\ref{disttree} and finally a distributional 
forest in Section~\ref{distforest}.

\subsection{Distributional fit}
\label{distfit}
Fitting a distributional model to a set of observations without considering any 
covariates is a well known procedure which can for example be done by applying 
the maximum likelihood method. This is also the chosen approach in the methodology 
described in this section. 
%%% <TH> is is repeated on page 5, remove here </TH>

The goal is to fit a distributional model $\mathcal{D}(Y, \theta)$ to a response variable 
$Y \in \mathcal{Y}$ for a \textit{k}-dimensional vector of distribution parameters 
$\theta \in \Theta$ in case of a \textit{k}-parametric distribution. The distribution 
family with density function $f$ has to be specified in advance such that a 
log-likelihood function $\ell(\theta; Y) = \log(f(Y; \theta)$ is provided. Using 
the GAMLSS framework (\citealt{Rigby+Stasinopoulos:2005}) a wide range of distributions 
can be selected where each distribution parameter can be specified separately. 
This enables the model to deal with various features that can make the task of 
fitting a model very challenging such as censoring, truncation or heavy tails.

In our application of precipitation forecasting the response variable, which is 
the power transformed total precipitation amount within 24 hours, is expected 
to be normally distributed with mean/location parameter $\mu$ and standard 
deviation/scale parameter $\sigma$ and left-censored with censoring point 0. 
(The power transformation is used to remove large parts of the skewness of 
the data as further explained in Section~\ref{data}.) Therefore, the 
corresponding log-likelihood function with parameter vector 
$\theta = (\mu, \sigma)$ is
$$
\ell(\mu, \sigma; Y) = 
\begin{cases}
    \log\left(\frac{1}{\sigma} \cdot \phi\left(\frac{Y - \mu}{\sigma}\right) \right), & \text{if } Y > 0\\
    \vspace{0.1cm}
    \log\left(\Phi\left(\frac{-\mu}{\sigma}\right)\right), & \text{if } Y = 0
\end{cases}
$$
where $\phi$ and $\Phi$ are the probability density function and the 
distribution function of the standard normal distribution $\mathcal{N}(0,1)$. 
Next to the censored normal distribution also the censored logistic 
distribution is often chosen for the modeling of precipitation amounts. In 
\cite{Gebetsberger+Messner+Mayr:2017} this choice has been made in order 
to be able to deal with heavy tails which often appear in precipitation 
data. Another frequent application of probabilistic forecasting in meteorology 
is the modeling of near surface air temperature. In this case a normal 
distribution is often selected.

With the specification of the distribution family and its log-likelihood 
function the task of fitting a distributional model turns into the task 
of determining the distribution parameter~$\theta$. This is done by the 
following maximization based on a learning sample $\{y_i\}_{i=1,\ldots,n}$ 
of the response variable $Y$.
$$
\hat{\theta} = \max_{\theta \in \Theta} \sum_{i=1}^n \ell(\theta; y_i)
$$
which is equal to solving 
$$
\sum_{i=1}^n \frac{\partial \ell}{\partial \theta}(\hat{\theta}; y_i) = 0.
$$
In that way, a whole distribution is specified with all its features, 
including location, scale, and shape. However, fitting one global model 
to the whole data set might be too much of a generalization and therefore 
not represent the data and its features in a reasonable way. Moreover, 
if covariates are available it is desirable to include them into the model 
as they might provide important additional information. In our application 
on precipitation data a wide range of covariates is available from a numerical 
ensemble prediction system considering precipitation, temperature, sea level 
pressure and other meteorological quantities which might be of interest and 
have an influence on the predictions.

A common way of including these covariates is to consider them as regressors 
in a distributional model such that each parameter $\theta$ depends on covariates 
$\bold{Z} = (Z_1,\ldots,Z_m)$. This could for example be done by a linear function 
$g$. In that way the fitted model would be specified as 
$\mathcal{D}(Y, \theta = g(\bold{Z}))$. In order to avoid the restriction to linear 
effects one might also use a set of smooth functions $\{f_i\}_{i = 1,\ldots,p}$ 
to specify a more flexible model 
$\mathcal{D}(Y, \theta = f_1(\bold{Z}) + \ldots + f_p(\bold{Z}))$ applying a GAM structure. 
Another approach to distributional regression is proposed by 
\cite{Klein+Kneib+Lang:2015} where each parameter of a possibly complex 
distribution can be modeled by an additive term consisting of a variety of 
different functional effect types. These include non-linear effects, spatial 
effects, random coefficients and interaction surfaces which extends the 
flexibility compared to parametric models such as GAMLSS modeling only 
smooth effects.

A different approach on how to include covariates that also allows for 
non-additive effects and interactions is considered in the following. 
In particular, the covariates can be used to separate the data into more 
homogeneous subgroups. Doing so after fitting a global model to the whole 
data set (without considering any covariates) and then fitting a local 
model to each subgroup should improve the model remarkably. This 
procedure of splitting a data set into subgroups can be done by 
applying a tree algorithm.


\subsection{Distributional tree}
\label{disttree}
Given the fit of a global distributional model $\mathcal{D}(Y, \bold{\hat{\theta}})$, 
the idea is to use the information provided by this model together with a 
set of covariates $\bold{Z} = (Z_1, \ldots, Z_m) \in \mathcal{Z}$ 
to decide whether the data set should be split into subgroups or not. 
In particular, the information provided by the fitted model are the scores 
$$
s(\hat{\theta}, y_i) = \frac{\partial \ell}{\partial \theta}(\hat{\theta}; y_i).
$$ 
These values express how well the model fits the data regarding each 
parameter. Ideally they should fluctuate randomly around zero, 
similar to residuals in ordinary least squares estimation, but in 
this case there is a score value for each pair of estimated parameter 
$\hat{\theta}_j \in (\hat{\theta}_1, \ldots, \hat{\theta}_k) = \bold{\hat{\theta}}$ 
and observation $y_j \in \{y_i\}_{i = 1,\ldots, n}$. This enables the 
model to be sensitive to changes in each of the parameters which is one 
of the main advantages of distributional trees over other tree models 
considering only the mean of the distribution.

Statistical tests are applied to check whether there is a significant 
dependency between the scores and each of the partitioning variables 
$Z_l \in \bold{Z} = (Z_1, \ldots, Z_m)$. If so, the data set is split 
into subgroups based on the values of the partitioning variable that 
shows the highest dependency. The exact way of determining how and where 
to split varies across different tree algorithms. For distributional trees 
one possible choice is a permutation test based algorithm which is explained 
in detail in the appendix. 

After splitting the data the next step is to 
fit a local distributional model in each of the subgroups. Repeating this 
procedure of fitting a model, applying statistical tests to evaluate parameter 
instability and splitting the data set depending on the test results leads 
to a tree structure. In that way the learning data set is separated into 
\textit{B} disjoint segments such that
$$
\mathcal{Z} = \dot{\bigcup\limits_{b = 1, \ldots, \textit{B}}} \mathcal{B}_b
$$
where each segment is represented by a terminal node of the tree.

Summed up, the steps of building a distributional tree are:
\begin{enumerate}
\item Specify a distribution family with log-likelihood function $\ell(\theta; y)$.
\item Estimate $\bold{\hat{\theta}}$ by solving $\sum_{i=1}^n s(\hat{\theta}, y_i) = 0$.
\item Test for associations or instabilities of the scores $s(\bold{\hat{\theta}}, y_i)$ 
and each partitioning variable~$Z_l$.
\item Split the sample along the partitioning variable with the strongest association 
or instability. Choose the breakpoint with the highest improvement in log-likelihood.
\item Repeat steps 2--4 recursively in the subgroups until some stopping criterion is reached.

\end{enumerate}
The stopping criterion in step 5 can for example be a minimum number of 
observations in a node or a p-value for the statistical tests applied in step 3.
%%% <TH> we need a little more detail here or a reference to a better
%%% description of mob/ctree OR appendix</TH>

Executing these steps results in a stepwise parametric model where a complete 
distribution is fit in each node of the tree.

For distributional trees a prediction for a (possibly new) set of covariates 
$\bold{z} = (z_1, \ldots, z_m)$ is made with the following strategy: 
All observations of the learning data set $\{(y_i,\bold{z}_i)\}_{i=1\ldots,n}$ 
which end up in the same terminal node of the tree as $\bold{z}$ are selected 
and a distributional model is fit on this subset. Selecting these observations
is done by including a weight vector 
$\bold{w}^{\text{tree}} = (w^{\text{tree}}_1, \ldots, w^{\text{tree}}_n)$ 
in the fitting process where
$$
w^{\text{tree}}_i(\bold{z}) = \sum_{b=1}^B I((\bold{z}_i \in \mathcal{B}_b) \land (\bold{z} \in \mathcal{B}_b)).
$$
The predicted distribution for $\bold{z}$ is then fully 
specified by the estimated parameter $\hat{\theta}(\bold{z})$ where
$$
\hat{\theta}(\bold{z}) = \max_{\theta \in \Theta} \sum_{i=1}^n w^{\text{tree}}_i(\bold{z}) \cdot \ell(\theta; y_i).
$$

While the clear structure of such a tree model is advantageous in 
interpreting and analyzing it, the abrupt changes are often too rough 
and impose steps on the model even if the true underlying effect is 
smooth. In these situations bagging can solve the problem by combining 
an ensemble of trees such that wrongly detected abrupt changes of the 
model are turned into almost smooth transitions. In that way, all kinds 
of effects can be modeled, which is also one the of the main advantages 
of forest models taking bagging one step further as the variance is 
reduced by sampling the covariates considered for the splitting process 
in each node.

%%% <TH> we need to be much more precise here: random forests is Breiman's and Cutler's
%%% random forest. Maybe it is better to not mention this method at any time. </TH>
Common forest models such as the original forest technique by 
\cite{Breiman+Cutler:2004} consist of trees only considering the mean of the 
distribution of the response variable. For that reason, changes in other 
parameters can only be detected if they are connected to changes in the 
location parameter, e.g. if they depend on the location parameter. 
As explained before, a distributional tree considers scores for all 
distribution parameters. Therefore, a forest consisting of 
distributional trees can also capture separate effects in each of the 
distribution parameters. For that reason distributional forests are
now going to be introduced.

\subsection{Distributional forest}
\label{distforest}
A distributional forest consists of an ensemble of $T$ distributional trees. 
Each of these trees is built on a different data set which can either be a bootstrap 
or a subset of the original data set. Moreover, in each node a subset of the 
partitioning variables is chosen randomly as candidates for splitting variables. 
In that way the correlation among the resulting trees is reduced and therefore also 
the variance of the model (\citealt{Breiman:2001}).

Forest models differ mainly in the way how the trees are combined to make predictions. 
Following the strategy explained and proposed by \cite{Hothorn+Zeileis:2017} we 
interpret random forests as adaptive local likelihood estimators as distributional 
models are fit locally in subgroups while the term ``adaptive'' refers to the 
dependence on the splitting variables $\bold{Z}$ and the resulting subgroups 
and weighing schemes. In this case the forest itself is only used to evaluate 
neighborhood relationships among the observations which can then be included 
in the fitting process. In transformation forests by \cite{Hothorn+Zeileis:2017} 
so-called ``nearest neighbor weights'' are calculated as a measure of distance 
between observations. This approach has also already been applied in several 
random forest-type methods such as in ``adaptive nearest neighbors'' by 
\cite{Lin+Jeon:2006}, in ``quantile regression forests''  by  \cite{Meinshausen:2006} 
and in ``bagging survival trees'' by \cite{Hothorn+Lausen+Benner:2004}. 
The idea of nearest neighbor weights is to measure similarity of a pair of 
observations by counting the number of trees in which it is asigned to the 
same terminal node.
%Similar observations have a high probability of ending up in the same terminal 
%node whereas this probability is low for quite different observations. 
%although the first random forest-type algorithm for the estimation of 
%conditional distribution functions was published more than a decade ago 
%(bagging survival trees, Hothorn et al. 2004). 
In particular, for a (possibly new) set of covariates 
$\bold{z} = (z_1, \ldots, z_m) $  such a set of weights 
$\bold{w}^{\text{forest}}~=~(w^{\text{forest}}_1, \ldots, w^{\text{forest}}_n)$ 
is obtained by the following procedure: For each tuple $(y_i,\bold{z}_i)$ in 
the learning data set $\{(y_i,\bold{z}_i)\}_{i=1\ldots,n}$ a weight is calculated 
by counting the number of trees in which $(y_i,\bold{z}_i)$ ends up in the same 
terminal node as $\bold{z}$:
$$
w^{\text{forest}}_i(\bold{z}) = \sum_{t=1}^T \sum_{b=1}^{B^t} I((\bold{z}_i \in \mathcal{B}^t_b) \land (\bold{z} \in \mathcal{B}^t_b))
$$
where $T$ is the number of trees of the forest, $B^t$ is the number of terminal 
nodes of the \textit{t}-th tree and $\mathcal{B}^t_b$ represents the \textit{b}-th 
terminal node of the \textit{t}-th tree.

Therefore, contrary to distributional trees, in distributional forests the whole 
learning data set can be used to fit a distributional model and specify the 
predicted distribution for a set of covariates. Compared to the formula for
predictions made by distributional trees the vector of tree weights 
$\bold{w}^{\text{tree}}$ containing only zeros and ones is now replaced by a 
vector of integer valued forest weights.
%%% <TH> we need references for the nearest neighbor weights, copy from
%%% "Transformation Forests" </TH>
%%%<TH> we need to cite "Transformation Forests" for the interpretation of
%%% random forests as adaptive local ML </TH>
This individual set of weights 
$\bold{w}^{\text{forest}}(\bold{z}) = (w^{\text{forest}}_1(\bold{z}), \ldots, w^{\text{forest}}_n(\bold{z}))$ 
for the set of covariates $\bold{z}$ can now be included in the 
estimation process of the distribution parameters and in that way leads 
to its individual parameter vector $\hat{\theta}(\bold{z})$ where
$$
\hat{\theta}(\bold{z}) =  \max_{\theta \in \Theta} \sum_{i=1}^n w^{\text{forest}}_i(\bold{z}) \cdot \ell(\theta; y_i)
$$
While in a distributional tree the same distribution is specified for all 
sets of covariates being in the same node, a distributional forest predicts 
and fits a different distribution for each different set of covariates due 
to the individual weights calculated separately for each set of covariates.

As this novel methodology combines the advantages of smooth and flexible 
parametric modeling with those of tree-structured modeling it provides a 
model that can deal with a wide range of situations. Abrupt changes and 
non-additive effects can be captured by its tree structure while smooth 
effects can be modeled reasonably well by combining various tree and 
smoothing the steps imposed by a single tree. As scores are considered 
for each distribution parameter (separate) effects in different parameters 
can be captured. At the same time, also interactions are detected 
automatically by splitting the data in a way that is sensitive to changes 
in either each of the parameters or combined in various parameters.







\SweaveOpts{eval = TRUE}

\section{Probabilistic precipitation forecasting in complex terrain}
\label{ProbabilisticForecasting}

Many weather forecasting models leverage the strengths of modern
numerical weather prediction (NWP) systems \citep[see][]{Bauer+Thorpe+Brunet:2015}
and produce high-quality forecasts adapted to specific sites/locations
through distributional regression models like EMOS
(ensemble model output statistics, \citealp{Gneiting+Raftery+Westveld:2005}).
In the case of precipiation forecasting, EMOS explains past precipitation
sums by the NWP output ``total precipitation'' (\emph{tp}), using its
ensemble mean as the predictor variable for the location parameter and
the ensemble standard deviation for the scale parameter.

While this approach alone is already highly effective in the plains,
it typically does not perform as well in complex terrain due to unresolved
effects in the NWP system. For example, in the Tyrolean Alps -- considered
in the following application case study -- the NWP grid cells of $50 \times 50$
km$^2$ are too coarse to capture mountains, valleys, etc. Therefore,
it is often possible to substantially improve the EMOS by including
further predictor variables, either from local observations or from
other NWP outputs. Unfortunately, it is typically unclear which
variables are relevant for improving the predictions. Simply including
all available variables may be computationally burdensome and
can lead to overfitting but, on the other hand, excluding too many variables
may result in a loss of valuable information. Therefore, selecting
the relevant variables and interactions is crucial for improving the
forecasting model.

In the following, it is illustrated how distributional regression forests
can solve this problem of automatically selecting relevant variables,
interactions, and potentially nonlinear effects. For fitting the forest
only the response distribution and the list of potential predictor variables
need to be specified (along with a few algorithmic details) and then
the model fit is determined by the forest in a data-driven way. Here, we
employ a zero-censored Gaussian distribution and 80~predictor variables
computed from ensemble means and spreads of various NWP outputs.
The predictive performance of the forest is compared to three other
zero-censored Gaussian models: (a) a standard basic EMOS, (b) a GAMLSS
with prespecified effects and interactions based on meteorological
knowedge/experience, and (c) a boosted GAMLSS with automatic
selection of smooth additive terms based on all 80~predictor variables.

\subsection{Data}
\label{data}

\begin{table}[t!]
%\resizebox{1.4\textwidth}{!}{
\begin{minipage}{\textwidth}
\begin{tabular}{ l  c  l }
\hline
Basic covariates & {\#} & Variations\\
\hline
\emph{tp}: total precipitation            & 12  & ensemble mean of sums over 24h, \\
\hspace*{0.5cm} power transformed (by $\frac{1}{1.6}$) &    & ensemble std. deviation of sums over 24h, \\ 
\emph{cape}: convective available     &    & ensemble minimum of sums over 24h, \\
\hspace*{0.9cm} potential energy &    & ensemble maximum of sums over 24h\\
\hspace*{0.9cm} power transformed (by $\frac{1}{1.6}$)&    & \qquad all for 6--30 UTC\\
\hspace*{\fill}                            &    & ensemble mean of sums over 6h\\
\hspace*{\fill}                            &    & \qquad for 6--12, 12--18, 18--24, 24--30 UTC\\
\hspace*{\fill}                            &    & ensemble std. deviation of sums over 6h\\
\hspace*{\fill}                            &    & \qquad for 6--12, 12--18, 18--24, 24--30 UTC\\
\hline
\emph{dswrf}: downwards short wave      & 6 & ensemble mean of mean values, \\
\hspace*{1.13cm} radiation flux (``sunshine'') &   & ensemble mean of minimal values,\\
\emph{msl}: mean sea level pressure     &   &  ensemble mean of maximal values,\\
\emph{pwat}: preciptable water          &   & ensemble std. deviation of mean values,\\
\emph{tmax}: 2m maximum temperature     &   & ensemble std. deviation of minimal values,\\
\hspace*{1.15cm}                          &   & ensemble std. deviation of maximal values,\\
\emph{tcolc}: total column-integrated   &   & \qquad all over 6--30 UTC \\
\hspace*{0.95cm} condensate               &   & \\
\emph{t500}: temperature on 500 hPa     &   & \\
\hspace*{1cm}                             &   & \\
\emph{t700}: temperature on 700 hPa     &   & \\
\hspace*{1cm}                             &   & \\
\emph{t850}: temperature on 850 hPa     &   & \\
\hspace{1cm}                              &   & \\
\hline
\emph{tdiff500850}: temperature         & 3 & ensemble mean of difference in mean,\\
\hspace*{\fill} difference 500 to 850 hPa &   & ensemble minimum of difference in mean,\\
\emph{tdiff500700}: temperature         &   & ensemble maximum of difference in mean\\
\hspace*{\fill} difference 500 to 700 hPa &   & \qquad all over 6--30 UTC\\
\emph{tdiff700850}: temperature         &   & \\
\hspace*{\fill} difference 700 to 850 hPa &   & \\
\hline
\emph{msl{\_}diff}: mean sea level pressure & 1 & \emph{msl{\_}mean{\_}max} $-$ \emph{msl{\_}mean{\_}min}\\
\hspace{1.6cm} difference            &   & \qquad over 6--30 UTC\\
\hline
\end{tabular}
\caption[Table caption text]{Basic covariates together with the number ({\#}) and the type of variations.}
\label{covariates}
\end{minipage} 
%}
\end{table}

Training and validation data consist of observed 
daily precipitation sums provided by the National Hydrographical Service 
(\citealt{ehyd}) and numerical weather forecasts from the U.S.~National
Oceanic and Atmospheric Administration (NOAA).
Both, observations and forecasts are available for 1985--2012 and
the analysis is exemplified using July as the month with the most precipitation
in Tyrol.

Observations are obtained for 95~stations all over Tyrol and surroundings,
providing 24~hour precipitation sums measured at 6~UTC and rigorously
quality-checked by the National Hydrographical Service. NWP outputs
are obtained from the second generation reforecast data set of the
global ensemble forecast system \citep[GEFS,][]{Hamill+Bates+Whitaker:2013}. 
This data set consists of an 11-member ensemble based on a fixed version of the 
numerical model and a horizontal grid spacing of about $50 \times 50$ 
km$^2$ initialized daily at 0~UTC from December 1984 to present 
providing forecasts on 26 hourly temporal resolution. Each of the 
11 ensemble members uses slightly different perturbed initial 
conditions trying to predict the situation specific uncertainty 
of the atmospheric state. 

From the GEFS outputs 14 basic forecast variables are considered with up 
to 12 variations of each of these variables such as mean/maximum/minimum
or different aggregation time ranges, etc. A detailed overview is
provided in Table~\ref{covariates}, yielding 80~predictor variables in total. (Since the minimum of the ``variable downwards short wave radiation flux'' (\emph{dswrf}) over 24 hours is always 0, the ensemble mean and standard deviation of these minimal values are not considered.) \fixme{Mention that dswrf\_mean/sprd\_min is not included?}

To remove large parts of the skewness of precipitation data, 
a power transformation \citep{Box+Cox:1964} is often applied, e.g., using
cubic \citep{Stidd:1973} or square root \citep{Hutchinson:1998} transformations.
However, the power parameter may vary for different climatic zones or temporal 
aggregation periods and hence we follow \cite{Stauffer+Mayr+Messner:2017} 
in their choice of $\frac{1}{1.6}$ as a suitable power parameter for the
region of Tyrol. The same power transformation is applied to both the
observed precipitation sums and the NWP outputs ``total precipitation'' (\emph{tp}) and
``convective available potential energy'' (\emph{cape}).


\subsection{Models and evaluation}

\begin{table}[t!]
\centering
\begin{tabular}{ l l l l }
\hline
Model & Type & Location ($\mu$) & Scale ($\log(\sigma)$)                                  \\ \hline
Distributional forest & recursive    & all                & all                           \\ 
                      & partitioning &                    &                               \\ \hline
EMOS                  & linear       & \emph{tp{\_}mean}  & \emph{tp{\_}sprd}             \\ \hline
Prespecified GAMLSS   & spline       & \emph{tp{\_}mean}, & \emph{tp{\_}sprd},            \\
                      & in each      & \emph{tp{\_}max},  & \emph{dswrf{\_}sprd{\_}mean}, \\
 & & \emph{tp{\_}mean1218} $\ast$ & \emph{tp{\_}sprd1218} $\ast$\\ 
 & & \quad \emph{cape{\_}mean1218}, & \quad \emph{cape{\_}mean1218},\\
 & & \emph{dswrf{\_}mean{\_}mean}, & \emph{tcolc{\_}sprd{\_}mean},\\
 & & \emph{tcolc{\_}mean{\_}mean}, & \emph{tdiff500850{\_}mean}\\
 & & \emph{pwat{\_}mean{\_}mean}, & \\
 & & \emph{tdiff500850{\_}mean}, & \\
 & & \emph{msl{\_}diff} & \\ \hline
Boosted GAMLSS        & spline  & all & all \\
                      & in each &     &     \\ \hline
\end{tabular}
\caption[Table caption text]{Overview of models with type of 
covariate dependency and included covariates for each distribution 
parameter.}
\label{model_specification}
\end{table}

The following zero-censored Gaussian regression models are employed
in the empirical case study, see also Table~\ref{model_specification} for
further details:
%
\begin{itemize}

\item \emph{Distributional forest:} All 80~predictor variables are
  considered for learning a forest of 100 trees. Bootstrap
  sampling is employed for each tree using a third of the predictors
  in each split of the tree (``mtry''). Parameters are estimated by
  adaptive local likelihood based on the forest weights, as described
  in Section~\ref{Methodology}.

\item \emph{EMOS:} Standard ensemble model output statistics are used
  with the total precipitation ensemble mean as regressor in the location
  submodel and the ensemble standard deviation in the scale submodel.
  The parameters are estimated by maximum likelihood, using an identity
  link for the location and a log link for the scale
  \citep[following the advice of][]{Gebetsberger+Messner+Mayr:2017}.

\item \emph{Prespecified GAMLSS:} Smooth additive splines are selected for
  the most relevant predictors, based on meteorological expert knowledge
  following \cite{Stauffer+Umlauf+Messner:2017}. More specifically, based
  on the 80~available variables, 8 terms are included in the location
  submodel and 6 in the scale submodel. Both involve an interaction of
  \emph{tp} and \emph{cape} in the afternoon (between 12 and 18 UTC)
  to capture the potential for thunderstorms that frequently occur in
  summer afternoons in the Alps. The model is estimated by maximum
  penalized likelihood using a backfitting algorithm \citep{Stasinopoulos+Rigby:2007}.

\item \emph{Boosted GAMLSS:} Smooth additive splines are selected
  automatically from all 80~available variables, using non-cyclic boosting
  for parameter estimation \citep{Hofner+Mayr+Schmid:2016,Messner+Mayr+Zeileis:2017}.
  This updates the predictor terms for the location or scale submodels iteratively
  by maximizing the log-likelihood only for the variable yielding the
  highest improvement. The iteration stops early -- before fully maximizing
  the in-sample likelihood -- based on a (computationally intensive)
  out-of-bag bootstrap estimate of the log-likelihood. The grid considered for
  the number of boosting iterations (``mstop'') is: $50, 75, \dots, 975, 1000$.

\end{itemize}

The predictive performance in terms of full probabilistic forecasts is
assessed using the continuous ranked probability score (CRPS, \fixme{reference}).
For each of the models this assesses the discrepancy of the
predicted distribution function $F$ from the observation $y$ by
$$
\text{CRPS}(y, F) = \int_{-\infty}^\infty (F(z) - \mathbf{1}(y \leq z))^2 dz
$$
where \(\mathbf{1}\) is the indicator function. In the subsequent
applications, the mean CRPS is always evaluated out of sample, 
either using cross-validation or a hold-out data set (2009--2012)
that was not used for learning (1985--2008). CRPS is a proper
scoring rule \citep{Gneiting+Raftery:2007} and lower values indicate a better fit.

To assess differences in the improvement of the forests and GAMLSS
models over the basic EMOS, a CRPS-based skill score with EMOS as
the reference method is computed as well:
$$
\text{SkillScore}_{\text{method}} = 1 - \frac{\text{CRPS}_{\text{method}}}{\text{CRPS}_{\text{EMOS}}}.
$$




\subsection{Application for one station}
\label{Application for one station}
First of all we focus on one observation station, Axams in Tyrol. This 
station is selected due to its geographical closeness to Innsbruck, 
the capital of Tyrol. As for all other stations, daily observations and 
numerical weather predictions are available for the month of July of 
28~years from 1985 through the year 2012. In Figure~\ref{axams_onestation} 
the results of fitting a distributional forest to the first 24 years of the 
data set and predicting the total amount of precipitation for one specific 
day of each of the four successive years have been shown as a motivational 
example for a probabilistic forecast. Staying with this fitted model,
variable importance of the distributional forest model will be analyzed and its
performance will be compared to those of the prespecified and the boosted GAMLSS 
and the EMOS model based on residual QQ-plots.

%%%<TH> always us the term "distributional forest"
As shown in Table~\ref{model_specification} the distributional forest 
includes all of the available covariates. A single tree model would allow 
to easily analyze which covariates are used as split variables in the nodes 
and hence have a significant impact on the response variable. In a forest 
model though, for each node in each of the trees a subset of the covariates 
is chosen randomly as possible split variables to select one from. Moreover, 
each tree is built on a resampled version of the learning data. For these 
reasons the choices of split variables vary over the trees of a forest making 
it difficult to analyze the impact of covariates just by investigating the 
different trees. To get an overview of which covariates have the most influential 
effect on the response variable a standard measurement of variable importance 
for forest models has been applied. Based on the mean decrease in accuracy 
regarding the CRPS the 10 leading covariates of the above described distributional 
forest for station Axams learned on the first 24 years are listed in 
Figure~\ref{varimp}. As expected, the numerical forecast for total precipitation (\emph{tp}) is the most influential variable. In particular, five variations of 
this variable are among the top ten of the list of covariates ordered by 
variable importance.  
  

\begin{figure}[t!]
\centering
\setkeys{Gin}{width=0.7\textwidth}
<<rain_axams_varim, fig=TRUE, echo=FALSE, width=6.5, height=4.5>>=
par(mar = c(3,10,2,2))
barplot(sort(vimp_crps, decreasing = FALSE)[(length(vimp_crps)-9):length(vimp_crps)], 
        horiz = TRUE, las = 1, axes = FALSE,
        xlab = "Variable importance: mean decrease in CRPS",
        font.axis = 3, #list(family="HersheySerif", face=3),
        names.arg = gsub("pow", "", names(sort(vimp_crps, decreasing = FALSE)[(length(vimp_crps)-9):length(vimp_crps)])))
axis(1, at = seq(0,1.6,0.2), las = 1, mgp=c(0,1,0))
@
\caption{\label{varimp} CRPS-based variable importance for the top 10 covariates 
in the distributional forest. Based on data for station Axams, learning period 
1985--2008 and assessed in 2009--2012.}
\end{figure}

In Figure~\ref{qqr_oosample} an impression of the quality of predictions made 
by the tested methods is given by residual QQ-plots for out-of-sample predictions 
of each of the models. For these plots 100 randomized quantile residuals are 
simulated per observation. The closer the points are to the diagonal line the 
better the estimated distributional model fits the data. Overall, it can be 
stated that all four models provide well calibrated forecasts. This is also 
supported by the corresponding probability integral transform (PIT) histograms 
(\citealt{Gneiting+Balabdaoui+Raftery:2007}) in the appendix.



\begin{figure}[t!]
\centering
\setkeys{Gin}{width=0.99\textwidth}
<<rain_axams_qqr_oosample, fig=TRUE, echo=FALSE, width = 6.5, pdf = FALSE, png = TRUE, resolution = 150>>=
# out of sample
set.seed(7)
par(mfrow = c(2,2))
qqrplot(pit_df_t, nsim = 100, main = "Distributional forest", ylim = c(-5,5), col = gray(0.04, alpha = 0.01), pch = 19)
qqrplot(pit_ml_t, nsim = 100, main = "EMOS", ylim = c(-5,5), col = gray(0.04, alpha = 0.01), pch = 19)
qqrplot(pit_g_t, nsim = 100, main = "Prespecified GAMLSS", ylim = c(-5,5), col = gray(0.04, alpha = 0.01), pch = 19)
qqrplot(pit_gb_t, nsim = 100, main = "Boosted GAMLSS", ylim = c(-5,5), col = gray(0.04, alpha = 0.01), pch = 19)
@
\caption{\label{qqr_oosample} Out-of-sample residual QQ-plots (2009--2012) 
for station Axams based on models learned on data from 1985--2008.}
\end{figure}

  
After analyzing the results of the models on only one data set where 
visualization is easier, the methods are now going to be evaluated
in a cross validation framework for the station Axams. 
The data set over all 28 years is split into 7 subsets each consisting of 
4 randomly selected years. While 6 of these subsets are used as learning 
data the 7th is the testing data set for which predictions are made and 
evaluated. This is done by calculating the CRPS value for each of the 
observations in the testing data set and then averaging them. Each of the 
7 parts is selected once as testing data which leads to 7 different settings 
of which again the average values are stored. This whole procedure is repeated 
10 times. The resulting skill scores are illustrated in 
Figure~\ref{boxplots_crps_axams} using the EMOS model as the reference model 
represented by the horizontal line at height 0. This CRPS skill score boxplot 
shows that, in this setting, both GAMLSS models and the distributional forest 
perform better than the EMOS model. While the two GAMLSS lead to an improvement 
of around 4 percent with the boosted version being slightly ahead, the 
distributional forest even reaches an improvement rate close to 6 percent.
<<echo=FALSE, results=hide>>=
#### cross validation rain
if(file.exists("rain_Axams_7x10.rda")){
  load("rain_Axams_7x10.rda")
} else {
  source("rain_cross_7x10.R")
  library("gamlss.cens")
  gen.cens("NO", type = "left")
  rainres <- rain_cross(stationname = "Axams", 
                        seedconst = 7, ntree = 100,
                        forest_mtry = 27,
                        gamboost_cvr = TRUE,
                        frac = FALSE)
  
  save(rainres, file = "rain_Axams_7x10.rda")
}

rain_rmse <- matrix(0, ncol = ncol(rainres[[1]]$rmse), length(rainres)-1)
rain_ll <- matrix(0, ncol = ncol(rainres[[1]]$ll), length(rainres)-1)
rain_crps <- matrix(0, ncol = ncol(rainres[[1]]$crps), length(rainres)-1)

for(i in 1:(length(rainres)-1)){
  rain_rmse[i,] <- colMeans(rainres[[i]]$rmse, na.rm = TRUE)
  rain_ll[i,] <- colMeans(rainres[[i]]$ll, na.rm = TRUE)
  rain_crps[i,] <- colMeans(rainres[[i]]$crps, na.rm = TRUE)
}

colnames(rain_rmse) <- colnames(rain_ll) <- colnames(rain_crps) <- colnames(rainres[[1]]$rmse)
@


\begin{figure}[t!]
\centering
<<rain_cross_axams_crps_skill_score, fig=TRUE, echo=FALSE, height=4.5, width = 7>>=
boxplot(1 - rain_crps[,c(2,3,4)] / rain_crps[,6], ylim = c(-0.005, 0.065),
        names = c("Distributional forest", "Prespecified GAMLSS", "Boosted GAMLSS"),
        ylab = "CRPS skill score", col = "lightgray") 
abline(h = 0, col = pal["EMOS"], lwd = 2)
@
\caption{\label{boxplots_crps_axams}CRPS skill score from the 10 times 
7-fold cross validation at station Axams (1985--2012). The horizontal orange 
line pertains to the reference model EMOS.}
\end{figure}




\SweaveOpts{eval = TRUE}
%\newpage
\subsection{Application for all stations}
Until now the focus has been on one observation station only. To test and 
compare the four methods under different geographical circumstances they
are applied on all 95 stations. For each station, the first 
24 years are used as learning data set and predictions are made for the 
ensuing 4 years of available observations. For these out-of-sample predictions 
the CRPS skill score with the EMOS model as reference model is calculated 
for each method at each station. In Figure~\ref{boxplot_crps_all} the boxplots 
of the CRPS skill score values are plotted illustrating that the distributional 
forest leads to the highest improvement averaged over all stations.
The green line represents the results for the station Axams.
%NAs in gamlss in 13 stations!!!\\
%for example in 13th station in complete{\_}stations\\
%\text{Error in glim.fit(f = mu.object, X = mu.X, y = y, w = w, fv = mu, os = mu.offset,  :}\\ 
%\text{NA's in the working vector or weights for parameter mu}
%\\
% sum(is.na(crps[,3]))
%<<eval = TRUE, echo = FALSE>>=
<<echo = FALSE>>=
#### prediction over all stations 24 - 4
if(file.exists("rain_pred_24to4.rda")){
  #load("~/svn/partykit/pkg/disttree/inst/draft/rain_pred_24to4.rda")
  load("rain_pred_24to4.rda")
} else {
  
  source("rain_pred_24to4.R")
  library("gamlss.cens")
  gen.cens("NO", type = "left")
  
  res <- rain_pred(seedconst = 7,
                   gamboost_cvr = TRUE,
                   frac = FALSE)
  
  save(res, file = "rain_pred_24to4.rda")
}
 
#ll_all <- res[[1]]$results["ll",]
#for(i in 2:(length(res)-1)) ll_all <- rbind(ll_all, res[[i]]$results["ll",])

#rmse_all <- res[[1]]$results["rmse",]
#for(i in 2:(length(res)-1)) rmse_all <- rbind(rmse_all, res[[i]]$results["rmse",])

crps_all <- res[[1]]$results["crps",]
for(i in 2:(length(res)-1))  crps_all <- rbind(crps_all, res[[i]]$results["crps",])

#colnames(ll_all) <- colnames(rmse_all) <- 
  colnames(crps_all) <- colnames(res[[1]]$results)

# skill score
s <- 1 - crps_all[, 2:4]/crps_all[,6]
colnames(s) <- c("Distributional forest", "Prespecified GAMLSS", "Boosted GAMLSS")

## prepare data for map which shows where distforest performed better than gamlss or gamboostLSS based on the crps

crps <- crps_all[,c("distforest", "gamlss", "gamboostLSS", "EMOS log")]  

## best method
bst <- apply(crps, 1, which.min)

## distance of forest to best other method
dst <- crps[,1] - crps[cbind(1:nrow(crps), apply(crps[, -1], 1, which.min) + 1)]

## breaks/groups
brk <- c(-0.1, -0.05, -0.005, 0.005, 0.05, 0.1)
#brk <- c(-0.1, -0.05, -0.01, 0.01, 0.05, 0.1)
grp <- cut(dst, breaks = brk)

## HCL colors (relatively flashy, essentially CARTO Tropic)
clr <- colorspace::diverge_hcl(5, h = c(195, 325), c = 80, l = c(50, 90), power = 1.3)



library("raster") # dem (digital elevation model)
library("sp")     # gadm www.gadm.org/country

load("plot_map_rain/demo/tirol.gadm.rda")
load("plot_map_rain/demo/tirol.dem.rda")
load("plot_map_rain/demo/ehyd.statlist.rda")
  
# Create SpatialPointsDataFrame from station list
sp <- SpatialPointsDataFrame(subset(ehyd.statlist[res$complete_stations,],
                                    select=c(lon,lat)),
                             data = subset(ehyd.statlist[res$complete_stations,],
                                           select = -c(lon,lat)),
                             proj4string = crs(tirol.dem))

#library("colorspace")
@



\begin{figure}[t!]
\centering
<<rain_all_crps_skill_score, fig=TRUE, echo=FALSE, width = 7>>=
  matplot(t(s[,]), type = "l", lwd = 2, 
          col = gray(0.5, alpha = 0.2),
          lty = 1, axes = FALSE, 
          xlab = "", ylab = "CRPS skill score", xlim = c(0.5, 3.5))
lines(s[70,], col = "limegreen", type = "o", pch = 19, lwd = 2)  
# Station Axams is the 77th station which is the 70th in the list of complete stations
  boxplot(s, add = TRUE, col = "transparent")
  abline(h = 0, col = pal["EMOS"], lwd = 2)
    
@
\caption{\label{boxplot_crps_all}CRPS skill score for each station (grey lines) 
and aggregated over all stations (boxplots). Station Axams is highlighted in green 
and the horizontal orange line pertains to the reference model EMOS. The models are 
learned on data from 24 years (1985--2008) and validated on 4 years (2009--2012).}
\end{figure}

Figure~\ref{map} shows a map of Tyrol (based on the SRTM 90m Digital Elevation 
Data, \cite{srtm}) with the location of all observation stations included in the 
data set. Each station is illustrated by a symbol with the type of symbol 
representing the method that performed best in terms of CRPS at this station. 
The color of the symbol indicates the difference between the CRPS of the 
distributional forest and the CRPS of the best performing among the other 
three models (prespecified GAMLSS, boosted GAMLSS and EMOS).
Even though for some stations, especially in East Tyrol, one of the two GAMLSS
models and for a few others the EMOS model leads to the best results, for the 
majority of stations the distributional forest performs best.

\begin{figure}[t!]
\centering
\setkeys{Gin}{width=0.99\textwidth}
<<map, fig=TRUE, echo=FALSE, height=6.1, width=9>>=
  
  layout(cbind(1, 2), width = c(9, 1))
  par(mar = c(5,4,4,0.1))
  raster::image(tirol.dem, col = rev(gray.colors(100)),
                main="Stations in Tyrol", ylab = "Latitude", xlab = "Longitude", 
                xlim = c(9.8,13.2), 
                ylim = c(46.6, 47.87))
  plot(tirol.gadm, add = TRUE)
  points(sp, pch = c(21, 24, 25, 22)[bst], bg = clr[grp], col = "black", las = 1, cex = 1.5)
  legend(x = 9.8, y = 47.815, pch = c(21, 24, 25, 22), legend = c("Distributional forest", "Prespecified GAMLSS", "Boosted GAMLSS", "EMOS"), cex = 1, bty = "n")
  text(x = 10.3, y = 47.82, labels = "Models with lowest CRPS")
  mtext("CRPS\ndifference", side=4, las = TRUE, at = c(x = 13.5, y = 47.76), line = 0.3)
  par(mar = c(0.5,0.2,0.5,2.3))
  ## legend
  plot(0, 0, type = "n", axes = FALSE, xlab = "", ylab = "",
       xlim = c(0, 1), ylim = c(-0.2, 0.2), xaxs = "i", yaxs = "i")
  rect(0, brk[-6], 0.5, brk[-1], col = rev(clr))
  axis(4, at = brk, las = 1, mgp=c(0,-0.5,-1))
  
@
\caption{\label{map}Map of Tyrol showing which model performed best at 
which station (type of symbol). The color codes whether the distributional 
forest had lower or higher CRPS (red or blue) compared to the best of the 
other three models (prespecified GAMLSS, boosted GAMLSS and EMOS).}
\end{figure}





\section{Discussion}

From the results in Section~\ref{ProbabilisticForecasting} we can conclude 
that distributional forests provide a powerful alternative framework requiring 
only little effort to set up a model which is competitive to commonly used 
methods or even outperforms them. Even though a proper selection of covariates 
and specification of interactions in advance can lead to better fitting 
(and probably less complex) models, this presupposes profound knowledge of 
all influencing factors. Otherwise, important aspects might be missed or 
effects which do not correspond to the truth might be imposed on the model. 
This risk can be avoided by applying a boosting algorithm for an automatic 
variable selection. In this case the trade-off is often a remarkably higher 
computational effort. To build a distributional forest the user is not 
required to provide any expert knowledge or additional information about 
the setting as variable selection and detection of interactions are done 
automatically by the tree-structure of the model. This feature is 
particularly advantageous in case of a very high number of covariates. 
Therefore, the above discussed application illustrates the strengths of 
distributional forests as they offer a very flexible framework which 
enables an easy use of the method and at the same time lead to an overall 
improvement in the predictive performance in this setting. Especially 
in situations where no or only little information is available about 
possible influences, effects or interactions this novel methodology 
can be seen as a good and recommendable compromise being able to deal 
with a wide range of modeling situations in a reasonable way.

%Even though many situations demand for the use of specifically tailored 
%models providing the needed features and therefore being clearly advantageous 
%and the best choice, it is often hard to tell in advance which features 
%will be required. In these cases a distributional forest is a 
%recommendable compromise providing reasonable models and performing 
%well without any prespecification or additional knowledge needed in advance. 



\section*{Computational details}
The corresponding implementation to the novel methods presented 
in this article is available on \textsf{R}-Forge in an \textsf{R} 
package called \textbf{disttree} within the project \textbf{partykit} 
(\url{https://R-Forge.R-project.org/projects/partykit/}). The function
\code{distfit} fits a distributional model and is called within the
tree-building function \code{disttree} and the forest-building
function \code{distforest}. 
Within \code{disttree} either the \code{mob} or \code{ctree} function 
can be applied as a framework to build a tree. Similarly, the function 
\code{distforest} is based on the function \code{cforest}. All three 
functions, \code{mob}, \code{ctree} and \code{cforest}, are provided 
in the package \textbf{partykit} (\citealt{Hothorn+Zeileis:2015}). 
To specify a distribution family the user can hand over one of the 
following objects as input argument in \code{distfit}, \code{disttree} 
and \code{distforest}: 
\begin{itemize}
\item a GAMLSS family object from the \textsf{R} package \textbf{gamlss.dist} 
(\citealt{Stasinopoulos+Rigby:2007}).
\item a family list generating function from the package \textbf{disttree}.
\item a complete family list from the package \textbf{disttree}.
\item a custom list containing all required information about the 
distribution family.
\end{itemize}

In Section~\ref{ProbabilisticForecasting} the prespecified GAMLSS were 
built using the function \code{gamlss} from the \textsf{R} package 
\textbf{gamlss} (\citealt{Stasinopoulos+Rigby:2007}). For the 
boosted GAMLSS the function \code{gamboostLSS} from the \textsf{R} 
package \textbf{gamboostLSS} (\citealt{Hofner+Mayr+Schmid:2016}) 
was applied. The EMOS models were built using the function 
\code{crch} from the \textsf{R} package \textbf{crch} 
(\citealt{Messner+Mayr+Zeileis:2016})


\bibliography{ref.bib}


\newpage
\begin{appendix}

\section{Tree algorithm}
\label{treealgorithm}
The tree algorithm used in the applications discussed in this paper 
is going to be explained following the ideas presented in 
\cite{Seibold+Zeileis+Hothorn:2017}. For notational simplicity, the testing 
and splitting procedure is described for the root node with observations 
$\{y_i\}_{i = 1,\ldots,n}$, $n \in \mathbb{N}$. In each child node the 
corresponding subset of observations depends on the assignment made 
by the foregoing split.

\begin{itemize}
\item A distributional model $\mathcal{D}(Y, \theta)$ is fit to the set of 
observations $\{y_i\}_{i = 1,\ldots,n}$ by estimating the distribution 
parameter $\hat{\theta}$. This parameter can be a multidimensional 
vector $(\hat{\theta}_1, \ldots, \hat{\theta}_K)$, $K \in \mathbb{N}$, 
depending on the type of distribution. The estimation is done by 
maximizing the log-likelihood function $\ell(\theta; Y)$, i.e.
$$
\hat{\theta} = \max_{\theta \in \Theta} \sum_{i=1}^n \ell(\theta; y_i)
$$
which is equal to solving
$$
\sum_{i=1}^n s(\theta, y_i) = \sum_{i=1}^n \frac{\partial \ell}{\partial \theta}(\theta; y_i) = 0
$$
for $\theta$.

\item Calculate score values 
$$
s(\hat{\theta}, y_i) = 
\begin{pmatrix} 
s(\hat{\theta}, y_1) \\
s(\hat{\theta}, y_2) \\
\vdots \\
s(\hat{\theta}, y_n)
\end{pmatrix} =
\begin{pmatrix} 
s(\hat{\theta}, y_1)_1 & s(\hat{\theta}, y_1)_2 & \ldots & s(\hat{\theta}, y_1)_K\\
s(\hat{\theta}, y_2)_1 & s(\hat{\theta}, y_2)_2 & \ldots & s(\hat{\theta}, y_2)_K\\
\vdots & \vdots & \ddots & \vdots \\
s(\hat{\theta}, y_n)_1 & s(\hat{\theta}, y_n)_2 & \ldots & s(\hat{\theta}, y_n)_K
\end{pmatrix}
$$

\item Test for independence between each column of scores $\left(s(\hat{\theta}, y_i)_k\right)_{i=1,\ldots,n}$ for $k\in\{1,\ldots,K\}$, and split variable $Z_j \in \{Z_1, \ldots, Z_m\}$.  
\begin{align*}
H_0^{1,j}:  \left(s(\hat{\theta}, y_i)_1\right)_{i=1,\ldots,n} \qquad \bot \qquad Z_j \\
H_0^{2,j}:  \left(s(\hat{\theta}, y_i)_2\right)_{i=1,\ldots,n} \qquad \bot \qquad Z_j \\
\vdots \hspace{3cm}\\
H_0^{K,j}:  \left(s(\hat{\theta}, y_i)_K\right)_{i=1,\ldots,n} \qquad \bot \qquad Z_j 
\end{align*}

To test these hypotheses permutation testing with the multivariate linear statistic
$$
T_{j} = \sum_{i=1}^n g_j(z_{ji}) \cdot s(\hat{\theta}, y_i)
$$
is applied. The type of the transformation function $g_j$ depends on 
the type of the split variable $Z_j$. If $Z_j$ is numeric then $g_j$ 
is simply the identity function $g_j(z_{ji}) = z_{ji}$. If $Z_j$ is 
a categorical variable with $L$ categories then 
$g_j(z_{ji}) = e_L(z_{ji}) = (\I(z_{ji} = 1), \ldots, \I(z_{ji} = L))$ 
such that $g_j$ is a unit vector where the element corresponding to 
the value of $z_{ji}$ is $1$. Observations with missing values are 
excluded from the sums.

With the conditional expectation $\mu_{j}$ and the covariance 
$\Sigma_{j}$ as derived by \cite{Strasser+Weber:1999} the test 
statistic can be standardized. For a numeric split variable this 
results in the Pearson correlation coefficient
$$
c(t_{j},\mu_{j},\Sigma_{j}) = \left|\frac{t_{j} - \mu_{j}}{\sqrt{\Sigma_{j}}}\right|.
$$
Otherwise the standardized form is
$$
c(t_{j},\mu_{jk},\Sigma_{j}) = \max_{l=1,\ldots,L}\left|\frac{(t_{j} - \mu_{j})_l}{\sqrt{(\Sigma_{j})_{ll}}}\right|.
$$
The smaller the p-value corresponding to the standardized test 
statistic $c(t_{j},\mu_{j},\Sigma_{j})$ is the stronger the discrepancy 
from the assumption of independence between the scores and the split 
variable $Z_j$.

\item After Bonferroni adjusting the p-values check whether any of 
the resulting p-values is beneath the selected significance level. 
If so, choose the partitioning variable $Z_{j^\ast}$ with the lowest 
p-value to the scores corresponding to the distribution parameters 
relevant for the split.

\item Choose the breakpoint that leads to the highest discrepancy 
between score functions in the two resulting subgroups. This 
difference can be measured by the linear statistic
$$
T_{j^{\ast}}^r = \sum_{i \in \mathcal{B}_{1r}} s(\hat{\theta}, y_i)
$$
where $\mathcal{B}_{1r}$ is the first of the two new subgroups that 
are defined by splitting in split point $r$ of variable $Z_{j^{\ast}}$. 
The split point is then chosen as follows:
$$
r^{\ast} = \argmin_{r} c(t_{j^{\ast}}^r,\mu_{j^{\ast}}^r,\Sigma_{j^{\ast}}^r).
$$

\item Repeat the testing and splitting procedure in each of the resulting 
subgroups until some stopping criterion is reached. This criterion 
can for example be a minimal number of observations in a node or a 
minimal p-value for the statistical tests. 

\end{itemize}

This permutation test based tree algorithm is presented in 
\cite{Hothorn+Hornik+Zeileis:2006} as the ctree algorithm. 
A different framework to build a tree is provided by the MOB algorithm 
which is based on M-fluctuation tests (\citealt{Zeileis+Hothorn+Hornik:2008}).


\section{Probability integral transform (PIT) histograms}
Probability integral transform (PIT) histograms 
\cite{Gneiting+Balabdaoui+Raftery:2007} are a commonly used tool to 
assess the predictive power of distributional forecasting methods, 
particularly in the field of meteorological forecasting.

Let $x$ be an observation with true distribution function $G$ and 
predicted distribution function $F$. Then the corresponding probability 
integral transform (PIT) value is $p = F(x)$. In the ideal case of a 
perfect prediction and $F$ being continuous $p$ follows a uniform distribution. 
This uniformity can be analysed applying a histogram. The closer the bars 
of a PIT histogram are to the value 1, the closer the distribution of $p$ 
is to a uniform distribution and therefore the better the predicted 
distribution models the true distribution.

The PIT histograms in Figure~\ref{pit_oosample} correspond to the forecasts 
presented in Section~\ref{Application for one station}. Daily observations 
of total precipitation from station Axams are considered and predictions 
are made for the 24th of July 2009, 2010, 2011 and 2012, based on data 
from the previous 24 years.


\begin{figure}[t!]
\centering
%\begin{subfigure}{0.8\textwidth}
<<rain_axams_pit_oosample, fig=TRUE, echo=FALSE, width = 6.5>>=
    
# out of sample
set.seed(4)

par(mfrow = c(2,2))
pithist(pit_df_t, nsim = 1000, breaks = seq(0, 1, length.out = 9), main = "Distributional forest", ylim = c(0,1.5))
pithist(pit_ml_t, nsim = 1000, breaks = seq(0, 1, length.out = 9), main = "EMOS", ylim = c(0,1.5))
pithist(pit_g_t, nsim = 1000, breaks = seq(0, 1, length.out = 9), main = "Prespecified GAMLSS", ylim = c(0,1.5))
pithist(pit_gb_t, nsim = 1000, breaks = seq(0, 1, length.out = 9), main = "Boosted GAMLSS", ylim = c(0,1.5))
      
@
%\end{subfigure}
\caption{\label{pit_oosample}Out-of-sample PIT histograms (2009--2012) for station Axams and models learned on data from 1985--2008.}
\end{figure}

\end{appendix}


\end{document}
