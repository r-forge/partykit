\documentclass[nojss]{jss}

%\VignetteIndexEntry{disttree: Distributional Regression Forests for Probabilistic Precipitation Forecasting in Complex Terrain}
%\VignetteDepends{partykit, gamlss.dist}
%\VignetteKeywords{regression trees, random forests, distributional regression, recursive partitioning}
%\VignettePackage{disttree}

%% packages
\usepackage{amstext}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{thumbpdf,lmodern}
%\usepackage{rotating}
%\usepackage{caption}
%\captionsetup{format=hang}
%\usepackage{subcaption}
\usepackage{Sweave}
%\usepackage{enumitem}
%\usepackage{graphicx}
\usepackage{array, makecell}
\usepackage{tikz}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{positioning,shapes,arrows,decorations.pathreplacing,calc,automata,mindmap}
%\usepackage[pagebackref=true]{hyperref}
%\usepackage{doi}

%% commands
\newcommand{\fixme}[1]{\emph{\marginpar{FIXME} (#1)}}
\renewcommand{\Prob}{\mathbb{P} }
\renewcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\Var}{\mathbb{V}}
\newcommand{\R}{\mathbb{R} }
\newcommand{\N}{\mathbb{N} }
\newcommand{\C}{\mathbb{C} }
\newcommand{\argmin}{\operatorname{argmin}\displaylimits}
\newcommand{\argmax}{\operatorname{argmax}\displaylimits}
\newcommand{\LS}{\mathcal{L}_n}
\newcommand{\TS}{\mathcal{T}_n}
\newcommand{\LSc}{\mathcal{L}_{\text{comb},n}}
\newcommand{\LSbc}{\mathcal{L}^*_{\text{comb},n}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\yn}{y_{\text{new}}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\sX}{\mathcal{X}}
\newcommand{\sY}{\mathcal{Y}}
\newcommand{\T}{\mathbf{T}}
\newcommand{\x}{\mathbf{x}}
\renewcommand{\a}{\mathbf{a}}
\newcommand{\xn}{\mathbf{x}_{\text{new}}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\ws}{\mathbf{w}_\cdot}
\renewcommand{\t}{\mathbf{t}}
\newcommand{\M}{\mathbf{M}}
\renewcommand{\vec}{\text{vec}}
\newcommand{\B}{\mathbf{B}}
\newcommand{\K}{\mathbf{K}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\cellx}{\pi_n[\x]}
\newcommand{\partn}{\pi_n(\mathcal{L}_n)}
\newcommand{\err}{\text{Err}}
\newcommand{\ea}{\widehat{\text{Err}}^{(a)}}
\newcommand{\ecv}{\widehat{\text{Err}}^{(cv1)}}
\newcommand{\ecvten}{\widehat{\text{Err}}^{(cv10)}}
\newcommand{\eone}{\widehat{\text{Err}}^{(1)}}
\newcommand{\eplus}{\widehat{\text{Err}}^{(.632+)}}
\newcommand{\eoob}{\widehat{\text{Err}}^{(oob)}}
\newcommand{\bft}{\mathbf{t}}

\hyphenation{Qua-dra-tic}

%% colors
\definecolor{HighlightOrange}{rgb}{0.9490196,0.5725490,0.0000000}
\definecolor{HighlightBlue}{rgb}{0.4784314,0.7490196,0.9803922}



\SweaveOpts{engine=R, eps=FALSE, keep.source=TRUE}
<<preliminaries, echo=FALSE, results=hide>>=
options(prompt = "R> ", continue = "+  ", useFancyQuotes = FALSE, width = 70)
library("ggplot2")
theme_set(theme_bw(base_size = 18))
library("disttree")
library("Formula")  # FIX ME: should be imported in disttree
library("gamlss")
library("randomForest")
library("lattice")
library("crch")
library("latex2exp")
library("parallel")
library("gamlss.cens")
gen.cens(NO, type = "left")
library("mgcv")
library("MASS")
library("countreg")

## HCL palette
pal <- hcl(c(10, 128, 260, 290, 50), 100, 50)
names(pal) <- c("forest", "tree", "gamlss", "gamboostLSS", "EMOS")

pallight <- hcl(c(10, 128, 260, 290, 70), 100, 50, alpha = 0.25)
names(pallight) <- c("forest", "tree", "gamlss", "gamboostLSS", "EMOS")

transpgrey <- rgb(0.190,0.190,0.190, alpha = 0.2)


# dist_list_cens_normal
{
  
  dist_list_cens_normal <- list()
  
  parnames <- c("mu", "sigma")
  etanames <- c("mu", "log(sigma)")
  
  ddist <-  function(y, eta, log = TRUE, weights = NULL, sum = FALSE, left = 0, right = Inf) {     
    par <- c(eta[1], exp(eta[2]))
    val <- crch::dcnorm(x = y, mean = par[1], sd = par[2], left = left, right = right, log = log)
    if(sum) {
      if(is.null(weights)) weights <- if(is.matrix(y)) rep.int(1, dim(y)[1]) else rep.int(1, length(y))
      val <- sum(weights * val, na.rm = TRUE)
    }
    return(val)
  }
  
  
  sdist <- function(y, eta, weights = NULL, sum = FALSE, left = 0, right = Inf) {   
    par <- c(eta[1], exp(eta[2]))
    # y[y==0] <- 1e-323
    
    score_m <- crch:::scnorm(x = y, mean = par[1], sd = par[2], which = "mu", left = left, right = right)
    score_s <- crch:::scnorm(x = y, mean = par[1], sd = par[2], which = "sigma", left = left, right = right) * exp(eta[2]) # inner derivation exp(eta[2])
    score <- cbind(score_m, score_s)
    score <- as.matrix(score)
    colnames(score) <- etanames
    if(sum) {
      if(is.null(weights)) weights <- rep.int(1, length(y)[1])
      # if score == Inf replace score with 1.7e308 because Inf*0 would lead to NaN (0 in weights)
      score[score==Inf] = 1.7e308
      score <- colSums(weights * score, na.rm = TRUE)
      #if(any(is.nan(score))) print(c(eta, "y", y))
    }
    return(score)
  }
  
  
  hdist <- function(y, eta, weights = NULL, left = 0, right = Inf) {    
    ny <- length(y)
    if(is.null(weights)) weights <- rep.int(1, ny)
    
    par <- c(eta[1], exp(eta[2]))                           
    # y[y==0] <- 1e-323
    
    d2mu <- crch:::hcnorm(x = y, mean = par[1], sd = par[2], which = "mu", left = left, right = right)
    d2sigma <- crch:::hcnorm(x = y, mean = par[1], sd = par[2], which = "sigma", left = left, right = right)
    dmudsigma <- crch:::hcnorm(x = y, mean = par[1], sd = par[2], which = "mu.sigma", left = left, right = right) # FIX: order?
    dsigmadmu <- crch:::hcnorm(x = y, mean = par[1], sd = par[2], which = "sigma.mu", left = left, right = right) # FIX: order?
    dsigma <- crch:::scnorm(x = y, mean = par[1], sd = par[2], which = "sigma", left = left, right = right)
    
    d2ld.etamu2 <- sum(weights * d2mu, na.rm = TRUE)
    d2ld.etamu.d.etasigma <- sum(weights * dmudsigma * par[2], na.rm = TRUE)
    d2ld.etasigma.d.etamu <- sum(weights * dsigmadmu * par[2], na.rm = TRUE)
    d2ld.etasigma2 <- sum(weights * (d2sigma * exp(2*eta[2]) + dsigma * par[2]), na.rm = TRUE)         
    
    hess <- matrix(c(d2ld.etamu2, d2ld.etamu.d.etasigma, d2ld.etasigma.d.etamu, d2ld.etasigma2), nrow = 2)
    colnames(hess) <- rownames(hess) <-  etanames
    
    return(hess)
  }
  
  
  ## additional functions pdist, qdist, rdist
  pdist <- function(q, eta, lower.tail = TRUE, log.p = FALSE) crch:::pcnorm(q, mean = eta[1], sd = eta[2], 
                                                                            lower.tail = lower.tail, log.p = log.p, 
                                                                            left = left, right = right)
  qdist <- function(p, eta, lower.tail = TRUE, log.p = FALSE) crch:::qcnorm(p, mean = eta[1], sd = eta[2], 
                                                                            lower.tail = lower.tail, log.p = log.p, 
                                                                            left = left, right = right)
  rdist <- function(n, eta) crch:::rcnorm(n, mean = eta[1], sd = eta[2], left = left, right = right)

  
  link <- c("identity", "log")
  
  linkfun <- function(par) {
    eta <- c(par[1], log(par[2]))
    names(eta) <- etanames
    return(eta)
  }
  
  
  linkinv <- function(eta) {
    par <- c(eta[1], exp(eta[2]))
    names(par) <- parnames
    return(par)
  }
  
  
  linkinvdr <- function(eta) {
    dpardeta <- c(1, exp(eta[2]))
    names(dpardeta) <- parnames
    return(dpardeta)
  }
  
  
  startfun <- function(y, weights = NULL){
    yc <- pmax(0,y)  # optional ?
    if(is.null(weights)) {
      mu <- mean(yc)
      sigma <- sqrt(1/length(yc) * sum((yc - mu)^2))
    } else {
      mu <- weighted.mean(yc, weights)
      sigma <- sqrt(1/sum(weights) * sum(weights * (yc - mu)^2))
    }
    starteta <- c(mu, log(sigma))
    names(starteta) <- etanames
    return(starteta)
  }
  
  mle <- FALSE
  
  dist_list_cens_normal <- list(family.name = "censored Normal Distribution",
                                ddist = ddist, 
                                sdist = sdist, 
                                hdist = hdist,
                                pdist = pdist,
                                qdist = qdist,
                                rdist = rdist,
                                link = link, 
                                linkfun = linkfun, 
                                linkinv = linkinv, 
                                linkinvdr = linkinvdr,
                                startfun = startfun,
                                mle = mle,
                                gamlssobj = FALSE,
                                censored = TRUE
  )
}





  



# plot RMSE
plot_rmse <- function(simres, type = c("exp", "par"), ylim = NULL, legend = TRUE){
  
  if(type == "exp"){
    rmse <- NULL
    colnames <- NULL
    col <- NULL
    legendnames <- NULL
    if("av.rmse.exp.true.dt" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.dt"], simres$resmat[,"av.rmse.exp.obs.dt"])
      colnames <- c(colnames, "dt.true", "dt.obs")
      col <- c(col, pal["tree"])
      legendnames <- c(legendnames, "tree")
    }
    if("av.rmse.exp.true.df" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.df"], simres$resmat[,"av.rmse.exp.obs.df"])
      colnames <- c(colnames, "df.true", "df.obs")
      col <- c(col, pal["forest"])
      legendnames <- c(legendnames, "distributional forest")
    }
    if("av.rmse.exp.true.g" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.g"], simres$resmat[,"av.rmse.exp.obs.g"])
      colnames <- c(colnames, "g.true", "g.obs")
      col <- c(col, pal["gamlss"])
      legendnames <- c(legendnames, "gamlss")
    }
    if("av.rmse.exp.true.b" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.b"], simres$resmat[,"av.rmse.exp.obs.b"])
      colnames <- c(colnames, "b.true", "b.obs")
      col <- c(col, pal["bamlss"])
      legendnames <- c(legendnames, "bamlss")
    }
    if("av.rmse.exp.true.gb" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.gb"], simres$resmat[,"av.rmse.exp.obs.gb"])
      colnames <- c(colnames, "gb.true", "gb.obs")
      col <- c(col, pal["gamboostLSS"])
      legendnames <- c(legendnames, "gamboostLSS")
    }
    if("av.rmse.exp.true.rf" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.rf"], simres$resmat[,"av.rmse.exp.obs.rf"])
      colnames <- c(colnames, "rf.true", "rf.obs")
      col <- c(col, pal["randomForest"])
      legendnames <- c(legendnames, "randomForest")
    }
    if("av.rmse.exp.true.cf" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.cf"], simres$resmat[,"av.rmse.exp.obs.cf"])
      colnames <- c(colnames, "cf.true","cf.obs")
      col <- c(col, pal["cforest"])
      legendnames <- c(legendnames, "cforest")
    }
    
    colnames(rmse) <- colnames
    if(is.null(ylim)) ylim <- c(min(na.omit(rmse)), max(na.omit(rmse)))
    
    plot(x = simres$x.axis, y = rmse[,1], type = "l", col = col[1], ylim = ylim,
         xlab = expression(kappa), ylab = "RMSE")
    lines(x = simres$x.axis, y = rmse[,2], type = "l", lty = 2, col = col[1])
    
    for(i in 1:(length(col)-1)){
      lines(x = simres$x.axis, y = rmse[,(2*i+1)], type = "l", col = col[i+1])
      lines(x = simres$x.axis, y = rmse[,(2*i+2)], type = "l", lty = 2, col = col[i+1])
    }
    if(legend) legend('topleft', legendnames, 
                      col = col, lty = 1, cex = 0.7)
  } 
  
  if(type == "par"){
    
    rmse <- NULL
    colnames <- NULL
    col <- NULL
    legendnames <- NULL
    if("av.rmse.exp.true.dt" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.dt"], simres$resmat[,"av.rmse.sigma.dt"])
      colnames <- c(colnames, "dt.true", "dt.obs")
      col <- c(col, pal["tree"])
      legendnames <- c(legendnames, "tree")
    }
    if("av.rmse.exp.true.df" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.df"], simres$resmat[,"av.rmse.sigma.df"])
      colnames <- c(colnames, "df.true", "df.obs")
      col <- c(col, pal["forest"])
      legendnames <- c(legendnames, "distributional forest")
    }
    if("av.rmse.exp.true.g" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.g"], simres$resmat[,"av.rmse.sigma.g"])
      colnames <- c(colnames, "g.true", "g.obs")
      col <- c(col, pal["gamlss"])
      legendnames <- c(legendnames, "gamlss")
    }
    if("av.rmse.exp.true.b" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.b"], simres$resmat[,"av.rmse.sigma.b"])
      colnames <- c(colnames, "b.true", "b.obs")
      col <- c(col, pal["bamlss"])
      legendnames <- c(legendnames, "bamlss")
    }
    if("av.rmse.exp.true.gb" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.gb"], simres$resmat[,"av.rmse.sigma.gb"])
      colnames <- c(colnames, "gb.true", "gb.obs")
      col <- c(col, pal["gamboostLSS"])
      legendnames <- c(legendnames, "gamboostLSS")
    }
    if("av.rmse.exp.true.rf" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.rf"], simres$resmat[,"av.rmse.sigma.rf"])
      colnames <- c(colnames, "rf.true", "rf.obs")
      col <- c(col, pal["randomForest"])
      legendnames <- c(legendnames, "randomForest")
    }
    if("av.rmse.exp.true.cf" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.cf"], simres$resmat[,"av.rmse.sigma.cf"])
      colnames <- c(colnames, "cf.true","cf.obs")
      col <- c(col, pal["cforest"])
      legendnames <- c(legendnames, "cforest")
    }
    
    colnames(rmse) <- colnames
    if(is.null(ylim)) ylim <- c(min(na.omit(rmse)), max(na.omit(rmse)))
    
    plot(x = simres$x.axis, y = rmse[,1], type = "l", col = col[1], ylim = ylim,
         xlab = expression(kappa), ylab = "RMSE")
    lines(x = simres$x.axis, y = rmse[,2], type = "l", lty = 2, col = col[1])
    
    for(i in 1:(length(col)-1)){
      lines(x = simres$x.axis, y = rmse[,(2*i+1)], type = "l", col = col[i+1])
      lines(x = simres$x.axis, y = rmse[,(2*i+2)], type = "l", lty = 2, col = col[i+1])
    }
    if(legend) legend('topleft', legendnames, 
                      col = col, lty = 1, cex = 0.7)
  }
}

# plot loglikelihood
plot_ll <- function(simres, ylim = NULL, legend = TRUE){
  
  ll <- NULL
  colnames <- NULL
  col <- NULL
  legendnames <- NULL
  if("av.rmse.exp.true.dt" %in% colnames(simres$resmat)) {
    ll <- cbind(ll, simres$resmat[,"av.loglik.dt"])
    colnames <- c(colnames, "dt.ll")
    col <- c(col, pal["tree"])
    legendnames <- c(legendnames, "tree")
  }
  if("av.rmse.exp.true.df" %in% colnames(simres$resmat)) {
    ll <- cbind(ll, simres$resmat[,"av.loglik.df"])
    colnames <- c(colnames, "df.ll")
    col <- c(col, pal["forest"])
    legendnames <- c(legendnames, "distributional forest")
  }
  if("av.rmse.exp.true.g" %in% colnames(simres$resmat)) {
    ll <- cbind(ll, simres$resmat[,"av.loglik.g"])
    colnames <- c(colnames, "g.true")
    col <- c(col, pal["gamlss"])
    legendnames <- c(legendnames, "gamlss")
  }
  if("av.rmse.exp.true.b" %in% colnames(simres$resmat)) {
    ll <- cbind(ll, simres$resmat[,"av.loglik.b"])
    colnames <- c(colnames, "b.true")
    col <- c(col, pal["bamlss"])
    legendnames <- c(legendnames, "bamlss")
  }
  if("av.rmse.exp.true.gb" %in% colnames(simres$resmat)) {
    ll <- cbind(ll, simres$resmat[,"av.loglik.gb"])
    colnames <- c(colnames, "gb.true")
    col <- c(col, pal["gamboostLSS"])
    legendnames <- c(legendnames, "gamboostLSS")
  }
  if("av.rmse.exp.true.rf" %in% colnames(simres$resmat)) {
    ll <- cbind(ll, simres$resmat[,"av.loglik.rf"])
    colnames <- c(colnames, "rf.true")
    col <- c(col, pal["randomForest"])
    legendnames <- c(legendnames, "randomForest")
  }
  if("av.rmse.exp.true.cf" %in% colnames(simres$resmat)) {
    ll <- cbind(ll, simres$resmat[,"av.loglik.cf"])
    colnames <- c(colnames, "cf.true")
    col <- c(col, pal["cforest"])
    legendnames <- c(legendnames, "cforest")
  }
  
  colnames(ll) <- colnames
  if(is.null(ylim)) ylim <- c(min(na.omit(ll)), max(na.omit(ll)))
  
  plot(x = simres$x.axis, y = ll[,1], type = "l", col = col[1], ylim = ylim,
       xlab = expression(kappa), ylab = "log-likelihood")
  
  for(i in 2:length(col)){
    lines(x = simres$x.axis, y = ll[,i], type = "l", col = col[i])
  }
  if(legend) legend('topleft', legendnames, 
                    col = col, lty = 1, cex = 0.7)
}

# plot crpse
plot_crps <- function(simres, ylim = NULL, legend = TRUE){
  
  crps <- NULL
  colnames <- NULL
  col <- NULL
  legendnames <- NULL
  if("av.rmse.exp.true.dt" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.dt"])
    colnames <- c(colnames, "dt.crps")
    col <- c(col, pal["tree"])
    legendnames <- c(legendnames, "tree")
  }
  if("av.rmse.exp.true.df" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.df"])
    colnames <- c(colnames, "df.crps")
    col <- c(col, pal["forest"])
    legendnames <- c(legendnames, "distributional forest")
  }
  if("av.rmse.exp.true.g" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.g"])
    colnames <- c(colnames, "g.true")
    col <- c(col, pal["gamlss"])
    legendnames <- c(legendnames, "gamlss")
  }
  if("av.rmse.exp.true.b" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.b"])
    colnames <- c(colnames, "b.true")
    col <- c(col, pal["bamlss"])
    legendnames <- c(legendnames, "bamlss")
  }
  if("av.rmse.exp.true.gb" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.gb"])
    colnames <- c(colnames, "gb.true")
    col <- c(col, pal["gamboostLSS"])
    legendnames <- c(legendnames, "gamboostLSS")
  }
  if("av.rmse.exp.true.rf" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.rf"])
    colnames <- c(colnames, "rf.true")
    col <- c(col, pal["randomForest"])
    legendnames <- c(legendnames, "randomForest")
  }
  if("av.rmse.exp.true.cf" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.cf"])
    colnames <- c(colnames, "cf.true")
    col <- c(col, pal["cforest"])
    legendnames <- c(legendnames, "cforest")
  }
  
  colnames(crps) <- colnames
  if(is.null(ylim)) ylim <- c(min(na.omit(crps)), max(na.omit(crps)))
  
  plot(x = simres$x.axis, y = crps[,1], type = "l", col = col[1], ylim = ylim,
       xlab = expression(kappa), ylab = "CRPS")
  
  for(i in 2:length(col)){
    lines(x = simres$x.axis, y = crps[,i], type = "l", col = col[i])
  }
  if(legend) legend('topright', legendnames, 
                    col = col, lty = 1, cex = 1, bty = "n")
}
@



\title{Distributional Regression Forests for Probabilistic Precipitation Forecasting in Complex Terrain}
\Shorttitle{Distributional Regression Forests for Probabilistic Precipitation Forecasting}

\author{Lisa Schlosser\\Universit\"at Innsbruck
   \And Torsten Hothorn\\Universit\"at Z\"urich
   \And Reto Stauffer\\Universit\"at Innsbruck
   \And Achim Zeileis\\Universit\"at Innsbruck}
\Plainauthor{Lisa Schlosser, Torsten Hothorn, Reto Stauffer, Achim Zeileis}

\Abstract{
While many regression models consider only the mean of the response variable recent advances in parametric modeling have made it possible to model all distribution parameters. In that way the underlying distribution can be fully specified allowing for a wide range of analysis and prediction methods which are particularly valuable in applications such as probabilistic weather forecasting. In generalized additive models for location, scale, and shape %(GAMLSS, \citealt{stasinopoulos2005}) 
each distribution parameter can be modeled separately by its own generalized additive model (GAM) capturing linear and non-linear effects. A different approach that would make it possible to also model non-additive effects and interactions is to apply tree-structured models such as regression trees %(\citealt{loh2011classification}) 
or random forests. %(\citealt{breiman2001random}). 
%%%<TH> "random forest" generisch; Breiman and Cutler's randomForest </TH>
This class of tree and forest models will now be integrated in probabilistic forecasting providing a novel general distributional framework for these methods. In particular, we are going to introduce distributional trees and forests which embed the idea of flexible parametric modeling into the framework of regression trees and random forests. 
%%%<TH> break-up very long sentence </TH>
Applying these novel approaches to precipitation data in a mountainous region with a very large number of numerical weather prediction quantities shows one of their main advantages in selecting variables automatically and illustrates their strengths as their performance is compared to GAMLSS models that are specified based either on subject-matter knowledge and experience from meteorology or based on a computationally more demanding boosting approach.


%For example in the field of probabilistic weather forecasting linear models such as ensemble model output statistics (EMOS, \citealt{gneiting2005calibrated}) provide common and widely used tools allowing for clear interpretations. Usually acting on the assumption of a normally distributed response variable both parameters, the mean and the variance, depend linearly on covariates in this type of model. However, the restriction to model only linear effects requires further extensions which allow for more flexibility. Generalized additive models for location, scale, and shape (GAMLSS, \citealt{stasinopoulos2005}) can also capture non-linear effects as each distribution parameter can be modeled separately by its own generalized additive model (GAM). A different approach that would make it possible to also model non-additive effects and interactions is to apply tree-structured models such as regression trees (\citealt{loh2011classification}) or random forests (\citealt{breiman2001random}). This class of models will now be integrated in probabilistic forecasting providing a novel general distributional framework for these methods. We are going to introduce distributional trees and forests which embed the idea of flexible parametric modeling into the framework of regression trees and random forests. Applying these novel approaches to precipitation data in a mountainous region with a large number of numerical weather prediction quantities illustrates their strengths as their performance is compared to GAMLSS models that are specified based either on subject-matter knowledge and experience from meteorology or based on a computationally more demanding boosting approach.

%In regression analysis one is interested in the relationship between a dependent variable and one or more explanatory variables. Various methods to fit statistical models to the data set have been developed, starting from linear models considering only the mean of the response variable and ranging to probabilistic models where all parameters of a distribution are fit to the given data set.\\
%If there is a strong variation within the data it might be advantageous to split the data first into more homogeneous subgroups based on given covariates and then fit a local model in each subgroup rather than fitting one global model to the whole data set. This can be done by applying regression trees and forests.\\
%Both of these two concepts, parametric modeling and algorithmic trees, have been investigated and developed further, however, mostly separated from each other. Therefore, our goal is to embed the progress made in the field of probabilistic modeling in the idea of algorithmic tree and forest models. In particular, more flexible models such as GAMLSS (\citealt{stasinopoulos2005}) should be fitted in the nodes of a tree in order to capture location, scale, and shape as well as censoring, tail behavior etc. while non-additive effects of the explanatory variables can be detected automatically by the splitting algorithm used to build the tree. In distributional forests an ensemble of distributional trees is built and used to calculate weights which are then included in the fitting process of a distributional model. These forest models can detect smooth effects as well as abrupt changes and interactions in a reasonable way without needing any kind of variable selection or information about the expected effects advance and therefore offers a good compromise particularly in complex settings. 
}
\Keywords{parametric models, regression trees, random forests, recursive partitioning, probabilistic forecasting, distributional regression}

\Address{
  Lisa Schlosser, Reto Stauffer, Achim Zeileis \\
  Universit\"at Innsbruck \\
  Department of Statistics \\
  Faculty of Economics and Statistics \\
  Universit\"atsstr.~15 \\
  6020 Innsbruck, Austria \\
  E-mail: \email{Lisa.Schlosser@uibk.ac.at}, \email{Reto.Stauffer@uibk.ac.at},\\
  \email{Achim.Zeileis@R-project.org} \\
  URL: \url{https://www.uibk.ac.at/statistics/personal/schlosser-lisa/},\\
  \url{http://retostauffer.org/}, \url{https://eeecon.uibk.ac.at/~zeileis/}\\
  
  Torsten Hothorn\\
  Universit\"at Z\"urich \\
  Institut f\"ur Epidemiologie, Biostatistik und Pr\"avention \\
  Hirschengraben 84\\
  CH-8001 Z\"urich, Switzerland \\
  E-mail: \email{Torsten.Hothorn@R-project.org}\\
  URL: \url{http://user.math.uzh.ch/hothorn/}\\
  
}

\begin{document}
\SweaveOpts{concordance=TRUE}


\SweaveOpts{eps=FALSE, keep.source=TRUE, eval = TRUE}


\SweaveOpts{eval = TRUE}
\section{Introduction}

In regression analysis a wide range of models has been developed to describe the
relationship between a response variable and a set of covariates. The classical
model is the linear model (LM) where the conditional mean of the response
is modeled through a linear function of the covariates (see the left panel of
Figure~\ref{Devel_parmod} for a schematic illustration). Over the last decades
this has been extended in various directions including:
\begin{itemize}
  \item \emph{Generalized linear models} (GLMs, \citealp{Nelder+Wedderburn:1972})
    encompassing an additional nonlinear link function for the conditional mean.
  \item \emph{Generalized additive models} (GAMs, \citealp{Hastie+Tibshirani:1986})
    allowing for smooth nonlinear effects in the covariates
    (Figure~\ref{Devel_parmod}, middle).
  \item \emph{Generalized additive models for location, scale, and shape}
    (GAMLSS, \citealt{stasinopoulos2005}) adopting a probabilistic modeling
    approach. In GAMLSS, each parameter of a statistical distribution can depend
    on an additive predictor of the covariates comprising linear and/or
    smooth nonlinear terms (Figure~\ref{Devel_parmod}, right).
\end{itemize}
Thus, the above-mentioned models provide a broad toolbox for capturing different
aspects of the response (mean only vs.\ full distribution) and different types
of dependencies on the covariates (linear vs.\ nonlinear additive terms).

\begin{figure}[t!]
\begin{tikzpicture}
\node (a) at (0,0){
\minipage{0.29\textwidth}
\centering
<<motivation_GLM, echo=FALSE, results=hide>>=
nobs <- 200
## GLM
set.seed(7)
x <- c(1:nobs)/nobs
ytrue <- 1+x*1.5
y <- ytrue + rnorm(nobs,0,0.3)
@

<<plot_motivation_GLM, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(y=y , x=x, xaxt="n", yaxt="n", ann=FALSE, col = "slategray", pch = 19)
box(lwd=5)
lines(x = x, y = ytrue, col = pal["forest"], lwd = 7, main = "")
@
\endminipage};
\node (b) at (5,0) 
{
\minipage{0.29\textwidth}
\centering
<<motivation_GAM, echo=FALSE, results=hide>>=
## GAM
set.seed(7)
x <- c(1:nobs)/nobs
x <- 2*(x-0.5)
ytrue <- x^3 
y <- ytrue + rnorm(nobs,0,0.3)
@

<<plot_motivation_GAM, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(y=y , x=x, xaxt="n", yaxt="n", ann=FALSE, col = "slategray", pch = 19)
box(lwd=5)
lines(x = x, y = ytrue, col = pal["forest"], lwd = 7, main = "")
@
\endminipage};
\node (c) at (10,0) 
{
\minipage{0.29\textwidth}
\centering
<<motivation_GAMLSS, echo=FALSE, results=hide>>=
## GAMLSS
set.seed(7)
x <- c(1:nobs)/nobs
x <- 2*(x-0.5)
ytrue <- x^3
var <- exp(-(2*x)^2)/2
y <- ytrue + rnorm(nobs, 0, 0.1 + var)
@

<<plot_motivation_GAMLSS, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(x, y, xaxt = "n", yaxt = "n", ann = FALSE, type = "n")
polygon(c(x, rev(x)), c(ytrue + 0.1 + var, rev(ytrue - 0.1 - var)),
  col = pallight["forest"], border = "transparent")
lines(x, ytrue, col = pal["forest"], lwd=7)
points(x, y, col = "slategray", pch = 19)
box(lwd = 5)
@
\endminipage};
\node (d) at (0,-2) 
{
\minipage{0.29\textwidth}
\centering
\vspace{0.0cm}
LM, GLM  
\endminipage};
\node (e) at (5,-2) 
{
\minipage{0.29\textwidth}
\centering
\vspace{0.0cm}
GAM 
\endminipage};
\node at (10,-2)
{
\minipage{0.28\textwidth}
\centering
\vspace{0.0cm}
GAMLSS 
\endminipage};
\draw[-{>[scale=4, length=2, width=3]},line width=0.4pt](2.1,0)--(2.9,0);
\draw[-{>[scale=4, length=2, width=3]},line width=0.4pt](7.1,0)--(7.9,0);
\end{tikzpicture}
\caption{\label{Devel_parmod}Parametric modeling developments. (Generalized) linear models (left), generalized additive models (middle), generalized additive models for location, scale, and shape (right).}
\end{figure}


While in many applications conditional mean regression models have been receiving
the most attention, there has been a paradigm shift over the last decade towards
distributional regression models. An important reason for this is that in many
fields forecasts of the mean are not the only (or not even the main) concern but
instead there is an increasing interest in probabilistic forecasts. Quantities of
interest typically include exceedence probabilities for certain thresholds of the
response or quantiles of the response distribution. As an example, consider
weather forecasting where there is less interest in the mean amount of
precipitation on the next day. Instead, the probability of rain vs.\ no rain
is typically more relevant or, in some situations, a prediction interval of
expected precipitation (say from the expected 10\% to 90\% quantile). Similar
considerations apply for other meteorological quantities and hence attention
in the weather forecasting literature has been shifting from classical linear
models \citep{Glahn+Lowry:1972} towards probabilistic models such as the
non-homogeneous Gaussian regression (NGR) of \cite{gneiting2005calibrated}.
The NGR typically describes the mean of some meteorological response variable through
the average of the corresponding quantity from an ensemble of physically-based
numerical weather predictions (NWPs). Similarly, the variance of the response
is captured through the variance of the ensemble of NWP outputs. Thus, the NGR
considers both the mean as well as the uncertainty of the NWP model outputs
to obtain probabilistic forecasts calibrated to a particular site.

%% late: introduce MOS and EMOS? more details about precipitation forecasting
%In ensemble model output statistics (EMOS, \cite{gneiting2005calibrated}) this 
%idea is taken one step further. Usually acting on the assumption of a normally distributed
%response variable this model allows for both parameters, the mean and the variance, to depend 
%linearly on covariates provided by NWPs. However, the restriction to model only linear effects 
%has risen the interest in applying more flexible models such as offered by the GAMLSS framework.

\begin{figure}[t!]
\begin{tikzpicture}
\node (a) at (0,0){
\minipage{0.29\textwidth}
\centering
<<motivation_regtree, echo=FALSE, results=hide>>=
## Reg. Tree
set.seed(7)
kappa <- 12
x <- c(1:nobs)/nobs
ytrue <- ytree <- yforest <- numeric(length = length(x))
for(i in 1:nobs) ytrue[i] <- if(x[i]<1/3) 0.5 else 1+(1-plogis(kappa*(2*(x[i]-0.2)-1)))
y <- ytrue + rnorm(nobs,0,0.3)
for(i in 1:nobs) ytree[i] <- if(x[i]<1/3) 0.5 else {if(x[i]<2/3) 2 else 1}
@

<<plot_motivation_regtree, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(x = x, y = y, xaxt="n", yaxt="n", ann=FALSE, col = "slategray", pch = 19)
box(lwd=5)
#lines(x = x, y = ytrue, col = "grey", lwd=5, main = "")
lines(x = x, y = ytree, col = pal["forest"], lwd=7)
@
\endminipage};
\node (b) at (5,0) 
{
\minipage{0.29\textwidth}
\centering
<<motivation_randforest, echo=FALSE, results=hide>>=
## Random Forest
for(i in 1:nobs) yforest[i] <- if(x[i]<0.27) 0.5 else { if(x[i]<0.39) 0.5 + 1.5*(plogis((x[i]-0.33)/6*700)) else 1+(1-plogis(kappa*(2*(x[i]-0.2)-1)))}
@

<<plot_motivation_randforest, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(x = x, y = y, xaxt="n", yaxt="n", ann=FALSE, col = "slategray", pch = 19)
box(lwd=5)
#lines(x = x, y = ytrue, col = "grey", lwd=5, main = "")
lines(x = x, y = yforest, col = pal["forest"], lwd=7, main = "")
@
\endminipage};
\node (c) at (10,0) 
{
\minipage{0.29\textwidth}
\centering
<<motivation_distforest, echo=FALSE, results=hide>>=
## distforest
set.seed(4723)
for(i in 1:nobs) yforest[i] <- if(x[i]<0.27) 0.5 else { if(x[i]<0.39) 0.5 + 1.5*(plogis((x[i]-0.33)/6*700)) else 1+(1-plogis(kappa*(2*(x[i]-0.2)-1)))}
xt <- 2*(x-0.5)
var <- exp(-(2*xt)^4)/2
y <- ytrue + rnorm(nobs, 0, 0.1 + var)
x <- c(1:nobs)/nobs
@

<<plot_motivation_distforest, fig=TRUE, echo=FALSE>>=
par(mar = c(2, 0, 2, 0))
plot(x, y, xaxt = "n", yaxt = "n", ann = FALSE, type = "n")
usr <- par("usr")
rect(usr[1], usr[3], usr[2], usr[4], col = gray(0.94), border = "transparent")
polygon(c(x, rev(x)), c(ytrue + 0.1 + var, rev(ytrue - 0.1 - var)),
  col = pallight["forest"], border = "transparent")
lines(x = x, y = yforest, col = pal["forest"], lwd=7, main = "")
points(x, y, col = "slategray", pch = 19)
box(lwd=8)
@
\endminipage};
\node (d) at (0,-2) 
{
\minipage{0.29\textwidth}
\centering
\vspace{0.0cm}
Regression Tree 
\endminipage};
\node (e) at (5,-2) 
{
\minipage{0.29\textwidth}
\centering
\vspace{0.0cm}
Random Forest 
\endminipage};
\node at (10,-2)
{
\minipage{0.28\textwidth}
\centering
\vspace{0.0cm}
Distributional Forest 
\endminipage};
\node (g) at (0,-5.4) 
{
\minipage{0.29\textwidth}
\vspace{0.2cm}
%\hspace{0.0cm}
\centering
<<motivation_disttree, echo=FALSE, results=hide>>=
## disttree
set.seed(74)
x <- c(1:nobs)/nobs
var <- ytree/3
y <- ytrue + rnorm(nobs,0,var)
@

<<plot_motivation_disttree, fig=TRUE, echo=FALSE>>=
par(mar = c(2, 0, 2, 0))
plot(x, y, xaxt="n", yaxt="n", ann=FALSE, type = "n")
usr <- par("usr")
rect(usr[1], usr[3], usr[2], usr[4], col = gray(0.94), border = "transparent")
lines(x = x, y = ytree, col = pal["forest"], lwd=7, main = "")
polygon(c(x, rev(x)), c(ytree + var, rev(ytree - var)),
  col = pallight["forest"], border = "transparent")
points(x, y, col = "slategray", pch = 19)
box(lwd=8)
@
\vspace{-0.2cm}%\\
Distributional Tree
\vspace{0.2cm}
\endminipage};
\draw[-{>[scale=4, length=2, width=3]},line width=0.4pt](2.1,0)--(2.9,0);
\draw[-{>[scale=4, length=2, width=3]},line width=0.9pt](7.1,0)--(7.9,0);
\draw[-{>[scale=4, length=2, width=3]},line width=0.9pt](0,-2.4)--(0,-3.2);
\draw[-{>[scale=4, length=2, width=3]},line width=0.9pt](2.2,-5.4)--(7.9,-2.5);
\end{tikzpicture}
\caption{\label{Devel_treeforest}Tree and forest developments.
Regression tree (top left), distributional tree (bottom left), random forest
(top middle), and distributional forest (top right).}
\end{figure}

In summary, the models discussed so far provide a broad and powerful toolset for
parametric distributional fits depending on a specified set of additive linear or
smooth nonlinear terms. A rather different approach to capturing the dependence on
covariates are tree-based models.
\begin{itemize}
  \item \emph{Regression trees} (\citealt{breiman1984classification}) recursively
    split the data into more homogeneous subgroups and can thus capture abrupt shifts
    (Figure~\ref{Devel_treeforest}, top left) and approximate nonlinear functions.
    Furthermore, trees automatically carry out a forward selection of covariates and
    their interactions. 
  \item \emph{Random forests} (\citealt{breiman2001random}) average the predictions of
    an ensemble of trees fitted to resampled versions of the learning data. This
    stabilizes the recursive partitions from individual trees and hence better
    approximates smooth functions (Figure~\ref{Devel_treeforest}, top middle)
\end{itemize}
While classical regression trees and random forests only model the mean of the response
we propose to follow the ideas from GAMLSS modeling -- as outlined
in Figure~\ref{Devel_parmod} -- and combine tree-based methods with parametric distributional models,
yielding two novel techniques:
\begin{itemize}
  \item \emph{Distributional trees} split the data into more homogenous groups with
    respect to a parametric distribution, thus capturing changes in any distribution
    paramer like location, scale, or shape (Figure~\ref{Devel_treeforest}, bottom left)
  \item \emph{Distributional forests} utilize an ensemble of distributional trees
    for obtaining stabilized and smoothed parametric predictions (Figure~\ref{Devel_treeforest},
    top right).
\end{itemize}
In the following, particular focus is given to distributional forests as a method for obtaining
probabilistic forecasts by leveraging the strengths of random forests: the ability to
capture both smooth and abruptly changing functions along with simultaneous selection of
variables and possibly complex interactions. Thus, these properties make the method particularly
appealing in case of many covariates with unknown effects where it would be challenging
to specify a GAMLSS.

In weather forecasting, these properties are appealing in mountainous regions and complex terrain
where small-scale effects are not well-resolved in the NWP outputs. As these are computed at
a coarser scale, there may be abrupt local effects in the meteorological observations within a single
NWP grid cell. To illustrate this in practice, precipitation forecasts are obtained with distributional
forests at 85 meteorological stations in a mountainous region in the Alps, covering mainly Tyrol,
Austria, and adjacent areas. More specifically, a zero-censored Gaussian distribution is
employed to model 24-hour total precipitation so that the zero-censored point mass describes the
probability of no precipitation on a given day (see Figure~\ref{axams_onestation}). Forecasts
for July are established based on data from the same month in the years 1985--2012 along with
a large number of NWP ensemble outputs as covariates. As Figure~\ref{axams_onestation} shows,
the station-wise models yield a full distributional forecast for each day -- here for the same
day (July~24) at one station (Axams) over four years (2009--2012) -- based on the previous 24~years as training data.
The corresponding observations conform reasonably well with the predictions.

<<echo=FALSE, results=hide>>=
#### Axams prediction 24 - 4
if(file.exists("rain_Axams_pred_24to4.rda")){
  #load("~/svn/partykit/pkg/disttree/inst/draft/rain_Axams_pred_24to4.rda")
  load("rain_Axams_pred_24to4.rda")
} else {
  #source("~/svn/partykit/pkg/disttree/inst/draft/rain_axams_pred_24to4.R")
  source("rain_axams_pred_24to4.R")
  library("gamlss.cens")
  gen.cens("NO", type = "left")
  res <- rain_axams_pred(seedconst = 7, ntree = 100,
                         forest_mtry = 27,   # if frac == TRUE: nvar/3 = 30
                         gamboost_cvr = TRUE,
                         frac = FALSE)
  
  save(res, file = "rain_Axams_pred_24to4.rda")
}

#### prepare data for plot of estimated density functions
# predictions for one day (in each of the four years) 
# (19th of July 2011 is missing)
pday <- 24  # 2 (hohe Beobachtung zu niedrig geschaetzt), 4, 15, evtl. auch 7, 8, 23 (eine 0-Beobachtung und 2 sehr aehnliche), 

pdays <- if(pday<19) c(pday, pday + 31, pday + 62, pday + 92) else c(pday, pday + 31, pday + 61, pday + 92)
pdf <- predict(res$df, newdata = res$testdata[pdays,], type = "parameter")
df_mu <- pdf$mu
df_sigma <- pdf$sigma
df_exp <- pnorm(df_mu/df_sigma) * (df_mu + df_sigma * (dnorm(df_mu/df_sigma) / pnorm(df_mu/df_sigma)))
cbind(df_exp, res$testdata[pdays,"robs"])


# plot predicted distributions together with observations
#set.seed(res$call$seedconst)
set.seed(7)
x <- c(0.01, sort(runif(500,0.01,8)))
y1 <- crch::dcnorm(x, mean = df_mu[1], sd = df_sigma[1], left = 0)
y2 <- crch::dcnorm(x, mean = df_mu[2], sd = df_sigma[2], left = 0)
y3 <- crch::dcnorm(x, mean = df_mu[3], sd = df_sigma[3], left = 0)
y4 <- crch::dcnorm(x, mean = df_mu[4], sd = df_sigma[4], left = 0)
dayending <- if(pday > 3) "th" else switch(pday, "1" = {dayending <- "st"}, "2" = {dayending <- "nd"}, "3" = {dayending <- "rd"})

# point mass (slightly shifted)
pm1 <- c(0.05, crch::dcnorm(-1, mean = df_mu[1], sd = df_sigma[1], left = 0))
pm2 <- c(0.01, crch::dcnorm(-1, mean = df_mu[2], sd = df_sigma[2], left = 0))
pm3 <- c(-0.03, crch::dcnorm(-1, mean = df_mu[3], sd = df_sigma[3], left = 0))
pm4 <- c(-0.07, crch::dcnorm(-1, mean = df_mu[4], sd = df_sigma[4], left = 0))

# predictions
pred1 <- c(res$testdata[pdays,"robs"][1], crch::dcnorm(res$testdata[pdays,"robs"][1], mean = df_mu[1], sd = df_sigma[1], left = 0))
pred2 <- c(res$testdata[pdays,"robs"][2], crch::dcnorm(res$testdata[pdays,"robs"][2], mean = df_mu[2], sd = df_sigma[2], left = 0))
pred3 <- c(res$testdata[pdays,"robs"][3], crch::dcnorm(res$testdata[pdays,"robs"][3], mean = df_mu[3], sd = df_sigma[3], left = 0))
pred4 <- c(res$testdata[pdays,"robs"][4], crch::dcnorm(res$testdata[pdays,"robs"][4], mean = df_mu[4], sd = df_sigma[4], left = 0))

#legendheight
lh1 <- crch::dcnorm(0.01, mean = df_mu[1], sd = df_sigma[1], left = 0)
lh2 <- crch::dcnorm(0.01, mean = df_mu[2], sd = df_sigma[2], left = 0)
lh3 <- crch::dcnorm(0.01, mean = df_mu[3], sd = df_sigma[3], left = 0)
lh4 <- crch::dcnorm(0.01, mean = df_mu[4], sd = df_sigma[4], left = 0)



## PIT histograms and qqr-plots

## get predicted parameter
# for learndata and testdata
{
  set.seed(7)
  ## disttree
  if(FALSE){
    # in sample
    pdt <- predict(res$dt, type = "parameter")
    dt_mu_l <- pdt$mu 
    dt_sigma_l <- pdt$sigma 
    pit_dt_l <- cbind(0, pnorm(res$learndata[,"robs"], mean = dt_mu_l, sd = dt_sigma_l))
    pit_dt_l[res$learndata[,"robs"]>0, 1] <- pit_dt_l[res$learndata[,"robs"]>0, 2]
  }  
  # out of sample
  pdt <- predict(res$dt, type = "parameter", newdata = res$testdata)
  dt_mu_t <- pdt$mu 
  dt_sigma_t <- pdt$sigma 
  pit_dt_t <- cbind(0, pnorm(res$testdata[,"robs"], mean = dt_mu_t, sd = dt_sigma_t))
  pit_dt_t[res$testdata[,"robs"]>0, 1] <- pit_dt_t[res$testdata[,"robs"]>0, 2]
  
  
  ## distforest
  if(FALSE){
    # in sample
    pdf <- predict(res$df, type = "parameter")
    df_mu_l <- pdf$mu
    df_sigma_l <- pdf$sigma
    pit_df_l <- cbind(0, pnorm(res$learndata[,"robs"], mean = df_mu_l, sd = df_sigma_l))
    pit_df_l[res$learndata[,"robs"]>0, 1] <- pit_df_l[res$learndata[,"robs"]>0, 2]
  }
  # out of sample
  pdf <- predict(res$df, type = "parameter", newdata = res$testdata)
  df_mu_t <- pdf$mu
  df_sigma_t <- pdf$sigma
  pit_df_t <- cbind(0, pnorm(res$testdata[,"robs"], mean = df_mu_t, sd = df_sigma_t))
  pit_df_t[res$testdata[,"robs"]>0, 1] <- pit_df_t[res$testdata[,"robs"]>0, 2]
  
  
  ## gamlss
  g_learndata <- res$learndata
  g_learndata$robs <- Surv(g_learndata$robs, g_learndata$robs>0, type="left")
  if(FALSE){
    # in sample
    g_mu_l <- predict(res$g, what = "mu", type = "response")
    g_sigma_l <- predict(res$g, what = "sigma", type = "response")
    pit_g_l <- cbind(0, pnorm(res$learndata[,"robs"], mean = g_mu_l, sd = g_sigma_l))
    pit_g_l[res$learndata[,"robs"]>0, 1] <- pit_g_l[res$learndata[,"robs"]>0, 2]
  }  
  # out of sample
  g_mu_t <- predict(res$g, what = "mu", type = "response", newdata = res$testdata)
  g_sigma_t <- predict(res$g, what = "sigma", type = "response", newdata = res$testdata)
  pit_g_t <- cbind(0, pnorm(res$testdata[,"robs"], mean = g_mu_t, sd = g_sigma_t))
  pit_g_t[res$testdata[,"robs"]>0, 1] <- pit_g_t[res$testdata[,"robs"]>0, 2]
  
  
  #gamboostLSS
  if(FALSE){
    #in sample
    pgb <- predict(res$gb, parameter = c("mu","sigma"), type = "response")
    gb_mu_l <- pgb$mu
    gb_sigma_l <- pgb$sigma
    pit_gb_l <- cbind(0, pnorm(res$learndata[,"robs"], mean = gb_mu_l, sd = gb_sigma_l))
    pit_gb_l[res$learndata[,"robs"]>0, 1] <- pit_gb_l[res$learndata[,"robs"]>0, 2]
  }  
  # out of sample
  pgb <- predict(res$gb, parameter = c("mu","sigma"), type = "response", newdata = res$testdata)
  gb_mu_t <- pgb$mu
  gb_sigma_t <- pgb$sigma
  pit_gb_t <- cbind(0, pnorm(res$testdata[,"robs"], mean = gb_mu_t, sd = gb_sigma_t))
  pit_gb_t[res$testdata[,"robs"]>0, 1] <- pit_gb_t[res$testdata[,"robs"]>0, 2]
  
  
  #EMOS
  if(FALSE){
    # in sample
    ml_mu_l <- predict(res$ml, type = "location")     # returns parameter on response scale
    ml_sigma_l <- predict(res$ml, type = "scale")
    pit_ml_l <- cbind(0, pnorm(res$learndata[,"robs"], mean = ml_mu_l, sd = ml_sigma_l))
    pit_ml_l[res$learndata[,"robs"]>0, 1] <- pit_ml_l[res$learndata[,"robs"]>0, 2]
  }  
  # out of sample
  ml_mu_t <- predict(res$ml, type = "location", newdata = res$testdata)     # returns parameter on response scale
  ml_sigma_t <- predict(res$ml, type = "scale", newdata = res$testdata)
  pit_ml_t <- cbind(0, pnorm(res$testdata[,"robs"], mean = ml_mu_t, sd = ml_sigma_t))
  pit_ml_t[res$testdata[,"robs"]>0, 1] <- pit_ml_t[res$testdata[,"robs"]>0, 2]
  
}


## Variable importance
if(file.exists("vimp_crps.rda")){
  #load("~/svn/partykit/pkg/disttree/inst/draft/vimp_crps.rda")
  load("vimp_crps.rda")
} else {
  
  # locally redefine logLik.distforest to use crps in varimp()
  logLik.distforest <- function(object, newdata = NULL, ...) {
    if(is.null(newdata)) {
      newdata <- object$data
    } 
    
    # predict parameter
    pdf <- predict(object, newdata = newdata, type = "parameter")
    df_mu <- pdf$mu
    df_sigma <- pdf$sigma
    
    # calculate CRPS
    crps <- mean(crps_cnorm(newdata$robs, location = df_mu, scale = df_sigma, lower = 0, upper = Inf), na.rm = TRUE)
  
    return(structure(crps, df = NA, class = "logLik"))
  }
  
  vimp_crps <- varimp(res$df, nperm = 1L)
  #vimp10_crps <- varimp(res$df, nperm = 10L)
  #vimp100_crps <- varimp(res$df, nperm = 100L)
  #vimp1000_crps <- varimp(res$df, nperm = 1000L)
  
  save(vimp_crps, file = "~/svn/partykit/pkg/disttree/inst/draft/vimp_crps.rda")
  #save(vimp10_crps, file = "~/svn/partykit/pkg/disttree/inst/draft/vimp10_crps.rda")
  #save(vimp100_crps, file = "~/svn/partykit/pkg/disttree/inst/draft/vimp100_crps.rda")
  #save(vimp1000_crps, file = "~/svn/partykit/pkg/disttree/inst/draft/vimp1000_crps.rda")
  
  rm(logLik.distforest) 
}
    
@


\begin{figure}[t!]
\centering
\setkeys{Gin}{width=0.8\textwidth}
<<rain_Axams_pred, fig=TRUE, echo=FALSE, width=6.5, height=4.5>>=
plot(x = x, y = y1, type = "l", col = "red", 
     main = paste0("July ", pday), ylab = "Density", 
     xlab = expression(Total~precipitation~"["~mm^(1/1.6)~"/"~"24h"~"]"),
     ylim = c(0,max(y1, y2, y3, y4, pm1, pm2, pm3, pm4) + 0.01),
     xlim = c(-1.5,8))
    
lines(x = x, y = y2, type = "l", col = "blue")
lines(x = x, y = y3, type = "l", col = "darkgreen")
lines(x = x, y = y4, type = "l", col = "purple")
legend("topright", c("Predicted distribution", "Point mass at censoring point", "Observation"),
       bty = "n", col = "black", lty = c(1, NA, NA), pch = c(NA, 19, 4), cex = 0.8)

# plot point mass
lines(x = c(pm1[1], pm1[1]), y = c(pm1[2], 0), col = "red", type = "l", lwd = 1)
lines(x = c(pm2[1], pm2[1]), y = c(pm2[2], 0), col = "blue", type = "l", lwd = 1)
lines(x = c(pm3[1], pm3[1]), y = c(pm3[2], 0), col = "darkgreen", type = "l", lwd = 1)
lines(x = c(pm4[1], pm4[1]), y = c(pm4[2], 0), col = "purple", type = "l", lwd = 1)

points(x = pm1[1], y = pm1[2], col = "red", pch = 19)
points(x = pm2[1], y = pm2[2], col = "blue", pch = 19)
points(x = pm3[1], y = pm3[2], col = "darkgreen", pch = 19)
points(x = pm4[1], y = pm4[2], col = "purple", pch = 19)


# plot predictions
points(x = pred1[1], y = pred1[2], col = "red", pch = 4)
points(x = pred2[1], y = pred2[2], col = "blue", pch = 4)
points(x = pred3[1], y = pred3[2], col = "darkgreen", pch = 4)
points(x = pred4[1], y = pred4[2], col = "purple", pch = 4)

lines(x = c(pred1[1], pred1[1]), y = c(pred1[2], 0), col = "darkgrey", type = "l", lty = 2)
lines(x = c(pred2[1], pred2[1]), y = c(pred2[2], 0), col = "darkgrey", type = "l", lty = 2)
lines(x = c(pred3[1], pred3[1]), y = c(pred3[2], 0), col = "darkgrey", type = "l", lty = 2)
lines(x = c(pred4[1], pred4[1]), y = c(pred4[2], 0), col = "darkgrey", type = "l", lty = 2)

# add labels
text(x = -0.8, y = lh1, labels = "2009", col = "red", cex = 0.8)
text(x = -0.8, y = lh2, labels = "2010", col = "blue", cex = 0.8)
text(x = -0.8, y = lh3, labels = "2011", col = "darkgreen", cex = 0.8)
text(x = -0.8, y = lh4, labels = "2012", col = "purple", cex = 0.8)
@
\caption{\label{axams_onestation}Predictions of total precipitation at station Axams for July 24 in 2009, 2010, 2011 and 2012 by a distributional forest learned on data from 1985--2008. Observations are left-censored at 0. The corresponding predicted point mass is shown at the censoring point (0).}
\end{figure}



\section{Methodology}
\label{Methodology}

\subsection{Motivation}
As illustrated in Figure~\ref{Devel_parmod} a lot of development has been made in the field of parametric modeling and a wide range of tools can be applied in research areas such as probabilistic forecasting. But as mentioned before, further features such as the ability to capture non-additive effects or interactions are desirable in many applications which demands for more flexible models. Since tree-structured models provide these required features a combination of them with parametric models can offer a framework that is able to deal with a wide range of different modeling situations. In this section we are going to introduce the novel methodology which embeds parametric modeling into the idea of tree-structured models step by step. We start with a simple distributional fit in Section~\ref{distfit}, then build a distributional tree in Section~\ref{disttree} and finally a distributional forest in Section~\ref{distforest}.

\subsection{Distributional fit}
\label{distfit}
Fitting a distributional model to a set of observations without considering any covariates is a well known procedure which can for example be done by applying the maximum likelihood method. This is also the chosen approach in the methodology described in this section. %%% <TH> is is repeated on page 5, remove here </TH>

The goal is to fit a distributional model $D(Y, \theta)$ to a response variable $Y \in \mathcal{Y}$ for a \textit{k}-dimensional vector of distribution parameters $\theta \in \Theta$ in case of a \textit{k}-parametric distribution. The distribution family has to be specified in advance such that a log-likelihood function $\ell(\theta; Y)$ is provided. Using the GAMLSS framework (\citealt{stasinopoulos2005}) a wide range of distributions can be selected where each distribution parameter can be specified separately. This enables the model to deal with various features that can usually be very challenging in model fitting such as censoring, truncation or heavy tails.

In our application of precipitation forecasting the response variable, which is the power transformed total precipitation amount within 24 hours, is expected to be normally distributed with mean/location parameter $\mu$ and standard deviation/scale parameter $\sigma$ and left-censored with censoring point 0. (The power transformation is used to remove large parts of the skewness of the data as further explained in Section~\ref{data}.) Therefore, the corresponding log-likelihood function with parameter vector $\theta = (\mu, \sigma)$ is
$$
\ell(\mu, \sigma; Y) = 
\begin{cases}
    \log\left(\frac{1}{\sigma} \cdot \phi\left(\frac{Y - \mu}{\sigma}\right) \right), & \text{if } Y > 0\\
    \vspace{0.1cm}
    \log\left(\Phi\left(\frac{-\mu}{\sigma}\right)\right), & \text{if } Y = 0
\end{cases}
$$
where $\phi$ and $\Phi$ are the probability density function and the distribution function of the standard normal distribution $\mathcal{N}(0,1)$. Next to the censored normal distribution the censored logistic distribution is also often chosen for the modeling of precipitation amounts. In \citealt{Gebetsberger2017} this choice has been made in order to be able to deal with heavy tails which often appear in precipitation data. Another frequent application of probabilistic forecasting in meteorology is the modeling of near surface air temperature. In this case a normal distribution is often selected.

With the specification of the distribution family and the log-likelihood function the task of fitting a distributional model turns into the task of determining the distribution parameter~$\theta$. This is done by the following maximization.
$$
\hat{\theta} = \max_{\theta \in \Theta} \sum_{i=1}^n \ell(\theta; y_i)
$$
where $\{y_i\}_{i=1,\ldots,n}$ are the observations of the response variable $Y$.

In that way, a whole distribution is specified with all its features, including location, scale, and shape. However, fitting one global model to the whole data set might be too much of a generalization and therefore not represent the data and its features in a reasonable way. Moreover, if covariates are available it is desirable to include them into the model as they might provide important additional information. In our application on precipitation data a wide range of covariates are available from a numerical ensemble prediction system considering precipitation, temperature, sea level pressure and other meteorological quantities which might be of interest and have an influence on the predictions.

A common way of including these covariates is to consider them as regressors in a distributional model such that each parameter $\theta$ depends on covariates $\bold{Z} = (Z_1,\ldots,Z_m)$. This could for example be done by a linear function $g$. In that way the fitted model would be specified as $D(Y, \theta = g(\bold{Z}))$. In order to avoid the restriction to linear effects one might also use a set of smooth functions $\{f_i\}_{i = 1,\ldots,p}$ to specify a more flexible model $D(Y, \theta = f_1(\bold{Z}) + \ldots + f_p(\bold{Z}))$ applying a GAM structure. Another approach to distributional regression is proposed by \citealt{Klein2015} where each parameter of a possibly complex distribution can be modeled by an additive term consisting of a variety of different functional effect types. These include non-linear effects, spatial effects, random coefficients and interaction surfaces which extends the flexibility compared to parametric models such as GAMLSS modeling only smooth effects.

However, the restriction to additive models remains. Therefore, a different approach on how to include covariates that also allows for non-additive effects and interactions is considered in the next subsection of this article. In particular, the covariates can be used to separate the data into more homogeneous subgroups. Doing so after fitting a global model to the whole data set (without considering any covariates) and then fitting a local model to each subgroup should improve the model remarkably. This procedure of splitting a data set into subgroups can be done by applying a tree algorithm.


\subsection{Distributional tree}
\label{disttree}
Given the fit of a global distributional model $D(Y, \bold{\hat{\theta}})$ the idea is to use the information provided by this model together with a set of covariates $\bold{Z} = (Z_1, \ldots, Z_m) \in \mathcal{Z}$ %(from now on denoted by $\bold{Z}$ instead of $\bold{X}$ to indicate that they are considered as split variables only and not as regressors)
%%% <TH> X -> Z is confusing. Either we always write theta = g(X) where
%%% g is now a tree or we explain that theta = g(X)(Z) is what we want.
%%% Here, we don't partition models with X variables, so we should stick to X. </TH>
to decide whether the data set should be split into subgroups or not. In particular, the information provided by the fitted model are the scores 
$$
s(\hat{\theta}, y_i) = \frac{\partial \ell}{\partial \theta}(\hat{\theta}; y_i).
$$ 
These values express how well the model fits the data regarding each parameter. Ideally they should fluctuate randomly around zero, similar to residuals in ordinary least squares estimations, but in this case there is a score value for each pair of estimated parameter $\hat{\theta}_j \in (\hat{\theta}_1, \ldots, \hat{\theta}_k) = \bold{\hat{\theta}}$ and observation $y_j \in \{y_i\}_{i = 1,\ldots, n}$. This enables the model to be sensitive to changes in each of the parameters which is one of the main advantages of distributional trees over other tree models considering only the mean of the distribution.

Statistical tests are applied to check whether there is a significant dependency between the scores and each of the partitioning variables $Z_l \in \bold{Z} = (Z_1, \ldots, Z_m)$. If so, the data set is split into subgroups based on the values of the partitioning variable that shows the highest dependency.

In a next step a local distributional model is fit in each of the subgroups. Repeating this procedure of fitting a model, applying statistical tests to evaluate parameter instability and splitting the data set depending on the test results leads to a tree structure. In that way the learning data set is separated into \textit{B} disjoint segments such that
$$
\mathcal{Z} = \dot{\bigcup\limits_{b = 1, \ldots, \textit{B}}} \mathcal{B}_b
$$
where each segment is represented by a terminal node of the tree.

The exact way of determining how and where to split varies across different tree algorithms. For distributional trees one possible choice is a permutation test based algorithm which is explained in detail in the appendix. Summed up, the steps of building a distributional tree are:
\begin{enumerate}
\item Specify a distribution with log-likelihood function $\ell(\theta; y)$.
\item Estimate $\bold{\hat{\theta}}$ by solving $\sum_{i=1}^n s(y_i, \hat{\theta}) = 0$.
\item Test for associations or instabilities of the scores $s(\bold{\hat{\theta}}, y_i)$ and each partitioning variable $Z_l$.
%\item Assess whether the \emph{model scores} are associated with
%    (or change along) any of the available covariates -- e.g.,
%    using parameter instability tests (\emph{strucchange}) or
%    conditional inference (\emph{coin}).
\item Split the sample along the partitioning variable with the strongest association or instability.
    Choose the breakpoint with the highest improvement in log-likelihood.
\item Repeat steps 2--4 recursively in the subgroups
    until some stopping criterion is reached.
    %-- e.g., for significance or sample size.
%\item Choose the variable with the strongest association
%\item Choose the split point which leads to the highest improvement of the model
%\item Split and repeat 2-6 in each node until a stopping criterion is met
\end{enumerate}
The stopping criterion in step 5 can for example be a minimum number of observations in a node or a p-value for the statistical tests applied in step 3.
%%% <TH> we need a little more detail here or a reference to a better
%%% description of mob/ctree OR appendix</TH>

Executing these steps results in a stepwise parametric model where a complete distribution is fit in each node of the tree.

For distributional trees a prediction for a new set of covariates $\bold{z} = (z_1, \ldots, z_m)$ is made with the following strategy: 
All observations of the learning data set $\{(y_i,\bold{z}_i)\}_{i=1\ldots,n}$ which end up in the same terminal node of the tree as the new set of covariates are selected and a distributional model is fit on the subset. To select this subset a weight vector $\bold{w}^{\text{tree}} = (w^{\text{tree}}_1, \ldots, w^{\text{tree}}_n)$ is included in the fitting process.
$$
w^{\text{tree}}_i(\bold{z}) = \sum_{b=1}^B I((\bold{z}_i \in \mathcal{B}_b) \land (\bold{z} \in \mathcal{B}_b))
$$
The predicted distribution of the new set of covariates is then fully specified by the estimated parameter $\hat{\theta}(\bold{z})$ where
$$
\hat{\theta}(\bold{z}) = \max_{\theta \in \Theta} \sum_{i=1}^n w^{\text{tree}}_i(\bold{z}) \cdot \ell(\theta; y_i).
$$

While the clear structure of such a tree model is advantageous in interpreting and analyzing it, the abrupt changes are often too rough and impose steps on the model even if the true underlying effect is smooth. In these situations bagging can solve the problem by combining an ensemble of trees such that wrongly detected abrupt changes of the model are turned into almost smooth transitions. In that way, all kinds of effects can be modeled, which is is also one the of the main advantages of forest models (taking bagging one step further as the variance is reduced by sampling the covariates considered for the splitting process in each node).

%%% <TH> we need to be much more precise here: random forests is Breiman's and Cutler's
%%% random forest. Maybe it is better to not mention this method at any time. </TH>
Common forest models such as Breiman and Cutler's randomForest consist of trees only considering the mean of the distribution of the response variable. For that reason, changes in other parameters can only be detected if they are connected to changes in the location parameter, e.g. if they depend on the location parameter. Therefore, building a forest of distributional trees which can also capture separate effects in each of the distribution parameters leads to an improvement of the model. This valuable feature of detecting separate effects is also provided by GAMLSS as each parameter is modeled separately. However, in this case this separation can also be a disadvantage as it hinders the capturing of interactions unless they are specified explicitly by the user. These two cases provide once again a good reason to combine the two frameworks of flexible parametric modeling and tree-structured models in order to profit from their advantages and compensate possible disadvantages. This is the pursued goal in introducing distributional forests.

\subsection{Distributional forest}
\label{distforest}
A distributional forest consists of an ensemble of $T$ distributional trees. Each of these trees is built on a different data set which can either be a bootstrap or a subset of the original data set. Moreover, in each node a subset of the partitioning variables is chosen randomly as candidates for splitting variables. In that way the correlation among the resulting trees is reduced and therefore also the variance of the model (\citealt{breiman2001random}).

Forest models differ mainly in the way how the trees are combined to make predictions. Following the strategy explained and proposed by \cite{transformationforests2017} we interpret random forests as adaptive local likelihood estimators as distributional models are fit locally in subgroups while the term ``adaptive'' refers to the dependence on the splitting variables $\bold{Z}$ and the resulting subgroups and weighing schemes. In this case the forest itself is only used to evaluate neighborhood relationships among the observations which can then be included in the fitting process. In transformation forests by \cite{transformationforests2017} so-called ``nearest neighbor weights'' are calculated as a measure of distance between observations. This approach has also already been applied in several random forest-type methods such as in ``adaptive nearest neighbors'' by \cite{lin2006}, in ``quantile regression forests''  by  \cite{meinshausen2006} and in ``bagging survival trees'' by \cite{hothorn2004}. The idea of nearest neighbor weights is to measure similarity of a pair of observations by counting the number of trees in which it is asigned to the same terminal node.
%Similar observations have a high probability of ending up in the same terminal node whereas this probability is low for quite different observations. 
%although the first random forest-type algorithm for the estimation of conditional distribution functions was published more than a decade ago (bagging survival trees, Hothorn et al. 2004). 
In particular, for a new set of covariates $\bold{z} = (z_1, \ldots, z_m) $ such a set of weights 
$\bold{w}^{\text{forest}}~=~(w^{\text{forest}}_1, \ldots, w^{\text{forest}}_n)$ is obtained by the following procedure: For each tuple $(y_i,\bold{z}_i)$ in the learning data set $\{(y_i,\bold{z}_i)\}_{i=1\ldots,n}$ a weight is calculated by counting the number of trees in which $(y_i,\bold{z}_i)$ ends up in the same terminal node as the new set of covariates $\bold{z}$:
$$
w^{\text{forest}}_i(\bold{z}) = \sum_{t=1}^T \sum_{b=1}^{B^t} I((\bold{z}_i \in \mathcal{B}^t_b) \land (\bold{z} \in \mathcal{B}^t_b))
$$
where $T$ is the number of trees of the forest, $B^t$ is the number of terminal nodes of the \textit{t}-th tree and $\mathcal{B}^t_b$ represents the \textit{b}-th terminal node of the \textit{t}-th tree.

Therefore, contrary to distributional trees, in distributional forests the whole learning data set can be used to fit the distributional model and specify the predicted distribution for a new set of covariates. Compared to the formula for predictions made by distributional trees the vector of tree weights $\bold{w}^{\text{tree}}$ containing only zeros and ones is now replaced by a vector of integer valued forest weights.
%%% <TH> we need references for the nearest neighbor weights, copy from
%%% "Transformation Forests" </TH>
%%%<TH> we need to cite "Transformation Forests" for the interpretation of
%%% random forests as adaptive local ML </TH>
This individual set of weights $\bold{w}^{\text{forest}}(\bold{z}) = (w^{\text{forest}}_1(\bold{z}), \ldots, w^{\text{forest}}_n(\bold{z}))$ for the new set of covariates $\bold{z}$ can now be included in the estimation process of the distribution parameters and in that way leads to its individual parameter vector $\hat{\theta}(\bold{z})$.
$$
\hat{\theta}(\bold{z}) =  \max_{\theta \in \Theta} \sum_{i=1}^n w^{\text{forest}}_i(\bold{z}) \cdot \ell(\theta; y_i)
$$
While in a distributional tree the same distribution is specified for all sets of covariates being in the same node, a distributional forest predicts and fits a different distribution for each different set of covariates due to the individual weights calculated separately for each set of covariates.

As this novel methodology combines the advantages of smooth and flexible parametric modeling with those of tree-structured modeling it provides a model that can deal with a wide range of situations. Abrupt changes and non-additive effects can be captured by its tree structure while smooth effects can be modeled reasonably well by combining various tree and smoothing the steps imposed by a single tree. As scores are considered for each distribution parameter (separate) effects in different parameters can be captured. At the same time, also interactions are detected automatically by splitting the data in a way that is sensitive to changes in either each of the parameters or combined in various parameters.











\SweaveOpts{eval = FALSE}
%\newpage
%\section{Simulation Study}
%\label{Simulation}
%Within this simulation study the performance of distributional trees and forests is tested and compared to other models, showing situations where one or the other method is more advantageous than others. 

%In the case of smooth parameter functions without any abrupt changes / non-additive effects a smooth parametric model such as GAMLSS is clearly the first choice while a stepfunction can be modeled best by applying a tree-structured model. Even though for both of these extreme situations a specifically advantageous method is provided, a forest model can be a good compromise and performs well in a wide range of settings including the two mentioned before. (Therefore, a forest is a recommendable choice particularly when no information about the type of parameter function is provided.)

%Generally, a smooth parametric model such as GAMLSS should be the most successful method if the parameter function is smooth with no rapid changes. However, if there are any jumps or non-additive effects or interactions, the trees should be more effective in detecting these steps and therefore also lead to a better fitting forest. But this situation might also appear for smooth functions which contain sections with very steep slopes. Therefore, it is now of great interest, whether the concept of smooth parametric models or of trees and forests covers a broader range of functions where it is superior to the other.\\


%The new methods are evaluated on generated data sets and their results are then compared to those of a GAMLSS (\citealt{stasinopoulos2005}) which fits a separate generalized additive model to each of the distribution parameter and in that way specifies a whole distribution and is a very efficient option for smooth effects of the covariates on the parameters.\\
%Each generated data set includes 500 samples consisiting of the value of a response variable $Y$ and six covariates $X_1, \ldots, X_6$ which are all independent and distributed as follows:
%$$
%X_1, X_2, X_3 \sim \textit{Unif}([-1,1])
%$$
%$$
%X_4, X_5, X_6 \sim \textit{Binom}(1, 0.5)
%$$

%The distribution of the response variable is a left-censored normal distribution with censoring point 0. The corresponding distribution parameters are defined by a parameter function $f$ depending on the covariates and an additional factor $\kappa$, also known as the smoothness constraint, which regulates the steepness of certain sections of the parameter function. For low values of $\kappa$ the resulting parameters change rather slowly over varying values of the covariates while high values of $\kappa$ lead to abrupt changes. \\ 
%The location parameter $\mu$ depends on $X_1, X_4$ and $\kappa$ and the scale parameter $\sigma$ depends on $\mu, X_1$ and $\kappa$. The other covariates $X_2, X_3, X_5$ and $X_6$ are used as noise variables.\\
%The latent variable $Y^{\ast}$ is generated based on the following setting.
%\begin{center}
%$$ 
%Y^{\ast} \sim \mathcal{N} (f(X_1, X_4, \kappa))
%$$
%with
%$$
%f(X_1, X_4, \kappa) = \left(\mu(X_1, X_4, \kappa) , \sigma(\mu,X_1,\kappa)\right)
%$$ 
%and
%\begin{align*}
%\mu(X_1, X_4, \kappa) = 0.2 
%+ \begin{cases}
%    7 \cdot \exp{(-(3 \cdot X_1-1.2)^{(2\cdot \kappa)})},
%    & \text{if } X_1 < 0.4 \text{ and } X_4 = 0\\
%    7 \cdot \exp{(-(3 \cdot X_1-1.2)^{(2\cdot \kappa)})} \cdot \frac{1}{2} + \frac{7}{2},       
%    & \text{if } X_1 \geq 0.4 \text{ and } X_4 = 0\\
%    10 \cdot \exp{(-(3 \cdot X_1-1.2)^{(2\cdot \kappa)})},
%    & \text{if } X_1 < 0.4 \text{ and } X_4 = 1\\
%    13 \cdot \exp{(-(3 \cdot X_1-1.2)^{(2\cdot \kappa)})} \cdot \frac{1}{2} + \frac{7}{2},       
%    & \text{if } X_1 \geq 0.4 \text{ and } X_4 = 1
%\end{cases}
%\end{align*}

%$$
%\sigma(\mu, X_1,\kappa) = 0.5 + \frac{\mu}{5} + 2 \cdot \mathbf{1}(X_1 < -0.4)
%$$
%%where $\Lambda$ is the cumulative distribution function of the logistic distribution
%%$$
%%\Lambda(\omega) = \frac{1}{1 + \exp(-\omega)}
%%$$
%The response variable $Y$ is defined as 
%$$
%Y = 
%\begin{cases}
%    Y^{\ast},& \text{if } Y^{\ast} \geq 0\\
%    0,       & \text{else}
%\end{cases}
%$$
%\end{center}


%To illustrate the parameter functions and the impact of $\kappa$ together with the results of the considered methods they are first applied on one generated data set only where $\kappa$ is set to~1 and then to a second data set where $\kappa$ is set to 10. In Figure~\ref{plot_sim_oneset_mu} the location parameter function $\mu(X_1, X_4, \kappa)$ is plotted along the covariate $X_1$ together with the estimated values of the different models for fixed covariates $x_4 = 0$ and $(x_2, x_3, x_5, x_6) = (0,0,0,0)$. \\

%\SweaveOpts{eval = TRUE}
<<eval = FALSE, echo=FALSE, results=hide>>=

source("oneset.R")
    
#if(file.exists("oneset1.rda")){
#  load("oneset1.rda")
#} else {
  
  #define parameter function
  #kappa <- 1
  fun1 <- function(x){
    mu <- 0.2 + 
      (7 * (exp(-(3*x[,1]-1.2)^(2*1)))) * as.numeric(x[,1] < 0.4) + 
      (7 * (exp(-(3*x[,1]-1.2)^(2*1))) * (1/2) + 7/2) * as.numeric(x[,1]>= 0.4) +
      (3 * (exp(-(3*x[,1]-1.2)^(2*1)))) * x[,4]
    sigma <- 0.5 + 
      mu/5 + 
      #(x[,1]>0.4)  #*x[,4]#+
      2*(x[,1] < -0.4) 
      #plogis((1^(1.8)) * 4 * (x[,2] + 0.7)) #* x[,4]
      #4*(1-plogis((1^(1.8)) * 2 * (x[,1] + 0.5))) * x[,4]
      #5 * abs(x[,1]) * x[,4]
    par <- cbind(mu, sigma)
    return(par)
  }
  
  library("gamlss.cens")
  gen.cens("NO", type = "left")
  
  oneset1 <- sim_oneset_vary_x1(parfun = fun1, nobs = 500, seedconst = 74, ntree = 100,
                                 formula = y~x1+x2+x3+x4+x5+x6, 
                                 tree_minsplit = 20, tree_minbucket = 7, tree_mincrit = 0.95, 
                                 forest_minsplit = 20, forest_minbucket = 7, forest_mincrit = 0, 
                                 forest_mtry = 3, type.tree = "ctree", 
                                 censNO = TRUE, 
                                 pred_fix_x2 = 0.9,
                                 pred_fix_x4 = 0,
                                 nrunif = 3,
                                 nrcovform = 6)

  
#  save(oneset1, file = "oneset1.rda")
#}

    
#if(file.exists("oneset10.rda")){
#  load("oneset10.rda")
#} else {
  
  #define parameter function
  #kappa <- 10
  fun10 <- function(x){
    mu <- 0.2 + 
      (7 * (exp(-(3*x[,1]-1.2)^(2*10)))) * as.numeric(x[,1] < 0.4) + 
      (7 * (exp(-(3*x[,1]-1.2)^(2*10))) * (1/2) + 7/2) * as.numeric(x[,1]>= 0.4) +
      (3 * (exp(-(3*x[,1]-1.2)^(2*10)))) * x[,4]
    sigma <- 0.5 + 
      mu/5 + 
      #(x[,1]>0.4)  #*x[,4]#+
      2*(x[,1] < -0.4) 
      #plogis((10^(1.8)) * 4 * (x[,2] + 0.7)) #* x[,4]
      #4*(1-plogis((10^(1.8)) * 2 * (x[,1] + 0.5))) * x[,4]
      #5 * abs(x[,1]) * x[,4]
    par <- cbind(mu, sigma)
    return(par)
  }
  
  library("gamlss.cens")
  gen.cens("NO", type = "left")
  
  oneset10 <- sim_oneset_vary_x1(parfun = fun10, nobs = 500, seedconst = 74, ntree = 100,
                                 formula = y~x1+x2+x3+x4+x5+x6, 
                                 tree_minsplit = 20, tree_minbucket = 7, tree_mincrit = 0.95, 
                                 forest_minsplit = 20, forest_minbucket = 7, forest_mincrit = 0, 
                                 forest_mtry = 3, type.tree = "ctree", 
                                 censNO = TRUE, 
                                 pred_fix_x2 = 0.9,
                                 pred_fix_x4 = 0,
                                 nrunif = 3,
                                 nrcovform = 6)

  
#  save(oneset10, file = "oneset10.rda")
#}
@


%\begin{figure}[t!]
%\begin{subfigure}{0.6\textwidth}
%<<sim_oneset1_mu, fig=TRUE, echo=FALSE>>=
%plot_oneset(oneset1, compare_mu =TRUE, add_dt = TRUE, add_df = TRUE, add_g = TRUE, plot_legend = TRUE, w_sigma = FALSE)
%@
%\end{subfigure}
%\hspace{-1cm}
%\begin{subfigure}{0.6\textwidth}
%<<sim_oneset10_mu, fig=TRUE, echo=FALSE>>=
%plot_oneset(oneset10, compare_mu =TRUE, add_dt = TRUE, add_df = TRUE, add_g = TRUE, plot_legend = TRUE, w_sigma = FALSE)
%@
%\end{subfigure}
%\caption{\label{plot_sim_oneset_mu} Smooth parameter function (left window, $\kappa = 1$) and parameter function with abrupt changes (right %window, $\kappa = 10$) together with the predicted location parameter $\mu$ of the tree model, the forest model and the GAMLSS,  plotted along %$x_1$ while all the other covariates are kept at fixed values.}
%\end{figure}

%In the case of smooth effects of the coefficients on the parameter function ($\kappa = 1$, left window in Figure~\ref{plot_sim_oneset_mu}) the fitted parameters of the GAMLSS are closest to the true parameters while the steps of the tree are cleary too rough. By combining various trees the forest can smooth out these steps, however, not as well as the GAMLSS.\\
%Looking at the other extreme situation in which the parameter functions include abrupt changes ($\kappa = 10$, right window in Figure~\ref{plot_sim_oneset_mu}) the tree captures them very well wich also leads to a good fit of the forest model. In contrast to the two tree-structured models the GAMLSS has its difficulties with these steps and therefore fluctuates around the true parameter functions.\\

%To get a more profound impression, the methods are evaluated on various data sets. The value of $\kappa$ is increasing from 1 to 10 by a stepsize of 1 and for each step 100 data sets are generated.\\
%%\subsubsection{Means of Comparison}
%In order to compare the performance of the chosen methods over various data sets, the continuous ranked probability score (CRPS) is calculated for each model on each data set. 

%three different values are calculated for the resulting models: the root of the mean squared error (RMSE), the log-likelihood value and the continuous ranked probability score (CRPS). All of them are evaluated out of sample, e.g. on a part of the data set that has not been used to build the model.\\
%Therefore, overfitting would lead to worse results.\\
%\\
%The RMSE represents how close the estimated expected values are to the true observations. For a response variable $Y$ and a set of observations $\textbf{y} = \{y_i\}_{i = 1,\ldots,n}$ with the corresponding estimated parameters $\hat{\boldsymbol{\theta}} = \{\hat{\theta}_i\}_{i = 1,\ldots,n}$ the RMSE is defined as follows.

%$$
%\text{RMSE}(Y, \boldsymbol{y}, \boldsymbol{\hat{\theta}}) = \sqrt{\frac{1}{n} \sum_{i=1}^n (\mathbb{E}_{\hat{\theta}_i}(Y) - y_i)^2}
%$$

%In most cases this value does not measure the distributional fit as only the expected value is considered. However, in the case of a censored normal distribution the expected value of the observed variable is a transformation of the expected value of the latent variable which includes the location and the scale parameter.\\
%\\
%A typical way to evaluate the distributional fit is provided by the log-likelihood value. For each observation $y$ and the corresponding predicted parameter $\hat{\theta}$ the value of the log-likelihood function $\ell(\hat{\theta}; y)$ is calculated and then averaged over all observations in the testing data set. The higher the value the better the predictions of the model are.\\
%\\
%The CRPS is another value that measures the accuracy of a probabilistic model. For one observation $y$ with the predicted distribution function $F$ the CRPS value is calculated by
%The CRPS measures the accuracy of a probabilistic model. For one observation $y$ with the predicted distribution function $F$ the CRPS value is calculated by
%$$
%\text{CRPS}(y, F) = \int_{-\infty}^\infty (F(z) - \mathbf{1}(y \leq z))^2 dz
%$$
%where \(\mathbf{1}\) is the indicator function. \\
%In this simulation study the CRPS is evaluated out of sample on a data set that has not been used to build the model but has been generated by the same process and with the same settings as the original data set. In the following this second data set is called testing data set while the original data set on which the models are built on is called learning data set.
%The mean CRPS over the whole testing data set is used for validation. As the CRPS can be seen as a generalization of the mean absolute error, a low value indicates a good fit. In applications, one big advantage in terms of interpretability is that the resulting value is on the scale of the observation. \\


%The methods are then compared by the out-of-sample RMSE for the location parameter $\mu$ and the shape parameter $\sigma$ as well as by %the out-of-sample log-likelihood of the fitted models, averaged within each step.
%(In that way it can be investigated how the results change for a changing value of $\kappa$.)


%\SweaveOpts{eval = FALSE}

<<eval = FALSE, echo=FALSE, results=hide>>=
if(file.exists("simres.rda")){
  load("simres.rda")
} else {
  #source("~/svn/partykit/pkg/disttree/inst/draft/.R")
  source("gensim.R")
  
  library("gamlss.cens")
  gen.cens("NO", type = "left")
  
  simfun <- function(x, kappa){
    mu <- 0.2 + 
      (7 * (exp(-(3*x[,1]-1.2)^(2*kappa)))) * as.numeric(x[,1] < 0.4) + 
      (7 * (exp(-(3*x[,1]-1.2)^(2*kappa))) * (1/2) + 7/2) * as.numeric(x[,1]>= 0.4) +
      (3 * (exp(-(3*x[,1]-1.2)^(2*kappa)))) * x[,4]
    sigma <- 0.5 + 
      mu/5 + 
      #(x[,1]>0.4)  #*x[,4]#+
      2*(x[,1] < -0.4) 
    #plogis((kappa^(1.8)) * 4 * (x[,2] + 0.7)) #* x[,4]
    #4*(1-plogis((kappa^(1.8)) * 2 * (x[,1] + 0.5))) * x[,4]
    #5 * abs(x[,1]) * x[,4]
    par <- cbind(mu, sigma)
    return(par)
  }
    
    
  simres <- gensim(parfun = simfun,
                   seedconst = 74, nrep = 100, ntree = 100,
                   nsteps = 10, stepsize = 1,
                   formula = y~x1+x2+x3+x4+x5+x6,
                   nobs = 500, testnobs = 500L,
                   tree_minsplit = 20, tree_minbucket = 7, tree_mincrit = 0.95,
                   forest_minsplit = 20, forest_minbucket = 7, forest_mincrit = 0, 
                   forest_mtry = 3,
                   censNO = TRUE,
                   gamboost_cvr = FALSE,
                   eval_disttree = TRUE,
                   eval_distforest = TRUE,
                   eval_gamlss = TRUE,
                   eval_bamlss = FALSE,
                   eval_gamboostLSS = FALSE,
                   eval_randomForest = FALSE,
                   eval_cforest = FALSE,
                   nrunif = 3,
                   nrcovform = 6)
  
  save(simres, file = "simres.rda")
}
@




\SweaveOpts{eval = TRUE}

\section{Probabilistic precipitation forecasting in complex terrain}
\label{ProbabilisticForecasting}


As explained in the introduction, distributional trees and forests provide a new approach in probabilistic forecasting which combines advantages of parametric models that are already used in this field and tree-structured models which provide features that could be particularly valuable in probabilistic forecasting but have not yet been applied in this kind of settings. To show their strengths in practice, these new methods are now applied to probabilistic precipitation forecasts in a mountainous region based on a large number of numerical weather prediction quantities. 

\subsection{Data}
\label{data}
The data set used for training and validation consists of observed daily precipitation sums provided by the National Hydrographical Service (\citealt{ehyd}) and numerical weather forecasts from the {U.S.} National Oceanic and Atmospheric Administration (NOAA).
Both, observations and weather forecasts are available from 1985 trough the year 2012, focusing on July only in this article.

Observations are available on a daily basis, each observation representing the total precipitation sum fallen within 24~hours observed at 6~UTC at 95 stations all over Tyrol and its close surrounding. The observations are undergoing a rigorous quality check before made available to the public.  As covariates the second generation reforecast data set of the global ensemble forecast system (GEFS) is used (\citealt{hamill2013}).  This data set consists of an 11-member ensemble based on a fixed version of the numerical model and a horizontal grid-spacing of about $50~\times~50 \text{km}^2$ initialized daily at 0~UTC from December 1984 to present providing forecasts on 26 hourly temporal resolution. Each of the 11 ensemble members uses slightly different perturbed initial conditions trying to predict the situation specific uncertainty of the atmospheric state. 


In particular, 14 basic forecast variables are considered with up to 12 variations of each of these variables being included as covariates. Since each forecast variable consists of an ensemble of individual predictions, the possible variations include the mean, the standard deviation, the minimum and the maximum over all ensemble members, noted as ensemble mean, ensemble std. deviation, ensemble minimum and ensemble maximum. All of these values can be calculated on the sums over 24 hours (6--6 UTC of the following day) or a 6 hour time window.\\
Another possible option is to consider the mean, minimum or maximum over 24 hours for each individual prediction and then take the ensemble mean or ensemble standard deviation, noted as ensemble mean of mean/minimal/maximal values or ensemble std. deviation of mean/minimal/maximal values respectively. Table~\ref{covariates} shows all variables together with the number of variations and the provided variations listed in the last column. Altogether 82 covariates are available in this setting.

%%% <TH> hm, now we _are_ using a transformation model without estimating the
%%% transformation. </TH>
To remove large parts of the skewness of precipitation data, a power transformation has been used frequently (Box and Cox 1964). In literature, cubic (\citealt{stidd1973}) or square root (\citealt{hutchinson1998b}) transformations have often been suggested but may vary for different climatic zones or temporal aggregation periods. In \citealt{stauffer2017a} optimization methods have been applied to find the best transformation factor which resulted in a value of $\frac{1}{1.6}$. Since the observation data set used for this optimization contains the observations considered in this article the same factor is chosen here for the power transformation of the response variable \emph{tp} (total precipitation). The resulting transformed variable is expected to be normally distributed and left-censored at censoring point 0. For simplicity the same transformation has been chosen for the variable \emph{cape} (convective available potential energy).

%\newpage
\begin{table}[t!]
%\resizebox{1.4\textwidth}{!}{
\begin{minipage}{\textwidth}
\begin{tabular}{ l  c  l }
\hline
Basic covariates & {\#} & Variations\\
\hline
\emph{tp}: total precipitation            & 12  & ensemble mean of sums over 24h, \\
\hspace*{0.5cm} power transformed (by $\frac{1}{1.6}$) &    & ensemble std. deviation of sums over 24h, \\ 
\emph{cape}: convective available     &    & ensemble minimum of sums over 24h, \\
\hspace*{0.9cm} potential energy &    & ensemble maximum of sums over 24h\\
\hspace*{0.9cm} power transformed (by $\frac{1}{1.6}$)&    & \qquad all for 6--30 UTC\\
\hspace*{\fill}                            &    & ensemble mean of sums over 6h\\
\hspace*{\fill}                            &    & \qquad for 6--12, 12--18, 18--24, 24--30 UTC\\
\hspace*{\fill}                            &    & ensemble std. deviation of sums over 6h\\
\hspace*{\fill}                            &    & \qquad for 6--12, 12--18, 18--24, 24--30 UTC\\
\hline
\emph{dswrf}: downwards short wave      & 6 & ensemble mean of mean values, \\
\hspace*{1.13cm} radiation flux (``sunshine'') &   & ensemble mean of minimal values,\\
\emph{msl}: mean sea level pressure     &   &  ensemble mean of maximal values,\\
\emph{pwat}: preciptable water          &   & ensemble std. deviation of mean values,\\
\emph{tmax}: 2m maximum temperature     &   & ensemble std. deviation of minimal values,\\
\hspace*{1.15cm}                          &   & ensemble std. deviation of maximal values,\\
\emph{tcolc}: total column-integrated   &   & \qquad all over 6--30 UTC \\
\hspace*{0.95cm} condensate               &   & \\
\emph{t500}: temperature on 500 hPa     &   & \\
\hspace*{1cm}                             &   & \\
\emph{t700}: temperature on 700 hPa     &   & \\
\hspace*{1cm}                             &   & \\
\emph{t850}: temperature on 850 hPa     &   & \\
\hspace{1cm}                              &   & \\
\hline
\emph{tdiff500850}: temperature         & 3 & ensemble mean of difference in mean,\\
\hspace*{\fill} difference 500 to 850 hPa &   & ensemble minimum of difference in mean,\\
\emph{tdiff500700}: temperature         &   & ensemble maximum of difference in mean\\
\hspace*{\fill} difference 500 to 700 hPa &   & \qquad all over 6--30 UTC\\
\emph{tdiff700850}: temperature         &   & \\
\hspace*{\fill} difference 700 to 850 hPa &   & \\
\hline
\emph{msl{\_}diff}: mean sea level pressure & 1 & \emph{msl{\_}mean{\_}max} $-$ \emph{msl{\_}mean{\_}min}\\
\hspace{1.6cm} difference            &   & \qquad over 6--30 UTC\\
\hline
\end{tabular}
\caption[Table caption text]{Basic covariates together with the number ({\#}) and the type of variations.}
\label{covariates}
\end{minipage} 
%}
\end{table}



\subsection{Models and evaluation}
Next to distributional forests three other methods are applied to this data set and their results are compared based on the continuous ranked probability score (CRPS). One of the reference models is an EMOS model as this is a common choice in this type of setting and research field. The other two models are both GAMs, once in a prespecified form in terms of variable selection for each distribution parameter and once applying a boosting algorithm. 
The chosen methods and their specifications are listed in Table~\ref{model_specification}.
\begin{table}[t!]
%\resizebox{1.4\textwidth}{!}{
%\begin{minipage}{\textwidth}
\centering
\begin{tabular}{ l l l l }
\hline
Model & Type & Location ($\mu$) & Scale ($\log(\sigma)$) \\
\hline
\textbf{Distributional forest} & recursive    & all & all \\ 
                & partitioning &     &     \\ 
\hline
\textbf{Prespecified GAM} & spline  & \emph{tp{\_}mean}, & \emph{tp{\_}sprd},\\
                            & in each & \emph{tp{\_}max},  & \emph{dswrf{\_}sprd{\_}mean},\\
 & & \emph{tp{\_}mean1218} $\ast$ & \emph{tp{\_}sprd1218} $\ast$\\ 
 & & \quad \emph{cape{\_}mean1218}, & \quad \emph{cape{\_}mean1218},\\
 & & \emph{dswrf{\_}mean{\_}mean}, & \emph{tcolc{\_}sprd{\_}mean},\\
 & & \emph{tcolc{\_}mean{\_}mean}, & \emph{tdiff500850{\_}mean}\\
 & & \emph{pwat{\_}mean{\_}mean}, & \\
 & & \emph{tdiff500850{\_}mean}, & \\
 & & \emph{msl{\_}diff} & \\
\hline
\textbf{Boosted GAM} & spline  & all & all \\
                       & in each &     &     \\
\hline
\textbf{EMOS} & linear & \emph{tp{\_}mean} & \emph{tp{\_}sprd} \\
\hline
\end{tabular}
\caption[Table caption text]{Overview of models with type of covariate dependency and included covariates for each distribution parameter.}
\label{model_specification}
%}
\end{table}

For the prespecified GAM model the covariates were selected manually based on meteorological knowledge and experience. Additionally, interactions are specified in advance, once between \emph{tp{\_}mean1218} and \emph{cape{\_}mean1218} for the location parameter and once between \emph{tp{\_}sprd1218} and \emph{cape{\_}mean1218} for the scale parameter. These interactions might capture events with a high potential for moderate or severe thunderstorms with large amounts of precipitation. By putting extra effort in the specification of the formulas the prespecified GAM model is provided with additional information and is therefore expected to profit from this advantage.

In the EMOS model the logarithmic function is chosen as the link function for the scale parameter. This choice has been made based on the results of \citealt{Gebetsberger2017} where different link functions (identity, quadratic and logarithmic function) have been compared. Moreover, testing the identity, quadratic and logarithmic function as link functions in this setting supported this choice as the logarithmic function lead to the best performance of the EMOS model in this application.

In order to compare the performance of the chosen methods, the continuous ranked probability score (CRPS) is calculated for each model. The CRPS measures the accuracy of a probabilistic model. For one observation $y$ with the predicted distribution function $F$ the CRPS value is calculated by
$$
\text{CRPS}(y, F) = \int_{-\infty}^\infty (F(z) - \mathbf{1}(y \leq z))^2 dz
$$
where \(\mathbf{1}\) is the indicator function.
%%% <TH> y > z??? </TH>

In this application the CRPS is evaluated out of sample on a data set that has not been used to build the model. The mean CRPS over this data set is used for validation. Since the CRPS can be seen as a generalization of the mean absolute error, a low value indicates a good fit. In applications, one big advantage in terms of interpretability is that the resulting value is on the scale of the observation.

For the purpose of comparing the performance of a distributional forest to those of other models the so-called skill score can be calculated with respect to the CRPS and one model being chosen as a reference. In that way we obtain a measure of how much a method improves the model performance in terms of CRPS compared to the reference method.
$$
\text{SkillsScore}_{\text{method}} = 1 - \frac{\text{CRPS}_{\text{method}}}{\text{CRPS}_{\text{reference}}}
$$




\subsection{Application for one station}
\label{Application for one station}
First of all we focus on one observation station, Axams in Tyrol. This station is selected due to its geographical closeness to Innsbruck, the capital of Tyrol. As for all other stations, daily observations of the month of July of 28 years are available from 1985 through the year 2012.

The results of fitting one distributional forest to the first 24 years of the data set and predicting the total amount of precipitation for one specific day of each of the four successive years have been shown in Figure~\ref{axams_onestation} as a motivational example for a probabilistic forecast. Staying with this fitted model we are now going to analyze variable importance of the distributional forest and compare its results to those of the GAM models and the EMOS model based on residual QQ-plots.
%%% <TH> wouldn't be a uniform QQ-plot be more appropriate? </TH>

%%%<TH> always us the term "distributional forest"
As shown in Table~\ref{model_specification} the distributional forest includes all 82 covariates. A single tree model allows to easily analyze which covariates are used as split variables in the nodes and hence have an impact on the response variable. In a distributional forest though, for each of the trees a subset of these covariates is chosen randomly as possible split variables which is one of the reasons leading to different splitting variables being chosen in each tree. To get an overview of which covariates have the most influential effect on the response variable a standard measurement of variable importance for forest models has been applied. Based on the mean decrease in accuracy regarding the CRPS the 10 leading covariates of the above described distributional forest for station Axams learned on the first 24 year are listed in Figure~\ref{varimp}. As expected, the numerical forecast for total precipitation is the most influential variable. In particular, five variations of this variable are among the top ten of the list of covariates ordered by variable importance.  
  

\begin{figure}[t!]
\centering
\setkeys{Gin}{width=0.7\textwidth}
<<rain_axams_varim, fig=TRUE, echo=FALSE, width=6.5, height=4.5>>=
par(mar = c(3,10,2,2))
barplot(sort(vimp_crps, decreasing = FALSE)[(length(vimp_crps)-9):length(vimp_crps)], 
        horiz = TRUE, las = 1, axes = FALSE,
        xlab = "Variable importance: mean decrease in CRPS",
        font.axis = 3, #list(family="HersheySerif", face=3),
        names.arg = gsub("pow", "", names(sort(vimp_crps, decreasing = FALSE)[(length(vimp_crps)-9):length(vimp_crps)])))
axis(1, at = seq(0,1.6,0.2), las = 1, mgp=c(0,1,0))
@
\caption{\label{varimp} CRPS-based variable importance for the top 10 covariates in the distributional forest. Based on data for station Axams, learning period 1985--2008 and assessed in 2009--2012.}
\end{figure}

In Figure~\ref{qqr_oosample} another impression of the quality of the predictions made by the tested methods is given by residual QQ-plots for out-of-sample predictions of each of the models. For these plots 100 randomized quantile residuals are simulated per observation. The closer the points are to the diagonal line the better the estimated distributional model fits the data. Overall, it can be stated that all four models provide well calibrated forecasts. In the appendix the corresponding probability integral transform (PIT) histograms (\citealt{gneiting2007}) can be found.



\begin{figure}[t!]
\centering
\setkeys{Gin}{width=0.99\textwidth}
<<rain_axams_qqr_oosample, fig=TRUE, echo=FALSE, width = 6.5>>=
# out of sample
set.seed(7)
par(mfrow = c(2,2))
qqrplot(pit_df_t, nsim = 100, main = "Distributional forest", ylim = c(-5,5), col = gray(0.1, alpha = 0.02), pch = 19)
qqrplot(pit_g_t, nsim = 100, main = "Prespecified GAM", ylim = c(-5,5), col = gray(0.1, alpha = 0.02), pch = 19)
qqrplot(pit_gb_t, nsim = 100, main = "Boosted GAM", ylim = c(-5,5), col = gray(0.1, alpha = 0.02), pch = 19)
qqrplot(pit_ml_t, nsim = 100, main = "EMOS", ylim = c(-5,5), col = gray(0.1, alpha = 0.02), pch = 19)

qqrplot(pit_ml_t, nsim = 100, main = "Boosted GAM", ylim = c(-5,5), col = gray(0.1, alpha = 0.02), pch = 19)
@
\caption{\label{qqr_oosample} Out-of-sample residual QQ-plot (2009--2012) for station Axams based on models learned on data from 1985--2008.}
\end{figure}

  
After analyzing the results of the models on one data set only where visualization is easier, we now want to get a more profound impression of the performances of the different methods. For that reason a cross validation framework is considered for the station Axams. The data set over all 28 years is split into 7 subsets each consisting of 4 randomly selected years. While 6 of these subsets are used as learning data the 7th is the testing data set for which predictions are made and evaluated. This is done by calculating the CRPS value for each of the observations in the testing data set and then averaging them. Each of the 7 parts is selected as testing data once which leads to 7 different settings over which, again, the average values are stored. This whole procedure is repeated 10 times. The resulting skill scores are illustrated in Figure~\ref{boxplots_crps_axams} using the EMOS model as the reference model represented by the horizontal line at height 0.
<<echo=FALSE, results=hide>>=
#### cross validation rain
if(file.exists("rain_Axams_7x10.rda")){
  load("rain_Axams_7x10.rda")
} else {
  source("rain_cross_7x10.R")
  library("gamlss.cens")
  gen.cens("NO", type = "left")
  rainres <- rain_cross(stationname = "Axams", 
                        seedconst = 7, ntree = 100,
                        forest_mtry = 27,
                        gamboost_cvr = TRUE,
                        frac = FALSE)
  
  save(rainres, file = "rain_Axams_7x10.rda")
}

rain_rmse <- matrix(0, ncol = ncol(rainres[[1]]$rmse), length(rainres)-1)
rain_ll <- matrix(0, ncol = ncol(rainres[[1]]$ll), length(rainres)-1)
rain_crps <- matrix(0, ncol = ncol(rainres[[1]]$crps), length(rainres)-1)

for(i in 1:(length(rainres)-1)){
  rain_rmse[i,] <- colMeans(rainres[[i]]$rmse, na.rm = TRUE)
  rain_ll[i,] <- colMeans(rainres[[i]]$ll, na.rm = TRUE)
  rain_crps[i,] <- colMeans(rainres[[i]]$crps, na.rm = TRUE)
}

colnames(rain_rmse) <- colnames(rain_ll) <- colnames(rain_crps) <- colnames(rainres[[1]]$rmse)
@


\begin{figure}[t!]
\centering
<<rain_cross_axams_crps_skill_score, fig=TRUE, echo=FALSE, height=4.5, width = 6.5>>=
boxplot(1 - rain_crps[,c(2,3,4)] / rain_crps[,6], ylim = c(-0.005, 0.065),
        names = c("Distributional forest", "Prespecified GAM", "Boosted GAM"),
        ylab = "CRPS skill score", col = "lightgray") 
abline(h = 0, col = pal["EMOS"], lwd = 2)
@
\caption{\label{boxplots_crps_axams}CRPS skill score from the 10 times 7-fold cross validation at station Axams (1985--2012). The horizontal orange line pertains to the reference model EMOS.}
\end{figure}

The CRPS skill score boxplot shows that, in this setting, both GAM models and the distributional forest perform better than the EMOS model. While the two GAM models lead to an improvement of around 4 percent with the boosted version being slightly ahead, the distributional forest even reaches an improvement rate close to 6 percent.

From this result we can conclude that the distributional forest provides a powerful alternative framework where only little or almost no effort is required to set up the model and which is competitive to existing methods that are commonly used for this kind of application or even outperforms them.

For the prespecified GAM model the covariates needed to be selected and interactions needed to be defined in advance. To do so, meteorological experience and knowledge is required and a lot of thought has to be put in these decisions. Moreover, making such decisions is always equivalent to influencing the resulting model and comes along with the risk of having missed important facts or even imposing certain effects on the model which might not correspond to the truth. This risk is avoided in the boosted GAM as all covariates are included and the selection of the influential covariates is done within a boosting framework. However, the trade off here is a remarkably higher computational effort. To build a distributional forest the user does not have to put any additional effort in it and therefore is not required to provide any expert knowledge or additional information about the setting. Variable selection is done automatically and interactions are detected by the tree structure of the model. In this particular setting, a very high number of covariates is available which intensifies the impact of the above mentioned strength of the distributional forest model. Therefore, in comparing these methods one advantage of distributional forests is clearly outlined as they offer a very flexible framework which enables an easy use of the method and at the same time lead to an improvement in the predictive performance.


\SweaveOpts{eval = TRUE}
%\newpage
\subsection{Application for all stations}
Until now the focus has been on one observation station only. To show that the presented results do not depend on the choice of this particular station we are now going to apply the methods on all 95 stations. At each of them, the first 24 years are used as learning data set and predictions are made for the last 4 years of available observations. For these out-of-sample predictions the CRPS skill score with respect to the EMOS model as reference is calculated for each method at each station. In Figure~\ref{boxplot_crps_all} the boxplots of the CRPS skill score values are plotted and show that the distributional forest leads to the highest improvement averaged over all stations. The green line represents the results for the station Axams.
%NAs in gamlss in 13 stations!!!\\
%for example in 13th station in complete{\_}stations\\
%\text{Error in glim.fit(f = mu.object, X = mu.X, y = y, w = w, fv = mu, os = mu.offset,  :}\\ 
%\text{NA's in the working vector or weights for parameter mu}
%\\
% sum(is.na(crps[,3]))
%<<eval = TRUE, echo = FALSE>>=
<<echo = FALSE>>=
#### prediction over all stations 24 - 4
if(file.exists("rain_pred_24to4.rda")){
  #load("~/svn/partykit/pkg/disttree/inst/draft/rain_pred_24to4.rda")
  load("rain_pred_24to4.rda")
} else {
  
  source("rain_pred_24to4.R")
  library("gamlss.cens")
  gen.cens("NO", type = "left")
  
  res <- rain_pred(seedconst = 7,
                   gamboost_cvr = TRUE,
                   frac = FALSE)
  
  save(res, file = "rain_pred_24to4.rda")
}
 
#ll_all <- res[[1]]$results["ll",]
#for(i in 2:(length(res)-1)) ll_all <- rbind(ll_all, res[[i]]$results["ll",])

#rmse_all <- res[[1]]$results["rmse",]
#for(i in 2:(length(res)-1)) rmse_all <- rbind(rmse_all, res[[i]]$results["rmse",])

crps_all <- res[[1]]$results["crps",]
for(i in 2:(length(res)-1))  crps_all <- rbind(crps_all, res[[i]]$results["crps",])

#colnames(ll_all) <- colnames(rmse_all) <- 
  colnames(crps_all) <- colnames(res[[1]]$results)

# skill score
s <- 1 - crps_all[, 2:4]/crps_all[,6]
colnames(s) <- c("Distributional forest", "Prespecified GAM", "Boosted GAM")

## prepare data for map which shows where distforest performed better than gamlss or gamboostLSS based on the crps

crps <- crps_all[,c("distforest", "gamlss", "gamboostLSS", "EMOS log")]  

## best method
bst <- apply(crps, 1, which.min)

## distance of forest to best other method
dst <- crps[,1] - crps[cbind(1:nrow(crps), apply(crps[, -1], 1, which.min) + 1)]

## breaks/groups
brk <- c(-0.1, -0.05, -0.005, 0.005, 0.05, 0.1)
#brk <- c(-0.1, -0.05, -0.01, 0.01, 0.05, 0.1)
grp <- cut(dst, breaks = brk)

## HCL colors (relatively flashy, essentially CARTO Tropic)
clr <- colorspace::diverge_hcl(5, h = c(195, 325), c = 80, l = c(50, 90), power = 1.3)



library("raster") # dem (digital elevation model)
library("sp")     # gadm www.gadm.org/country

load("plot_map_rain/demo/tirol.gadm.rda")
load("plot_map_rain/demo/tirol.dem.rda")
load("plot_map_rain/demo/ehyd.statlist.rda")
  
# Create SpatialPointsDataFrame from station list
sp <- SpatialPointsDataFrame(subset(ehyd.statlist[res$complete_stations,],
                                    select=c(lon,lat)),
                             data = subset(ehyd.statlist[res$complete_stations,],
                                           select = -c(lon,lat)),
                             proj4string = crs(tirol.dem))

#library("colorspace")
@



\begin{figure}[t!]
\centering
<<rain_all_crps_skill_score, fig=TRUE, echo=FALSE, width = 6.5>>=
  matplot(t(s[,]), type = "l", lwd = 2, 
          col = gray(0.5, alpha = 0.2),
          lty = 1, axes = FALSE, 
          xlab = "", ylab = "CRPS skill score", xlim = c(0.5, 3.5))
lines(s[70,], col = "limegreen", type = "o", pch = 19, lwd = 2)  
# Station Axams is the 77th station which is the 70th in the list of complete stations
  boxplot(s, add = TRUE, col = "transparent")
  abline(h = 0, col = pal["EMOS"], lwd = 2)
    
@
\caption{\label{boxplot_crps_all}CRPS skill score for each station (grey lines) and aggregated over all stations (boxplots). Station Axams is highlighted in green and the horizontal orange line pertains to the reference model EMOS. The models are learned on data from 24 years (1985--2008) and validated on 4 years (2009--2012).}
\end{figure}

Figure~\ref{map} shows the location of all stations in the data set. Each observation station is illustrated by a symbol with the type of symbol representing the method that performed best in terms of CRPS at this station. The color of the symbol indicates the difference of the CRPS of the distributional forest and the CRPS of the best performing among the other three models (prespecified GAM, boosted GAM and EMOS).

\begin{figure}[t!]
\centering
\setkeys{Gin}{width=0.99\textwidth}
<<map, fig=TRUE, echo=FALSE, height=6.1, width=9>>=
  
  layout(cbind(1, 2), width = c(9, 1))
  par(mar = c(5,4,4,0.1))
  raster::image(tirol.dem, col = rev(gray.colors(100)),
                main="Stations in Tyrol", ylab = "Latitude", xlab = "Longitude", 
                xlim = c(9.8,13.2), 
                ylim = c(46.6, 47.85))
  plot(tirol.gadm, add = TRUE)
  points(sp, pch = c(21, 24, 25, 22)[bst], bg = clr[grp], col = "black", las = 1, cex = 1.5)
  legend(x = 9.8, y = 47.795, pch=c(21,24,25,22), legend = c("Distributional forest", "Prespecified GAM", "Boosted GAM", "EMOS"), cex = 1, bty = "n")
  text(x = 10.3, y = 47.8, labels = "Models with lowest CRPS")
  mtext("CRPS\ndifference", side=4, las = TRUE, at = c(x = 13.5, y = 47.74), line = 0.3)
  par(mar = c(0.5,0.2,0.5,2.3))
  ## legend
  plot(0, 0, type = "n", axes = FALSE, xlab = "", ylab = "",
       xlim = c(0, 1), ylim = c(-0.2, 0.2), xaxs = "i", yaxs = "i")
  rect(0, brk[-6], 0.5, brk[-1], col = rev(clr))
  axis(4, at = brk, las = 1, mgp=c(0,-0.5,-1))
  
@
\caption{\label{map}Map of Tyrol showing which model performed best at which station (type of symbol). The color codes whether the distributional forest had lower or higher CRPS (red or blue) compared to the best of the other three models (prespecified GAM, boosted GAM and EMOS).}
\end{figure}





\section{Discussion}

%%% <TH> this should go into the discussion or the interpretation of the
%%% empirical results </TH>
Even though many situations demand for the use of specifically tailored models providing the needed features and therefore being clearly advantageous and the best choice, it is often hard to tell in advance which features will be required. In these cases a distributional forest is a recommendable compromise providing reasonable models and performing well without any prespecification or additional knowledge needed in advance. 



\section*{Computational details}
The corresponding implementation to the novel methods presented in this article is available on R-Forge in an \textsf{R} package called \textbf{disttree} within the project \textbf{partykit} (\url{https://R-Forge.R-project.org/projects/partykit/}). The function \code{disttree} uses either the \code{mob} or \code{ctree} function as a framework to build a tree. Similarly, the function \code{distforest} is based on the \code{cforest} function which provides a framework to build a distributional forest. All three functions, \code{mob}, \code{ctree} and \code{cforest}, are provided in the package \textbf{partykit} (\citealt{hothorn2015partykit}).












%\newpage
\nocite{Messner2016, stasinopoulos2007generalized, Seibold2016, Seibold2017, yee2010vgam, gamboostLSS2016, srtm}
% zeileis2008implementing, 
% zeileis2010party, 
% hothorn2015package,

%\newpage
%\newpage
\bibliography{ref.bib}


\newpage
\begin{appendix}

\section{Tree algorithm}
The tree algorithm used in the applications discussed in this paper is explained in the following. For notational simplicity, the testing and splitting procedure is described for the root node with observations $\{y_i\}_{i = 1,\ldots,n}$, $n \in \mathbb{N}$. In each child node the corresponding subset of observations depends on the assignment made by the foregoing split.\\

\begin{itemize}
\item A distributional model $D(Y, \theta)$ is fit to the set of observations $\{y_i\}_{i = 1,\ldots,n}$ by estimating the distribution parameter $\hat{\theta}$. This parameter can be a multidimensional vector $(\hat{\theta}_1, \ldots, \hat{\theta}_K)$, $K \in \mathbb{N}$, depending on the type of distribution. The estimation is done by maximizing the log-likelihood function $\ell(\theta; Y)$, i.e.
$$
\hat{\theta} = \max_{\theta \in \Theta} \sum_{i=1}^n \ell(\theta; y_i)
$$
which is equal to solving
$$
\sum_{i=1}^n s(\theta, y_i) = \sum_{i=1}^n \frac{\partial \ell}{\partial \theta}(\theta; y_i) = 0
$$
for $\theta$.

\item Calculate score values 
$$
s(\hat{\theta}, y_i) = 
\begin{pmatrix} 
s(\hat{\theta}, y_1) \\
s(\hat{\theta}, y_2) \\
\vdots \\
s(\hat{\theta}, y_n)
\end{pmatrix} =
\begin{pmatrix} 
s(\hat{\theta}, y_1)_1 & s(\hat{\theta}, y_1)_2 & \ldots & s(\hat{\theta}, y_1)_K\\
s(\hat{\theta}, y_2)_1 & s(\hat{\theta}, y_2)_2 & \ldots & s(\hat{\theta}, y_2)_K\\
\vdots & \vdots & \ddots & \vdots \\
s(\hat{\theta}, y_n)_1 & s(\hat{\theta}, y_n)_2 & \ldots & s(\hat{\theta}, y_n)_K
\end{pmatrix}
$$

\item Test for independence between each column of scores $\left(s(\hat{\theta}, y_i)_k\right)_{i=1,\ldots,n}$ for $k\in\{1,\ldots,K\}$, and split variable $Z_j \in \{Z_1, \ldots, Z_m\}$.  
\begin{align*}
H_0^{1,j}:  \left(s(\hat{\theta}, y_i)_1\right)_{i=1,\ldots,n} \qquad \bot \qquad Z_j \\
H_0^{2,j}:  \left(s(\hat{\theta}, y_i)_2\right)_{i=1,\ldots,n} \qquad \bot \qquad Z_j \\
\vdots \hspace{3cm}\\
H_0^{K,j}:  \left(s(\hat{\theta}, y_i)_K\right)_{i=1,\ldots,n} \qquad \bot \qquad Z_j 
\end{align*}

To test these hypotheses permutation testing with the multivariate linear statistic
$$
T_{j} = \sum_{i=1}^n g_j(z_{ji}) \cdot s(\hat{\theta}, y_i)
$$
is applied. The type of the transformation function $g_j$ depends on the type of the split variable $Z_j$. If $Z_j$ is numeric then $g_j$ is simply the identity function $g_j(z_{ji}) = z_{ji}$. If $Z_j$ is a categorical variable with $L$ categories then $g_j(z_{ji}) = e_L(z_{ji}) = (\I(z_{ji} = 1), \ldots, \I(z_{ji} = L))$ such that $g_j$ is a unit vector where the element corresponding to the value of $z_{ji}$ is $1$. Observations with missing values are excluded from the sums.

With the conditional expectation $\mu_{j}$ and the covariance $\Sigma_{j}$ as derived by \cite{strasser1999} the test statistic can be standardized. For a numeric split variable this results in the Pearson correlation coefficient
$$
c(t_{j},\mu_{j},\Sigma_{j}) = \left|\frac{t_{j} - \mu_{j}}{\sqrt{\Sigma_{j}}}\right|.
$$
Otherwise the standardized form is
$$
c(t_{j},\mu_{jk},\Sigma_{j}) = \max_{l=1,\ldots,L}\left|\frac{(t_{j} - \mu_{j})_l}{\sqrt{(\Sigma_{j})_{ll}}}\right|.
$$
The smaller the p-value corresponding to the standardized test statistic $c(t_{j},\mu_{j},\Sigma_{j})$ is the stronger the discrepancy from the assumption of independence between the scores and the split variable $Z_j$.

\item After Bonferroni adjusting the p-values check whether any of the resulting p-values is beneath the selected significance level. If so, choose the partitioning variable $Z_{j^\ast}$ with the lowest p-value to the scores corresponding to the distribution parameters relevant for the split.

\item Choose the breakpoint that leads to the highest discrepancy between score functions in the two resulting subgroups. This difference can be measured by the linear statistic
$$
T_{j^{\ast}}^r = \sum_{i \in \mathcal{B}_{1r}} s(\hat{\theta}, y_i)
$$
where $\mathcal{B}_{1r}$ is the first of the two new subgroups that are defined by splitting in split point $r$ of variable $Z_{j^{\ast}}$. The split point is then chosen as follows:
$$
r^{\ast} = \argmin_{r} c(t_{j^{\ast}}^r,\mu_{j^{\ast}}^r,\Sigma_{j^{\ast}}^r).
$$

\item Repeat the testing and splitting procedure in each of the resulting subgroups until some stopping criterion is reached. This criterion can for example be a minimal number of observations in a node or a minimal p-value for the statistical tests. 

\end{itemize}

This permutation test based tree algorithm is presented in \citealt{hothorn2006unbiased} as the ctree algorithm. A different framework to build a tree is provided by the MOB algorithm which is based on M-fluctuation tests (\citealt{zeileis2008model}).



%\begin{figure}[t!]
%\begin{subfigure}{0.5\textwidth}
%<<rain_axams_pit_insample, fig=TRUE, echo=FALSE>>=
%
%# in sample
%set.seed(7)
%par(mfrow = c(2,2))
%pithist(pit_df_l, nsim = 1000, breaks = seq(0, 1, length.out = 9))
%pithist(pit_g_l, nsim = 1000, breaks = seq(0, 1, length.out = 9))
%pithist(pit_gb_l, nsim = 1000, breaks = seq(0, 1, length.out = 9))
%pithist(pit_ml_l, nsim = 1000, breaks = seq(0, 1, length.out = 9))
%
%@
%\caption{\label{pit_insample} PIT histograms for station Axams (in sample)}
%\end{figure}

\section{Probability integral transform (PIT) histograms}
Probability integral transform (PIT) histograms \cite{gneiting2007} are a commonly used tool to assess the predictive power of distributional forecasting methods, particularly in the field of meteorological forecasting \\
Let $x$ be an observation with true distribution function $G$ and predicted distribution function $F$. Then the corresponding probability integral transform (PIT) value is $p = F(x)$. In the ideal case of a perfect prediction and $F$ being continuous $p$ follows a uniform distribution. This uniformity can be analysed applying a histogram. The closer the bars of a PIT histogram are to the value 1, the closer the distribution of $p$ is to a uniform distribution and therefore the better the predicted distribution models the true distribution.

The PIT histograms in Figure~\ref{pit_oosample} correspond to the forecasts presented in Section~\ref{Application for one station}. 
Daily observations of total precipitation from station Axams are considered and predictions are made for the 24th of July 2009, 2010, 2011 and 2012, based on data from the previous 24 years.


\begin{figure}[t!]
\centering
%\begin{subfigure}{0.8\textwidth}
<<rain_axams_pit_oosample, fig=TRUE, echo=FALSE, width = 6.5>>=
    
# out of sample
set.seed(4)

par(mfrow = c(2,2))
pithist(pit_df_t, nsim = 1000, breaks = seq(0, 1, length.out = 9), main = "Distributional forest", ylim = c(0,1.5))
pithist(pit_g_t, nsim = 1000, breaks = seq(0, 1, length.out = 9), main = "Prespecified GAM", ylim = c(0,1.5))
pithist(pit_gb_t, nsim = 1000, breaks = seq(0, 1, length.out = 9), main = "Boosted GAM", ylim = c(0,1.5))
pithist(pit_ml_t, nsim = 1000, breaks = seq(0, 1, length.out = 9), main = "EMOS", ylim = c(0,1.5))
      
@
%\end{subfigure}
\caption{\label{pit_oosample}Out-of-sample PIT histograms (2009--2012) for station Axams and models learned on data from 1985--2008.}
\end{figure}

\end{appendix}


\end{document}
