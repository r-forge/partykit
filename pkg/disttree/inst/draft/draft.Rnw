\documentclass[nojss]{jss}

%\VignetteIndexEntry{disttree: Distributional Trees and Forests}
%\VignetteDepends{partykit, gamlss.dist}
%\VignetteKeywords{regression trees, random forests, distributional regression, recursive partitioning}
%\VignettePackage{disttree}

%% packages
\usepackage{amstext}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{thumbpdf,lmodern}
%\usepackage{rotating}
%\usepackage{caption}
%\captionsetup{format=hang}
%\usepackage{subcaption}
\usepackage{Sweave}
%\usepackage{enumitem}
%\usepackage{graphicx}
\usepackage{array, makecell}
\usepackage{tikz}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{positioning,shapes,arrows,decorations.pathreplacing,calc,automata,mindmap}
%\usepackage[pagebackref=true]{hyperref}
%\usepackage{doi}

%% commands
\newcommand{\fixme}[1]{\emph{\marginpar{FIXME} (#1)}}
\renewcommand{\Prob}{\mathbb{P} }
\renewcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\Var}{\mathbb{V}}
\newcommand{\R}{\mathbb{R} }
\newcommand{\N}{\mathbb{N} }
\newcommand{\C}{\mathbb{C} }
\newcommand{\argmin}{\operatorname{argmin}\displaylimits}
\newcommand{\argmax}{\operatorname{argmax}\displaylimits}
\newcommand{\LS}{\mathcal{L}_n}
\newcommand{\TS}{\mathcal{T}_n}
\newcommand{\LSc}{\mathcal{L}_{\text{comb},n}}
\newcommand{\LSbc}{\mathcal{L}^*_{\text{comb},n}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\yn}{y_{\text{new}}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\sX}{\mathcal{X}}
\newcommand{\sY}{\mathcal{Y}}
\newcommand{\T}{\mathbf{T}}
\newcommand{\x}{\mathbf{x}}
\renewcommand{\a}{\mathbf{a}}
\newcommand{\xn}{\mathbf{x}_{\text{new}}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\ws}{\mathbf{w}_\cdot}
\renewcommand{\t}{\mathbf{t}}
\newcommand{\M}{\mathbf{M}}
\renewcommand{\vec}{\text{vec}}
\newcommand{\B}{\mathbf{B}}
\newcommand{\K}{\mathbf{K}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\cellx}{\pi_n[\x]}
\newcommand{\partn}{\pi_n(\mathcal{L}_n)}
\newcommand{\err}{\text{Err}}
\newcommand{\ea}{\widehat{\text{Err}}^{(a)}}
\newcommand{\ecv}{\widehat{\text{Err}}^{(cv1)}}
\newcommand{\ecvten}{\widehat{\text{Err}}^{(cv10)}}
\newcommand{\eone}{\widehat{\text{Err}}^{(1)}}
\newcommand{\eplus}{\widehat{\text{Err}}^{(.632+)}}
\newcommand{\eoob}{\widehat{\text{Err}}^{(oob)}}
\newcommand{\bft}{\mathbf{t}}

\hyphenation{Qua-dra-tic}

%% colors
\definecolor{HighlightOrange}{rgb}{0.9490196,0.5725490,0.0000000}
\definecolor{HighlightBlue}{rgb}{0.4784314,0.7490196,0.9803922}



\SweaveOpts{engine=R, eps=FALSE, keep.source=TRUE}
<<preliminaries, echo=FALSE, results=hide>>=
options(prompt = "R> ", continue = "+  ", useFancyQuotes = FALSE, width = 70)
library("ggplot2")
theme_set(theme_bw(base_size = 18))
library("disttree")
library("Formula")  # FIX ME: should be imported in disttree
library("gamlss")
library("randomForest")
library("lattice")
library("crch")
library("latex2exp")
library("parallel")
library("gamlss.cens")
gen.cens(NO, type = "left")
library("mgcv")
library("MASS")
library("countreg")

## HCL palette
pal <- hcl(c(10, 128, 260, 290, 50), 100, 50)
names(pal) <- c("forest", "tree", "gamlss", "gamboostLSS", "EMOS")

pallight <- hcl(c(10, 128, 260, 290, 70), 100, 50, alpha = 0.25)
names(pallight) <- c("forest", "tree", "gamlss", "gamboostLSS", "EMOS")

transpgrey <- rgb(0.190,0.190,0.190, alpha = 0.2)

## define distribution list:
# dist_list_normal
{
  
  dist_list_normal <- list()
  
  parnames <- c("mu", "sigma")
  etanames <- c("mu", "log(sigma)")
  
  
  ddist <-  function(y, eta, log = TRUE, weights = NULL, sum = FALSE) {     
    
    val <- -1/2 * (log(2*pi) + 2*eta[2] + exp(log((y-eta[1])^2) - 2*eta[2]))
    if(!log) val <- exp(val)
    
    # par <- c(eta[1], exp(eta[2]))
    # val <- dnorm(y, mean = par[1], sd = par[2], log = log)
    
    if(sum) {
      if(is.null(weights)) weights <- rep.int(1, length(y))
      val <- sum(weights * val, na.rm = TRUE)
    }
    return(val)
  }
  
  
  sdist <- function(y, eta, weights = NULL, sum = FALSE) {   
    
    score <- cbind(exp(-2*eta[2]) * (y-eta[1]), 
                   -1 + exp(-2*eta[2] + log((y-eta[1])^2)))
    
    # par <- c(eta[1], exp(eta[2])) 
    # score <- cbind(1/par[2]^2 * (y-par[1]), 
    #                (-1/par[2] + ((y - par[1])^2)/(par[2]^3)) * exp(eta[2]))
    
    score <- as.matrix(score)
    colnames(score) <- etanames
    if(sum) {
      if(is.null(weights)) weights <- rep.int(1, length(y))
      # if score == Inf replace score with 1.7e308 because Inf*0 would lead to NaN -> gradient is NaN
      score[score==Inf] = 1.7e308
      score <- colSums(weights * score, na.rm = TRUE)
    }
    return(score)
  }
  
  
  hdist <- function(y, eta, weights = NULL) {    
    ny <- length(y)
    if(is.null(weights)) weights <- rep.int(1, ny)
    
    d2ld.etamu2 <- sum(weights * rep.int(-exp(-2*eta[2]), ny))
    d2ld.etamu.d.etasigma <- sum(weights * (-2)*(y-eta[1]) * exp(-2*eta[2]), na.rm = TRUE)          # should be 0 for exact parameters (here: observed hess)
    d2ld.etasigma2 <- sum(weights * (-2)*exp(log((y-eta[1])^2) - 2*eta[2]), na.rm = TRUE)    
    
    # par <- c(eta[1], exp(eta[2]))                           
    # d2ld.etamu2 <- sum(weights * rep.int(-1/par[2]^2, ny))
    # d2ld.etamu.d.etasigma <- sum(weights * (-2)*(y-par[1])/par[2]^2), na.rm = TRUE)          # should be 0 for exact parameters (here: observed hess)
    # d2ld.etasigma2 <- sum(weights * (-2)*(y-par[1])^2/par[2]^2, na.rm = TRUE)         
    
    hess <- matrix(c(d2ld.etamu2, d2ld.etamu.d.etasigma, d2ld.etamu.d.etasigma, d2ld.etasigma2), nrow = 2)
    colnames(hess) <- rownames(hess) <-  etanames
    
    return(hess)
  }
  
  
  ## additional functions pdist, qdist, rdist
  pdist <- pnorm
  qdist <- qnorm
  rdist <- rnorm  
  
  
  link <- c("identity", "log")
  
  linkfun <- function(par) {
    eta <- c(par[1], log(par[2]))
    names(eta) <- etanames
    return(eta)
  }
  
  
  linkinv <- function(eta) {
    par <- c(eta[1], exp(eta[2]))
    names(par) <- parnames
    return(par)
  }
  
  
  linkinvdr <- function(eta) {
    dpardeta <- c(1, exp(eta[2]))
    names(dpardeta) <- parnames
    return(dpardeta)
  }
  
  
  startfun <- function(y, weights = NULL){
    if(is.null(weights)) {
      mu <- mean(y)
      sigma <- sqrt(1/length(y) * sum((y - mu)^2))
    } else {
      mu <- weighted.mean(y, weights)
      sigma <- sqrt(1/sum(weights) * sum(weights * (y - mu)^2))
    }
    starteta <- c(mu, log(sigma))
    names(starteta) <- etanames
    return(starteta)
  }
  
  mle <- TRUE
  
  dist_list_normal <- list(family.name = "Normal Distribution",
                           ddist = ddist, 
                           sdist = sdist, 
                           hdist = hdist,
                           pdist = pdist,
                           qdist = qdist,
                           rdist = rdist,
                           link = link, 
                           linkfun = linkfun, 
                           linkinv = linkinv, 
                           linkinvdr = linkinvdr,
                           startfun = startfun,
                           mle = mle,
                           gamlssobj = FALSE,
                           censored = FALSE
  )
}


# dist_list_cens_normal
{
  
  dist_list_cens_normal <- list()
  
  parnames <- c("mu", "sigma")
  etanames <- c("mu", "log(sigma)")
  
  ddist <-  function(y, eta, log = TRUE, weights = NULL, sum = FALSE, left = 0, right = Inf) {     
    par <- c(eta[1], exp(eta[2]))
    val <- crch::dcnorm(x = y, mean = par[1], sd = par[2], left = left, right = right, log = log)
    if(sum) {
      if(is.null(weights)) weights <- if(is.matrix(y)) rep.int(1, dim(y)[1]) else rep.int(1, length(y))
      val <- sum(weights * val, na.rm = TRUE)
    }
    return(val)
  }
  
  
  sdist <- function(y, eta, weights = NULL, sum = FALSE, left = 0, right = Inf) {   
    par <- c(eta[1], exp(eta[2]))
    # y[y==0] <- 1e-323
    
    score_m <- crch:::scnorm(x = y, mean = par[1], sd = par[2], which = "mu", left = left, right = right)
    score_s <- crch:::scnorm(x = y, mean = par[1], sd = par[2], which = "sigma", left = left, right = right) * exp(eta[2]) # inner derivation exp(eta[2])
    score <- cbind(score_m, score_s)
    score <- as.matrix(score)
    colnames(score) <- etanames
    if(sum) {
      if(is.null(weights)) weights <- rep.int(1, length(y)[1])
      # if score == Inf replace score with 1.7e308 because Inf*0 would lead to NaN (0 in weights)
      score[score==Inf] = 1.7e308
      score <- colSums(weights * score, na.rm = TRUE)
      #if(any(is.nan(score))) print(c(eta, "y", y))
    }
    return(score)
  }
  
  
  hdist <- function(y, eta, weights = NULL, left = 0, right = Inf) {    
    ny <- length(y)
    if(is.null(weights)) weights <- rep.int(1, ny)
    
    par <- c(eta[1], exp(eta[2]))                           
    # y[y==0] <- 1e-323
    
    d2mu <- crch:::hcnorm(x = y, mean = par[1], sd = par[2], which = "mu", left = left, right = right)
    d2sigma <- crch:::hcnorm(x = y, mean = par[1], sd = par[2], which = "sigma", left = left, right = right)
    dmudsigma <- crch:::hcnorm(x = y, mean = par[1], sd = par[2], which = "mu.sigma", left = left, right = right) # FIX: order?
    dsigmadmu <- crch:::hcnorm(x = y, mean = par[1], sd = par[2], which = "sigma.mu", left = left, right = right) # FIX: order?
    dsigma <- crch:::scnorm(x = y, mean = par[1], sd = par[2], which = "sigma", left = left, right = right)
    
    d2ld.etamu2 <- sum(weights * d2mu, na.rm = TRUE)
    d2ld.etamu.d.etasigma <- sum(weights * dmudsigma * par[2], na.rm = TRUE)
    d2ld.etasigma.d.etamu <- sum(weights * dsigmadmu * par[2], na.rm = TRUE)
    d2ld.etasigma2 <- sum(weights * (d2sigma * exp(2*eta[2]) + dsigma * par[2]), na.rm = TRUE)         
    
    hess <- matrix(c(d2ld.etamu2, d2ld.etamu.d.etasigma, d2ld.etasigma.d.etamu, d2ld.etasigma2), nrow = 2)
    colnames(hess) <- rownames(hess) <-  etanames
    
    return(hess)
  }
  
  
  ## additional functions pdist, qdist, rdist
  pdist <- function(q, eta, lower.tail = TRUE, log.p = FALSE) crch:::pcnorm(q, mean = eta[1], sd = eta[2], 
                                                                            lower.tail = lower.tail, log.p = log.p, 
                                                                            left = left, right = right)
  qdist <- function(p, eta, lower.tail = TRUE, log.p = FALSE) crch:::qcnorm(p, mean = eta[1], sd = eta[2], 
                                                                            lower.tail = lower.tail, log.p = log.p, 
                                                                            left = left, right = right)
  rdist <- function(n, eta) crch:::rcnorm(n, mean = eta[1], sd = eta[2], left = left, right = right)

  
  link <- c("identity", "log")
  
  linkfun <- function(par) {
    eta <- c(par[1], log(par[2]))
    names(eta) <- etanames
    return(eta)
  }
  
  
  linkinv <- function(eta) {
    par <- c(eta[1], exp(eta[2]))
    names(par) <- parnames
    return(par)
  }
  
  
  linkinvdr <- function(eta) {
    dpardeta <- c(1, exp(eta[2]))
    names(dpardeta) <- parnames
    return(dpardeta)
  }
  
  
  startfun <- function(y, weights = NULL){
    yc <- pmax(0,y)  # optional ?
    if(is.null(weights)) {
      mu <- mean(yc)
      sigma <- sqrt(1/length(yc) * sum((yc - mu)^2))
    } else {
      mu <- weighted.mean(yc, weights)
      sigma <- sqrt(1/sum(weights) * sum(weights * (yc - mu)^2))
    }
    starteta <- c(mu, log(sigma))
    names(starteta) <- etanames
    return(starteta)
  }
  
  mle <- FALSE
  
  dist_list_cens_normal <- list(family.name = "censored Normal Distribution",
                                ddist = ddist, 
                                sdist = sdist, 
                                hdist = hdist,
                                pdist = pdist,
                                qdist = qdist,
                                rdist = rdist,
                                link = link, 
                                linkfun = linkfun, 
                                linkinv = linkinv, 
                                linkinvdr = linkinvdr,
                                startfun = startfun,
                                mle = mle,
                                gamlssobj = FALSE,
                                censored = TRUE
  )
}





  



# plot RMSE
plot_rmse <- function(simres, type = c("exp", "par"), ylim = NULL, legend = TRUE){
  
  if(type == "exp"){
    rmse <- NULL
    colnames <- NULL
    col <- NULL
    legendnames <- NULL
    if("av.rmse.exp.true.dt" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.dt"], simres$resmat[,"av.rmse.exp.obs.dt"])
      colnames <- c(colnames, "dt.true", "dt.obs")
      col <- c(col, pal["tree"])
      legendnames <- c(legendnames, "tree")
    }
    if("av.rmse.exp.true.df" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.df"], simres$resmat[,"av.rmse.exp.obs.df"])
      colnames <- c(colnames, "df.true", "df.obs")
      col <- c(col, pal["forest"])
      legendnames <- c(legendnames, "forest")
    }
    if("av.rmse.exp.true.g" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.g"], simres$resmat[,"av.rmse.exp.obs.g"])
      colnames <- c(colnames, "g.true", "g.obs")
      col <- c(col, pal["gamlss"])
      legendnames <- c(legendnames, "gamlss")
    }
    if("av.rmse.exp.true.b" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.b"], simres$resmat[,"av.rmse.exp.obs.b"])
      colnames <- c(colnames, "b.true", "b.obs")
      col <- c(col, pal["bamlss"])
      legendnames <- c(legendnames, "bamlss")
    }
    if("av.rmse.exp.true.gb" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.gb"], simres$resmat[,"av.rmse.exp.obs.gb"])
      colnames <- c(colnames, "gb.true", "gb.obs")
      col <- c(col, pal["gamboostLSS"])
      legendnames <- c(legendnames, "gamboostLSS")
    }
    if("av.rmse.exp.true.rf" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.rf"], simres$resmat[,"av.rmse.exp.obs.rf"])
      colnames <- c(colnames, "rf.true", "rf.obs")
      col <- c(col, pal["randomForest"])
      legendnames <- c(legendnames, "randomForest")
    }
    if("av.rmse.exp.true.cf" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.cf"], simres$resmat[,"av.rmse.exp.obs.cf"])
      colnames <- c(colnames, "cf.true","cf.obs")
      col <- c(col, pal["cforest"])
      legendnames <- c(legendnames, "cforest")
    }
    
    colnames(rmse) <- colnames
    if(is.null(ylim)) ylim <- c(min(na.omit(rmse)), max(na.omit(rmse)))
    
    plot(x = simres$x.axis, y = rmse[,1], type = "l", col = col[1], ylim = ylim,
         xlab = expression(kappa), ylab = "RMSE")
    lines(x = simres$x.axis, y = rmse[,2], type = "l", lty = 2, col = col[1])
    
    for(i in 1:(length(col)-1)){
      lines(x = simres$x.axis, y = rmse[,(2*i+1)], type = "l", col = col[i+1])
      lines(x = simres$x.axis, y = rmse[,(2*i+2)], type = "l", lty = 2, col = col[i+1])
    }
    if(legend) legend('topleft', legendnames, 
                      col = col, lty = 1, cex = 0.7)
  } 
  
  if(type == "par"){
    
    rmse <- NULL
    colnames <- NULL
    col <- NULL
    legendnames <- NULL
    if("av.rmse.exp.true.dt" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.dt"], simres$resmat[,"av.rmse.sigma.dt"])
      colnames <- c(colnames, "dt.true", "dt.obs")
      col <- c(col, pal["tree"])
      legendnames <- c(legendnames, "tree")
    }
    if("av.rmse.exp.true.df" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.df"], simres$resmat[,"av.rmse.sigma.df"])
      colnames <- c(colnames, "df.true", "df.obs")
      col <- c(col, pal["forest"])
      legendnames <- c(legendnames, "forest")
    }
    if("av.rmse.exp.true.g" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.g"], simres$resmat[,"av.rmse.sigma.g"])
      colnames <- c(colnames, "g.true", "g.obs")
      col <- c(col, pal["gamlss"])
      legendnames <- c(legendnames, "gamlss")
    }
    if("av.rmse.exp.true.b" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.b"], simres$resmat[,"av.rmse.sigma.b"])
      colnames <- c(colnames, "b.true", "b.obs")
      col <- c(col, pal["bamlss"])
      legendnames <- c(legendnames, "bamlss")
    }
    if("av.rmse.exp.true.gb" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.gb"], simres$resmat[,"av.rmse.sigma.gb"])
      colnames <- c(colnames, "gb.true", "gb.obs")
      col <- c(col, pal["gamboostLSS"])
      legendnames <- c(legendnames, "gamboostLSS")
    }
    if("av.rmse.exp.true.rf" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.rf"], simres$resmat[,"av.rmse.sigma.rf"])
      colnames <- c(colnames, "rf.true", "rf.obs")
      col <- c(col, pal["randomForest"])
      legendnames <- c(legendnames, "randomForest")
    }
    if("av.rmse.exp.true.cf" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.cf"], simres$resmat[,"av.rmse.sigma.cf"])
      colnames <- c(colnames, "cf.true","cf.obs")
      col <- c(col, pal["cforest"])
      legendnames <- c(legendnames, "cforest")
    }
    
    colnames(rmse) <- colnames
    if(is.null(ylim)) ylim <- c(min(na.omit(rmse)), max(na.omit(rmse)))
    
    plot(x = simres$x.axis, y = rmse[,1], type = "l", col = col[1], ylim = ylim,
         xlab = expression(kappa), ylab = "RMSE")
    lines(x = simres$x.axis, y = rmse[,2], type = "l", lty = 2, col = col[1])
    
    for(i in 1:(length(col)-1)){
      lines(x = simres$x.axis, y = rmse[,(2*i+1)], type = "l", col = col[i+1])
      lines(x = simres$x.axis, y = rmse[,(2*i+2)], type = "l", lty = 2, col = col[i+1])
    }
    if(legend) legend('topleft', legendnames, 
                      col = col, lty = 1, cex = 0.7)
  }
}

# plot loglikelihood
plot_ll <- function(simres, ylim = NULL, legend = TRUE){
  
  ll <- NULL
  colnames <- NULL
  col <- NULL
  legendnames <- NULL
  if("av.rmse.exp.true.dt" %in% colnames(simres$resmat)) {
    ll <- cbind(ll, simres$resmat[,"av.loglik.dt"])
    colnames <- c(colnames, "dt.ll")
    col <- c(col, pal["tree"])
    legendnames <- c(legendnames, "tree")
  }
  if("av.rmse.exp.true.df" %in% colnames(simres$resmat)) {
    ll <- cbind(ll, simres$resmat[,"av.loglik.df"])
    colnames <- c(colnames, "df.ll")
    col <- c(col, pal["forest"])
    legendnames <- c(legendnames, "forest")
  }
  if("av.rmse.exp.true.g" %in% colnames(simres$resmat)) {
    ll <- cbind(ll, simres$resmat[,"av.loglik.g"])
    colnames <- c(colnames, "g.true")
    col <- c(col, pal["gamlss"])
    legendnames <- c(legendnames, "gamlss")
  }
  if("av.rmse.exp.true.b" %in% colnames(simres$resmat)) {
    ll <- cbind(ll, simres$resmat[,"av.loglik.b"])
    colnames <- c(colnames, "b.true")
    col <- c(col, pal["bamlss"])
    legendnames <- c(legendnames, "bamlss")
  }
  if("av.rmse.exp.true.gb" %in% colnames(simres$resmat)) {
    ll <- cbind(ll, simres$resmat[,"av.loglik.gb"])
    colnames <- c(colnames, "gb.true")
    col <- c(col, pal["gamboostLSS"])
    legendnames <- c(legendnames, "gamboostLSS")
  }
  if("av.rmse.exp.true.rf" %in% colnames(simres$resmat)) {
    ll <- cbind(ll, simres$resmat[,"av.loglik.rf"])
    colnames <- c(colnames, "rf.true")
    col <- c(col, pal["randomForest"])
    legendnames <- c(legendnames, "randomForest")
  }
  if("av.rmse.exp.true.cf" %in% colnames(simres$resmat)) {
    ll <- cbind(ll, simres$resmat[,"av.loglik.cf"])
    colnames <- c(colnames, "cf.true")
    col <- c(col, pal["cforest"])
    legendnames <- c(legendnames, "cforest")
  }
  
  colnames(ll) <- colnames
  if(is.null(ylim)) ylim <- c(min(na.omit(ll)), max(na.omit(ll)))
  
  plot(x = simres$x.axis, y = ll[,1], type = "l", col = col[1], ylim = ylim,
       xlab = expression(kappa), ylab = "log-likelihood")
  
  for(i in 2:length(col)){
    lines(x = simres$x.axis, y = ll[,i], type = "l", col = col[i])
  }
  if(legend) legend('topleft', legendnames, 
                    col = col, lty = 1, cex = 0.7)
}

# plot crpse
plot_crps <- function(simres, ylim = NULL, legend = TRUE){
  
  crps <- NULL
  colnames <- NULL
  col <- NULL
  legendnames <- NULL
  if("av.rmse.exp.true.dt" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.dt"])
    colnames <- c(colnames, "dt.crps")
    col <- c(col, pal["tree"])
    legendnames <- c(legendnames, "tree")
  }
  if("av.rmse.exp.true.df" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.df"])
    colnames <- c(colnames, "df.crps")
    col <- c(col, pal["forest"])
    legendnames <- c(legendnames, "forest")
  }
  if("av.rmse.exp.true.g" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.g"])
    colnames <- c(colnames, "g.true")
    col <- c(col, pal["gamlss"])
    legendnames <- c(legendnames, "gamlss")
  }
  if("av.rmse.exp.true.b" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.b"])
    colnames <- c(colnames, "b.true")
    col <- c(col, pal["bamlss"])
    legendnames <- c(legendnames, "bamlss")
  }
  if("av.rmse.exp.true.gb" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.gb"])
    colnames <- c(colnames, "gb.true")
    col <- c(col, pal["gamboostLSS"])
    legendnames <- c(legendnames, "gamboostLSS")
  }
  if("av.rmse.exp.true.rf" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.rf"])
    colnames <- c(colnames, "rf.true")
    col <- c(col, pal["randomForest"])
    legendnames <- c(legendnames, "randomForest")
  }
  if("av.rmse.exp.true.cf" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.cf"])
    colnames <- c(colnames, "cf.true")
    col <- c(col, pal["cforest"])
    legendnames <- c(legendnames, "cforest")
  }
  
  colnames(crps) <- colnames
  if(is.null(ylim)) ylim <- c(min(na.omit(crps)), max(na.omit(crps)))
  
  plot(x = simres$x.axis, y = crps[,1], type = "l", col = col[1], ylim = ylim,
       xlab = expression(kappa), ylab = "CRPS")
  
  for(i in 2:length(col)){
    lines(x = simres$x.axis, y = crps[,i], type = "l", col = col[i])
  }
  if(legend) legend('topright', legendnames, 
                    col = col, lty = 1, cex = 1, bty = "n")
}
@



\title{Distributional Trees and Forests}
\Plaintitle{disttree: Distributional Trees and Forests} 

\author{Lisa Schlosser\\Universit\"at Innsbruck
   \And Torsten Hothorn\\Universit\"at Z\"urich
   \And Reto Stauffer\\Universit\"at Innsbruck
   \And Achim Zeileis\\Universit\"at Innsbruck}
\Plainauthor{Lisa Schlosser, Torsten Hothorn, Reto Stauffer, Achim Zeileis}

\Abstract{
In the field of probabilistic forecasting linear models such as ensemble model output statistics (EMOS, \citealt{gneiting2005calibrated}) provide common and widely used tools. This is due to their simplicity and interpretability. However, the restriction to model only linear effects requires further extensions which allow for more flexibility. Generalized additive models for location, scale, and shape (GAMLSS, \citealt{stasinopoulos2005}) can also capture non-linear effects as each distribution parameter can be modeled separately by its own generalized additive model (GAM). A different approach that would make it possible to also model non-additive effects and interactions is to apply tree structured models such as regression trees (\citealt{loh2011classification}) or random forests (\citealt{breiman2001random}). However, up to now, this class of models has not yet been integrated in probabilistic forecasting as no general distributional framework is available for these methods. Therefore, we are going to introduce distributional trees and forests which embed the idea of flexible parametric modeling into the framework of regression trees and random forests. Applying these novel approaches on precipitation data in a mountainous region with a large number of numerical weather prediction quantities illustrates their strengths as their performance is compared to GAMLSS models that are specified based either on subject-matter knowledge and experience from meteorology or based on a computationally more demanding boosting approach.

%In regression analysis one is interested in the relationship between a dependent variable and one or more explanatory variables. Various methods to fit statistical models to the data set have been developed, starting from linear models considering only the mean of the response variable and ranging to probabilistic models where all parameters of a distribution are fit to the given data set.\\
%If there is a strong variation within the data it might be advantageous to split the data first into more homogeneous subgroups based on given covariates and then fit a local model in each subgroup rather than fitting one global model to the whole data set. This can be done by applying regression trees and forests.\\
%Both of these two concepts, parametric modeling and algorithmic trees, have been investigated and developed further, however, mostly separated from each other. Therefore, our goal is to embed the progress made in the field of probabilistic modeling in the idea of algorithmic tree and forest models. In particular, more flexible models such as GAMLSS (\citealt{stasinopoulos2005}) should be fitted in the nodes of a tree in order to capture location, scale and shape as well as censoring, tail behavior etc. while non-additive effects of the explanatory variables can be detected automatically by the splitting algorithm used to build the tree. In distributional forests an ensemble of distributional trees is built and used to calculate weights which are then included in the fitting process of a distributional model. These forest models can detect smooth effects as well as abrupt changes and interactions in a reasonable way without needing any kind of variable selection or information about the expected effects advance and therefore offers a good compromise particularly in complex settings. 
}
\Keywords{parametric models, regression trees, random forests, recursive partitioning, probabilistic forecasting, distributional regression}

\Address{
  Lisa Schlosser, Reto Stauffer, Achim Zeileis \\
  Department of Statistics \\
  Faculty of Economics and Statistics \\
  Universit\"at Innsbruck \\
  Universit\"atsstr.~15 \\
  6020 Innsbruck, Austria \\
  E-mail: \email{Lisa.Schlosser@uibk.ac.at}, \email{Reto.Stauffer@uibk.ac.at}, \email{Achim.Zeileis@R-project.org} \\
  URL: \url{https://www.uibk.ac.at/statistics/personal/schlosser-lisa/lisa-schlosser.html}, \url{http://retostauffer.org/}, \url{https://eeecon.uibk.ac.at/~zeileis/}\\
  
  Torsten Hothorn\\
  Institut f\"ur Sozial- und Pr\"aventivmedizin, Abteilung Biostatistik \\
  Universit\"at Z\"urich \\
  Hirschengraben 84\\
  CH-8001 Z\"urich, Switzerland \\
  E-mail: \email{Torsten.Hothorn@R-project.org}\\
  URL: \url{http://user.math.uzh.ch/hothorn/}\\
  
}

\begin{document}
\SweaveOpts{concordance=TRUE}


\SweaveOpts{eps=FALSE, keep.source=TRUE, eval = TRUE}

\SweaveOpts{eval = TRUE}
\section{Introduction}
A typical application in meteorology is to improve probabilistic precipitation forecasts. The main interest lies in estimating the probability of precipitation as well as predicting the expected amount of precipitation. In this research area various methods have been developed and tested in the last decades. (Multiple) Linear models such as ensemble model output statistics (EMOS) were introduced more than a decade ago (\citealt{gneiting2005calibrated}) and are still common and widely used. Despite their simplicity and interpretability, the limitation to modeling only linear effects has risen the interest in applying more flexible models. This feature is provided by generalized additive models (GAM) which can also model non-linear effects. However, often only the expectation of the underlying distribution is estimated. To obtain probabilistic predictions, generalized additive models for location, scale, and shape (GAMLSS, \citealt{stasinopoulos2005}) offer a framework in which each distribution parameter can be modeled separately by its own GAM. In Figure~\ref{Devel_parmod} the development of parametric models starting from simple linear models (LM) and ranging to GAMLSS is illustrated.\\
These flexible parametric methods provide well fitting models when applied on precipitation data in a rather smooth and homogeneous setting, e.g. areas where no abrupt changes in meteorological/climatological events appear. However, for example in complex terrain in mountainous regions various factors can change rapidly and their effects are difficult to be represented by a smooth parametric model. In this situation a different approach which can deal with these abrupt changes would be to apply tree structured models such as regression trees (\citealt{loh2011classification}) or random forests (\citealt{breiman2001random}). Moreover, these models are particularly advantageous in case of many covariates with unknown effects or interactions as the variable selection is done automatically within the model fitting process. Therefore, no additional effort or knowledge for the specification of the model is required in advance. However, even though their main advantage of being able to model non-additive effects and interactions without any necessary prespecification can play to their strength, regression trees are not an adequate choice if (additional) smooth effects appear as well. In this situation a random forest provides a good compromise as it combines a set of trees such that the steps imposed by the trees are smoothed out in case of smooth effects while abrupt changes are still captured by the tree structure of the model.\\
For these reasons, regression trees and particularly random forests provide valuable features which can contribute to improving models for probabilistic forecasting. But, up to now, no general distributional framework is available for these methods. Therefore, we are going to introduce distributional trees and forests which embed the idea of flexible parametric modeling into the framework of regression trees and random forests. The developments in the field of tree-structured models together with the novel approaches of distributional trees and forests are illustrated in Figure~\ref{Devel_treeforest}.\\







%In the field of regression analysis many statistical models have already been developed to model the relationship between a response variable and one or more explanatory variables.\\
%Among the first were linear models (LM) which predict the (conditional) expected value of the response variable as a linear combination of the explanatory variables. This idea was extended to generalized linear models (GLM) which broadened the range of possible distributions of the response variable to the exponential family. (In R these models can be built using the functions \code{lm()} and \code{glm()} provided by the package \pkg{stats}).\\
%Then, in order to also allow for non-linear effects, generalized additive models (GAM) were introduced where smooth functions of the explanatory variables are summed up instead of the linear combination. (In the fitting function \code{gam} types of smoothing terms can be selected.)\\
%However, with these methods only the mean of the distribution of the response variable can be modeled. Generalized additive models for location, scale and shape
%(GAMLSS) (\citealt{stasinopoulos2005}) made it possible to model each of the parameters of a distribution separately by its own GAM. In that way a whole distribution can be specified. The R-packages \pkg{gamlss} and \pkg{gamlss.dist} offer the corresponding software with a wide variety of distributions.\\
%This development in the field of parametric modeling is illustrated in Figure~\ref{Devel_parmod}.\\


\begin{figure}[t!]
\begin{tikzpicture}
\node (a) at (0,0){
\minipage{0.29\textwidth}
\begin{center}
<<motivation_GLM, echo=FALSE, results=hide>>=
nobs <- 200
## GLM
set.seed(7)
x <- c(1:nobs)/nobs
ytrue <- 1+x*1.5
y <- ytrue + rnorm(nobs,0,0.3)
@

<<plot_motivation_GLM, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(y=y , x=x, xaxt="n", yaxt="n", ann=FALSE, col = "slategray", pch = 19)
box(lwd=5)
lines(x = x, y = ytrue, col = pal["forest"], lwd = 7, main = "")
@
\end{center}
\endminipage};
\node (b) at (5,0) 
{
\minipage{0.29\textwidth}
\begin{center}
<<motivation_GAM, echo=FALSE, results=hide>>=
## GAM
set.seed(7)
x <- c(1:nobs)/nobs
x <- 2*(x-0.5)
ytrue <- x^3 
y <- ytrue + rnorm(nobs,0,0.3)
@

<<plot_motivation_GAM, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(y=y , x=x, xaxt="n", yaxt="n", ann=FALSE, col = "slategray", pch = 19)
box(lwd=5)
lines(x = x, y = ytrue, col = pal["forest"], lwd = 7, main = "")
@
\end{center}
\endminipage};
\node (c) at (10,0) 
{
\minipage{0.29\textwidth}
\begin{center}
<<motivation_GAMLSS, echo=FALSE, results=hide>>=
## GAMLSS
set.seed(7)
x <- c(1:nobs)/nobs
x <- 2*(x-0.5)
ytrue <- x^3
var <- exp(-(2*x)^2)/2
y <- ytrue + rnorm(nobs, 0, 0.1 + var)
@

<<plot_motivation_GAMLSS, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(x, y, xaxt = "n", yaxt = "n", ann = FALSE, type = "n")
polygon(c(x, rev(x)), c(ytrue + 0.1 + var, rev(ytrue - 0.1 - var)),
  col = pallight["forest"], border = "transparent")
lines(x, ytrue, col = pal["forest"], lwd=7)
points(x, y, col = "slategray", pch = 19)
box(lwd = 5)
@
\end{center}
\endminipage};
\node (d) at (0,-2) 
{
\minipage{0.29\textwidth}
\begin{center}
\vspace{0.0cm}
LM, GLM  
\end{center}
\endminipage};
\node (e) at (5,-2) 
{
\minipage{0.29\textwidth}
\begin{center}
\vspace{0.0cm}
GAM 
\end{center}
\endminipage};
\node at (10,-2)
{
\minipage{0.28\textwidth}
\begin{center}
\vspace{0.0cm}
GAMLSS 
\end{center}
\endminipage};
\draw[-{>[scale=4, length=2, width=3]},line width=0.4pt](2.1,0)--(2.9,0);
\draw[-{>[scale=4, length=2, width=3]},line width=0.4pt](7.1,0)--(7.9,0);
\end{tikzpicture}
\caption{\label{Devel_parmod}Development in the field of parametric modeling.}
\end{figure}




\begin{figure}[t!]
\begin{tikzpicture}
\node (a) at (0,0){
\minipage{0.29\textwidth}
\begin{center}
<<motivation_regtree, echo=FALSE, results=hide>>=
## Reg. Tree
set.seed(7)
kappa <- 12
x <- c(1:nobs)/nobs
ytrue <- ytree <- yforest <- numeric(length = length(x))
for(i in 1:nobs) ytrue[i] <- if(x[i]<1/3) 0.5 else 1+(1-plogis(kappa*(2*(x[i]-0.2)-1)))
y <- ytrue + rnorm(nobs,0,0.3)
for(i in 1:nobs) ytree[i] <- if(x[i]<1/3) 0.5 else {if(x[i]<2/3) 2 else 1}
@

<<plot_motivation_regtree, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(x = x, y = y, xaxt="n", yaxt="n", ann=FALSE, col = "slategray", pch = 19)
box(lwd=5)
#lines(x = x, y = ytrue, col = "grey", lwd=5, main = "")
lines(x = x, y = ytree, col = pal["forest"], lwd=7)
@
\end{center}
\endminipage};
\node (b) at (5,0) 
{
\minipage{0.29\textwidth}
\begin{center}
<<motivation_randforest, echo=FALSE, results=hide>>=
## Random Forest
for(i in 1:nobs) yforest[i] <- if(x[i]<0.27) 0.5 else { if(x[i]<0.39) 0.5 + 1.5*(plogis((x[i]-0.33)/6*700)) else 1+(1-plogis(kappa*(2*(x[i]-0.2)-1)))}
@

<<plot_motivation_randforest, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(x = x, y = y, xaxt="n", yaxt="n", ann=FALSE, col = "slategray", pch = 19)
box(lwd=5)
#lines(x = x, y = ytrue, col = "grey", lwd=5, main = "")
lines(x = x, y = yforest, col = pal["forest"], lwd=7, main = "")
@
\end{center}
\endminipage};
\node (c) at (10,0) 
{
\minipage{0.29\textwidth}
\begin{center}
<<motivation_distforest, echo=FALSE, results=hide>>=
## distforest
set.seed(4723)
for(i in 1:nobs) yforest[i] <- if(x[i]<0.27) 0.5 else { if(x[i]<0.39) 0.5 + 1.5*(plogis((x[i]-0.33)/6*700)) else 1+(1-plogis(kappa*(2*(x[i]-0.2)-1)))}
xt <- 2*(x-0.5)
var <- exp(-(2*xt)^4)/2
y <- ytrue + rnorm(nobs, 0, 0.1 + var)
x <- c(1:nobs)/nobs
@

<<plot_motivation_distforest, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(x, y, xaxt = "n", yaxt = "n", ann = FALSE, type = "n")
polygon(c(x, rev(x)), c(ytrue + 0.1 + var, rev(ytrue - 0.1 - var)),
  col = pallight["forest"], border = "transparent")
lines(x = x, y = yforest, col = pal["forest"], lwd=7, main = "")
points(x, y, col = "slategray", pch = 19)
box(lwd=5)
@
\end{center}
\endminipage};
\node (d) at (0,-2) 
{
\minipage{0.29\textwidth}
\begin{center}
\vspace{0.0cm}
Regression Tree 
\end{center}
\endminipage};
\node (e) at (5,-2) 
{
\minipage{0.29\textwidth}
\begin{center}
\vspace{0.0cm}
Random Forest 
\end{center}
\endminipage};
\node at (10,-2)
{
\minipage{0.28\textwidth}
\begin{center}
\vspace{0.0cm}
Distributional Forest 
\end{center}
\endminipage};
\node (g) at (0,-5.4) 
{
\minipage{0.29\textwidth}
\vspace{0.2cm}
%\hspace{0.0cm}
\begin{center}
<<motivation_disttree, echo=FALSE, results=hide>>=
## disttree
set.seed(74)
x <- c(1:nobs)/nobs
var <- ytree/3
y <- ytrue + rnorm(nobs,0,var)
@

<<plot_motivation_disttree, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(x = x, y = y, xaxt="n", yaxt="n", ann=FALSE, col = "slategray", pch = 19)
lines(x = x, y = ytree, col = pal["forest"], lwd=7, main = "")
polygon(c(x, rev(x)), c(ytree + var, rev(ytree - var)),
  col = pallight["forest"], border = "transparent")
box(lwd=5)
@
\vspace{-0.2cm}%\\
Distributional Tree
\vspace{0.2cm}
\end{center}
\endminipage};
\draw[-{>[scale=4, length=2, width=3]},line width=0.4pt](2.1,0)--(2.9,0);
\draw[-{>[scale=4, length=2, width=3]},line width=0.4pt](7.1,0)--(7.9,0);
\draw[-{>[scale=4, length=2, width=3]},line width=0.4pt](0,-2.4)--(0,-3.2);
\draw[-{>[scale=4, length=2, width=3]},line width=0.4pt](2.2,-5.4)--(7.9,-2.5);
\end{tikzpicture}
\caption{\label{Devel_treeforest}Development in the field of algorithmic trees and forests.}
\end{figure}


%\SweaveOpts{eval = FALSE}

%All these parametric models work very well as long as only additive effects appear in the considered data. A different type of model that can capture non-additive effects as well are regression trees. Here the basic idea is to split the data set into more homogeneous subgroups based on information provided by the explanatory variables and to fit a model/response to each terminal node. One problem that might appear when applying tree algorithms is that they impose steps and abrupt changes on the model, even in situations where effects are rather smooth, as it can be seen in the top left plot of Figure~\ref{Devel_treeforest}. One way how to tackle this problem is to not just consider one tree but an ensemble of trees. This is the idea of random forests. A set of trees is built, each on a slightly different data set which can either be a bootstrap or a subsample of the original data set. Then the results of all trees are combined to get a final model which unlike a tree model can also deal with smooth effects in a reasonable way.\\ 
%In distributional trees and forests we now want to combine all these ideas from different fields of statistical modeling in order to benefit from all their advantages.\\
%\\
%In the past the combination of parametric data models and algorithmic tree models has gained more and more interest since it allows for a more flexible and clear structure. \\
%Algorithms that only fit constants to each final node and therefore only model the mean of the response variable (such as CART, Breiman 1984) might lead to very large trees which complicates analyzing and interpreting. Algorithms such as GUIDE (Loh 2002), CRUISE (Kim and Loh 2001) and LOTUS (Chan and Loh 2004) were among the first to provide trees with parametric models in each terminal node. The MOB algorithm (Zeileis, Hothorn and Hornik 2008), an algorithm for model-based recursive partitioning, extended this idea by fitting parametric models to each node and using the gained information of the fitted model to create further splits in inner nodes. Using this algorithm as a framework to build trees LMs and GLMs can be fitted in the nodes of the trees. However, up to now trees which only model the mean of the response variable in its nodes are still more common and widely used.\\
%Random forests (\citealt{breiman2001random}) applying the CART algorithm to build the trees and cforest applying the ctree algorithm (\citealt{hothorn2015partykit}, \citealt{hothorn2006unbiased}) have used this idea of combining an ensemble of trees to get smoother effects of the covariates after combining or averaging over the parameters of the trees.\\
%\\
%But as mentioned before, until now the above listed development in the field of probabilistic modeling has not yet been fully integrated in the idea of algorithmic tree and forest models. Probabilistic models are rarely used in combination with tree algorithms and more flexible models such as GAMLSSs have not yet been applied as models in the nodes of a tree.\\
%\\
%Therefore, our main goal is to embed the progress made in the field of probabilistic modeling in the idea of regression trees and forests. In particular, more flexible models such as GAMLSSs should be fitted in the nodes of a tree in order to capture location, scale and shape as well as censoring, tail behavior etc. while non-additive effects or interactions of the explanatory variables can be detected automatically by the splitting algorithm used to build the tree. In that way, no variable selection or predefining of variable interactions is necessary and a whole distribution is specified in each node which leads to a wide range of inference methods that can be applied.\\
%These (future) steps are illustrated in Figure~\ref{Devel_treeforest}, embedded in the development made in the field of tree and forest algorithms.\\


After explaining the novel methodology in Section~\ref{Methodology} of this paper the focus will then be put on an application in the field of probabilistic forecasting in Section~\ref{ProbabilisticForecasting}. The new method is applied on precipitation data in a mountainous region with a large number of numerical weather prediction quantities and the results are compared to those of other methods commonly used in this research area. In a first setting only one observation station is taken into consideration. The models are trained on data from 24 years and probabilistic predictions of the consecutive 4 years are validated. A brief outlook on the results of this setting is given by Figure~\ref{axams_onestation}. For the 24th of July of 2009, 2010, 2011 and 2012 the density functions of the distributions estimated by a forest model are plotted together with the corresponding observations. This illustrates that well fitting predicted expected values are provided by this model as they are close to the observed values. Beside the expected values the predictions provided by distributional models offer a wide range of information as they specify the whole distribution and therefore enable further methods of inference and analysis. In section \ref{ProbabilisticForecasting} this example will be further discussed and it will be shown that the predictions made by the forest model are reasonably well fitting, not only for the mean parameter of the distribution but also for its second parameter, the scale parameter, and therefore the whole distribution.


<<echo=FALSE, results=hide>>=
#### Axams prediction 24 - 4
if(file.exists("rain_Axams_pred_24to4.rda")){
  #load("~/svn/partykit/pkg/disttree/inst/draft/rain_Axams_pred_24to4.rda")
  load("rain_Axams_pred_24to4.rda")
} else {
  #source("~/svn/partykit/pkg/disttree/inst/draft/rain_axams_pred_24to4.R")
  source("rain_axams_pred_24to4.R")
  library("gamlss.cens")
  gen.cens("NO", type = "left")
  res <- rain_axams_pred(seedconst = 7, ntree = 100,
                         forest_mtry = 27,   # if frac == TRUE: nvar/3 = 30
                         gamboost_cvr = TRUE,
                         frac = FALSE)
  
  save(res, file = "rain_Axams_pred_24to4.rda")
}

#### prepare data for plot of estimated density functions
# predictions for one day (in each of the four years) 
# (19th of July 2011 is missing)
pday <- 24  # 2 (hohe Beobachtung zu niedrig geschaetzt), 4, 15, evtl. auch 7, 8, 23 (eine 0-Beobachtung und 2 sehr aehnliche), 

pdays <- if(pday<19) c(pday, pday + 31, pday + 62, pday + 92) else c(pday, pday + 31, pday + 61, pday + 92)
pdf <- predict(res$df, newdata = res$testdata[pdays,], type = "parameter")
df_mu <- pdf$mu
df_sigma <- pdf$sigma
df_exp <- pnorm(df_mu/df_sigma) * (df_mu + df_sigma * (dnorm(df_mu/df_sigma) / pnorm(df_mu/df_sigma)))
cbind(df_exp, res$testdata[pdays,"robs"])


# plot predicted distributions together with observations
#set.seed(res$call$seedconst)
set.seed(7)
x <- c(0.01, sort(runif(500,0.01,8)))
y1 <- crch::dcnorm(x, mean = df_mu[1], sd = df_sigma[1], left = 0)
y2 <- crch::dcnorm(x, mean = df_mu[2], sd = df_sigma[2], left = 0)
y3 <- crch::dcnorm(x, mean = df_mu[3], sd = df_sigma[3], left = 0)
y4 <- crch::dcnorm(x, mean = df_mu[4], sd = df_sigma[4], left = 0)
dayending <- if(pday > 3) "th" else switch(pday, "1" = {dayending <- "st"}, "2" = {dayending <- "nd"}, "3" = {dayending <- "rd"})

# point mass (slightly shifted)
pm1 <- c(0.05, crch::dcnorm(-1, mean = df_mu[1], sd = df_sigma[1], left = 0))
pm2 <- c(0.01, crch::dcnorm(-1, mean = df_mu[2], sd = df_sigma[2], left = 0))
pm3 <- c(-0.03, crch::dcnorm(-1, mean = df_mu[3], sd = df_sigma[3], left = 0))
pm4 <- c(-0.07, crch::dcnorm(-1, mean = df_mu[4], sd = df_sigma[4], left = 0))

# predictions
pred1 <- c(res$testdata[pdays,"robs"][1], crch::dcnorm(res$testdata[pdays,"robs"][1], mean = df_mu[1], sd = df_sigma[1], left = 0))
pred2 <- c(res$testdata[pdays,"robs"][2], crch::dcnorm(res$testdata[pdays,"robs"][2], mean = df_mu[2], sd = df_sigma[2], left = 0))
pred3 <- c(res$testdata[pdays,"robs"][3], crch::dcnorm(res$testdata[pdays,"robs"][3], mean = df_mu[3], sd = df_sigma[3], left = 0))
pred4 <- c(res$testdata[pdays,"robs"][4], crch::dcnorm(res$testdata[pdays,"robs"][4], mean = df_mu[4], sd = df_sigma[4], left = 0))

#legendheight
lh1 <- crch::dcnorm(0.01, mean = df_mu[1], sd = df_sigma[1], left = 0)
lh2 <- crch::dcnorm(0.01, mean = df_mu[2], sd = df_sigma[2], left = 0)
lh3 <- crch::dcnorm(0.01, mean = df_mu[3], sd = df_sigma[3], left = 0)
lh4 <- crch::dcnorm(0.01, mean = df_mu[4], sd = df_sigma[4], left = 0)



## PIT histograms and qqr-plots

## get predicted parameter
# for learndata and testdata
{
  set.seed(7)
  ## disttree
  if(FALSE){
    # in sample
    pdt <- predict(res$dt, type = "parameter")
    dt_mu_l <- pdt$mu 
    dt_sigma_l <- pdt$sigma 
    pit_dt_l <- cbind(0, pnorm(res$learndata[,"robs"], mean = dt_mu_l, sd = dt_sigma_l))
    pit_dt_l[res$learndata[,"robs"]>0, 1] <- pit_dt_l[res$learndata[,"robs"]>0, 2]
  }  
  # out of sample
  pdt <- predict(res$dt, type = "parameter", newdata = res$testdata)
  dt_mu_t <- pdt$mu 
  dt_sigma_t <- pdt$sigma 
  pit_dt_t <- cbind(0, pnorm(res$testdata[,"robs"], mean = dt_mu_t, sd = dt_sigma_t))
  pit_dt_t[res$testdata[,"robs"]>0, 1] <- pit_dt_t[res$testdata[,"robs"]>0, 2]
  
  
  ## distforest
  if(FALSE){
    # in sample
    pdf <- predict(res$df, type = "parameter")
    df_mu_l <- pdf$mu
    df_sigma_l <- pdf$sigma
    pit_df_l <- cbind(0, pnorm(res$learndata[,"robs"], mean = df_mu_l, sd = df_sigma_l))
    pit_df_l[res$learndata[,"robs"]>0, 1] <- pit_df_l[res$learndata[,"robs"]>0, 2]
  }
  # out of sample
  pdf <- predict(res$df, type = "parameter", newdata = res$testdata)
  df_mu_t <- pdf$mu
  df_sigma_t <- pdf$sigma
  pit_df_t <- cbind(0, pnorm(res$testdata[,"robs"], mean = df_mu_t, sd = df_sigma_t))
  pit_df_t[res$testdata[,"robs"]>0, 1] <- pit_df_t[res$testdata[,"robs"]>0, 2]
  
  
  ## gamlss
  g_learndata <- res$learndata
  g_learndata$robs <- Surv(g_learndata$robs, g_learndata$robs>0, type="left")
  if(FALSE){
    # in sample
    g_mu_l <- predict(res$g, what = "mu", type = "response")
    g_sigma_l <- predict(res$g, what = "sigma", type = "response")
    pit_g_l <- cbind(0, pnorm(res$learndata[,"robs"], mean = g_mu_l, sd = g_sigma_l))
    pit_g_l[res$learndata[,"robs"]>0, 1] <- pit_g_l[res$learndata[,"robs"]>0, 2]
  }  
  # out of sample
  g_mu_t <- predict(res$g, what = "mu", type = "response", newdata = res$testdata)
  g_sigma_t <- predict(res$g, what = "sigma", type = "response", newdata = res$testdata)
  pit_g_t <- cbind(0, pnorm(res$testdata[,"robs"], mean = g_mu_t, sd = g_sigma_t))
  pit_g_t[res$testdata[,"robs"]>0, 1] <- pit_g_t[res$testdata[,"robs"]>0, 2]
  
  
  #gamboostLSS
  if(FALSE){
    #in sample
    pgb <- predict(res$gb, parameter = c("mu","sigma"), type = "response")
    gb_mu_l <- pgb$mu
    gb_sigma_l <- pgb$sigma
    pit_gb_l <- cbind(0, pnorm(res$learndata[,"robs"], mean = gb_mu_l, sd = gb_sigma_l))
    pit_gb_l[res$learndata[,"robs"]>0, 1] <- pit_gb_l[res$learndata[,"robs"]>0, 2]
  }  
  # out of sample
  pgb <- predict(res$gb, parameter = c("mu","sigma"), type = "response", newdata = res$testdata)
  gb_mu_t <- pgb$mu
  gb_sigma_t <- pgb$sigma
  pit_gb_t <- cbind(0, pnorm(res$testdata[,"robs"], mean = gb_mu_t, sd = gb_sigma_t))
  pit_gb_t[res$testdata[,"robs"]>0, 1] <- pit_gb_t[res$testdata[,"robs"]>0, 2]
  
  
  #EMOS
  if(FALSE){
    # in sample
    ml_mu_l <- predict(res$ml, type = "location")     # returns parameter on response scale
    ml_sigma_l <- predict(res$ml, type = "scale")
    pit_ml_l <- cbind(0, pnorm(res$learndata[,"robs"], mean = ml_mu_l, sd = ml_sigma_l))
    pit_ml_l[res$learndata[,"robs"]>0, 1] <- pit_ml_l[res$learndata[,"robs"]>0, 2]
  }  
  # out of sample
  ml_mu_t <- predict(res$ml, type = "location", newdata = res$testdata)     # returns parameter on response scale
  ml_sigma_t <- predict(res$ml, type = "scale", newdata = res$testdata)
  pit_ml_t <- cbind(0, pnorm(res$testdata[,"robs"], mean = ml_mu_t, sd = ml_sigma_t))
  pit_ml_t[res$testdata[,"robs"]>0, 1] <- pit_ml_t[res$testdata[,"robs"]>0, 2]
  
}


## Variable importance
if(file.exists("vimp_crps.rda")){
  #load("~/svn/partykit/pkg/disttree/inst/draft/vimp_crps.rda")
  load("vimp_crps.rda")
} else {
  
  # locally redefine logLik.distforest to use crps in varimp()
  logLik.distforest <- function(object, newdata = NULL, ...) {
    if(is.null(newdata)) {
      newdata <- object$data
    } 
    
    # predict parameter
    pdf <- predict(object, newdata = newdata, type = "parameter")
    df_mu <- pdf$mu
    df_sigma <- pdf$sigma
    
    # calculate CRPS
    crps <- mean(crps_cnorm(newdata$robs, location = df_mu, scale = df_sigma, lower = 0, upper = Inf), na.rm = TRUE)
  
    return(structure(crps, df = NA, class = "logLik"))
  }
  
  vimp_crps <- varimp(res$df, nperm = 1L)
  #vimp10_crps <- varimp(res$df, nperm = 10L)
  #vimp100_crps <- varimp(res$df, nperm = 100L)
  #vimp1000_crps <- varimp(res$df, nperm = 1000L)
  
  save(vimp_crps, file = "~/svn/partykit/pkg/disttree/inst/draft/vimp_crps.rda")
  #save(vimp10_crps, file = "~/svn/partykit/pkg/disttree/inst/draft/vimp10_crps.rda")
  #save(vimp100_crps, file = "~/svn/partykit/pkg/disttree/inst/draft/vimp100_crps.rda")
  #save(vimp1000_crps, file = "~/svn/partykit/pkg/disttree/inst/draft/vimp1000_crps.rda")
  
  rm(logLik.distforest) 
}
    
@


\begin{figure}[t!]
\begin{center}
<<rain_Axams_pred, fig=TRUE, echo=FALSE, width=6.5, height=4.5>>=
plot(x = x, y = y1, type = "l", col = "red", 
     main = paste0("July ", pday), ylab = "Density", 
     xlab = expression(Total~precipitation~"["~mm^(1/1.6)~"/"~"24h"~"]"),
     #xlab = expression(Total~precipitation~group("[", mm^(1/1.6)~per~day,"]")),
     #{\frac{1}{1.6}}$,
     ylim = c(0,max(y1, y2, y3, y4, pm1, pm2, pm3, pm4) + 0.01),
     xlim = c(-1.5,8))
    
lines(x = x, y = y2, type = "l", col = "blue")
lines(x = x, y = y3, type = "l", col = "darkgreen")
lines(x = x, y = y4, type = "l", col = "purple")
legend('topright', c("predicted distribution", "point mass at censoring point", "observation"),
       bty = "n", col = "black", lty = c(1, NA, NA), pch = c(NA, 19, 4), cex = 0.8)

# plot point mass
lines(x = c(pm1[1], pm1[1]), y = c(pm1[2], 0), col = "red", type = "l", lwd = 1)
lines(x = c(pm2[1], pm2[1]), y = c(pm2[2], 0), col = "blue", type = "l", lwd = 1)
lines(x = c(pm3[1], pm3[1]), y = c(pm3[2], 0), col = "darkgreen", type = "l", lwd = 1)
lines(x = c(pm4[1], pm4[1]), y = c(pm4[2], 0), col = "purple", type = "l", lwd = 1)

points(x = pm1[1], y = pm1[2], col = "red", pch = 19)
points(x = pm2[1], y = pm2[2], col = "blue", pch = 19)
points(x = pm3[1], y = pm3[2], col = "darkgreen", pch = 19)
points(x = pm4[1], y = pm4[2], col = "purple", pch = 19)


# plot predictions
points(x = pred1[1], y = pred1[2], col = "red", pch = 4)
points(x = pred2[1], y = pred2[2], col = "blue", pch = 4)
points(x = pred3[1], y = pred3[2], col = "darkgreen", pch = 4)
points(x = pred4[1], y = pred4[2], col = "purple", pch = 4)

lines(x = c(pred1[1], pred1[1]), y = c(pred1[2], 0), col = "darkgrey", type = "l", lty = 2)
lines(x = c(pred2[1], pred2[1]), y = c(pred2[2], 0), col = "darkgrey", type = "l", lty = 2)
lines(x = c(pred3[1], pred3[1]), y = c(pred3[2], 0), col = "darkgrey", type = "l", lty = 2)
lines(x = c(pred4[1], pred4[1]), y = c(pred4[2], 0), col = "darkgrey", type = "l", lty = 2)

# add labels
text(x = -0.8, y = lh1, labels = "2009", col = "red", cex = 0.8)
text(x = -0.8, y = lh2, labels = "2010", col = "blue", cex = 0.8)
text(x = -0.8, y = lh3, labels = "2011", col = "darkgreen", cex = 0.8)
text(x = -0.8, y = lh4, labels = "2012", col = "purple", cex = 0.8)
@
\caption{\label{axams_onestation}Predictions of total precipitation at station Axams for July 24 in 2009, 2010, 2011 and 2012 by a distributional forest model learned on data from 1985--2008. Observations and predictions are left censored at 0. The corresponding point mass is shown at the censoring point (0).}
\end{center}
\end{figure}






%\newpage
\section{Methodology}
\label{Methodology}

\subsection{Motivation}
As illustrated in Figure~\ref{Devel_parmod} a lot of development has been made in the field of parametric modeling and a wide range of tools can be applied in research areas such as probabilistic forecasting. But as mentioned before, further features such as the ability to capture non-additive effects or interactions are desirable in many applications which demands for more flexible models. Since tree structured models provide these required features a combination of them with parametric models can offer a framework that is able to deal with a wide range of different modeling situations. In this section we are going to introduce the novel methodology which embeds parametric modeling into the idea of tree structured models step by step. We start with a simple distributional fit in Subsection~\ref{distfit}, then build a distributional tree in Subsection~\ref{disttree} and finally a distributional forest in Subsection~\ref{distforest}.

\newpage
\subsection{Distributional Fit}
\label{distfit}
Fitting a distributional model to a set of observations without considering any covariates is a well known procedure which can for example be done by applying the maximum likelihood method. This is also the chosen approach in the methodology described in this section.\\
The goal is to fit a distributional model $D(Y, \theta)$ to a response variable $Y \in \mathcal{Y}$ for a \textit{k}-dimensional vector of distribution parameters $\theta \in \Theta$ in case of a \textit{k}-parametric distribution. The distribution family has to be specified in advance such that a log-likelihood function $\ell(\theta; Y)$ is provided. Using the GAMLSS framework (\citealt{stasinopoulos2005}) a wide range of distributions can be selected where each distribution parameter can be specified separately. This enables the model to deal with various features that can usually be very challenging in model fitting such as censoring, truncation or heavy tails.\\
In our application of precipitation forecasting the response variable, which is the power transformed total precipitation amount within 24 hours, is expected to be normally distributed with mean/location parameter $\mu$ and standard deviation/scale parameter $\sigma$ and left censored with censoring point 0. (The power transformation is used to remove large parts of the skewness of the data as further explained in \ref{data}.) Therefore, the corresponding log-likelihood function with parameter vector $\theta = (\mu, \sigma)$ is
$$
\ell(\mu, \sigma; Y) = 
\begin{cases}
    \log\left(\frac{1}{\sigma} \cdot \phi(\frac{Y - \mu}{\sigma}) \right), & \text{if } Y > 0\\
    \vspace{0.1cm}
    \log\left(\Phi(\frac{-\mu}{\sigma})\right), & \text{if } Y = 0
\end{cases}
$$
where $\phi$ and $\Phi$ are the probability density function and the distribution function of the standard normal distribution $\mathcal{N}(0,1)$. Next to the censored normal distribution the censored logistic distribution is also often chosen for the modeling of precipitation amounts. In \citealt{Gebetsberger2017} this choice has been made in order to be able to deal with heavy tails which often appear in precipitation data. Another frequent application of probabilistic forecasting in meteorology is the modeling of near surface air temperature. In this case a normal distribution is often selected.\\
With the specification of the distribution family and the log-likelihood function the task of fitting a distributional model turns into the task of determining the distribution parameter $\theta$. This is done by the following maximization.
$$
\hat{\theta} = \max_{\theta \in \Theta} \sum_{i=1}^n \ell(\theta; y_i)
$$
where $\{y_i\}_{i=1,\ldots,n}$ are the observations of the response variable $Y$.\\
In that way, a whole distribution is specified with all its features, including location, scale and shape. However, fitting one global model to the whole data set might be too much of a generalization and therefore not represent the data and its features in a reasonable way. Moreover, if covariates are available it is desirable to include them into the model as they might provide important additional information. In our application on precipitation data a wide range of covariates are available from a numerical ensemble prediction system considering precipitation, temperature, sea level pressure and other meteorological quantities which might be of interest and have an influence on the predictions. \\
A common way of including these covariates is to consider them as regressors in a distributional model such that each parameter $\theta$ depends on covariates $\bold{X} = (X_1,\ldots,X_m)$. This could for example be done by a linear function $g$. In that way the fitted model would be specified as $D(Y, \theta = g(\bold{X}))$. In order to avoid the restriction to linear effects one might also use a set of smooth functions $\{f_i\}_{i = 1,\ldots,p}$ to specify a more flexible model $D(Y, \theta = f_1(\bold{X}) + \ldots + f_p(\bold{X}))$ applying a GAM structure. Another approach to distributional regression is proposed by \citealt{Klein2015} where each parameter of a possibly complex distribution can be modeled by an additive term consisting of a variety of different functional effect types. These include non-linear effects, spatial effects, random coefficients and interaction surfaces which extends the flexibility compared to parametric models such as GAMLSS modeling only smooth effects.\\
However, the restriction to additive models remains. Therefore, a different approach on how to include covariates that also allows for non-additive effects and interactions is considered in the next subsection of this article. In particular, the covariates can be used to separate the data into more homogeneous subgroups. Doing so after fitting a global model to the whole data set (without considering any covariates) and then fitting a local model to each subgroup should improve the model remarkably. This procedure of splitting a data set into subgroups can be done by applying a tree algorithm.\\


\subsection{Distributional Tree}
\label{disttree}
Given the fit of a global distributional model $D(Y, \bold{\hat{\theta}})$ the idea is to use the information provided by this model together with a set of covariates $\bold{Z} = (Z_1, \ldots, Z_m) \in \mathcal{Z}$ (from now on denoted by $\bold{Z}$ instead of $\bold{X}$ to indicate that they are considered as split variables only and not as regressors) to decide whether the data set should be split into subgroups or not. In particular, the information provided by the fitted model are the scores 
$$
s(\hat{\theta}, y_i) = \frac{\partial \ell}{\partial \theta}(\hat{\theta}; y_i).
$$ 
These values express how well the model fits the data regarding each parameter. Ideally they should fluctuate randomly around zero, similar to residuals in ordinary least squares estimations, but in this case there is a score value for each pair of estimated parameter $\hat{\theta}_j \in (\hat{\theta}_1, \ldots, \hat{\theta}_k) = \bold{\hat{\theta}}$ and observation $y_j \in \{y_i\}_{i = 1,\ldots, n}$. This enables the model to be sensitive to changes in each of the parameters which is one of the main advantages of distributional trees over other tree models considering only the mean of the distribution.\\
Statistical tests are applied to check whether there is a significant dependency between the scores and each of the partitioning variables $Z_l \in \bold{Z} = (Z_1, \ldots, Z_m)$. If so, the data set is split into subgroups based on the values of the partitioning variable that shows the highest dependency.\\
In a next step a local distributional model is fit in each of the subgroups. Repeating this procedure of fitting a model, applying statistical tests to evaluate the goodness of fit and splitting the data set depending on the test results leads to a tree structure. In that way the learning data set is separated into \textit{B} disjoint segments such that
$$
\mathcal{Z} = \dot{\bigcup\limits_{b = 1, \ldots, \textit{B}}} \mathcal{B}_b
$$
where each segment is represented by a terminal node of the tree.\\

The exact way of determining how and where to split varies across different tree algorithms. For distributional trees, one of two algorithms can be chosen as a framework to build the tree: either the MOB algorithm (\citealt{zeileis2008model}) or the ctree algorithm (\citealt{hothorn2006unbiased}). Summed up, the steps of building a distributional tree using one of these algorithms are
\begin{enumerate}
\item Specify a distribution with log-likelihood function $\ell(\theta; y)$.
\item Estimate $\bold{\hat{\theta}}$ via maximum likelihood.
\item Test for associations or instabilities of the scores $s(\bold{\hat{\theta}}, y_i)$ and each partitioning variable $Z_l$.
%\item Assess whether the \emph{model scores} are associated with
%    (or change along) any of the available covariates -- e.g.,
%    using parameter instability tests (\emph{strucchange}) or
%    conditional inference (\emph{coin}).
\item Split the sample along the partitioning variable with the strongest association or instability.
    Choose the breakpoint with the highest improvement in log-likelihood.
\item Repeat steps 2--4 recursively in the subgroups
    until some stopping criterion is reached.
    %-- e.g., for significance or sample size.
%\item Choose the variable with the strongest association
%\item Choose the split point which leads to the highest improvement of the model
%\item Split and repeat 2-6 in each node until a stopping criterion is met
\end{enumerate}
The stopping criterion in step 5 can for example be a minimum number of observations in a node or a p-value for the statistical tests applied in step 3.\\
Executing these steps results in a stepwise parametric model where a complete distribution is fit in each node of the tree.\\
\\
For distributional trees the strategy of how to make a prediction for a new set of covariates $\bold{z}^{\ast} = (z_1^{\ast}, \ldots, z_m^{\ast})$ is very simple and straight forward. All observations of the learning data set $\{(y_i,\bold{z}_i)\}_{i=1\ldots,n}$ which end up in the same terminal node of the tree as the new set of covariates are selected and a distributional model is fit on the subset. To select this subset a weight vector $\bold{\omega}^{tree} = (\omega^{tree}_1, \ldots, \omega^{tree}_n)$ is included in the fitting process.
$$
\omega^{tree}_i(\bold{z}^{\ast}) = \sum_{b=1}^B I((\bold{z}_i \in \mathcal{B}_b) \land (\bold{z}^{\ast} \in \mathcal{B}_b))
$$
The predicted distribution of the new set of covariates is then fully specified by the estimated parameter $\hat{\theta}(\bold{z}^{\ast})$ where
$$
\hat{\theta}(\bold{z}^{\ast}) = \max_{\theta \in \Theta} \sum_{i=1}^n \omega^{tree}_i(\bold{z}^{\ast}) \cdot \ell(\theta; y_i).
$$

While the clear structure of such a tree model is advantageous in interpreting and analyzing it, the abrupt changes are often too rough and impose steps on the model even if the true underlying effect is smooth. In these situations bagging can solve the problem by combining an ensemble of trees such that wrongly detected abrupt changes of the model are turned into almost smooth transitions. In that way, all kinds of effects can be modeled, which is is also one the of the main advantages of forest models (taking bagging one step further as the variance is reduced by sampling the covariates considered for the splitting process in each node). \\
\\
Common forest models such as random forests consist of trees only considering the mean of the distribution of the response variable. For that reason, changes in other parameters can only be detected if they are connected to changes in the location parameter, e.g. if they depend on the location parameter. Therefore, building a forest of distributional trees which can also capture separate effects in each of the distribution parameters leads to an improvement of the model. This valuable feature of detecting separate effects is also provided by GAMLSS as each parameter is modeled separately. However, in this case this separation can also be a disadvantage as it hinders the capturing of interactions unless they are specified explicitly by the user. These two cases provide once again a good reason to combine the two frameworks of flexible parametric modeling and tree structured models in order to profit from their advantages and compensate possible disadvantages. This is the pursued goal in introducing distributional forests.

\subsection{Distributional Forest}
\label{distforest}
A distributional forest consists of an ensemble of distributional trees $\{t_s\}_{s=1,\ldots,T}$. Each of these trees is built on a different data set which can either be a bootstrap or a subset of the original data set. Moreover, in each node a subset of the partitioning variables is chosen randomly as candidates for splitting variables. In that way the correlation among the resulting trees is reduced and therefore also the variance of the model (\citealt{breiman2001random}).\\
\\
Forest models differ mainly in the way how the trees are combined to make predictions. In this case the forest itself is only used to evaluate neighborhood relationships among the observations which can then be included in the fitting process.\\
Contrary to distributional trees, in distributional forests the whole learning data set can be used to fit the distributional model and specify the predicted distribution for a new set of covariates. This difference is represented in the weight vector as the tree weights $\bold{\omega}^{tree}$ containing only zeros and ones are now replaced by integer valued forest weights \\
$\bold{\omega}^{forest} = (\omega^{forest}_1, \ldots, \omega^{forest}_n)$.\\
For a new set of covariates $\bold{z}^{\ast} = (z^{\ast}_1, \ldots, z^{\ast}_m) $ the set of weights is obtained by the following procedure: For each tuple $(y_j,\bold{z}_j)$ in the learning data set $\{(y_i,\bold{z}_i)\}_{i=1\ldots,n}$ a weight is calculated by counting the number of trees in which $(y_j,\bold{z}_j)$ ends up in the same terminal node as the new set of covariates $\bold{z}^{\ast}$. 
$$
\omega^{forest}_i(\bold{z}^{\ast}) = \sum_{s=1}^T \sum_{b=1}^{B^s} I((\bold{z}_i \in \mathcal{B}^s_b) \land (\bold{z}^{\ast} \in \mathcal{B}^s_b))
$$
where $T$ is the number of trees of the forest, $B^s$ is the number of terminal nodes of the \textit{s}-th tree and $\mathcal{B}^s_b$ represents the \textit{b}-th terminal node of the \textit{s}-th tree.\\
This individual set of weights $\bold{\omega}^{forest}(\bold{z}^{\ast}) = (\omega^{forest}_1(\bold{z}^{\ast}), \ldots, \omega^{forest}_n(\bold{z}^{\ast}))$ for the new set of covariates $\bold{z}^{\ast}$ can now be included in the estimation process of the distribution parameters and in that way leads to its individual parameter vector $\hat{\theta}(\bold{z}^{\ast})$.
$$
\hat{\theta}(\bold{z}^{\ast}) =  \max_{\theta \in \Theta} \sum_{i=1}^n \omega^{forest}_i(\bold{z}^{\ast}) \cdot \ell(\theta; y_i)
$$
While in a distributional tree the same distribution is specified for all sets of covariates being in the same node, a distributional forest predicts and fits a different distribution for each different set of covariates due to the individual weights calculated separately for each set of covariates.\\
\\
As this novel methodology combines the advantages of smooth and flexible parametric modeling with those of tree structured modeling it provides a model that can deal with a wide range of situations. Abrupt changes and non-additive effects can be captured by its tree structure while smooth effects can be modeled reasonably well by combining various tree and smoothing the steps imposed by a single tree. As scores are considered for each distribution parameter (separate) effects in different parameters can be captured. At the same time, also interactions are detected automatically by splitting the data in a way that is sensitive to changes in either each of the parameters or combined in various parameters.\\
Even though many situations demand for the use of specifically tailored models providing the needed features and therefore being clearly advantageous and the best choice, it is often hard to tell in advance which features will be required. In these cases a distributional forest is a recommendable compromise providing reasonable models and performing well without any prespecification or additional knowledge needed in advance. 











\SweaveOpts{eval = FALSE}
%\newpage
%\section{Simulation Study}
%\label{Simulation}
%Within this simulation study the performance of distributional trees and forests is tested and compared to other models, showing situations where one or the other method is more advantageous than others. 

%In the case of smooth parameter functions without any abrupt changes / non-additive effects a smooth parametric model such as GAMLSS is clearly the first choice while a stepfunction can be modeled best by applying a tree-structured model. Even though for both of these extreme situations a specifically advantageous method is provided, a forest model can be a good compromise and performs well in a wide range of settings including the two mentioned before. (Therefore, a forest is a recommendable choice particularly when no information about the type of parameter function is provided.)

%Generally, a smooth parametric model such as GAMLSS should be the most successful method if the parameter function is smooth with no rapid changes. However, if there are any jumps or non-additive effects or interactions, the trees should be more effective in detecting these steps and therefore also lead to a better fitting forest. But this situation might also appear for smooth functions which contain sections with very steep slopes. Therefore, it is now of great interest, whether the concept of smooth parametric models or of trees and forests covers a broader range of functions where it is superior to the other.\\


%The new methods are evaluated on generated data sets and their results are then compared to those of a GAMLSS (\citealt{stasinopoulos2005}) which fits a separate generalized additive model to each of the distribution parameter and in that way specifies a whole distribution and is a very efficient option for smooth effects of the covariates on the parameters.\\
%Each generated data set includes 500 samples consisiting of the value of a response variable $Y$ and six covariates $X_1, \ldots, X_6$ which are all independent and distributed as follows:
%$$
%X_1, X_2, X_3 \sim \textit{Unif}([-1,1])
%$$
%$$
%X_4, X_5, X_6 \sim \textit{Binom}(1, 0.5)
%$$

%The distribution of the response variable is a left censored normal distribution with censoring point 0. The corresponding distribution parameters are defined by a parameter function $f$ depending on the covariates and an additional factor $\kappa$, also known as the smoothness constraint, which regulates the steepness of certain sections of the parameter function. For low values of $\kappa$ the resulting parameters change rather slowly over varying values of the covariates while high values of $\kappa$ lead to abrupt changes. \\ 
%The location parameter $\mu$ depends on $X_1, X_4$ and $\kappa$ and the scale parameter $\sigma$ depends on $\mu, X_1$ and $\kappa$. The other covariates $X_2, X_3, X_5$ and $X_6$ are used as noise variables.\\
%The latent variable $Y^{\ast}$ is generated based on the following setting.
%\begin{center}
%$$ 
%Y^{\ast} \sim \mathcal{N} (f(X_1, X_4, \kappa))
%$$
%with
%$$
%f(X_1, X_4, \kappa) = \left(\mu(X_1, X_4, \kappa) , \sigma(\mu,X_1,\kappa)\right)
%$$ 
%and
%\begin{align*}
%\mu(X_1, X_4, \kappa) = 0.2 
%+ \begin{cases}
%    7 \cdot \exp{(-(3 \cdot X_1-1.2)^{(2\cdot \kappa)})},
%    & \text{if } X_1 < 0.4 \text{ and } X_4 = 0\\
%    7 \cdot \exp{(-(3 \cdot X_1-1.2)^{(2\cdot \kappa)})} \cdot \frac{1}{2} + \frac{7}{2},       
%    & \text{if } X_1 \geq 0.4 \text{ and } X_4 = 0\\
%    10 \cdot \exp{(-(3 \cdot X_1-1.2)^{(2\cdot \kappa)})},
%    & \text{if } X_1 < 0.4 \text{ and } X_4 = 1\\
%    13 \cdot \exp{(-(3 \cdot X_1-1.2)^{(2\cdot \kappa)})} \cdot \frac{1}{2} + \frac{7}{2},       
%    & \text{if } X_1 \geq 0.4 \text{ and } X_4 = 1
%\end{cases}
%\end{align*}

%$$
%\sigma(\mu, X_1,\kappa) = 0.5 + \frac{\mu}{5} + 2 \cdot \mathbf{1}(X_1 < -0.4)
%$$
%%where $\Lambda$ is the cumulative distribution function of the logistic distribution
%%$$
%%\Lambda(\omega) = \frac{1}{1 + \exp(-\omega)}
%%$$
%The response variable $Y$ is defined as 
%$$
%Y = 
%\begin{cases}
%    Y^{\ast},& \text{if } Y^{\ast} \geq 0\\
%    0,       & \text{else}
%\end{cases}
%$$
%\end{center}


%To illustrate the parameter functions and the impact of $\kappa$ together with the results of the considered methods they are first applied on one generated data set only where $\kappa$ is set to~1 and then to a second data set where $\kappa$ is set to 10. In Figure~\ref{plot_sim_oneset_mu} the location parameter function $\mu(X_1, X_4, \kappa)$ is plotted along the covariate $X_1$ together with the estimated values of the different models for fixed covariates $x_4 = 0$ and $(x_2, x_3, x_5, x_6) = (0,0,0,0)$. \\

%\SweaveOpts{eval = TRUE}
<<eval = FALSE, echo=FALSE, results=hide>>=

source("oneset.R")
    
#if(file.exists("oneset1.rda")){
#  load("oneset1.rda")
#} else {
  
  #define parameter function
  #kappa <- 1
  fun1 <- function(x){
    mu <- 0.2 + 
      (7 * (exp(-(3*x[,1]-1.2)^(2*1)))) * as.numeric(x[,1] < 0.4) + 
      (7 * (exp(-(3*x[,1]-1.2)^(2*1))) * (1/2) + 7/2) * as.numeric(x[,1]>= 0.4) +
      (3 * (exp(-(3*x[,1]-1.2)^(2*1)))) * x[,4]
    sigma <- 0.5 + 
      mu/5 + 
      #(x[,1]>0.4)  #*x[,4]#+
      2*(x[,1] < -0.4) 
      #plogis((1^(1.8)) * 4 * (x[,2] + 0.7)) #* x[,4]
      #4*(1-plogis((1^(1.8)) * 2 * (x[,1] + 0.5))) * x[,4]
      #5 * abs(x[,1]) * x[,4]
    par <- cbind(mu, sigma)
    return(par)
  }
  
  library("gamlss.cens")
  gen.cens("NO", type = "left")
  
  oneset1 <- sim_oneset_vary_x1(parfun = fun1, nobs = 500, seedconst = 74, ntree = 100,
                                 formula = y~x1+x2+x3+x4+x5+x6, 
                                 tree_minsplit = 20, tree_minbucket = 7, tree_mincrit = 0.95, 
                                 forest_minsplit = 20, forest_minbucket = 7, forest_mincrit = 0, 
                                 forest_mtry = 3, type.tree = "ctree", 
                                 censNO = TRUE, 
                                 pred_fix_x2 = 0.9,
                                 pred_fix_x4 = 0,
                                 nrunif = 3,
                                 nrcovform = 6)

  
#  save(oneset1, file = "oneset1.rda")
#}

    
#if(file.exists("oneset10.rda")){
#  load("oneset10.rda")
#} else {
  
  #define parameter function
  #kappa <- 10
  fun10 <- function(x){
    mu <- 0.2 + 
      (7 * (exp(-(3*x[,1]-1.2)^(2*10)))) * as.numeric(x[,1] < 0.4) + 
      (7 * (exp(-(3*x[,1]-1.2)^(2*10))) * (1/2) + 7/2) * as.numeric(x[,1]>= 0.4) +
      (3 * (exp(-(3*x[,1]-1.2)^(2*10)))) * x[,4]
    sigma <- 0.5 + 
      mu/5 + 
      #(x[,1]>0.4)  #*x[,4]#+
      2*(x[,1] < -0.4) 
      #plogis((10^(1.8)) * 4 * (x[,2] + 0.7)) #* x[,4]
      #4*(1-plogis((10^(1.8)) * 2 * (x[,1] + 0.5))) * x[,4]
      #5 * abs(x[,1]) * x[,4]
    par <- cbind(mu, sigma)
    return(par)
  }
  
  library("gamlss.cens")
  gen.cens("NO", type = "left")
  
  oneset10 <- sim_oneset_vary_x1(parfun = fun10, nobs = 500, seedconst = 74, ntree = 100,
                                 formula = y~x1+x2+x3+x4+x5+x6, 
                                 tree_minsplit = 20, tree_minbucket = 7, tree_mincrit = 0.95, 
                                 forest_minsplit = 20, forest_minbucket = 7, forest_mincrit = 0, 
                                 forest_mtry = 3, type.tree = "ctree", 
                                 censNO = TRUE, 
                                 pred_fix_x2 = 0.9,
                                 pred_fix_x4 = 0,
                                 nrunif = 3,
                                 nrcovform = 6)

  
#  save(oneset10, file = "oneset10.rda")
#}
@


%\begin{figure}[t!]
%\begin{subfigure}{0.6\textwidth}
%<<sim_oneset1_mu, fig=TRUE, echo=FALSE>>=
%plot_oneset(oneset1, compare_mu =TRUE, add_dt = TRUE, add_df = TRUE, add_g = TRUE, plot_legend = TRUE, w_sigma = FALSE)
%@
%\end{subfigure}
%\hspace{-1cm}
%\begin{subfigure}{0.6\textwidth}
%<<sim_oneset10_mu, fig=TRUE, echo=FALSE>>=
%plot_oneset(oneset10, compare_mu =TRUE, add_dt = TRUE, add_df = TRUE, add_g = TRUE, plot_legend = TRUE, w_sigma = FALSE)
%@
%\end{subfigure}
%\caption{\label{plot_sim_oneset_mu} Smooth parameter function (left window, $\kappa = 1$) and parameter function with abrupt changes (right %window, $\kappa = 10$) together with the predicted location parameter $\mu$ of the tree model, the forest model and the GAMLSS,  plotted along %$x_1$ while all the other covariates are kept at fixed values.}
%\end{figure}

%In the case of smooth effects of the coefficients on the parameter function ($\kappa = 1$, left window in Figure~\ref{plot_sim_oneset_mu}) the fitted parameters of the GAMLSS are closest to the true parameters while the steps of the tree are cleary too rough. By combining various trees the forest can smooth out these steps, however, not as well as the GAMLSS.\\
%Looking at the other extreme situation in which the parameter functions include abrupt changes ($\kappa = 10$, right window in Figure~\ref{plot_sim_oneset_mu}) the tree captures them very well wich also leads to a good fit of the forest model. In contrast to the two tree structured models the GAMLSS has its difficulties with these steps and therefore fluctuates around the true parameter functions.\\

%To get a more profound impression, the methods are evaluated on various data sets. The value of $\kappa$ is increasing from 1 to 10 by a stepsize of 1 and for each step 100 data sets are generated.\\
%%\subsubsection{Means of Comparison}
%In order to compare the performance of the chosen methods over various data sets, the continuous ranked probability score (CRPS) is calculated for each model on each data set. 

%three different values are calculated for the resulting models: the root of the mean squared error (RMSE), the log-likelihood value and the continuous ranked probability score (CRPS). All of them are evaluated out of sample, e.g. on a part of the data set that has not been used to build the model.\\
%Therefore, overfitting would lead to worse results.\\
%\\
%The RMSE represents how close the estimated expected values are to the true observations. For a response variable $Y$ and a set of observations $\textbf{y} = \{y_i\}_{i = 1,\ldots,n}$ with the corresponding estimated parameters $\hat{\boldsymbol{\theta}} = \{\hat{\theta}_i\}_{i = 1,\ldots,n}$ the RMSE is defined as follows.

%$$
%\text{RMSE}(Y, \boldsymbol{y}, \boldsymbol{\hat{\theta}}) = \sqrt{\frac{1}{n} \sum_{i=1}^n (\mathbb{E}_{\hat{\theta}_i}(Y) - y_i)^2}
%$$

%In most cases this value does not measure the distributional fit as only the expected value is considered. However, in the case of a censored normal distribution the expected value of the observed variable is a transformation of the expected value of the latent variable which includes the location and the scale parameter.\\
%\\
%A typical way to evaluate the distributional fit is provided by the log-likelihood value. For each observation $y$ and the corresponding predicted parameter $\hat{\theta}$ the value of the log-likelihood function $\ell(\hat{\theta}; y)$ is calculated and then averaged over all observations in the testing data set. The higher the value the better the predictions of the model are.\\
%\\
%The CRPS is another value that measures the accuracy of a probabilistic model. For one observation $y$ with the predicted distribution function $F$ the CRPS value is calculated by
%The CRPS measures the accuracy of a probabilistic model. For one observation $y$ with the predicted distribution function $F$ the CRPS value is calculated by
%$$
%\text{CRPS}(y, F) = \int_{-\infty}^\infty (F(z) - \mathbf{1}(y \leq z))^2 dz
%$$
%where \(\mathbf{1}\) is the indicator function. \\
%In this simulation study the CRPS is evaluated out of sample on a data set that has not been used to build the model but has been generated by the same process and with the same settings as the original data set. In the following this second data set is called testing data set while the original data set on which the models are built on is called learning data set.
%The mean CRPS over the whole testing data set is used for validation. As the CRPS can be seen as a generalization of the mean absolute error, a low value indicates a good fit. In applications, one big advantage in terms of interpretability is that the resulting value is on the scale of the observation. \\


%The methods are then compared by the out-of-sample RMSE for the location parameter $\mu$ and the shape parameter $\sigma$ as well as by %the out-of-sample log-likelihood of the fitted models, averaged within each step.
%(In that way it can be investigated how the results change for a changing value of $\kappa$.)


%\SweaveOpts{eval = FALSE}

<<eval = FALSE, echo=FALSE, results=hide>>=
if(file.exists("simres.rda")){
  load("simres.rda")
} else {
  #source("~/svn/partykit/pkg/disttree/inst/draft/.R")
  source("gensim.R")
  
  library("gamlss.cens")
  gen.cens("NO", type = "left")
  
  simfun <- function(x, kappa){
    mu <- 0.2 + 
      (7 * (exp(-(3*x[,1]-1.2)^(2*kappa)))) * as.numeric(x[,1] < 0.4) + 
      (7 * (exp(-(3*x[,1]-1.2)^(2*kappa))) * (1/2) + 7/2) * as.numeric(x[,1]>= 0.4) +
      (3 * (exp(-(3*x[,1]-1.2)^(2*kappa)))) * x[,4]
    sigma <- 0.5 + 
      mu/5 + 
      #(x[,1]>0.4)  #*x[,4]#+
      2*(x[,1] < -0.4) 
    #plogis((kappa^(1.8)) * 4 * (x[,2] + 0.7)) #* x[,4]
    #4*(1-plogis((kappa^(1.8)) * 2 * (x[,1] + 0.5))) * x[,4]
    #5 * abs(x[,1]) * x[,4]
    par <- cbind(mu, sigma)
    return(par)
  }
    
    
  simres <- gensim(parfun = simfun,
                   seedconst = 74, nrep = 100, ntree = 100,
                   nsteps = 10, stepsize = 1,
                   formula = y~x1+x2+x3+x4+x5+x6,
                   nobs = 500, testnobs = 500L,
                   tree_minsplit = 20, tree_minbucket = 7, tree_mincrit = 0.95,
                   forest_minsplit = 20, forest_minbucket = 7, forest_mincrit = 0, 
                   forest_mtry = 3,
                   censNO = TRUE,
                   gamboost_cvr = FALSE,
                   eval_disttree = TRUE,
                   eval_distforest = TRUE,
                   eval_gamlss = TRUE,
                   eval_bamlss = FALSE,
                   eval_gamboostLSS = FALSE,
                   eval_randomForest = FALSE,
                   eval_cforest = FALSE,
                   nrunif = 3,
                   nrcovform = 6)
  
  save(simres, file = "simres.rda")
}
@



%\begin{figure}[t!]
%\begin{subfigure}{0.6\textwidth}
%<<sim_rmse_exp, fig=TRUE, echo=FALSE>>=
%plot_rmse(simres, type = "exp")
%@
%\end{subfigure}
%\hspace{-1cm}
%\begin{subfigure}{0.6\textwidth}
%<<sim_rmse_par, fig=TRUE, echo=FALSE>>=
%plot_rmse(simres, type = "par")
%@
%\end{subfigure}
%\caption{\label{plot_sim_rmse}Results of the simulation study (RMSE).\\
%left: continuous lines represent the difference between the true and the estimated expected value, dotted lines represent the difference %between the observations and the estimated expected value\\
%right: continuous lines represent the location parameter, dotted lines the scale parameter}
%\end{figure}

%\begin{figure}[t!]
%\begin{center}
%%\begin{subfigure}{0.6\textwidth}
%<<sim_crps, fig=TRUE, echo=FALSE>>=
%plot_crps(simres)
%@
%\caption{\label{plot_sim_crps}CRPS of the three models evaluated on various data sets over increasing $\kappa$} 
%%\code{disttree} vs. \code{distforest} vs. \code{gamlss} vs. \code{randomForest} vs. %\code{bamlss} vs. \code{gamboostLSS}\\
%%right: levelplot of the parameter function for $\kappa=5$}
%\end{center}
%\end{figure}

%\end{subfigure}
%\hspace{-1cm}
%\begin{subfigure}{0.6\textwidth}
%<<sim_levelplot, fig=TRUE, echo=FALSE>>=
%levelfun <- function(x) simres$fun(x,5)
%
%ncuts <- 100
%colR <- colorRampPalette(c("blue","green"))                                       
%cols <- colR(ncuts+1)
%griddata <- expand.grid(list(x1 = seq(-1, 1, 0.01), x4 = seq(0, 1, 0.2)))
%griddata$mu <- levelfun(griddata)
%levelplot(mu ~ x1 * x4, data = griddata, xlab = "x1", ylab = "x4", 
%          region = TRUE, cuts = ncuts, col.regions = cols)

%@
%\end{subfigure}
%\caption{\label{plot_sim_ll}left: results of the simulation study: log-likelihood\\ %\code{disttree} vs. \code{distforest} vs. \code{gamlss} vs. \code{randomForest} vs. %\code{bamlss} vs. \code{gamboostLSS}\\
%right: levelplot of the parameter function for $\kappa=5$}
%\end{figure}



%Figure~\ref{plot_sim_crps} clearly illustrates the strengths of the different methods applied in this simulation study and supports the assumptions made above based on the results for the two specific situations in Figure~\ref{plot_sim_oneset_mu}. Moreover, it can be observed how the stepwise transition from the one extreme situation to the other influences the performance of the applied methods.\\
%The parametric GAMLSS can easily deal with the smooth effects that appear for low values of $\kappa$ and outperforms the tree and forest models in this situation. However, it clearly starts to struggle as $\kappa$ increases and the effects turn into abrupt changes. %This behaviour has already been detected and discussed above when looking at only single data sets and is now proven by an almost monotone increase of the CRPS.\\
%On the contrary, the imposed/enforced steps of the tree model lead to a high CRPS in the situation of smooth effects while decreasing quickly as the slopes in the parameter functions get steeper. For high values of $\kappa$ resulting almost in step functions the tree model perfomes the best as stated by the low CRPS value.\\
%Therefore, both extreme situations show to have a preferable method to choose which clearly outperforms the others. But overall the forest model stays more or less on the same level of goodness of fit regardless of the type of effects in the parameter functions. As this level lies inbetween the ones of the other two methods the forest model provides a good compromise. If the type of effects is known in advance one should choose the corresponding adequate method. But as this is usually not the case, applying a distributional forest offers a good and flexible solution which can deal with a wide variety of situations and in that way ensures a reasonable well fit for a wide rango of situations where no prior information on the form of the effects is available. 

%Even though \code{gamlss} can easily deal with each of the smooth functions it struggles with the separation in subgroups. This is the point where the tree structure provides a better solution. However, a single tree can not keep up with the performance of \code{gamlss} when it comes to modeling the smooth functions within the subgroups. Offering a good compromise the forest takes advantages of the tree structure to find subgroups but can also model smooth effects if enough trees are included.\\
%However, once the increasing value of $\kappa$ leads to very steep sections within the parameter functions of each of the subgroups the tree turns out to be the most successful method in this specific situation.\\
%\\
%Looking at the resulting values of the RMSE it can be observed that trees and forests cleary outperform gamlss in this situation. For $\kappa=1$ the performance of all three methods is similar. However, as $\kappa$ increases the RMSE of gamlss increases while the RMSE of trees and forests stays at almost the same level or even decreases sligthly as soon as $\kappa$ is greater or equal 2. The same conclusion can be drawn from looking at the log-likelihood where the resulting values of gamlss decrease rapidly while the values of trees and forests only decrease a little at the beginning but then increase.\\
%This is the result that was expected as trees and forests can deal with rapid changes of the distribution functions more easily.\\
%For low values of $\kappa$ the forests are a little bit more efficient than the trees, but the higher $\kappa$ and therefore also the steeper the parameter function is the better the trees become. Especially regarding the scale parameter the trees are clearly advantageous over the forests.   


%(As it has already been demonstrated in the first examples on single data sets the performances of \code{distforest} and \code{disttree} are not as successful as the one of \code{gamlss} for low values of $\kappa$ but remarkably improve as this values increases and are superior to \code{gamlss} for high values of $\kappa$. This is illustrated by an almost constant RMSE of \code{distforest} and \code{disttree} while the RMSE of \code{gamlss} increases. Comparing the tree and the forest model it can be observed that the forest seems to perform better for lower $\kappa$ while very steep sections lead to a better performance of trees.













\SweaveOpts{eval = TRUE}
%\newpage
%\newpage
\section{Probabilistic Forecasting of Precipitation in Complex Terrain}
\label{ProbabilisticForecasting}


As explained in the introduction, distributional trees and forests provide a new approach in probabilistic forecasting which combines advantages of parametric models that are already used in this field and tree structured models which provide features that could be particularly valuable in probabilistic forecasting but have not yet been applied in this kind of settings. To show their strengths in practice, these new methods are now applied to probabilistic precipitation forecasts in a mountainous region based on a large number of numerical weather prediction quantities. 

\subsection{Data}
\label{data}
The data set used for training and validation consists of observed daily precipitation sums provided by the National Hydrographical Service (\citealt{ehyd}) and numerical weather forecasts from the {U.S.} National Oceanic and Atmospheric Administration (NOAA).
Both, observations and weather forecasts are available from 1985 trough the year 2012, focusing on July only in this article.\\ 
Observations are available on a daily basis, each observation representing the total precipitation sum fallen within 24~hours observed at 6~UTC at 95 stations all over Tyrol and its close surrounding. The observations are undergoing a rigorous quality check before made available to the public.  As covariates the second generation reforecast data set of the global ensemble forecast system (GEFS) is used (\citealt{hamill2013}).  This data set consists of an 11-member ensemble based on a fixed version of the numerical model and a horizontal grid-spacing of about $50~\times~50 \text{km}^2$ initialized daily at 0~UTC from December 1984 to present providing forecasts on 26 hourly temporal resolution. Each of the 11 ensemble members uses slightly different perturbed initial conditions trying to predict the situation specific uncertainty of the atmospheric state. 


%The data set consists of daily 24h precipitation sums for the month of July from 28 years, starting in 1985 until 2012, both years included. The observations were measured at 95 (?) stations all over Tyrol (and regions close to the frontier). The considered data is provided by .....
%Next to the variable of interest which is the measured total precipitation within 24 hours of each day (measured from 6 UTC to 6 UTC of the following day), numerical forecasts (from ... ) are used as covariates. These forecasts are ensemble predictions consisting of ... individual forecasts based on varying/perturbed initial conditions.\\

In particular, 14 basic forecast variables are considered with up to 12 variations of each of these variables being included as covariates. Since each forecast variable consists of an ensemble of individual predictions, the possible variations include the mean, the standard deviation, the minimum and the maximum over all ensemble members, noted as ensemble mean, ensemble std dev, ensemble minimum and ensemble maximum. All of these values can be calculated on the sums over 24 hours (6--6 UTC of the following day) or a 6 hour time window.\\
Another possible option is to consider the mean, minimum or maximum over 24 hours for each individual prediction and then take the ensemble mean or ensemble standard deviation, noted as ensemble mean of mean/minimal/maximal values or ensemble std dev of mean/minimal/maximal values respectively.\\
Table~\ref{covariates} shows all variables together with the number of variations and the provided variations listed in the last column. Altogether 82 covariates are available in this setting.\\
To remove large parts of the skewness of precipitation data, a power transformation has been used frequently (Box and Cox 1964). In literature, cubic (\citealt{stidd1973}) or square root (\citealt{hutchinson1998b}) transformations have often been suggested but may vary for different climatic zones or temporal aggregation periods. In \citealt{stauffer2017a} optimization methods have been applied to find the best transformation factor which resulted in a value of $\frac{1}{1.6}$. Since the observation data set used for this optimization contains the observations considered in this article the same factor is chosen here for the power transformation of the response variable \texttt{tp} (total precipitation). The resulting transformed variable is expected to be normally distributed and left censored at censoring point 0. For simplicity the same transformation has been chosen for the variable \texttt{cape} (convective available potential energy).

%\newpage
\begin{table}[t!]
%\resizebox{1.4\textwidth}{!}{
\begin{minipage}{\textwidth}
\begin{tabular}{ l  c  l }
\hline
Basic covariates & {\#} & Variations\\
\hline
\textbf{tp}: total precipitation            & 12  & ensemble mean of sums over 24h, \\
\hspace*{\fill} power transformed (by $\frac{1}{1.6}$) &    & ensemble std dev of sums over 24h, \\ 
\textbf{cape}: convective                  &    & ensemble minimum of sums over 24h, \\
\hspace*{0.4cm} available potential energy &    & ensemble maximum of sums over 24h\\
\hspace*{\fill}  power transformed (by $\frac{1}{1.6}$)&    & \qquad all for 6--30 UTC\\
\hspace*{\fill}                            &    & ensemble mean of sums over 6h\\
\hspace*{\fill}                            &    & \qquad for 6--12, 12--18, 18--24, 24--30 UTC\\
\hspace*{\fill}                            &    & ensemble std dev of sums over 6h\\
\hspace*{\fill}                            &    & \qquad for 6--12, 12--18, 18--24, 24--30 UTC\\
\hline
\textbf{dswrf}: downwards short wave      & 6 & ensemble mean of mean values, \\
\hspace*{\fill} radiation flux (``sunshine'') &   & ensemble mean of minimal values,\\
\textbf{msl}: mean sea level pressure     &   &  ensemble mean of maximal values,\\
\textbf{pwat}: preciptable water          &   & ensemble std dev of mean values,\\
\textbf{tmax}: 2m maximum temperature     &   & ensemble std dev of minimal values,\\
\hspace*{1.15cm}                          &   & ensemble std dev of maximal values,\\
\textbf{tcolc}: total column-integrated   &   & \qquad all over 6--30 UTC \\
\hspace*{1.05cm} condensate               &   & \\
\textbf{t500}: temperature on 500 hPa     &   & \\
\hspace*{1cm}                             &   & \\
\textbf{t700}: temperature on 700 hPa     &   & \\
\hspace*{1cm}                             &   & \\
\textbf{t850}: temperature on 850 hPa     &   & \\
\hspace{1cm}                              &   & \\
\hline
\textbf{tdiff500850}: temperature         & 3 & ensemble mean of difference in mean,\\
\hspace*{\fill} difference 500 to 850 hPa &   & ensemble minimum of difference in mean,\\
\textbf{tdiff500700}: temperature         &   & ensemble maximum of difference in mean\\
\hspace*{\fill} difference 500 to 700 hPa &   & \qquad all over 6--30 UTC\\
\textbf{tdiff700850}: temperature         &   & \\
\hspace*{\fill} difference 700 to 850 hPa &   & \\
\hline
\textbf{msl{\_}diff}: mean sea level pressure & 1 & msl{\_}mean{\_}max - msl{\_}mean{\_}min\\
\hspace{1.45cm} difference            &   & \qquad over 6--30 UTC\\
\hline
\end{tabular}
\caption[Table caption text]{Basic covariates together with the number ({\#}) and the type of variations.}
\label{covariates}
\end{minipage} 
%}
\end{table}



\subsection{Models and Evaluation}
Next to distributional forests three other methods are applied to this data set and their results are compared based on the continuous ranked probability score (CRPS). One of the reference models is an EMOS model as this is a common choice in this type of setting and research field. The other two models are both GAMs, once in a prespecified form in terms of variable selection for each distribution parameter and once applying a boosting algorithm. 
The chosen methods and their specifications are listed in Table~\ref{model_specification}.
\begin{table}[t!]
%\resizebox{1.4\textwidth}{!}{
%\begin{minipage}{\textwidth}
\begin{center}
\begin{tabular}{ l l l l }
\hline
Model & Type & Location ($\mu$) & Scale ($\log(\sigma)$) \\
\hline
\textbf{Forest} & recursive    & all & all \\ 
                & partitioning &     &     \\ 
\hline
\textbf{prespecified GAM} & spline  & tp{\_}mean, & tp{\_}sprd,\\
                            & in each & tp{\_}max,  & dswrf{\_}sprd{\_}mean,\\
 & & tp{\_}mean1218 $\ast$ & tp{\_}sprd1218 $\ast$\\ 
 & & \quad cape{\_}mean1218, & \quad  cape{\_}mean1218,\\
 & & dswrf{\_}mean{\_}mean, & tcolc{\_}sprd{\_}mean,\\
 & & tcolc{\_}mean{\_}mean, & tdiff500850{\_}mean\\
 & & pwat{\_}mean{\_}mean, & \\
 & & tdiff500850{\_}mean, & \\
 & & msl{\_}diff & \\
\hline
\textbf{boosted GAM} & spline  & all & all \\
                       & in each &     &     \\
\hline
\textbf{EMOS} & linear & tp{\_}mean & tp{\_}sprd \\
\hline
\end{tabular}
\caption[Table caption text]{Table of applied methods together with their type (way of including the covariates) and the included covariates for each distribution parameter.}
\label{model_specification}
%}
\end{center}
\end{table}

For the prespecified GAM model the covariates were selected manually based on meteorological knowledge and experience. Additionally, interactions are specified in advance, once between \texttt{tp{\_}mean1218} and \texttt{cape{\_}mean1218} for the location parameter and once between \texttt{tp{\_}sprd1218} and \texttt{cape{\_}mean1218} for the scale parameter. These interactions might capture events with a high potential for moderate or severe thunderstorms with large amounts of precipitation. By putting extra effort in the specification of the formulas the prespecified GAM model is provided with additional information and is therefore expected to profit from this advantage.\\
In the EMOS model the logarithmic function is chosen as the link function for the scale parameter. This choice has been made based on the results of \citealt{Gebetsberger2017} where different link functions (identity, quadratic and logarithmic function) have been compared. Moreover, testing the identity, quadratic and logarithmic function as link functions in this setting supported this choice as the logarithmic function lead to the best performance of the EMOS model in this application.\\
\\
In order to compare the performance of the chosen methods, the continuous ranked probability score (CRPS) is calculated for each model. The CRPS measures the accuracy of a probabilistic model. For one observation $y$ with the predicted distribution function $F$ the CRPS value is calculated by
$$
\text{CRPS}(y, F) = \int_{-\infty}^\infty (F(z) - \mathbf{1}(y \leq z))^2 dz
$$
where \(\mathbf{1}\) is the indicator function. \\
In this application the CRPS is evaluated out of sample on a data set that has not been used to build the model. The mean CRPS over this data set is used for validation. Since the CRPS can be seen as a generalization of the mean absolute error, a low value indicates a good fit. In applications, one big advantage in terms of interpretability is that the resulting value is on the scale of the observation. \\
For the purpose of comparing the performance of a distributional forest to those of other models the so-called skills score can be calculated with respect to the CRPS and one model being chosen as a reference. In that way we obtain a measure of how much a method improves the model performance in terms of CRPS compared to the reference method.
$$
\text{SkillsScore}_{\text{method}} = 1 - \frac{\text{CRPS}_{\text{method}}}{\text{CRPS}_{\text{reference}}}
$$




%\SweaveOpts{eval = TRUE}
\subsection{Application for one station}
First of all we focus on one observation station, Axams in Tyrol. This station is selected due to its geographical closeness to Innsbruck, the capital of Tyrol. As for all other stations, daily observations of the month of July of 28 years are available from 1985 through the year 2012.\\ 
The results of fitting one distributional forest to the first 24 years of the data set and predicting the total amount of precipitation for one specific day of each of the four successive years have been shown in Figure~\ref{axams_onestation} as a motivational example for a probabilistic forecast. Staying with this fitted model we are now going to analyze variable importance of this forest model and compare its results to those of the GAM models and the EMOS model based on probability integral transform (PIT) histograms.\\
\\
As shown in Table~\ref{model_specification} the forest model includes all 82 covariates. A single tree model allows to easily analyze which covariates are used as split variables in the nodes and hence have an impact on the response variable. In a forest model though, for each of the trees a subset of these covariates is chosen randomly as possible split variables which is one of the reasons leading to different splitting variables being chosen in each tree. To get an overview of which covariates have the most influential effect on the response variable a standard measurement of variable importance for forest models has been applied. Based on the mean decrease in accuracy regarding the CRPS the 10 leading covariates of the above described forest model for station Axams learned on the first 24 year are listed in Figure~\ref{varimp}. As expected, the numerical forecast for total precipitation is the most influential variable. In particular, five variations of this variable are among the top ten of the list of covariates ordered by variable importance.  
  

\begin{figure}[t!]
\begin{center}
%\begin{subfigure}{0.7\textwidth}
<<rain_axams_varim, fig=TRUE, echo=FALSE, width=6.5, height=4.5>>=
par(mar = c(2,10,2,2))
barplot(sort(vimp_crps, decreasing = FALSE)[(length(vimp_crps)-9):length(vimp_crps)], 
        horiz = TRUE, las = 1, cex.names = 1.2, axes = TRUE, 
        names.arg = gsub("pow", "", names(sort(vimp_crps, decreasing = FALSE)[(length(vimp_crps)-9):length(vimp_crps)])))
#axis(1, at = seq(0,1.6,0.2), las = 1, mgp=c(-1,0.5,0))
@
%\end{subfigure}
%\hspace{-1cm}
%\begin{subfigure}{0.5\textwidth}
%<<rain_axams_varim_margeffects1_sigma, fig=TRUE, echo=FALSE>>=
%plot(x = sp1[,id_mi[1]], y = sp1[, "sigma"], type = "l", xlab = names_mi[1], ylab = %"sigma")
%@
%\end{subfigure}
\caption{\label{varimp} Variable importance of the forest model for station Axams learned on data from 1985--2008 and assessed in 2009--2012 based on mean decrease in accuracy regarding the CRPS (top 10 list).}
\end{center}
\end{figure}

To get another impression of the quality of the predictions made by the tested methods a PIT histogram is plotted in Figure~\ref{pit_oosample} for out-of-sample predictions of each of the models. 
A perfectly calibrated model would lead to a PIT histogram with a uniform distribution (a density of 1 for each bin as indicated by the red horizontal line). Overall, it can be stated that all four model provide well calibrated forecasts.

  

%\begin{figure}[t!]
%\begin{subfigure}{0.5\textwidth}
%<<rain_axams_pit_insample, fig=TRUE, echo=FALSE>>=
%
%# in sample
%set.seed(7)
%par(mfrow = c(2,2))
%pithist(pit_df_l, nsim = 1000, breaks = seq(0, 1, length.out = 9))
%pithist(pit_g_l, nsim = 1000, breaks = seq(0, 1, length.out = 9))
%pithist(pit_gb_l, nsim = 1000, breaks = seq(0, 1, length.out = 9))
%pithist(pit_ml_l, nsim = 1000, breaks = seq(0, 1, length.out = 9))
%
%@
%\caption{\label{pit_insample} PIT histograms for station Axams (in sample)}
%\end{figure}

\begin{figure}[t!]
\begin{center}
%\begin{subfigure}{0.8\textwidth}
<<rain_axams_pit_oosample, fig=TRUE, echo=FALSE, width = 6.5>>=
    
# out of sample
set.seed(7)

par(mfrow = c(2,2))
pithist(pit_df_t, nsim = 1000, breaks = seq(0, 1, length.out = 9), main = "Forest", ylim = c(0,1.5))
pithist(pit_g_t, nsim = 1000, breaks = seq(0, 1, length.out = 9), main = "prespecified GAM", ylim = c(0,1.5))
pithist(pit_gb_t, nsim = 1000, breaks = seq(0, 1, length.out = 9), main = "boosted GAM", ylim = c(0,1.5))
pithist(pit_ml_t, nsim = 1000, breaks = seq(0, 1, length.out = 9), main = "EMOS", ylim = c(0,1.5))
      
@
%\end{subfigure}
\caption{\label{pit_oosample}Out-of-sample PIT histograms for station Axams assessed in 2009--2012 based on models learned on data from 1985--2008.}
\end{center}
\end{figure}

%\begin{figure}[t!]
%\begin{subfigure}{0.5\textwidth}
%<<rain_axams_qqr_insample, fig=TRUE, echo=FALSE>>=
%# in sample
%set.seed(7)
%par(mfrow = c(2,2))
%set.seed(1)
%qqrplot(pit_df_l, nsim = 1, main = "distforest (in bag)")
%qqrplot(pit_g_l, nsim = 1, main = "gamlss (in bag)")
%qqrplot(pit_gb_l, nsim = 1, main = "gamboostLSS (in bag)")
%qqrplot(pit_ml_l, nsim = 1, main = "EMOS (in bag)")

%@
%\caption{\label{qqr_insample} QQR-Plot for station Axams (in sample)}
%\end{figure}

%\begin{figure}[t!]
%%\begin{subfigure}{0.5\textwidth}
%<<rain_axams_qqr_oosample, fig=TRUE, echo=FALSE>>=
%    
%# out of sample
%set.seed(7)
%par(mfrow = c(2,2))
%set.seed(1)
%qqrplot(pit_df_t, nsim = 1, main = "distforest (out of bag)")
%qqrplot(pit_g_t, nsim = 1, main = "gamlss (out of bag)")
%qqrplot(pit_gb_t, nsim = 1, main = "gamboostLSS (out of bag)")
%qqrplot(pit_ml_t, nsim = 1, main = "EMOS (out of bag)")

%@
%\caption{\label{qqr_oosample} QQR-Plot for station Axams (out of sample)}
%\end{figure}


  






%\SweaveOpts{eval = TRUE}
%\subsubsection{Cross-Validation}
\indent After analyzing the results of the models on one data set only where visualization is easier, we now want to get a more profound impression of the performances of the different methods. For that reason a cross validation framework is considered for the station Axams. The data set over all 28 years is split into 7 subsets each consisting of 4 randomly selected years. While 6 of these subsets are used as learning data the 7th is the testing data set for which predictions are made and evaluated. This is done by calculating the CRPS value for each of the observations in the testing data set and then averaging them. Each of the 7 parts is selected as testing data once which leads to 7 different settings over which, again, the average values are stored. This whole procedure is repeated 10 times. The resulting skills scores are illustrated in Figure~\ref{boxplots_crps_axams} using the EMOS model as the reference model represented by the horizontal line at height 0.
<<echo=FALSE, results=hide>>=
#### cross validation rain
if(file.exists("rain_Axams_7x10.rda")){
  #load("~/svn/partykit/pkg/disttree/inst/draft/rain_Axams_7x10.rda")
  load("rain_Axams_7x10.rda")
} else {
  #source("~/svn/partykit/pkg/disttree/inst/draft/rain_cross_7x10.R")
  source("rain_cross_7x10.R")
  library("gamlss.cens")
  gen.cens("NO", type = "left")
  rainres <- rain_cross(stationname = "Axams", 
                        seedconst = 7, ntree = 100,
                        forest_mtry = 27,
                        gamboost_cvr = TRUE,
                        frac = FALSE)
  
  save(rainres, file = "rain_Axams_7x10.rda")
}

rain_rmse <- matrix(0, ncol = ncol(rainres[[1]]$rmse), length(rainres)-1)
rain_ll <- matrix(0, ncol = ncol(rainres[[1]]$ll), length(rainres)-1)
rain_crps <- matrix(0, ncol = ncol(rainres[[1]]$crps), length(rainres)-1)

for(i in 1:(length(rainres)-1)){
  rain_rmse[i,] <- colMeans(rainres[[i]]$rmse, na.rm = TRUE)
  rain_ll[i,] <- colMeans(rainres[[i]]$ll, na.rm = TRUE)
  rain_crps[i,] <- colMeans(rainres[[i]]$crps, na.rm = TRUE)
}

colnames(rain_rmse) <- colnames(rain_ll) <- colnames(rain_crps) <-
  colnames(rainres[[1]]$rmse)

@


\begin{figure}[t!]
\begin{center}
\center
<<rain_cross_axams_crps_skills_score, fig=TRUE, echo=FALSE, height=4.5>>=
boxplot(1 - rain_crps[,c(2,3,4)] / rain_crps[,6], ylim = c(-0.005, 0.065),
        names = c("Forest", "prespecified GAM", "boosted GAM"),
        ylab = "CRPS skills score", col = "lightgray") 
abline(h = 0, col = pal["EMOS"], lwd = 2)
@
\center
\caption{\label{boxplots_crps_axams}CRPS skills score with reference model EMOS from the 10 times 7-fold cross validation at station Axams.}
\end{center}
\end{figure}

The CRPS skills score boxplot shows that, in this setting, both GAM models and the forest perform better than the EMOS model. While the two GAM models lead to an improvement of around 4 percent with the boosted version being slightly ahead, the forest even reaches an improvement rate close to 6 percent.\\
From this result we can conclude that the distributional forest provides a powerful alternative framework where only little or almost no effort is required to set up the model and which is competitive to existing methods that are commonly used for this kind of application or even outperforms them.\\
For the prespecified GAM model the covariates needed to be selected and interactions needed to be defined in advance. To do so, meteorological experience and knowledge is required and a lot of thought has to be put in these decisions. Moreover, making such decisions is always equivalent to influencing the resulting model and comes along with the risk of having missed important facts or even imposing certain effects on the model which might not correspond to the truth.\\
This risk is avoided in the boosted GAM as all covariates are included and the selection of the influential covariates is done within a boosting framework. However, the trade off here is a remarkably higher computational effort.\\
To build a distributional forest the user does not have to put any additional effort in it and therefore is not required to provide any expert knowledge or additional information about the setting. Variable selection is done automatically and interactions are detected by the tree structure of the model.\\
In this particular setting, a very high number of covariates is available which intensifies the impact of the above mentioned strength of the forest model.\\
Therefore, in comparing these methods one advantage of distributional forests is clearly outlined as they offer a very flexible framework which enables an easy use of the method and at the same time lead to an improvement in the predictive performance.

\SweaveOpts{eval = TRUE}
%\newpage
\subsection{Application for all stations}
Until now the focus has been on one observation station only. To show that the presented results do not depend on the choice of this particular station we are now going to apply the methods on all 95 stations. At each of them, the first 24 years are used as learning data set and predictions are made for the last 4 years of available observations. For these out-of-sample predictions the CRPS skills score with respect to the EMOS model as reference is calculated for each method at each station. In Figure~\ref{boxplot_crps_all} the boxplots of the CRPS skills score values are plotted and show that the distributional forest leads to the highest improvement averaged over all stations. The green line represents the results for the station Axams.
%NAs in gamlss in 13 stations!!!\\
%for example in 13th station in complete{\_}stations\\
%\text{Error in glim.fit(f = mu.object, X = mu.X, y = y, w = w, fv = mu, os = mu.offset,  :}\\ 
%\text{NA's in the working vector or weights for parameter mu}
%\\
% sum(is.na(crps[,3]))
%<<eval = TRUE, echo = FALSE>>=
<<echo = FALSE>>=
#### prediction over all stations 24 - 4
if(file.exists("rain_pred_24to4.rda")){
  #load("~/svn/partykit/pkg/disttree/inst/draft/rain_pred_24to4.rda")
  load("rain_pred_24to4.rda")
} else {
  
  source("rain_pred_24to4.R")
  library("gamlss.cens")
  gen.cens("NO", type = "left")
  
  res <- rain_pred(seedconst = 7,
                   gamboost_cvr = TRUE,
                   frac = FALSE)
  
  save(res, file = "rain_pred_24to4.rda")
}
 
#ll_all <- res[[1]]$results["ll",]
#for(i in 2:(length(res)-1)) ll_all <- rbind(ll_all, res[[i]]$results["ll",])

#rmse_all <- res[[1]]$results["rmse",]
#for(i in 2:(length(res)-1)) rmse_all <- rbind(rmse_all, res[[i]]$results["rmse",])

crps_all <- res[[1]]$results["crps",]
for(i in 2:(length(res)-1))  crps_all <- rbind(crps_all, res[[i]]$results["crps",])

#colnames(ll_all) <- colnames(rmse_all) <- 
  colnames(crps_all) <- colnames(res[[1]]$results)

# skills score
s <- 1 - crps_all[, 2:4]/crps_all[,6]
colnames(s) <- c("Forest", "prespecified GAM", "boosted GAM")

## prepare data for map which shows where distforest performed better than gamlss or gamboostLSS based on the crps

crps <- crps_all[,c("distforest", "gamlss", "gamboostLSS", "EMOS log")]  

## best method
bst <- apply(crps, 1, which.min)

## distance of forest to best other method
dst <- crps[,1] - crps[cbind(1:nrow(crps), apply(crps[, -1], 1, which.min) + 1)]

## breaks/groups
brk <- c(-0.1, -0.05, -0.005, 0.005, 0.05, 0.1)
#brk <- c(-0.1, -0.05, -0.01, 0.01, 0.05, 0.1)
grp <- cut(dst, breaks = brk)

## HCL colors (relatively flashy, essentially CARTO Tropic)
clr <- colorspace::diverge_hcl(5, h = c(195, 325), c = 80, l = c(50, 90), power = 1.3)



library("raster") # dem (digital elevation model)
library("sp")     # gadm www.gadm.org/country

load("plot_map_rain/demo/tirol.gadm.rda")
load("plot_map_rain/demo/tirol.dem.rda")
load("plot_map_rain/demo/ehyd.statlist.rda")
  
# Create SpatialPointsDataFrame from station list
sp <- SpatialPointsDataFrame(subset(ehyd.statlist[res$complete_stations,],
                                    select=c(lon,lat)),
                             data = subset(ehyd.statlist[res$complete_stations,],
                                           select = -c(lon,lat)),
                             proj4string = crs(tirol.dem))

#library("colorspace")
@



\begin{figure}[h!]
\begin{center}
<<rain_all_crps_skills_score, fig=TRUE, echo=FALSE>>=
  matplot(t(s[,]), type = "l", lwd = 2, 
          col = gray(0.5, alpha = 0.2),
          lty = 1, axes = FALSE, 
          xlab = "", ylab = "CRPS skills score", xlim = c(0.5, 3.5))
lines(s[70,], col = "limegreen", type = "o", pch = 19, lwd = 2)  
# Station Axams is the 77th station which is the 70th in the list of complete stations
  boxplot(s, add = TRUE, col = "transparent")
  abline(h = 0, col = pal["EMOS"], lwd = 2)
    
@
\caption{\label{boxplot_crps_all}CRPS skills score over all stations with reference model EMOS. The models are learned on data from 24 years (1985--2008) and validated on 4 years (2009--2012).}
\end{center}
\end{figure}

Figure~\ref{map} shows the location of all stations in the data set. Each observation station is illustrated by a symbol with the type of symbol representing the method that performed best in terms of CRPS at this station. The color of the symbol indicates the difference of the CRPS of the forest model and the CRPS of the best performing among the other three models (prespecified GAM, boosted GAM and EMOS).

\begin{figure}[h!]
\begin{center}
<<map, fig=TRUE, echo=FALSE, height=6, width=9>>=
  
  layout(cbind(1, 2), width = c(9, 1))
  par(mar = c(5,4,4,0.1))
  raster::image(tirol.dem, col = rev(gray.colors(100)),
                main="Stations in Tyrol", ylab = "Latitude", xlab = "Longitude", 
                xlim = c(9.8,13.2), 
                ylim = c(46.6, 47.8))
  plot(tirol.gadm, add = TRUE)
  points(sp, pch = c(21, 24, 25, 22)[bst], bg = clr[grp], col = "black", las = 1, cex = 1.5)
  legend("topleft", pch=c(21,24,25,22), legend = c("Forest", "prespecified GAM", "boosted GAM", "EMOS"), cex = 1, bty = "n")
  
  par(mar = c(0.5,0.2,0.5,2.3))
  ## legend
  plot(0, 0, type = "n", axes = FALSE, xlab = "", ylab = "",
       xlim = c(0, 1), ylim = c(-0.2, 0.2), xaxs = "i", yaxs = "i")
  rect(0, brk[-6], 0.5, brk[-1], col = rev(clr))
  axis(4, at = brk, las = 1, mgp=c(0,-0.5,-1))
  
@
\caption{\label{map}Map of Tyrol with all observation stations. The type of symbol indicates which model performed best with respect to the CRPS. The color indicates the difference of the CRPS of the forest model and the CRPS of the best performing among the other three models (prespecified GAM, boosted GAM and EMOS).}
\end{center}
\end{figure}





\section{Discussion}




\section*{Computational Details}
The corresponding implementation to the novel methods presented in this article is available on R-Forge in an \textsf{R} package called \textbf{disttree} within the project \textbf{partykit} (\url{https://R-Forge.R-project.org/projects/partykit/}). The function \code{disttree} uses either the \code{mob} or \code{ctree} function as a framework to build a tree. Similarly, the function \code{distforest} is based on the \code{cforest} function which provides a framework to build a forest. All three functions, \code{mob}, \code{ctree} and \code{cforest}, are provided in the package \textbf{partykit} (\citealt{hothorn2015partykit}).





%\newpage
%\section{References}

%Hothorn T, Hornik K, Zeileis A (2006). 
%``Unbiased Recursive Partitioning: A Conditional Inference Framework.''
%\textit{Journal of Computational and Graphical Statistics}, 
%\textbf{15}(3), 651--674. 
%\doi{10.1198/106186006X133933}\\
%\\
%Zeileis A, Hothorn T, Hornik K (2008). 
%``Model-Based Recursive Partitioning.''
%\textit{Journal of Computational and Graphical Statistics}, 
%\textbf{17}(2), 492--514. 
%\doi{10.1198/106186008X319331}\\
%\\
%Hothorn T, Zeileis A (2015). 
%``partykit: A Modular Toolkit for Recursive Partytioning in R.''
%\textit{Journal of Machine Learning Research}, 
%\textbf{16}, 3905--3909. 
%\url{http://www.jmlr.org/papers/v16/hothorn15a.html}\\
%\\
%Stasinopoulos DM, Rigby RA (2007).
%``Generalized Additive Models for Location Scale and Shape (GAMLSS) in R.''
%\textit{Journal of Statistical Software}, 
%\textbf{23}(7), 1--46.
%\doi{10.18637/jss.v023.i07}\\
%\\
%Stasinopoulos DM, Rigby RA (2005).
%``Generalized Additive Models for Location Scale and Shape (with discussion)''
%\textit{Applied Statistics},
%\textbf{54}(3), 507--554.
%\doi{10.1111/j.1467-9876.2005.00510.x}\\
%\\
%Seibold H, Zeileis A, Hothorn T (2017).
%``Individual Treatment Effect Prediction for Amyotrophic Lateral Sclerosis Patients.''
%\textit{Statistical Methods in Medical Research}, 
%\textbf{12}(1), 45--63.
%\doi{10.1177/0962280217693034}\\
%\\
%Hothorn T, Zeileis A (2017).
%``Transformation Forests.''
%\emph{arXiv 1701.02110}, arXiv.org E-Print Archive.
%\url{http://arxiv.org/abs/1701.02110}\\
%\\
%Messner JW, Mayr GJ, Zeileis A (2016).
%``Heteroscedastic Censored and Truncated Regression with {crch}''
%\textit{The R Journal},
%\textbf{8}(1), 173--181.
%\url{https://journal.R-project.org/archive/2016-1/messner-mayr-zeileis.pdf}\\
%\\
%Zeileis A, Wiel MA, Hornik K, Hothorn T (2008).
%``Implementing a {Class} of {Permutation} {Tests}: {The} {Coin} {Package}''
%\textit{Journal of Statistical Software},
%\textbf{28}(8), 1--23.
%\doi{10.18637/jss.v028.i08}\\
%%publisher={American Statistical Association}
%\\
%Zeileis A, Hothorn T, Hornik K (2010).
%``Party with the {MOB}: {Model-Based} {Recursive} {Partitioning} in {R}''
%\textit{R package version 0.9-9999}.
%\url{https://cran.r-project.org/web/packages/party/vignettes/MOB.pdf}\\
%\\
%Breiman L (2001).
%``Random {Forests}''
%\textit{Machine Learning},
%\textbf{45}(1), 5--32
%%\doi{10.1023/A:1010933404324}
%%publisher={Springer}\\
%\\
%Loh WY (2011).
%``Classification and {Regression} {Trees}''
%\textit{Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
%\textbf{1}(1), 14--23.
%\doi{10.1002/widm.8}
%%publisher={Wiley Online Library}\\
%\\
%Hothorn T (Maintainer), Hornik K, Strobl C, Zeileis A (2015).
%``Package 'party' ''
%\textit{Package Reference Manual for Party Version 0.9--0.998},
%\textbf{16}, 37.\\
%%  \doi{}
%\\
%Yee TW (2010).
%``The {VGAM} {Package} for {Categorical} {Data} {Analysis}''
%\textit{Journal of Statistical Software},
%\textbf{32}(10), 1--34.
%\url{http://www.jstatsoft.org/v32/i10/}\\
%\\
%Umlauf N, Klein N, Zeileis A (2017).
%``{BAMLSS}: {B}ayesian Additive Models for Location, Scale
%  and Shape (and Beyond)''
%\textit{Working Papers in Economics and Statistics, Research
%  Platform Empirical and Experimental Economics, Universit\"at
%  Innsbruck},
%\textbf{2017-05},
%\url{http://EconPapers.RePEc.org/RePEc:inn:wpaper:2017-05}\\
%%type = {Working Paper},
%%number = {2017-05},
%%month = {February},
%\\
%Benjamin Hofner and Andreas Mayr and Nora Fenske and
%  Matthias Schmid (2017). 
%``{gamboostLSS}: Boosting Methods for {GAMLSS} Models''
%\textit{{R} package version 2.0-0}.
%\url{https://CRAN.R-project.org/package=gamboostLSS}\\
%\\
%Liaw A, Wiener M (2002).
%``Classification and Regression by randomForest''
%\textit{R News},
%\textbf{2}(3), 18--22.
%\url{http://CRAN.R-project.org/doc/Rnews/}
%\\
%Gneiting T, Raftery A E, Westveld III A H, Goldman T (2005).
%``Calibrated probabilistic forecasting using ensemble model output statistics and minimum CRPS estimation''
%\textit{Monthly Weather Review},
%\textbf{133}(5), 1098--1118.
%\doi{https://doi.org/10.1175/MWR2904.1}










%\newpage
\nocite{hothorn2006unbiased, zeileis2008model, hothorn2015partykit, Messner2016, loh2011classification, stasinopoulos2007generalized, Seibold2016, Seibold2017, breiman2001random, stasinopoulos2005, yee2010vgam, gamboostLSS2016}
% zeileis2008implementing, 
% zeileis2010party, 
% hothorn2015package,

%\newpage
%\newpage
\bibliography{ref.bib}

%\newpage
%\section*{Appendix}

%\begin{figure}[h!]
%\begin{center}
%%\begin{subfigure}{0.5\textwidth}
%<<rain_cross_axams_rmse, fig=TRUE, echo=FALSE, width=10, height=7>>=
%boxplot(rain_rmse[,c(2,3,4,6)], ylab = "RMSE",
%        names = c("Forest", "prespecified GAM", "boosted GAM", "EMOS")) 
%@
%%\end{subfigure}
%\caption{\label{boxplot_rmse_cross}RMSE values of the 10x10 cross validation at station Axams.}
%\end{center}
%\end{figure}
%%\hspace{-1cm}
%\begin{figure}[h!]
%\begin{center}
%%\begin{subfigure}{0.5\textwidth}
%<<rain_cross_axams_ll, fig=TRUE, echo=FALSE, width=10, height=7>>=
%boxplot(rain_ll[,c(2,3,4,6)], ylim = c(-2.8,-1.3), ylab = "Log-likelihood",
%        names = c("Forest", "prespecified GAM", "boosted GAM", "EMOS")) 
%#boxplot(rain_ll)
%@
%%\end{subfigure}
%%\caption{\label{boxplots_rain}Results 10x10 cross validation at station Axams\\
%%left: RMSE, right: log-likelihood}
%\caption{\label{boxplot_cross_ll}Log-likelihood values of the 10x10 cross validation at station Axams (extracted interval [-2.8,-1.3]).}
%\end{center}
%\end{figure}


%\begin{figure}[h!]
%\begin{center}
%<<rain_all_rmse, fig=TRUE, echo=FALSE, width=10, height=7.4>>=
%  matplot(t(rmse[,c(2,3,4,6)]), type = "l", lwd = 1, 
%          col = gray(0.5, alpha = 0.5), lty = 1, axes = FALSE, 
%          xlab = "", ylab = "RMSE", xlim = c(0.5, 4.5))
%  boxplot(rmse[,c(2,3,4,6)], add = TRUE,
%        names = c("Forest", "prespecified GAM", "boosted GAM", "EMOS")) 
%@
%\caption{\label{boxplot_rmse_all}RMSE for predictions over all stations.}
%\end{center}
%\end{figure}


%\begin{figure}[h!]
%\begin{center}
%<<rain_all_ll, fig=TRUE, echo=FALSE, width=10, height=7.4>>=
%  matplot(t(ll[,c(2,3,4,6)]), type = "l", lwd = 1, col = gray(0.5, alpha = 0.5), 
%          lty = 1, axes = FALSE, 
%          xlab = "", ylab = "Log-likelihood", xlim = c(0.5, 4.5), ylim = c(-4,-1))
%  boxplot(ll[,c(2,3,4,6)], add = TRUE,
%        names = c("Forest", "prespecified GAM", "boosted GAM", "EMOS")) 
%@
%\caption{\label{boxplot_ll_all}Log-likelihood values for predictions over all stations (extracted interval [-4,-1]).}
%\end{center}
%\end{figure}

\end{document}