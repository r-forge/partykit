\documentclass[nojss]{jss}

%\VignetteIndexEntry{disttree: Distributional Trees and Forests}
%\VignetteDepends{partykit, gamlss.dist}
%\VignetteKeywords{regression trees, random forests, distributional regression, recursive partitioning}
%\VignettePackage{disttree}

%% packages
\usepackage{amstext}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{thumbpdf}
\usepackage{rotating}
\usepackage{caption}
\captionsetup{format=hang}
\usepackage{subcaption}
\usepackage{Sweave}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{array, makecell}
\usepackage{tikz}
\usetikzlibrary{positioning,shapes,arrows,decorations.pathreplacing,calc,automata,mindmap}
%\usepackage[pagebackref=true]{hyperref}
%\usepackage{doi}

%% commands
\newcommand{\fixme}[1]{\emph{\marginpar{FIXME} (#1)}}
\renewcommand{\Prob}{\mathbb{P} }
\renewcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\Var}{\mathbb{V}}
\newcommand{\R}{\mathbb{R} }
\newcommand{\N}{\mathbb{N} }
\newcommand{\C}{\mathbb{C} }
\newcommand{\argmin}{\operatorname{argmin}\displaylimits}
\newcommand{\argmax}{\operatorname{argmax}\displaylimits}
\newcommand{\LS}{\mathcal{L}_n}
\newcommand{\TS}{\mathcal{T}_n}
\newcommand{\LSc}{\mathcal{L}_{\text{comb},n}}
\newcommand{\LSbc}{\mathcal{L}^*_{\text{comb},n}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\yn}{y_{\text{new}}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\sX}{\mathcal{X}}
\newcommand{\sY}{\mathcal{Y}}
\newcommand{\T}{\mathbf{T}}
\newcommand{\x}{\mathbf{x}}
\renewcommand{\a}{\mathbf{a}}
\newcommand{\xn}{\mathbf{x}_{\text{new}}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\ws}{\mathbf{w}_\cdot}
\renewcommand{\t}{\mathbf{t}}
\newcommand{\M}{\mathbf{M}}
\renewcommand{\vec}{\text{vec}}
\newcommand{\B}{\mathbf{B}}
\newcommand{\K}{\mathbf{K}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\cellx}{\pi_n[\x]}
\newcommand{\partn}{\pi_n(\mathcal{L}_n)}
\newcommand{\err}{\text{Err}}
\newcommand{\ea}{\widehat{\text{Err}}^{(a)}}
\newcommand{\ecv}{\widehat{\text{Err}}^{(cv1)}}
\newcommand{\ecvten}{\widehat{\text{Err}}^{(cv10)}}
\newcommand{\eone}{\widehat{\text{Err}}^{(1)}}
\newcommand{\eplus}{\widehat{\text{Err}}^{(.632+)}}
\newcommand{\eoob}{\widehat{\text{Err}}^{(oob)}}
\newcommand{\bft}{\mathbf{t}}

\hyphenation{Qua-dra-tic}

%% colors
\definecolor{HighlightOrange}{rgb}{0.9490196,0.5725490,0.0000000}
\definecolor{HighlightBlue}{rgb}{0.4784314,0.7490196,0.9803922}



\SweaveOpts{engine=R, eps=FALSE, keep.source=TRUE}
<<preliminaries, echo=FALSE, results=hide>>=
options(prompt = "R> ", continue = "+  ", useFancyQuotes = FALSE, width = 70)
library("ggplot2")
theme_set(theme_bw(base_size = 18))
library("disttree")
library("Formula")  # FIX ME: should be imported in disttree
library("gamlss")
library("randomForest")
library("lattice")
library("crch")
library("latex2exp")
library("parallel")
library("gamlss.cens")
gen.cens(NO, type = "left")
library("mgcv")
library("MASS")
library("countreg")

## HCL palette
pal <- hcl(c(10, 128, 260, 290, 50), 100, 50)
names(pal) <- c("forest", "tree", "gamlss", "gamboostLSS", "EMOS")

pallight <- hcl(c(10, 128, 260, 290, 70), 100, 50, alpha = 0.25)
names(pallight) <- c("forest", "tree", "gamlss", "gamboostLSS", "EMOS")

transpgrey <- rgb(0.190,0.190,0.190, alpha = 0.2)

## define distribution list:
# dist_list_normal
{
  
  dist_list_normal <- list()
  
  parnames <- c("mu", "sigma")
  etanames <- c("mu", "log(sigma)")
  
  
  ddist <-  function(y, eta, log = TRUE, weights = NULL, sum = FALSE) {     
    
    val <- -1/2 * (log(2*pi) + 2*eta[2] + exp(log((y-eta[1])^2) - 2*eta[2]))
    if(!log) val <- exp(val)
    
    # par <- c(eta[1], exp(eta[2]))
    # val <- dnorm(y, mean = par[1], sd = par[2], log = log)
    
    if(sum) {
      if(is.null(weights)) weights <- rep.int(1, length(y))
      val <- sum(weights * val, na.rm = TRUE)
    }
    return(val)
  }
  
  
  sdist <- function(y, eta, weights = NULL, sum = FALSE) {   
    
    score <- cbind(exp(-2*eta[2]) * (y-eta[1]), 
                   -1 + exp(-2*eta[2] + log((y-eta[1])^2)))
    
    # par <- c(eta[1], exp(eta[2])) 
    # score <- cbind(1/par[2]^2 * (y-par[1]), 
    #                (-1/par[2] + ((y - par[1])^2)/(par[2]^3)) * exp(eta[2]))
    
    score <- as.matrix(score)
    colnames(score) <- etanames
    if(sum) {
      if(is.null(weights)) weights <- rep.int(1, length(y))
      # if score == Inf replace score with 1.7e308 because Inf*0 would lead to NaN -> gradient is NaN
      score[score==Inf] = 1.7e308
      score <- colSums(weights * score, na.rm = TRUE)
    }
    return(score)
  }
  
  
  hdist <- function(y, eta, weights = NULL) {    
    ny <- length(y)
    if(is.null(weights)) weights <- rep.int(1, ny)
    
    d2ld.etamu2 <- sum(weights * rep.int(-exp(-2*eta[2]), ny))
    d2ld.etamu.d.etasigma <- sum(weights * (-2)*(y-eta[1]) * exp(-2*eta[2]), na.rm = TRUE)          # should be 0 for exact parameters (here: observed hess)
    d2ld.etasigma2 <- sum(weights * (-2)*exp(log((y-eta[1])^2) - 2*eta[2]), na.rm = TRUE)    
    
    # par <- c(eta[1], exp(eta[2]))                           
    # d2ld.etamu2 <- sum(weights * rep.int(-1/par[2]^2, ny))
    # d2ld.etamu.d.etasigma <- sum(weights * (-2)*(y-par[1])/par[2]^2), na.rm = TRUE)          # should be 0 for exact parameters (here: observed hess)
    # d2ld.etasigma2 <- sum(weights * (-2)*(y-par[1])^2/par[2]^2, na.rm = TRUE)         
    
    hess <- matrix(c(d2ld.etamu2, d2ld.etamu.d.etasigma, d2ld.etamu.d.etasigma, d2ld.etasigma2), nrow = 2)
    colnames(hess) <- rownames(hess) <-  etanames
    
    return(hess)
  }
  
  
  ## additional functions pdist, qdist, rdist
  pdist <- pnorm
  qdist <- qnorm
  rdist <- rnorm  
  
  
  link <- c("identity", "log")
  
  linkfun <- function(par) {
    eta <- c(par[1], log(par[2]))
    names(eta) <- etanames
    return(eta)
  }
  
  
  linkinv <- function(eta) {
    par <- c(eta[1], exp(eta[2]))
    names(par) <- parnames
    return(par)
  }
  
  
  linkinvdr <- function(eta) {
    dpardeta <- c(1, exp(eta[2]))
    names(dpardeta) <- parnames
    return(dpardeta)
  }
  
  
  startfun <- function(y, weights = NULL){
    if(is.null(weights)) {
      mu <- mean(y)
      sigma <- sqrt(1/length(y) * sum((y - mu)^2))
    } else {
      mu <- weighted.mean(y, weights)
      sigma <- sqrt(1/sum(weights) * sum(weights * (y - mu)^2))
    }
    starteta <- c(mu, log(sigma))
    names(starteta) <- etanames
    return(starteta)
  }
  
  mle <- TRUE
  
  dist_list_normal <- list(family.name = "Normal Distribution",
                           ddist = ddist, 
                           sdist = sdist, 
                           hdist = hdist,
                           pdist = pdist,
                           qdist = qdist,
                           rdist = rdist,
                           link = link, 
                           linkfun = linkfun, 
                           linkinv = linkinv, 
                           linkinvdr = linkinvdr,
                           startfun = startfun,
                           mle = mle,
                           gamlssobj = FALSE,
                           censored = FALSE
  )
}


# dist_list_cens_normal
{
  
  dist_list_cens_normal <- list()
  
  parnames <- c("mu", "sigma")
  etanames <- c("mu", "log(sigma)")
  
  ddist <-  function(y, eta, log = TRUE, weights = NULL, sum = FALSE, left = 0, right = Inf) {     
    par <- c(eta[1], exp(eta[2]))
    val <- crch::dcnorm(x = y, mean = par[1], sd = par[2], left = left, right = right, log = log)
    if(sum) {
      if(is.null(weights)) weights <- if(is.matrix(y)) rep.int(1, dim(y)[1]) else rep.int(1, length(y))
      val <- sum(weights * val, na.rm = TRUE)
    }
    return(val)
  }
  
  
  sdist <- function(y, eta, weights = NULL, sum = FALSE, left = 0, right = Inf) {   
    par <- c(eta[1], exp(eta[2]))
    # y[y==0] <- 1e-323
    
    score_m <- crch:::scnorm(x = y, mean = par[1], sd = par[2], which = "mu", left = left, right = right)
    score_s <- crch:::scnorm(x = y, mean = par[1], sd = par[2], which = "sigma", left = left, right = right) * exp(eta[2]) # inner derivation exp(eta[2])
    score <- cbind(score_m, score_s)
    score <- as.matrix(score)
    colnames(score) <- etanames
    if(sum) {
      if(is.null(weights)) weights <- rep.int(1, length(y)[1])
      # if score == Inf replace score with 1.7e308 because Inf*0 would lead to NaN (0 in weights)
      score[score==Inf] = 1.7e308
      score <- colSums(weights * score, na.rm = TRUE)
      #if(any(is.nan(score))) print(c(eta, "y", y))
    }
    return(score)
  }
  
  
  hdist <- function(y, eta, weights = NULL, left = 0, right = Inf) {    
    ny <- length(y)
    if(is.null(weights)) weights <- rep.int(1, ny)
    
    par <- c(eta[1], exp(eta[2]))                           
    # y[y==0] <- 1e-323
    
    d2mu <- crch:::hcnorm(x = y, mean = par[1], sd = par[2], which = "mu", left = left, right = right)
    d2sigma <- crch:::hcnorm(x = y, mean = par[1], sd = par[2], which = "sigma", left = left, right = right)
    dmudsigma <- crch:::hcnorm(x = y, mean = par[1], sd = par[2], which = "mu.sigma", left = left, right = right) # FIX: order?
    dsigmadmu <- crch:::hcnorm(x = y, mean = par[1], sd = par[2], which = "sigma.mu", left = left, right = right) # FIX: order?
    dsigma <- crch:::scnorm(x = y, mean = par[1], sd = par[2], which = "sigma", left = left, right = right)
    
    d2ld.etamu2 <- sum(weights * d2mu, na.rm = TRUE)
    d2ld.etamu.d.etasigma <- sum(weights * dmudsigma * par[2], na.rm = TRUE)
    d2ld.etasigma.d.etamu <- sum(weights * dsigmadmu * par[2], na.rm = TRUE)
    d2ld.etasigma2 <- sum(weights * (d2sigma * exp(2*eta[2]) + dsigma * par[2]), na.rm = TRUE)         
    
    hess <- matrix(c(d2ld.etamu2, d2ld.etamu.d.etasigma, d2ld.etasigma.d.etamu, d2ld.etasigma2), nrow = 2)
    colnames(hess) <- rownames(hess) <-  etanames
    
    return(hess)
  }
  
  
  ## additional functions pdist, qdist, rdist
  pdist <- function(q, eta, lower.tail = TRUE, log.p = FALSE) crch:::pcnorm(q, mean = eta[1], sd = eta[2], 
                                                                            lower.tail = lower.tail, log.p = log.p, 
                                                                            left = left, right = right)
  qdist <- function(p, eta, lower.tail = TRUE, log.p = FALSE) crch:::qcnorm(p, mean = eta[1], sd = eta[2], 
                                                                            lower.tail = lower.tail, log.p = log.p, 
                                                                            left = left, right = right)
  rdist <- function(n, eta) crch:::rcnorm(n, mean = eta[1], sd = eta[2], left = left, right = right)

  
  link <- c("identity", "log")
  
  linkfun <- function(par) {
    eta <- c(par[1], log(par[2]))
    names(eta) <- etanames
    return(eta)
  }
  
  
  linkinv <- function(eta) {
    par <- c(eta[1], exp(eta[2]))
    names(par) <- parnames
    return(par)
  }
  
  
  linkinvdr <- function(eta) {
    dpardeta <- c(1, exp(eta[2]))
    names(dpardeta) <- parnames
    return(dpardeta)
  }
  
  
  startfun <- function(y, weights = NULL){
    yc <- pmax(0,y)  # optional ?
    if(is.null(weights)) {
      mu <- mean(yc)
      sigma <- sqrt(1/length(yc) * sum((yc - mu)^2))
    } else {
      mu <- weighted.mean(yc, weights)
      sigma <- sqrt(1/sum(weights) * sum(weights * (yc - mu)^2))
    }
    starteta <- c(mu, log(sigma))
    names(starteta) <- etanames
    return(starteta)
  }
  
  mle <- FALSE
  
  dist_list_cens_normal <- list(family.name = "censored Normal Distribution",
                                ddist = ddist, 
                                sdist = sdist, 
                                hdist = hdist,
                                pdist = pdist,
                                qdist = qdist,
                                rdist = rdist,
                                link = link, 
                                linkfun = linkfun, 
                                linkinv = linkinv, 
                                linkinvdr = linkinvdr,
                                startfun = startfun,
                                mle = mle,
                                gamlssobj = FALSE,
                                censored = TRUE
  )
}





  
  
  
## function to estimate standard deviation of randomForest for a new observation
# (in randomForest the argument 'keep.inbag' must be set to TRUE)
rf_getsd <- function(rf, newdata = NULL, rfdata){
  
  if(is.null(newdata)) newdata <- rfdata
  
  rf_sd <- numeric(length = NROW(newdata))
  
  for(k in 1:NROW(newdata)){
    newobs <- newdata[k,]
    # get predictions for the new observations from all trees
    pred.newobs <- predict(rf, predict.all = TRUE, newdata = newobs)
    
    # vector where the standard deviations from all trees are stored
    sd_trees <- numeric(length = rf$ntree)
    
    # loop over all trees of the forest
    for(i in 1:rf$ntree){
      
      # get data used to build this tree
      obsid <- rep.int(c(1:NROW(rfdata)), as.vector(rf$inbag[,i]))
      obs_tree <- rfdata[obsid,]
      rownames(obs_tree) <- c(1:NROW(obs_tree))
      # get predictions for this data from this tree
      pred.obs_tree <- predict(rf, newdata = obs_tree, predict.all = TRUE)$individual[,i]
      
      # get prediction for the new observation from this tree
      pred.newobs_tree <- pred.newobs$individual[,i]
      
      # get part of the data that ends up in the same terminal node (has the same prediction)
      obs_node <- obs_tree[pred.obs_tree == pred.newobs_tree,]
      
      sd_trees[i] <- sd(obs_node$y)
    }
    
    # average of sd over all trees
    sd_newobs <- mean(sd_trees, na.rm = TRUE)
    rf_sd[k] <- sd_newobs
  }
  return(rf_sd)   
}



## function to estimate standard deviation of cforest for a new observation
cf_getsd <- function(cf, newdata = NULL){
  
  # get IDs of predicted nodes for the learning data cfdata (does not have to be handed over as cf was learned on cfdata)
  pred.node.learn <- predict(cf, type = "node")
  
  # get IDs of predicted nodes for the new observations
  if(is.null(newdata)) {
    pred.node.new <- pred.node.learn
    newdata <- cf$data
  } else {
    pred.node.new <- predict(cf, newdata = newdata, type = "node")
  }
  
  sdnew <- numeric(length = NROW(newdata))
  
  for(i in 1:NROW(newdata)){
    for(t in 1:cf$info$call$ntree){
      nodedata <- cf$data[(pred.node.learn[[t]] == pred.node.new[[t]][i]),]
      sdnew[i] <- sd(nodedata[,paste(cf$terms[[2]])])
    }
  }
  
  return(sdnew)   
}



## simulation plot functions
# plot RMSE
plot_rmse <- function(simres, type = c("exp", "par"), ylim = NULL, legend = TRUE){
  
  if(type == "exp"){
    rmse <- NULL
    colnames <- NULL
    col <- NULL
    legendnames <- NULL
    if("av.rmse.exp.true.dt" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.dt"], simres$resmat[,"av.rmse.exp.obs.dt"])
      colnames <- c(colnames, "dt.true", "dt.obs")
      col <- c(col, pal["tree"])
      legendnames <- c(legendnames, "disttree")
    }
    if("av.rmse.exp.true.df" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.df"], simres$resmat[,"av.rmse.exp.obs.df"])
      colnames <- c(colnames, "df.true", "df.obs")
      col <- c(col, pal["forest"])
      legendnames <- c(legendnames, "distforest")
    }
    if("av.rmse.exp.true.g" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.g"], simres$resmat[,"av.rmse.exp.obs.g"])
      colnames <- c(colnames, "g.true", "g.obs")
      col <- c(col, pal["gamlss"])
      legendnames <- c(legendnames, "gamlss")
    }
    if("av.rmse.exp.true.b" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.b"], simres$resmat[,"av.rmse.exp.obs.b"])
      colnames <- c(colnames, "b.true", "b.obs")
      col <- c(col, pal["bamlss"])
      legendnames <- c(legendnames, "bamlss")
    }
    if("av.rmse.exp.true.gb" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.gb"], simres$resmat[,"av.rmse.exp.obs.gb"])
      colnames <- c(colnames, "gb.true", "gb.obs")
      col <- c(col, pal["gamboostLSS"])
      legendnames <- c(legendnames, "gamboostLSS")
    }
    if("av.rmse.exp.true.rf" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.rf"], simres$resmat[,"av.rmse.exp.obs.rf"])
      colnames <- c(colnames, "rf.true", "rf.obs")
      col <- c(col, pal["randomForest"])
      legendnames <- c(legendnames, "randomForest")
    }
    if("av.rmse.exp.true.cf" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.exp.true.cf"], simres$resmat[,"av.rmse.exp.obs.cf"])
      colnames <- c(colnames, "cf.true","cf.obs")
      col <- c(col, pal["cforest"])
      legendnames <- c(legendnames, "cforest")
    }

    colnames(rmse) <- colnames
    if(is.null(ylim)) ylim <- c(min(na.omit(rmse)), max(na.omit(rmse)))
    
    plot(x = simres$x.axis, y = rmse[,1], type = "l", col = col[1], ylim = ylim,
         xlab = "kappa", ylab = "RMSE")
    lines(x = simres$x.axis, y = rmse[,2], type = "l", lty = 2, col = col[1])
    
    for(i in 1:(length(col)-1)){
      lines(x = simres$x.axis, y = rmse[,(2*i+1)], type = "l", col = col[i+1])
      lines(x = simres$x.axis, y = rmse[,(2*i+2)], type = "l", lty = 2, col = col[i+1])
    }
    if(legend) legend('topleft', legendnames, 
           col = col, lty = 1, cex = 0.7)
  } 
  
  if(type == "par"){
    
    rmse <- NULL
    colnames <- NULL
    col <- NULL
    legendnames <- NULL
    if("av.rmse.exp.true.dt" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.dt"], simres$resmat[,"av.rmse.sigma.dt"])
      colnames <- c(colnames, "dt.true", "dt.obs")
      col <- c(col, pal["tree"])
      legendnames <- c(legendnames, "disttree")
    }
    if("av.rmse.exp.true.df" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.df"], simres$resmat[,"av.rmse.sigma.df"])
      colnames <- c(colnames, "df.true", "df.obs")
      col <- c(col, pal["forest"])
      legendnames <- c(legendnames, "distforest")
    }
    if("av.rmse.exp.true.g" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.g"], simres$resmat[,"av.rmse.sigma.g"])
      colnames <- c(colnames, "g.true", "g.obs")
      col <- c(col, pal["gamlss"])
      legendnames <- c(legendnames, "gamlss")
    }
    if("av.rmse.exp.true.b" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.b"], simres$resmat[,"av.rmse.sigma.b"])
      colnames <- c(colnames, "b.true", "b.obs")
      col <- c(col, pal["bamlss"])
      legendnames <- c(legendnames, "bamlss")
    }
    if("av.rmse.exp.true.gb" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.gb"], simres$resmat[,"av.rmse.sigma.gb"])
      colnames <- c(colnames, "gb.true", "gb.obs")
      col <- c(col, pal["gamboostLSS"])
      legendnames <- c(legendnames, "gamboostLSS")
    }
    if("av.rmse.exp.true.rf" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.rf"], simres$resmat[,"av.rmse.sigma.rf"])
      colnames <- c(colnames, "rf.true", "rf.obs")
      col <- c(col, pal["randomForest"])
      legendnames <- c(legendnames, "randomForest")
    }
    if("av.rmse.exp.true.cf" %in% colnames(simres$resmat)) {
      rmse <- cbind(rmse, simres$resmat[,"av.rmse.mu.cf"], simres$resmat[,"av.rmse.sigma.cf"])
      colnames <- c(colnames, "cf.true","cf.obs")
      col <- c(col, pal["cforest"])
      legendnames <- c(legendnames, "cforest")
    }

    colnames(rmse) <- colnames
    if(is.null(ylim)) ylim <- c(min(na.omit(rmse)), max(na.omit(rmse)))
    
    plot(x = simres$x.axis, y = rmse[,1], type = "l", col = col[1], ylim = ylim,
         xlab = "kappa", ylab = "RMSE")
    lines(x = simres$x.axis, y = rmse[,2], type = "l", lty = 2, col = col[1])
    
    for(i in 1:(length(col)-1)){
      lines(x = simres$x.axis, y = rmse[,(2*i+1)], type = "l", col = col[i+1])
      lines(x = simres$x.axis, y = rmse[,(2*i+2)], type = "l", lty = 2, col = col[i+1])
    }
    if(legend) legend('topleft', legendnames, 
           col = col, lty = 1, cex = 0.7)
  }
}


# plot loglikelihood
plot_ll <- function(simres, ylim = NULL, legend = TRUE){
  
    ll <- NULL
    colnames <- NULL
    col <- NULL
    legendnames <- NULL
    if("av.rmse.exp.true.dt" %in% colnames(simres$resmat)) {
      ll <- cbind(ll, simres$resmat[,"av.loglik.dt"])
      colnames <- c(colnames, "dt.ll")
      col <- c(col, pal["tree"])
      legendnames <- c(legendnames, "disttree")
    }
    if("av.rmse.exp.true.df" %in% colnames(simres$resmat)) {
      ll <- cbind(ll, simres$resmat[,"av.loglik.df"])
      colnames <- c(colnames, "df.ll")
      col <- c(col, pal["forest"])
      legendnames <- c(legendnames, "distforest")
    }
    if("av.rmse.exp.true.g" %in% colnames(simres$resmat)) {
      ll <- cbind(ll, simres$resmat[,"av.loglik.g"])
      colnames <- c(colnames, "g.true")
      col <- c(col, pal["gamlss"])
      legendnames <- c(legendnames, "gamlss")
    }
    if("av.rmse.exp.true.b" %in% colnames(simres$resmat)) {
      ll <- cbind(ll, simres$resmat[,"av.loglik.b"])
      colnames <- c(colnames, "b.true")
      col <- c(col, pal["bamlss"])
      legendnames <- c(legendnames, "bamlss")
    }
    if("av.rmse.exp.true.gb" %in% colnames(simres$resmat)) {
      ll <- cbind(ll, simres$resmat[,"av.loglik.gb"])
      colnames <- c(colnames, "gb.true")
      col <- c(col, pal["gamboostLSS"])
      legendnames <- c(legendnames, "gamboostLSS")
    }
    if("av.rmse.exp.true.rf" %in% colnames(simres$resmat)) {
      ll <- cbind(ll, simres$resmat[,"av.loglik.rf"])
      colnames <- c(colnames, "rf.true")
      col <- c(col, pal["randomForest"])
      legendnames <- c(legendnames, "randomForest")
    }
    if("av.rmse.exp.true.cf" %in% colnames(simres$resmat)) {
      ll <- cbind(ll, simres$resmat[,"av.loglik.cf"])
      colnames <- c(colnames, "cf.true")
      col <- c(col, pal["cforest"])
      legendnames <- c(legendnames, "cforest")
    }

    colnames(ll) <- colnames
    if(is.null(ylim)) ylim <- c(min(na.omit(ll)), max(na.omit(ll)))
    
    plot(x = simres$x.axis, y = ll[,1], type = "l", col = col[1], ylim = ylim,
         xlab = "kappa", ylab = "log-likelihood")
    
    for(i in 2:length(col)){
      lines(x = simres$x.axis, y = ll[,i], type = "l", col = col[i])
    }
    if(legend) legend('topleft', legendnames, 
           col = col, lty = 1, cex = 0.7)
}


# plot crpse
plot_crps <- function(simres, ylim = NULL, legend = TRUE){
  
  crps <- NULL
  colnames <- NULL
  col <- NULL
  legendnames <- NULL
  if("av.rmse.exp.true.dt" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.dt"])
    colnames <- c(colnames, "dt.crps")
    col <- c(col, pal["tree"])
    legendnames <- c(legendnames, "disttree")
  }
  if("av.rmse.exp.true.df" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.df"])
    colnames <- c(colnames, "df.crps")
    col <- c(col, pal["forest"])
    legendnames <- c(legendnames, "distforest")
  }
  if("av.rmse.exp.true.g" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.g"])
    colnames <- c(colnames, "g.true")
    col <- c(col, pal["gamlss"])
    legendnames <- c(legendnames, "gamlss")
  }
  if("av.rmse.exp.true.b" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.b"])
    colnames <- c(colnames, "b.true")
    col <- c(col, pal["bamlss"])
    legendnames <- c(legendnames, "bamlss")
  }
  if("av.rmse.exp.true.gb" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.gb"])
    colnames <- c(colnames, "gb.true")
    col <- c(col, pal["gamboostLSS"])
    legendnames <- c(legendnames, "gamboostLSS")
  }
  if("av.rmse.exp.true.rf" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.rf"])
    colnames <- c(colnames, "rf.true")
    col <- c(col, pal["randomForest"])
    legendnames <- c(legendnames, "randomForest")
  }
  if("av.rmse.exp.true.cf" %in% colnames(simres$resmat)) {
    crps <- cbind(crps, simres$resmat[,"av.crps.cf"])
    colnames <- c(colnames, "cf.true")
    col <- c(col, pal["cforest"])
    legendnames <- c(legendnames, "cforest")
  }
  
  colnames(crps) <- colnames
  if(is.null(ylim)) ylim <- c(min(na.omit(crps)), max(na.omit(crps)))
  
  plot(x = simres$x.axis, y = crps[,1], type = "l", col = col[1], ylim = ylim,
       xlab = "kappa", ylab = "CRPS")
  
  for(i in 2:length(col)){
    lines(x = simres$x.axis, y = crps[,i], type = "l", col = col[i])
  }
  if(legend) legend('topleft', legendnames, 
                    col = col, lty = 1, cex = 0.7)
}
@



\title{Distributional Trees and Forests}
\Plaintitle{disttree: Distributional Trees and Forests} 

\author{Lisa Schlosser\\Universit\"at Innsbruck
   \And Torsten Hothorn\\Universit\"at Z\"urich
   \And Reto Stauffer\\Universit\"at Innsbruck
   \And Achim Zeileis\\Universit\"at Innsbruck}
\Plainauthor{Lisa Schlosser, Torsten Hothorn, Reto Stauffer, Achim Zeileis}

\Abstract{
In the field of probabilistic forecasting linear models such as ensemble model output statistics (EMOS, \cite{gneiting2005calibrated}) provide common and widely used tools. This is due to their simplicity and their interpretability. However, the restriction to modeling only linear effects requires further extensions which allow for more flexibility. Generalized additive models for location, scale, and shape (GAMLSS, \cite{stasinopoulos2005}) can also capture non-linear effects as each distribution parameter can be modeled separately by its own generalized additive model (GAM). A different approach that would make it possible to also model non-additive effects and interactions is to apply tree structured models such as regression trees (\cite{loh2011classification}) or random forests (\cite{breiman2001random}). However, up to now, this class of models has not yet been integrated in probabilistic forecasting as no general distributional framework is available for these methods. Therefore, we are going to introduce distributional trees and forests which embed the idea of flexible parametric modeling into the framework of regression trees and random forests. Applying these novel approaches on precipitation data in a mountainous region with a large number of numerical weather prediction quantities illustrates their strengths as their performance is compared to GAMLSS models that are specified based either on subject-matter knowledge/experience from the meteorology or based on a computationally more demanding boosting approach.

%In regression analysis one is interested in the relationship between a dependent variable and one or more explanatory variables. Various methods to fit statistical models to the data set have been developed, starting from linear models considering only the mean of the response variable and ranging to probabilistic models where all parameters of a distribution are fit to the given data set.\\
%If there is a strong variation within the data it might be advantageous to split the data first into more homogeneous subgroups based on given covariates and then fit a local model in each subgroup rather than fitting one global model to the whole data set. This can be done by applying regression trees and forests.\\
%Both of these two concepts, parametric modeling and algorithmic trees, have been investigated and developed further, however, mostly separated from each other. Therefore, our goal is to embed the progress made in the field of probabilistic modeling in the idea of algorithmic tree and forest models. In particular, more flexible models such as GAMLSS (\cite{stasinopoulos2005}) should be fitted in the nodes of a tree in order to capture location, scale and shape as well as censoring, tail behavior etc. while non-additive effects of the explanatory variables can be detected automatically by the splitting algorithm used to build the tree. In distributional forests an ensemble of distributional trees is built and used to calculate weights which are then included in the fitting process of a distributional model. These forest models can detect smooth effects as well as abrupt changes and interactions in a reasonable way without needing any kind of variable selection or information about the expected effects advance and therefore offers a good compromise particularly in complex settings. 
}
\Keywords{parametric models, regression trees, random forests, recursive partitioning, probabilistic forecasting, distributional regression}

\Address{
  Lisa Schlosser \\
  Department of Statistics \\
  Faculty of Economics and Statistics \\
  Universit\"at Innsbruck \\
  Universit\"atsstr.~15 \\
  6020 Innsbruck, Austria \\
  E-mail: \email{Lisa.Schlosser@uibk.ac.at} \\
  URL: \url{https://www.uibk.ac.at/statistics/personal/schlosser-lisa/lisa-schlosser.html}\\
  
  Torsten Hothorn\\
  Institut f\"ur Sozial- und Pr\"aventivmedizin, Abteilung Biostatistik \\
  Universit\"at Z\"urich \\
  Hirschengraben 84\\
  CH-8001 Z\"urich, Switzerland \\
  E-mail: \email{Torsten.Hothorn@R-project.org}\\
  URL: \url{http://user.math.uzh.ch/hothorn/}\\
  
  Reto Stauffer \\
  Department of Statistics \\
  Faculty of Economics and Statistics \\
  Universit\"at Innsbruck \\
  Universit\"atsstr.~15 \\
  6020 Innsbruck, Austria \\
  E-mail: \email{Reto.Stauffer@uibk.ac.at} \\
  URL: \url{https://www.uibk.ac.at/statistics/personal/stauffer/reto-stauffer.html}\\
  
  Achim Zeileis \\
  Department of Statistics \\
  Faculty of Economics and Statistics \\
  Universit\"at Innsbruck \\
  Universit\"atsstr.~15 \\
  6020 Innsbruck, Austria \\
  E-mail: \email{Achim.Zeileis@R-project.org} \\
  URL: \url{http://eeecon.uibk.ac.at/~zeileis/}

}

\begin{document}
\SweaveOpts{concordance=TRUE}


\SweaveOpts{eps=FALSE, keep.source=TRUE, eval = TRUE}

\SweaveOpts{eval = TRUE}
\section{Introduction}
A typical application in probabilistic forecasting in meteorology is the analysis of precipitation data. The two main interests lie in estimating the probability of rain as well as predicting the expected amount of precipitation. In this research area various methods have been developed and tested in the last decades. (Multiple) Linear models such as ensemble model output statistics (EMOS) were introduced more than a decade ago (\cite{gneiting2005calibrated}) and are still common and widely used. Despite their simplicity and interpretability, the limitation to modeling only linear effects has risen the interest in applying more flexible models. This feature is provided by generalized additive models (GAM) which can also model non-linear effects. However, often only the expectation of the underlying distribution is estimated. To obtain probabilistic predictions, generalized additive models for location, scale, and shape (GAMLSS, \cite{stasinopoulos2005}) offer a framework in which each distribution parameter can be modeled separately by its own GAM. In Figure \ref{Devel_parmod} the development of parametric models starting from simple linear models (LM) and ranging to GAMLSS is illustrated.\\
These flexible parametric methods provide well fitting models when applied on precipitation data in a rather smooth/homogeneous setting, e.g. areas where no abrupt changes in meteorological/climatological events appear. However, for example in complex terrain/mountainous regions various factors can change rapidly and their effects are difficult to be modeled/represented by a smooth parametric model. In this situation a different approach which can deal with these abrupt changes would be to apply tree structured models such as regression trees (\cite{loh2011classification}) or random forests (\cite{breiman2001random}). Moreover, these models are particularly advantageous in case of many covariates with unknown effects or interactions as the variable selection is done automatically within the model fitting process. Therefore, no additional effort or knowledge for the specification of the model in advance is required. However, even though their main advantage of being able to model non-additive effects and interactions without any necessary prespecification can play to their strength, regression trees are not an adequate choice if (additional) smooth effects appear as well. In this situation a random forest provides a good compromise as it combines a set of trees such that the steps imposed by the trees are smoothed out in case of smooth effects while abrupt changes are still captured by the tree structure of the model.\\
For these reasons, regression trees and particularly random forests provide valuable features which can contribute to improving models for probabilistic forecasting. But, up to now, no general distributional framework is available for these methods. Therefore, we are going to introduce distributional trees and forests which embed the idea of flexible parametric modeling into the framework of regression trees and random forests. The developments in the field of tree-structured models together with the novel approaches of distributional trees and forests are illustrated in Figure \ref{Devel_treeforest}.\\







%In the field of regression analysis many statistical models have already been developed to model the relationship between a response variable and one or more explanatory variables.\\
%Among the first were linear models (LM) which predict the (conditional) expected value of the response variable as a linear combination of the explanatory variables. This idea was extended to generalized linear models (GLM) which broadened the range of possible distributions of the response variable to the exponential family. (In R these models can be built using the functions \code{lm()} and \code{glm()} provided by the package \pkg{stats}).\\
%Then, in order to also allow for non-linear effects, generalized additive models (GAM) were introduced where smooth functions of the explanatory variables are summed up instead of the linear combination. (In the fitting function \code{gam} types of smoothing terms can be selected.)\\
%However, with these methods only the mean of the distribution of the response variable can be modeled. Generalized additive models for location, scale and shape
%(GAMLSS) (\cite{stasinopoulos2005}) made it possible to model each of the parameters of a distribution separately by its own GAM. In that way a whole distribution can be specified. The R-packages \pkg{gamlss} and \pkg{gamlss.dist} offer the corresponding software with a wide variety of distributions.\\
%This development in the field of parametric modeling is illustrated in Figure \ref{Devel_parmod}.\\


\begin{figure}[t!]
\minipage{0.29\textwidth}
\begin{center}
<<motivation_GLM, echo=FALSE, results=hide>>=
nobs <- 200
## GLM
set.seed(7)
x <- c(1:nobs)/nobs
ytrue <- 1+x*1.5
y <- ytrue + rnorm(nobs,0,0.3)
@

<<plot_motivation_GLM, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(y=y , x=x, xaxt="n", yaxt="n", ann=FALSE, col = "slategray", pch = 19)
box(lwd=5)
lines(x = x, y = ytrue, col = pal["forest"], lwd = 7, main = "")
@
\end{center}
\endminipage
{\LARGE$\rightarrow$}
\minipage{0.29\textwidth}
\begin{center}
<<motivation_GAM, echo=FALSE, results=hide>>=
## GAM
set.seed(7)
x <- c(1:nobs)/nobs
x <- 2*(x-0.5)
ytrue <- x^3 
y <- ytrue + rnorm(nobs,0,0.3)
@

<<plot_motivation_GAM, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(y=y , x=x, xaxt="n", yaxt="n", ann=FALSE, col = "slategray", pch = 19)
box(lwd=5)
lines(x = x, y = ytrue, col = pal["forest"], lwd = 7, main = "")
@
\end{center}
\endminipage
{{\LARGE$\rightarrow$}}
\minipage{0.29\textwidth}
\begin{center}
<<motivation_GAMLSS, echo=FALSE, results=hide>>=
## GAMLSS
set.seed(7)
x <- c(1:nobs)/nobs
x <- 2*(x-0.5)
ytrue <- x^3
var <- exp(-(2*x)^2)/2
y <- ytrue + rnorm(nobs, 0, 0.1 + var)
@

<<plot_motivation_GAMLSS, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(x, y, xaxt = "n", yaxt = "n", ann = FALSE, type = "n")
polygon(c(x, rev(x)), c(ytrue + 0.1 + var, rev(ytrue - 0.1 - var)),
  col = pallight["forest"], border = "transparent")
lines(x, ytrue, col = pal["forest"], lwd=7)
points(x, y, col = "slategray", pch = 19)
box(lwd = 5)
@
\end{center}
\endminipage
\vspace{0.0cm}
\minipage{0.25\textwidth}
\begin{center}
\hspace{0.4cm}
LM, GLM 
\end{center}
\endminipage
\hspace{1.7cm}
\minipage{0.25\textwidth}
\begin{center}
GAM
\end{center}
\endminipage
\hspace{1.7cm}
\minipage{0.25\textwidth}
\begin{center}
GAMLSS
\end{center}
\endminipage
\caption{\label{Devel_parmod}Development in the field of parametric modeling.}
\end{figure}




\begin{figure}[t!]
\minipage{0.29\textwidth}
\begin{center}
<<motivation_regtree, echo=FALSE, results=hide>>=
## Reg. Tree
set.seed(7)
kappa <- 12
x <- c(1:nobs)/nobs
ytrue <- ytree <- yforest <- numeric(length = length(x))
for(i in 1:nobs) ytrue[i] <- if(x[i]<1/3) 0.5 else 1+(1-plogis(kappa*(2*(x[i]-0.2)-1)))
y <- ytrue + rnorm(nobs,0,0.3)
for(i in 1:nobs) ytree[i] <- if(x[i]<1/3) 0.5 else {if(x[i]<2/3) 2 else 1}
@

<<plot_motivation_regtree, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(x = x, y = y, xaxt="n", yaxt="n", ann=FALSE, col = "slategray", pch = 19)
box(lwd=5)
#lines(x = x, y = ytrue, col = "grey", lwd=5, main = "")
lines(x = x, y = ytree, col = pal["forest"], lwd=7)
@
\end{center}
\endminipage
{\LARGE$\rightarrow$}
\minipage{0.29\textwidth}
\begin{center}
<<motivation_randforest, echo=FALSE, results=hide>>=
## Random Forest
for(i in 1:nobs) yforest[i] <- if(x[i]<0.27) 0.5 else { if(x[i]<0.39) 0.5 + 1.5*(plogis((x[i]-0.33)/6*700)) else 1+(1-plogis(kappa*(2*(x[i]-0.2)-1)))}
@

<<plot_motivation_randforest, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(x = x, y = y, xaxt="n", yaxt="n", ann=FALSE, col = "slategray", pch = 19)
box(lwd=5)
#lines(x = x, y = ytrue, col = "grey", lwd=5, main = "")
lines(x = x, y = yforest, col = pal["forest"], lwd=7, main = "")
@
\end{center}
\endminipage
{\LARGE$\rightarrow$}
\minipage{0.29\textwidth}
\begin{center}
<<motivation_distforest, echo=FALSE, results=hide>>=
## distforest
set.seed(4723)
for(i in 1:nobs) yforest[i] <- if(x[i]<0.27) 0.5 else { if(x[i]<0.39) 0.5 + 1.5*(plogis((x[i]-0.33)/6*700)) else 1+(1-plogis(kappa*(2*(x[i]-0.2)-1)))}
xt <- 2*(x-0.5)
var <- exp(-(2*xt)^4)/2
y <- ytrue + rnorm(nobs, 0, 0.1 + var)
x <- c(1:nobs)/nobs
@

<<plot_motivation_distforest, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(x, y, xaxt = "n", yaxt = "n", ann = FALSE, type = "n")
polygon(c(x, rev(x)), c(ytrue + 0.1 + var, rev(ytrue - 0.1 - var)),
  col = pallight["forest"], border = "transparent")
lines(x = x, y = yforest, col = pal["forest"], lwd=7, main = "")
points(x, y, col = "slategray", pch = 19)
box(lwd=5)
@
\end{center}
\endminipage


\minipage{0.29\textwidth}
\begin{center}
\vspace{0.0cm}
Regression Tree 
\end{center}
\endminipage
\hspace{0.65cm}
\minipage{0.29\textwidth}
\begin{center}
\vspace{0.0cm}
Random Forest 
\end{center}
\endminipage
\hspace{0.65cm}
\minipage{0.28\textwidth}
\begin{center}
\vspace{0.0cm}
Distributional Forest 
\end{center}
\endminipage\\
\minipage{0.29\textwidth}
\vspace{0.2cm}
\center {\LARGE$\downarrow$}\\
\vspace{-0.4cm}
%\hspace{0.0cm}
\begin{center}
<<motivation_disttree, echo=FALSE, results=hide>>=
## disttree
set.seed(74)
x <- c(1:nobs)/nobs
var <- ytree/3
y <- ytrue + rnorm(nobs,0,var)
@

<<plot_motivation_disttree, fig=TRUE, echo=FALSE>>=
par(mar=c(2,0,2,0))
plot(x = x, y = y, xaxt="n", yaxt="n", ann=FALSE, col = "slategray", pch = 19)
lines(x = x, y = ytree, col = pal["forest"], lwd=7, main = "")
polygon(c(x, rev(x)), c(ytree + var, rev(ytree - var)),
  col = pallight["forest"], border = "transparent")
box(lwd=5)
@
\vspace{-0.2cm}\\
Distributional Tree
\vspace{0.2cm}
\end{center}
\endminipage
\caption{\label{Devel_treeforest}Development in the field of algorithmic trees and forests}
\end{figure}


%All these parametric models work very well as long as only additive effects appear in the considered data. A different type of model that can capture non-additive effects as well are regression trees. Here the basic idea is to split the data set into more homogeneous subgroups based on information provided by the explanatory variables and to fit a model/response to each terminal node. One problem that might appear when applying tree algorithms is that they impose steps and abrupt changes on the model, even in situations where effects are rather smooth, as it can be seen in the top left plot of Figure \ref{Devel_treeforest}. One way how to tackle this problem is to not just consider one tree but an ensemble of trees. This is the idea of random forests. A set of trees is built, each on a slightly different data set which can either be a bootstrap or a subsample of the original data set. Then the results of all trees are combined to get a final model which unlike a tree model can also deal with smooth effects in a reasonable way.\\ 
%In distributional trees and forests we now want to combine all these ideas from different fields of statistical modeling in order to benefit from all their advantages.\\
%\\
%In the past the combination of parametric data models and algorithmic tree models has gained more and more interest since it allows for a more flexible and clear structure. \\
%Algorithms that only fit constants to each final node and therefore only model the mean of the response variable (such as CART, Breiman 1984) might lead to very large trees which complicates analyzing and interpreting. Algorithms such as GUIDE (Loh 2002), CRUISE (Kim and Loh 2001) and LOTUS (Chan and Loh 2004) were among the first to provide trees with parametric models in each terminal node. The MOB algorithm (Zeileis, Hothorn and Hornik 2008), an algorithm for model-based recursive partitioning, extended this idea by fitting parametric models to each node and using the gained information of the fitted model to create further splits in inner nodes. Using this algorithm as a framework to build trees LMs and GLMs can be fitted in the nodes of the trees. However, up to now trees which only model the mean of the response variable in its nodes are still more common and widely used.\\
%Random forests (\cite{breiman2001random}) applying the CART algorithm to build the trees and cforest applying the ctree algorithm (\cite{hothorn2015partykit}, \cite{hothorn2006unbiased}) have used this idea of combining an ensemble of trees to get smoother effects of the covariates after combining or averaging over the parameters of the trees.\\
%\\
%But as mentioned before, until now the above listed development in the field of probabilistic modeling has not yet been fully integrated in the idea of algorithmic tree and forest models. Probabilistic models are rarely used in combination with tree algorithms and more flexible models such as GAMLSSs have not yet been applied as models in the nodes of a tree.\\
%\\
%Therefore, our main goal is to embed the progress made in the field of probabilistic modeling in the idea of regression trees and forests. In particular, more flexible models such as GAMLSSs should be fitted in the nodes of a tree in order to capture location, scale and shape as well as censoring, tail behavior etc. while non-additive effects or interactions of the explanatory variables can be detected automatically by the splitting algorithm used to build the tree. In that way, no variable selection or predefining of variable interactions is necessary and a whole distribution is specified in each node which leads to a wide range of inference methods that can be applied.\\
%These (future) steps are illustrated in Figure \ref{Devel_treeforest}, embedded in the development made in the field of tree and forest algorithms.\\


After explaining the novel methodology in Section \ref{Methodology} of this paper and presenting results from a simulation study in Section \ref{Simulation} the focus will then be put on an application in the field of probabilistic forecasting in Section \ref{ProbabilisticForecasting}. The new method is applied on precipitation data in a mountainous region with a large number of numerical weather prediction quantities and the results are compared to those of other methods commonly used in this research area. In a first setting only one observation station is taken into consideration and based on the data/information from 25 years probabilistic predictions are made for the 3 successive years. A brief outlook on the results of this setting is given by Figure \ref{axams_onestation}. For the 24th of July of 2010, 2011 and 2012 the density functions of the estimated distributions for this day of all three prediction years are plotted together with the corresponding observations. The estimated distribution parameters for the 24th of July of 2010 are comparably high with location $3.146$ and scale $1.902$ which leads to a well fitting predicted expectation of $3.185$ close to the observed value of $3.001$. Also for 2011 and 2012 the estimated expected values of 1.085 and 0.483 provide reasonable predictions as the observed values are 1.121 and 0 respectively.\\
Next to the expected values the predictions provided by these distributional models offer a wide range of information as they specify the whole distribution and therefore enable further methods of inference and analysis. 



<<echo=FALSE, results=hide>>=
#### Axams prediction 25 - 3
if(file.exists("rain_Axams_pred.rda")){
  #load("~/svn/partykit/pkg/disttree/inst/draft/rain_Axams_pred.rda")
  load("rain_Axams_pred.rda")
} else {
  #source("~/svn/partykit/pkg/disttree/inst/draft/rain_axams_pred.R")
  source("rain_axams_pred.R")
  library("gamlss.cens")
  gen.cens("NO", type = "left")
  res <- rain_axams_pred(seedconst = 7, ntree = 100,
                         forest_mtry = 27,   # if frac == TRUE: nvar/3 = 30
                         gamboost_cvr = TRUE,
                         frac = FALSE)
  
  save(res, file = "rain_Axams_pred.rda")
}

# prepare data for plot of estimated density functions
pday <- 24  # 2 (hohe Beobachtung zu niedrig geschaetzt), 4, 15, evtl. auch 7, 8, 23 (eine 0-Beobachtung und 2 sehr aehnliche), 

pdays <- if(pday<19) c(pday, pday + 31, pday + 61) else c(pday, pday + 30, pday + 61)
pdf <- predict(res$df, newdata = res$testdata[pdays,], type = "parameter")
df_mu <- pdf$mu
df_sigma <- pdf$sigma
df_exp <- pnorm(df_mu/df_sigma) * (df_mu + df_sigma * (dnorm(df_mu/df_sigma) / pnorm(df_mu/df_sigma)))
cbind(df_exp, res$testdata[pdays,"robs"])


# plot predicted distributions together with observations
#set.seed(res$call$seedconst)
set.seed(7)
x <- c(0.01, sort(runif(500,0.01,8)))
y1 <- crch::dcnorm(x, mean = df_mu[1], sd = df_sigma[1], left = 0)
y2 <- crch::dcnorm(x, mean = df_mu[2], sd = df_sigma[2], left = 0)
y3 <- crch::dcnorm(x, mean = df_mu[3], sd = df_sigma[3], left = 0)
dayending <- if(pday > 3) "th" else switch(pday, "1" = {dayending <- "st"}, "2" = {dayending <- "nd"}, "3" = {dayending <- "rd"})

# point mass (slightly shifted)
pm1 <- c(0.04, crch::dcnorm(-1, mean = df_mu[1], sd = df_sigma[1], left = 0))
pm2 <- c(-0.03, crch::dcnorm(-1, mean = df_mu[2], sd = df_sigma[2], left = 0))
pm3 <- c(-0.1, crch::dcnorm(-1, mean = df_mu[3], sd = df_sigma[3], left = 0))

# predictions
pred1 <- c(res$testdata[pdays,"robs"][1], crch::dcnorm(res$testdata[pdays,"robs"][1], mean = df_mu[1], sd = df_sigma[1], left = 0))
pred2 <- c(res$testdata[pdays,"robs"][2], crch::dcnorm(res$testdata[pdays,"robs"][2], mean = df_mu[2], sd = df_sigma[2], left = 0))
pred3 <- c(res$testdata[pdays,"robs"][3], crch::dcnorm(res$testdata[pdays,"robs"][3], mean = df_mu[3], sd = df_sigma[3], left = 0))

#legendheight
lh1 <- crch::dcnorm(0.01, mean = df_mu[1], sd = df_sigma[1], left = 0)
lh2 <- crch::dcnorm(0.01, mean = df_mu[2], sd = df_sigma[2], left = 0)
lh3 <- crch::dcnorm(0.01, mean = df_mu[3], sd = df_sigma[3], left = 0)



## PIT histograms and qqr-plots

## get predicted parameter
# for learndata and testdata
{
  ## disttree
  if(FALSE){
    # in sample
    pdt <- predict(res$dt, type = "parameter")
    dt_mu_l <- pdt$mu 
    dt_sigma_l <- pdt$sigma 
    pit_dt_l <- cbind(0, pnorm(res$learndata[,"robs"], mean = dt_mu_l, sd = dt_sigma_l))
    pit_dt_l[res$learndata[,"robs"]>0, 1] <- pit_dt_l[res$learndata[,"robs"]>0, 2]
  }  
  # out of sample
  pdt <- predict(res$dt, type = "parameter", newdata = res$testdata)
  dt_mu_t <- pdt$mu 
  dt_sigma_t <- pdt$sigma 
  pit_dt_t <- cbind(0, pnorm(res$testdata[,"robs"], mean = dt_mu_t, sd = dt_sigma_t))
  pit_dt_t[res$testdata[,"robs"]>0, 1] <- pit_dt_t[res$testdata[,"robs"]>0, 2]
  
  
  ## distforest
  if(FALSE){
    # in sample
    pdf <- predict(res$df, type = "parameter")
    df_mu_l <- pdf$mu
    df_sigma_l <- pdf$sigma
    pit_df_l <- cbind(0, pnorm(res$learndata[,"robs"], mean = df_mu_l, sd = df_sigma_l))
    pit_df_l[res$learndata[,"robs"]>0, 1] <- pit_df_l[res$learndata[,"robs"]>0, 2]
  }
  # out of sample
  pdf <- predict(res$df, type = "parameter", newdata = res$testdata)
  df_mu_t <- pdf$mu
  df_sigma_t <- pdf$sigma
  pit_df_t <- cbind(0, pnorm(res$testdata[,"robs"], mean = df_mu_t, sd = df_sigma_t))
  pit_df_t[res$testdata[,"robs"]>0, 1] <- pit_df_t[res$testdata[,"robs"]>0, 2]
  
  
  ## gamlss
  g_learndata <- res$learndata
  g_learndata$robs <- Surv(g_learndata$robs, g_learndata$robs>0, type="left")
  if(FALSE){
    # in sample
    g_mu_l <- predict(res$g, what = "mu", type = "response")
    g_sigma_l <- predict(res$g, what = "sigma", type = "response")
    pit_g_l <- cbind(0, pnorm(res$learndata[,"robs"], mean = g_mu_l, sd = g_sigma_l))
    pit_g_l[res$learndata[,"robs"]>0, 1] <- pit_g_l[res$learndata[,"robs"]>0, 2]
  }  
  # out of sample
  g_mu_t <- predict(res$g, what = "mu", type = "response", newdata = res$testdata)
  g_sigma_t <- predict(res$g, what = "sigma", type = "response", newdata = res$testdata)
  pit_g_t <- cbind(0, pnorm(res$testdata[,"robs"], mean = g_mu_t, sd = g_sigma_t))
  pit_g_t[res$testdata[,"robs"]>0, 1] <- pit_g_t[res$testdata[,"robs"]>0, 2]
  
  
  #gamboostLSS
  if(FALSE){
    #in sample
    pgb <- predict(res$gb, parameter = c("mu","sigma"), type = "response")
    gb_mu_l <- pgb$mu
    gb_sigma_l <- pgb$sigma
    pit_gb_l <- cbind(0, pnorm(res$learndata[,"robs"], mean = gb_mu_l, sd = gb_sigma_l))
    pit_gb_l[res$learndata[,"robs"]>0, 1] <- pit_gb_l[res$learndata[,"robs"]>0, 2]
  }  
  # out of sample
  pgb <- predict(res$gb, parameter = c("mu","sigma"), type = "response", newdata = res$testdata)
  gb_mu_t <- pgb$mu
  gb_sigma_t <- pgb$sigma
  pit_gb_t <- cbind(0, pnorm(res$testdata[,"robs"], mean = gb_mu_t, sd = gb_sigma_t))
  pit_gb_t[res$testdata[,"robs"]>0, 1] <- pit_gb_t[res$testdata[,"robs"]>0, 2]
  
  
  #EMOS
  if(FALSE){
    # in sample
    ml_mu_l <- predict(res$ml, type = "location")     # returns parameter on response scale
    ml_sigma_l <- predict(res$ml, type = "scale")
    pit_ml_l <- cbind(0, pnorm(res$learndata[,"robs"], mean = ml_mu_l, sd = ml_sigma_l))
    pit_ml_l[res$learndata[,"robs"]>0, 1] <- pit_ml_l[res$learndata[,"robs"]>0, 2]
  }  
  # out of sample
  ml_mu_t <- predict(res$ml, type = "location", newdata = res$testdata)     # returns parameter on response scale
  ml_sigma_t <- predict(res$ml, type = "scale", newdata = res$testdata)
  pit_ml_t <- cbind(0, pnorm(res$testdata[,"robs"], mean = ml_mu_t, sd = ml_sigma_t))
  pit_ml_t[res$testdata[,"robs"]>0, 1] <- pit_ml_t[res$testdata[,"robs"]>0, 2]
  
}
@


\begin{figure}[t!]
\begin{center}
<<rain_Axams_pred, fig=TRUE, echo=FALSE, width=6.5, height=4.5>>=
plot(x = x, y = y1, type = "l", col = "red", 
     main = paste0("July ", pday, dayending), ylab = "Density", 
     xlab = "Total precipitation (power transformed)",
     ylim = c(0,max(y1, y2, y3, pm1, pm2, pm3) + 0.01),
     xlim = c(-1.5,8))
    
lines(x = x, y = y2, type = "l", col = "blue")
    lines(x = x, y = y3, type = "l", col = "darkgreen")
    #legend('topright', c("2010", "2011", "2012"), col = c("red", "blue", "darkgreen"), lty = 1, cex = 1)
    
    # plot point mass
    lines(x = c(pm1[1], pm1[1]), y = c(pm1[2], 0), col = "red", type = "l", lwd = 1)
    lines(x = c(pm2[1], pm2[1]), y = c(pm2[2], 0), col = "blue", type = "l", lwd = 1)
    lines(x = c(pm3[1], pm3[1]), y = c(pm3[2], 0), col = "darkgreen", type = "l", lwd = 1)
    points(x = pm1[1], y = pm1[2], col = "red", pch = 19)
    points(x = pm2[1], y = pm2[2], col = "blue", pch = 19)
    points(x = pm3[1], y = pm3[2], col = "darkgreen", pch = 19)
    
    
    # plot predictions
    points(x = pred1[1], y = pred1[2], col = "red", pch = 4)
    points(x = pred2[1], y = pred2[2], col = "blue", pch = 4)
    points(x = pred3[1], y = pred3[2], col = "darkgreen", pch = 4)
    
    lines(x = c(pred1[1], pred1[1]), y = c(pred1[2], 0), col = "darkgrey", type = "l", lty = 2)
    lines(x = c(pred2[1], pred2[1]), y = c(pred2[2], 0), col = "darkgrey", type = "l", lty = 2)
    lines(x = c(pred3[1], pred3[1]), y = c(pred3[2], 0), col = "darkgrey", type = "l", lty = 2)
    
    # add labels
    text(x = -0.8, y = lh1, labels = "2010", col = "red", cex = 0.8)
    text(x = -0.8, y = lh2, labels = "2011", col = "blue", cex = 0.8)
    text(x = -0.8, y = lh3, labels = "2012", col = "darkgreen", cex = 0.8)
@
\caption{\label{axams_onestation}Predictions at station Axams for one day in July of 2010, 2011 and 2012.}
\end{center}
\end{figure}






\newpage
\section{Methodology}
\label{Methodology}

\subsection{Motivation (Review/Problems)}
As illustrated in Figure \ref{Devel_parmod} a lot of development has been made in the field of parametric modeling and a wide range of tools can be applied in research areas such as probabilistic forecasting. But as mentioned before, further features such as the ability to capture non-additive effects or interactions are desirable in many applications which demands for more flexible models. Since tree structured models provide these required features a combination of them with parametric models can offer a framework that is able to deal with a wide range of different modeling situations. In this section we are going to introduce the novel methodology which embeds parametric modeling into the idea of tree structured models step by step starting with a simple distributional fit in Subsection \ref{distfit}, then building a distributional tree in Subsection \ref{disttree} and finally a distributional forest in Subsection \ref{distforest}.


\subsection{Distributional Fit}
\label{distfit}
Fitting a distributional model to a set of observations without considering any covariates is a well known procedure which can for example be done by applying the maximum likelihood method. This is also the chosen approach in the methodology described in this section.\\
The goal is to fit a distributional model $D(Y, \theta)$ to a response variable $Y \in \mathcal{Y}$ for a \textit{k}-dimensional vector of distribution parameters $\theta \in \Theta$ in case of a \textit{k}-parametric distribution. The distribution family has to be specified in advance such that a log-likelihood function $\ell(\theta; Y)$ is provided. Using the GAMLSS framework (\cite{stasinopoulos2005}) a wide range of distributions can be selected where each distribution parameter can be specified separately. This enables the model to deal with various features that can usually be very challenging in model fitting such as censoring, truncation or heavy tails.\\
In our application of precipitation forecasting the response variable, which is the power transformed total precipitation amount within 24 hours, is expected to be normally distributed with mean/location parameter $\mu$ and standard deviation/scale parameter $\sigma$ and left censored with censoring point 0. Therefore, the corresponding log-likelihood function with parameter vector $\theta = (\mu, \sigma)$ is
$$
\ell(\mu, \sigma; Y) = 
\begin{cases}
    \log\left(\frac{1}{\sigma} \cdot \phi(\frac{Y - \mu}{\sigma}) \right), & \text{if } Y > 0\\
    \vspace{0.1cm}
    \log\left(\Phi(\frac{-\mu}{\sigma})\right), & \text{if } Y = 0
\end{cases}
$$
where $\phi$ and $\Phi$ are the probability density function and the distribution function of the standard normal distribution $\mathcal{N}(0,1)$. Next to the censored normal distribution the censored logistic distribution is also often chosen for the modeling of precipitation amounts. In \cite{Gebetsberger2017} this choice has been made in order to be able to deal with heavy tails which often appear in precipitation data. Another frequent application of probabilistic forecasting in meteorology is the modeling of temperature. In this case a normal distribution is often selected.\\
With the specification of the distribution family and the log-likelihood function the task of fitting a distributional model turns into the task of determining the distribution parameter $\theta$. This is done by the following maximization.
$$
\hat{\theta} = \max_{\theta \in \Theta} \sum_{i=1}^n \ell(\theta; y_i)
$$
where $\{y_i\}_{i=1,\ldots,n}$ are the observations of the response variable $Y$.\\
In that way, a whole distribution is specified with all its features, including location, scale and shape. However, fitting one global model to the whole data set might be too much of a generalization and therefore not represent the data and its features in a reasonable way. Moreover, if covariates are available it is desirable to include them into the model as they might provide important information. In our application on precipitation data a wide range of covariates are available from numerical ensemble predictions considering precipitation, temperature, sea level pressure and other meteorological quantities which might be of interest/have an influence on the predictions. \\
A common way of including these covariates is to consider them as regressors in a distributional model such that each parameter $\theta$ depends on covariates $\bold{X} = (X_1,\ldots,X_m)$. This could for example be done by a linear function $g$. In that way the fitted model would be specified as $D(Y, \theta = g(\bold{X}))$. In order to avoid the restriction to linear effects one might also use a set of smooth functions $\{f_i\}_{i = 1,\ldots,p}$ to specify a more flexible model $D(Y, \theta = f_1(\bold{X}) + \ldots + f_p(\bold{X}))$ applying a GAM structure. Another approach to distributional regression is proposed by \cite{Klein2015} where each parameter of a possibly complex distribution can be modeled by an additive term consisting of a variety of different functional effect types. These include non-linear effects, spatial effects, random coefficients and interaction surfaces which extends the flexibility compared to parametric models such as GAMLSS modeling only smooth effects.\\
However, the restriction to additive models remains and therefore a different approach of how to include covariates that also allows for non-additive effects and interactions is now considered. In particular, the covariates can be used to separate the data into more homogeneous subgroups. Doing so after fitting a global model to the whole data set (without considering any covariates) and then fitting a local model to each subgroup should improve the model remarkably. This procedure of splitting a data set into subgroups can be done by applying a tree algorithm.\\


\subsection{Distributional Tree}
\label{disttree}
After having fit a global distributional model $D(Y, \bold{\hat{\theta}})$ the idea is now to use the information provided by this model together with a set of covariates $\bold{Z} = (Z_1, \ldots, Z_m) \in \mathcal{Z}$ (from now on denoted by $\bold{Z}$ instead of $\bold{X}$ to indicate that they are considered as split variables only and not as regressors) to decide if the data set should be split into subgroups. In particular, the information provided by the fitted model are the scores 
$$
s(\hat{\theta}, y_i) = \frac{\partial \ell}{\partial \theta}(\hat{\theta}; y_i).
$$ 
These values express how well the model fits the data regarding each parameter. Ideally they should fluctuate randomly around zero, similar to residuals in ordinary least squares estimations, but in this case there is a score value for each estimated parameter $\hat{\theta}_j \in (\hat{\theta}_1, \ldots, \hat{\theta}_k) = \bold{\hat{\theta}}$ and each observation.\\
Statistical tests are applied to find out whether any significant connection/dependency between the scores and each of the partitioning variables $Z_l \in \bold{Z} = (Z_1, \ldots, Z_m)$ is given. If so, the data set is split into subgroups based on the values of the partitioning variable that shows the highest connection/dependency.\\
In a next step a local distributional model is fit in each of the subgroups. Repeating this procedure of fitting a model, applying statistical tests to evaluate the goodness of fit and splitting the data set depending on the test results leads to a tree structure. In that way the learning data set is separated into \textit{B} disjoint segments such that
$$
\mathcal{Z} = \dot{\bigcup\limits_{b = 1, \ldots, \textit{B}}} \mathcal{B}_b
$$
where each segment is represented by a terminal node of the tree.\\

The exact way of determining how and where to split varies over different tree algorithms. (For distributional trees, one of two algorithms can be chosen as a framework to build the tree: either the MOB algorithm or the ctree algorithm.) Summed up, the steps of building a distributional tree using one of these algorithms are
\begin{enumerate}
\item Specify a distribution with log-likelihood function $\ell(\theta; y)$.
\item Estimate $\bold{\hat{\theta}}$ via maximum likelihood.
\item Test for associations or instabilities of the scores $s(\bold{\hat{\theta}}, y_i)$ and each partitioning variable $Z_l$.
%\item Assess whether the \emph{model scores} are associated with
%    (or change along) any of the available covariates -- e.g.,
%    using parameter instability tests (\emph{strucchange}) or
%    conditional inference (\emph{coin}).
\item Split the sample along the partitioning variable with the strongest association or instability.
    Choose the breakpoint with the highest improvement in log-likelihood.
\item Repeat steps 2--4 recursively in the subgroups
    until some stopping criterion is met.
    %-- e.g., for significance or sample size.
%\item Choose the variable with the strongest association
%\item Choose the split point which leads to the highest improvement of the model
%\item Split and repeat 2-6 in each node until a stopping criterion is met
\end{enumerate}
The stopping criterion in step 5 can for example be a minimum number of observations in a node or a p-value for the statistical tests applied in step 3.\\
Executing these steps results in a stepwise parametric model where a complete distribution is fit in each node of the tree.\\
\\
For distributional trees the strategy of how to make a prediction for a new observation $\bold{z}^{\ast} = (z_1^{\ast}, \ldots, z_m^{\ast})$ is very simple and straight forward. All observations of the learning data set $\{(y_i,\bold{z}_i)\}_{i=1\ldots,n}$ which end up in the same terminal node of the tree as the new observation are selected and a distributional model is fit on the subset. To select this subset a weight vector $\bold{\omega^{tree}} = (\omega^{tree}_1, \ldots, \omega^{tree}_n)$ is included in the fitting process.
$$
\omega^{tree}_i(\bold{z}^{\ast}) = \sum_{b=1}^B I((\bold{z}_i \in \mathcal{B}_b) \land (\bold{z}^{\ast} \in \mathcal{B}_b))
$$
The predicted distribution of the new observation is then fully specified by the estimated parameter $\hat{\theta}(\bold{z}^{\ast})$ where
$$
\hat{\theta}(\bold{z}^{\ast}) = \max_{\theta \in \Theta} \sum_{i=1}^n \omega^{tree}_i(\bold{z}^{\ast}) \cdot \ell(\theta; y_i).
$$

While the clear structure of such a model is advantageous in analyzing it, the abrupt changes are often too rough and impose steps on the model even in case of smooth effects. In these situations bagging can solve the problem by combining an ensemble of trees such that wrongly detected abrupt changes of the model are turned into almost smooth transitions. In that way, all kinds of effects can be modeled, which is is also one the of the main advantages of forest models (taking bagging one step further as the variance is reduced by sampling the covariates considered for the splitting process in each node).

\subsection{Distributional Forest}
\label{distforest}
A distributional forest consists of an ensemble of distributional trees $\{t_s\}_{s=1,\ldots,T}$. Each of these trees is built on a different data set which can either be a bootstrap or a subset of the original data set. Moreover, in each node a subset of the partitioning variables is chosen randomly as candidates for splitting variables. In that way the correlation among the resulting trees is reduced and therefore also the variance of the model (\cite{breiman2001random}).\\
\\
Forest models differ mainly in the way how the trees are combined to make predictions. In this case the forest itself is only used to evaluate neighborhood relationships among the observations which can then be included in the fitting process.\\
Contrary to distributional trees, in distributional forests the whole learning data set can be used to fit the distributional model and specify the predicted distribution for a new observation. This difference is represented in the weight vector, as the tree weights $\bold{\omega}^{tree}$ containing only zeros and ones are now replaced by integer valued forest weights \\
$\bold{\omega}^{forest} = (\omega^{forest}_1, \ldots, \omega^{forest}_n)$.\\
For a new observation $\bold{z}^{\ast} = (z^{\ast}_1, \ldots, z^{\ast}_m) $ this set of weights is obtained by the following procedure: For each observation $(y_j,\bold{z}_j)$ in the learning data set $\{(y_i,\bold{z}_i)\}_{i=1\ldots,n}$ a weight is calculated by counting the number of trees in which it ends up in the same terminal node as the new observation $\bold{z}^{\ast}$. 
$$
\omega^{forest}_i(\bold{z}^{\ast}) = \sum_{s=1}^T \sum_{b=1}^{B^s} I((\bold{z}_i \in \mathcal{B}^s_b) \land (\bold{z}^{\ast} \in \mathcal{B}^s_b))
$$
where $T$ is the number of trees of the forest, $B^s$ is the number of terminal nodes of the \textit{s}-th tree and $\mathcal{B}^s_b$ represents the \textit{b}-th terminal node of the \textit{s}-th tree.\\
This individual set of weights $\bold{\omega}^{forest}(\bold{z}^{\ast}) = (\omega^{forest}_1(\bold{z}^{\ast}), \ldots, \omega^{forest}_n(\bold{z}^{\ast}))$ for the new observation $\bold{z}^{\ast}$ can now be included in the estimation process of the distribution parameters and in that way, leads to its individual parameter vector $\hat{\theta}(\bold{z}^{\ast})$.
$$
\hat{\theta}(\bold{z}^{\ast}) =  \max_{\theta \in \Theta} \sum_{i=1}^n \omega^{forest}_i(\bold{z}^{\ast}) \cdot \ell(\theta; y_i)
$$
While in a distributional tree the same distribution is specified for all observations being in the same node a distributional forest predicts and fits a different distribution for each different observation which is due to the individual weights calculated separately for each observation.











\newpage
\section{Simulation Study}
\label{Simulation}
Within this simulation study the performance of distributional trees and forests is tested and compared to other models, showing situations where one or the other method is more advantageous than others. 

%In the case of smooth parameter functions without any abrupt changes / non-additive effects a smooth parametric model such as GAMLSS is clearly the first choice while a stepfunction can be modeled best by applying a tree-structured model. Even though for both of these extreme situations a specifically advantageous method is provided, a forest model can be a good compromise and performs well in a wide range of settings including the two mentioned before. (Therefore, a forest is a recommendable choice particularly when no information about the type of parameter function is provided.)

%Generally, a smooth parametric model such as GAMLSS should be the most successful method if the parameter function is smooth with no rapid changes. However, if there are any jumps or non-additive effects or interactions, the trees should be more effective in detecting these steps and therefore also lead to a better fitting forest. But this situation might also appear for smooth functions which contain sections with very steep slopes. Therefore, it is now of great interest, whether the concept of smooth parametric models or of trees and forests covers a broader range of functions where it is superior to the other.\\


The new methods are evaluated on generated data sets and their results are then compared to those of a GAMLSS (\cite{stasinopoulos2005}) which fits a separate generalized additive model to each of the distribution parameter and in that way specifies a whole distribution and is a very efficient option for smooth effects of the covariates on the parameters.\\
The generated data sets include 500 observations which each includes the value of a response variable $Y$ and six covariates $X_1, \ldots, X_6$ which are all independent and distributed as follows:
$$
X_1, X_2, X_3 \sim \textit{Unif}([-1,1])
$$
$$
X_4, X_5, X_6 \sim \textit{Binom}(1, 0.5)
$$

The distribution of the response variable is a left censored normal distribution with censoring point 0. The corresponding distribution parameters are defined by a parameter function $f$ depending on the covariates and an additional factor $\kappa$ which regulates the steepness of certain sections of the parameter function. For low values of $\kappa$ the resulting parameters change rather slowly over varying values of the covariates while high values of $\kappa$ lead to abrupt changes. \\ 
The location parameter $\mu$ depends on $X_1, X_4$ and $\kappa$ and the scale parameter $\sigma$ depends on $\mu, X_1$ and $\kappa$. The other covariates $X_2, X_3, X_5$ and $X_6$ are used as noise variables.\\
The latent variable $Y^{\ast}$ is generated based on the following setting.
\begin{center}
$$ 
Y^{\ast} \sim \mathcal{N} (f(X_1, X_4, \kappa))
$$
with
$$
f(X_1, X_4, \kappa) = \left(\mu(X_1, X_4, \kappa) , \sigma(\mu,X_1,\kappa)\right)
$$ 
and
\begin{align*}
\mu(X_1, X_4, \kappa) = 0.2 
+ \begin{cases}
    7 \cdot \exp{(-(3 \cdot X_1-1.2)^{(2\cdot \kappa)})},
    & \text{if } X_1 < 0.4 \text{ and } X_4 = 0\\
    7 \cdot \exp{(-(3 \cdot X_1-1.2)^{(2\cdot \kappa)})} \cdot \frac{1}{2} + \frac{7}{2},       
    & \text{if } X_1 \geq 0.4 \text{ and } X_4 = 0\\
    10 \cdot \exp{(-(3 \cdot X_1-1.2)^{(2\cdot \kappa)})},
    & \text{if } X_1 < 0.4 \text{ and } X_4 = 1\\
    10 \cdot \exp{(-(3 \cdot X_1-1.2)^{(2\cdot \kappa)})} \cdot \frac{1}{2} + \frac{10}{2},       
    & \text{if } X_1 \geq 0.4 \text{ and } X_4 = 1
\end{cases}
\end{align*}

$$
\sigma(\mu, X_1,\kappa) = 0.5 + \frac{\mu}{5} + 2 \cdot \mathbf{1}(X_1 < -0.7)
$$
%where $\Lambda$ is the cumulative distribution function of the logistic distribution
%$$
%\Lambda(\omega) = \frac{1}{1 + \exp(-\omega)}
%$$
The response variable $Y$ is defined as 
$$
Y = 
\begin{cases}
    Y^{\ast},& \text{if } Y^{\ast} \geq 0\\
    0,       & \text{else}
\end{cases}
$$
\end{center}


To illustrate the parameter functions and the impact of $\kappa$ together with the results of the considered methods they are first applied on one generated data set only where $\kappa$ is set to 1 (Figure \ref{plot_sim_oneset1}) and then to a second data set where $\kappa$ is set to 10 (bottom plots in Figure \ref{plot_sim_oneset10}).\\
In these plot the location parameter function $\mu(X_1, X_4, \kappa)$ (left window) and the the scale parameter function $\sigma(\mu, X_1, \kappa)$ (right window) are plotted along the covariate $X_1$ together with the estimated values of the different models for fixed covariates $x_4 = 0$ and $(x_2, x_3, x_5, x_6) = (0,0, \ldots, 0)$. \\

\SweaveOpts{eval = TRUE}
<<echo=FALSE, results=hide>>=

if(!(file.exists("plot_oneset1_mu.jpeg") & file.exists("plot_oneset1_sigma.jpeg") &
   file.exists("plot_oneset10_mu.jpeg") & file.exists("plot_oneset10_sigma.jpeg"))){
  
  library("gamlss.cens")
  gen.cens("NO", type = "left")
  source("oneset.R")
  
}

@


\begin{figure}[t!]
\begin{subfigure}{0.6\textwidth}
\includegraphics{plot_oneset1_mu.jpeg}
\end{subfigure}
\hspace{-1cm}
\begin{subfigure}{0.6\textwidth}
\includegraphics{plot_oneset1_sigma.jpeg}
\end{subfigure}
\caption{\label{plot_sim_oneset1} Smooth parameter functions ($\kappa = 1$) together with the predicted parameter of the tree model, the forest model and the GAMLSS.}
\end{figure}


\begin{figure}[t!]
\begin{subfigure}{0.6\textwidth}
\includegraphics{plot_oneset10_mu.jpeg}
\end{subfigure}
\hspace{-1cm}
\begin{subfigure}{0.6\textwidth}
\includegraphics{plot_oneset10_sigma.jpeg}
\end{subfigure}
\caption{\label{plot_sim_oneset10} Parameter functions with abrupt changes ($\kappa = 10$) together with the predicted parameter of the tree model, the forest model and the GAMLSS.}
\end{figure}

In the case of smooth effects of the coefficients on the parameter function ($\kappa = 1$, Figure \ref{plot_sim_oneset1}) the fitted parameters of the GAMLSS are closest to the true parameters while the steps of the tree are cleary too rough. By combining various trees the forest can smooth out these steps, however, not as well as the GAMLSS.\\
Looking at the other extreme situation in which the parameter functions include abrupt changes ($\kappa = 10$, Figure \ref{plot_sim_oneset10}) the tree captures them very well wich also leads to a good fit of the forest model. In contrast to the two tree structured models the GAMLSS has its difficulties with these steps and therefore fluctuates around the true parameter functions.\\

Now, to get a more profound impression, the methods are evaluated on various data sets. The value of $\kappa$ is increasing from 1 to 10 by a stepsize of 1 and for each step 100 data sets are generated.\\
%\subsubsection{Means of Comparison}
In order to compare the performance of the chosen methods over various data sets, the continuous ranked probability score (CRPS) is calculated for each model on each data set. 

%three different values are calculated for the resulting models: the root of the mean squared error (RMSE), the log-likelihood value and the continuous ranked probability score (CRPS). All of them are evaluated out of sample, e.g. on a part of the data set that has not been used to build the model.\\
%Therefore, overfitting would lead to worse results.\\
%\\
%The RMSE represents how close the estimated expected values are to the true observations. For a response variable $Y$ and a set of observations $\textbf{y} = \{y_i\}_{i = 1,\ldots,n}$ with the corresponding estimated parameters $\hat{\boldsymbol{\theta}} = \{\hat{\theta}_i\}_{i = 1,\ldots,n}$ the RMSE is defined as follows.

%$$
%\text{RMSE}(Y, \boldsymbol{y}, \boldsymbol{\hat{\theta}}) = \sqrt{\frac{1}{n} \sum_{i=1}^n (\mathbb{E}_{\hat{\theta}_i}(Y) - y_i)^2}
%$$

%In most cases this value does not measure the distributional fit as only the expected value is considered. However, in the case of a censored normal distribution the expected value of the observed variable is a transformation of the expected value of the latent variable which includes the location and the scale parameter.\\
%\\
%A typical way to evaluate the distributional fit is provided by the log-likelihood value. For each observation $y$ and the corresponding predicted parameter $\hat{\theta}$ the value of the log-likelihood function $\ell(\hat{\theta}; y)$ is calculated and then averaged over all observations in the testing data set. The higher the value the better the predictions of the model are.\\
%\\
%The CRPS is another value that measures the accuracy of a probabilistic model. For one observation $y$ with the predicted distribution function $F$ the CRPS value is calculated by
The CRPS measures the accuracy of a probabilistic model. For one observation $y$ with the predicted distribution function $F$ the CRPS value is calculated by
$$
\text{CRPS}(y, F) = \int_{-\infty}^\infty (F(z) - \mathbf{1}(y \leq z))^2 dz
$$
where \(\mathbf{1}\) is the indicator function. \\
In this simulation study the CRPS is evaluated out of sample, e.g. on a data set that has not been used to build the model but has been generated by the same process and with the same settings as the original data set. In the following this second data set on which the CRPS is evaluated is called testing data set while the original data set on which the models are built on is called learning data set.
The CRPS value is first calculated for each observation in the testing data set and then the average of these values is taken. As the CRPS can be seen as a generalization of the mean absolute error, a low value indicates a good fit. In applications, one big advantage in terms of interpretability is that the resulting value is on the scale of the observation. \\


%The methods are then compared by the out-of-sample RMSE for the location parameter $\mu$ and the shape parameter $\sigma$ as well as by %the out-of-sample log-likelihood of the fitted models, averaged within each step.
%(In that way it can be investigated how the results change for a changing value of $\kappa$.)


\SweaveOpts{eval = TRUE}

<<echo=FALSE, results=hide>>=
if(file.exists("simres.rda")){
  load("simres.rda")
} else {
  #source("~/svn/partykit/pkg/disttree/inst/draft/.R")
  source("gensim.R")
  simres <- gensim(seedconst = 7, nrep = 20, ntree = 100,
                   nsteps = 5, stepsize = 2,
                   formula = y~x1+x2+x3+x4+x5+x6,
                   nobs = 400, testnobs = 200L,
                   tree_minsplit = 25, tree_minbucket = 10, tree_mincrit = 0.95,
                   forest_minsplit = 25, forest_minbucket = 10, forest_mincrit = 0, 
                   forest_mtry = 3,
                   fix.mu = FALSE,
                   fix.sigma = FALSE,
                   mu.sigma.interaction = FALSE,
                   mubase = 1,
                   sigmabase = 3,
                   censNO = TRUE,
                   gamboost_cvr = FALSE,
                   eval_disttree = TRUE,
                   eval_distforest = TRUE,
                   eval_gamlss = TRUE,
                   eval_bamlss = FALSE,
                   eval_gamboostLSS = FALSE,
                   eval_randomForest = FALSE,
                   eval_cforest = FALSE)
  
  save(simres, file = "simres.rda")
}
@



%\begin{figure}[t!]
%\begin{subfigure}{0.6\textwidth}
%<<sim_rmse_exp, fig=TRUE, echo=FALSE>>=
%plot_rmse(simres, type = "exp")
%@
%\end{subfigure}
%\hspace{-1cm}
%\begin{subfigure}{0.6\textwidth}
%<<sim_rmse_par, fig=TRUE, echo=FALSE>>=
%plot_rmse(simres, type = "par")
%@
%\end{subfigure}
%\caption{\label{plot_sim_rmse}Results of the simulation study (RMSE).\\
%left: continuous lines represent the difference between the true and the estimated expected value, dotted lines represent the difference %between the observations and the estimated expected value\\
%right: continuous lines represent the location parameter, dotted lines the scale parameter}
%\end{figure}

\begin{figure}[t!]
\begin{center}
%\begin{subfigure}{0.6\textwidth}
<<sim_crps, fig=TRUE, echo=FALSE>>=
plot_crps(simres)
@
\caption{\label{plot_sim_crps}Results of the simulation study: CRPS} %\code{disttree} vs. \code{distforest} vs. \code{gamlss} vs. \code{randomForest} vs. %\code{bamlss} vs. \code{gamboostLSS}\\
%right: levelplot of the parameter function for $\kappa=5$}
\end{center}
\end{figure}

%\end{subfigure}
%\hspace{-1cm}
%\begin{subfigure}{0.6\textwidth}
%<<sim_levelplot, fig=TRUE, echo=FALSE>>=
%levelfun <- function(x) simres$fun(x,5)
%
%ncuts <- 100
%colR <- colorRampPalette(c("blue","green"))                                       
%cols <- colR(ncuts+1)
%griddata <- expand.grid(list(x1 = seq(-1, 1, 0.01), x4 = seq(0, 1, 0.2)))
%griddata$mu <- levelfun(griddata)
%levelplot(mu ~ x1 * x4, data = griddata, xlab = "x1", ylab = "x4", 
%          region = TRUE, cuts = ncuts, col.regions = cols)

%@
%\end{subfigure}
%\caption{\label{plot_sim_ll}left: results of the simulation study: log-likelihood\\ %\code{disttree} vs. \code{distforest} vs. \code{gamlss} vs. \code{randomForest} vs. %\code{bamlss} vs. \code{gamboostLSS}\\
%right: levelplot of the parameter function for $\kappa=5$}
%\end{figure}



Figure \ref{plot_sim_crps} clearly illustrates the strengths of the different methods applied in this simulation study and supports the assumptions made above based on the results for the two specific situations in Figures \ref{plot_sim_oneset10} and \ref{plot_sim_oneset10}. Moreover, it can be observed how the stepwise transition from the one extreme situation to the other influences the performance of the applied methods.\\
The parametric GAMLSS can easily deal with the smooth effects that appear for low values of $\kappa$ and outperforms the tree and forest models in this situation. However, it clearly starts to struggle as $\kappa$ increases and the effects turn into abrupt changes. %This behaviour has already been detected and discussed above when looking at only single data sets and is now proven by an almost monotone increase of the CRPS.\\
On the contrary, the imposed/enforced steps of the tree model lead to a high CRPS value in the situation of smooth effects while this value decreases quickly as the slopes in the parameter functions get steeper. For high values of $\kappa$ resulting almost in step functions the tree model perfomes the best as stated by the low CRPS value.\\
Therefore, both extreme situations show to have a preferable method to choose which clearly outperforms the others. But overall the forest model stays more or less on the same level of goodness of fit regardless of the type of effects in the parameter functions. As this level lies inbetween the ones of the other two methods the forest model provides a good compromise. Of course, if the type of effects is known in advance one should choose the corresponding adequate method, but as this is usually not the case, applying a distributional forest offers a solution which can deal with a wide variety of situations and in that way secures a good fit of the resulting model. 

%Even though \code{gamlss} can easily deal with each of the smooth functions it struggles with the separation in subgroups. This is the point where the tree structure provides a better solution. However, a single tree can not keep up with the performance of \code{gamlss} when it comes to modeling the smooth functions within the subgroups. Offering a good compromise the forest takes advantages of the tree structure to find subgroups but can also model smooth effects if enough trees are included.\\
%However, once the increasing value of $\kappa$ leads to very steep sections within the parameter functions of each of the subgroups the tree turns out to be the most successful method in this specific situation.\\
%\\
%Looking at the resulting values of the RMSE it can be observed that trees and forests cleary outperform gamlss in this situation. For $\kappa=1$ the performance of all three methods is similar. However, as $\kappa$ increases the RMSE of gamlss increases while the RMSE of trees and forests stays at almost the same level or even decreases sligthly as soon as $\kappa$ is greater or equal 2. The same conclusion can be drawn from looking at the log-likelihood where the resulting values of gamlss decrease rapidly while the values of trees and forests only decrease a little at the beginning but then increase.\\
%This is the result that was expected as trees and forests can deal with rapid changes of the distribution functions more easily.\\
%For low values of $\kappa$ the forests are a little bit more efficient than the trees, but the higher $\kappa$ and therefore also the steeper the parameter function is the better the trees become. Especially regarding the scale parameter the trees are clearly advantageous over the forests.   


%(As it has already been demonstrated in the first examples on single data sets the performances of \code{distforest} and \code{disttree} are not as successful as the one of \code{gamlss} for low values of $\kappa$ but remarkably improve as this values increases and are superior to \code{gamlss} for high values of $\kappa$. This is illustrated by an almost constant RMSE of \code{distforest} and \code{disttree} while the RMSE of \code{gamlss} increases. Comparing the tree and the forest model it can be observed that the forest seems to perform better for lower $\kappa$ while very steep sections lead to a better performance of trees.













\SweaveOpts{eval = TRUE}
\newpage
\newpage
\section{Probabilistic Forecasting of Precipitation}
\label{ProbabilisticForecasting}


As explained in the introduction, distributional trees and forests provide a new approach in probabilistic forecasting which combines advantages of parametric models that are already used in this field and tree structured models which provide features that could be particularly valuable in probabilistic forecasting but have not yet been applied in this kind of settings. To show their strengths in practice, these new methods are now applied to probabilistic precipitation forecasts in a mountainous region based on a large number of numerical weather prediction quantities. 

\subsection{Setting}
The data set used for training and validation consists of observed daily
precipitation sums provided by the national hydrographical service
(\citealt{ehyd}) and numerical weather forecasts from the {U.S.} national
oceanic and atmospheric administration (NOAA).
Both, observations and weather forecasts are available from 1985 trough the
year 2012, focussing on July only in this article.\\
Observations are available on a daily basis, each observation representing the
total precipitation sum fallen within 24~hours observed at 6~UTC at \textbf{95}
stations all over Tyrol and it's close surrounding. The observations are
undergoing a rigorous quality check before made available to the public.  As
covariates the second generation reforecast data set of the global ensemble
forecast system (GEFS) is used (\citealt{hamill2013}).  This data set consists
of an 11-member ensemble based on a fixed version of the numerical model and a
horizontal grid-spacing of about $50~\times~50 km^2$ initialized at 0~UTC from
Dezember 1984 to precent. Each of the 11 ensemble members uses slightly
different perturbed initial conditions trying to predict the situation specific
uncertenty of the atmospheric state. 


%The data set consists of daily 24h precipitation sums for the month of July from 28 years, starting in 1985 until 2012, both years included. The observations were measured at 95 (?) stations all over Tyrol (and regions close to the frontier). The considered data is provided by .....
%Next to the variable of interest which is the measured total precipitation within 24 hours of each day (measured from 6 UTC to 6 UTC of the following day), numerical forecasts (from ... ) are used as covariates. These forecasts are ensemble predictions consisting of ... individual forecasts based on varying/perturbed initial conditions.\\

In particular, 14 basic forecast variables are considered with up to 12 variations of each of these variables being included as covariates. Since each forecast variable consists of an ensemble of individual predictions, the possible variations include the mean, the standard deviation, the minimum and the maximum over all these individual predictions, noted as ensemble mean, ensemble std dev, ensemble minimum and ensemble maximum. All of these values can be calculated on the sums over a whole day (6--30 UTC) or a 6h time window of the day.\\
Another possible option is to consider the mean, minimum or maximum over 24 hours for each individual prediction and then take the ensemble mean or ensemble standard deviation, noted as ensemble mean of mean/minimal/maximal values or ensemble std dev of mean/minimal/maximal values respectively.\\
Table \ref{covariates} shows all variables together with the number of variations and the provided variations listed in the last column. Altogether 82 covariates are available in this setting.\\
To remove the skewness of ... (the distribution of variables representing precipitation amounts), a power transformation has been used frequently (Box and Cox 1964). In literature, cubic (\citealt{stidd1973}) or square root (\citealt{hutchinson1998b}) transformations have often been suggested but may vary for different climatic zones or temporal aggregation periods.
The response variable \texttt{tp} (total precipitation) is power transformed by a factor of 1.6. This factor is chosen due to experience and knowledge about possible distributions of precipitation measurements. The resulting transformed variable is expected to be normally distributed and left censored at censoring point 0. The same transformation has been chosen for the variable \texttt{cape} (convective available potential energy).




%\newpage
\begin{table}[t!]
%\resizebox{1.4\textwidth}{!}{
\begin{minipage}{\textwidth}
\begin{tabular}{ l  c  l }
\hline
Basic covariates & {\#} & Variations\\
\hline
\textbf{tp}: total precipitation            & 12 variations:  & ensemble mean of sums over 24h, \\
\hspace*{\fill} power transformed (by 1.6) &    & ensemble std dev of sums over 24h, \\ 
\textbf{cape}: convective                  &    & ensemble minimum of sums over 24h, \\
\hspace*{0.4cm} available potential energy &    & ensemble maximum of sums over 24h\\
\hspace*{\fill}  power transformed (by 1.6)&    & \qquad all for 6--30 UTC\\
\hspace*{\fill}                            &    & ensemble mean of sums over 6h\\
\hspace*{\fill}                            &    & \qquad for 6--12, 12--18, 18--24, 24--30 UTC\\
\hspace*{\fill}                            &    & ensemble std dev of sums over 6h\\
\hspace*{\fill}                            &    & \qquad for 6--12, 12--18, 18--24, 24--30 UTC\\
\hline
\textbf{dswrf}: downwards short ware      & 6 variations: & ensemble mean of mean values, \\
\hspace*{\fill} radiation flux (sunshine) &   & ensemble mean of minimal values,\\
\textbf{msl}: mean sea level pressure     &   &  ensemble mean of maximal values,\\
\textbf{pwat}: preciptable water          &   & ensemble std dev of mean values,\\
\textbf{tmax}: 2m maximum                 &   & ensemble std dev of minimal values,\\
\hspace*{1.15cm} temperature              &   & ensemble std dev of maximal values,\\
\textbf{tcolc}: total column-integrated   &   & \qquad all over 6--30 UTC \\
\hspace*{1.05cm} condensate  &   & \\
\textbf{t500}: temperature   &   & \\
\hspace*{1cm} on 500 hPA     &   & \\
\textbf{t700}: temperature   &   & \\
\hspace*{1cm} on 700 hPA     &   & \\
\textbf{t850}: temperature   &   & \\
\hspace{1cm} on 850 hPA      &   & \\
\hline
\textbf{tdiff500850}: temperature         & 3 variations: & ensemble mean of difference in mean,\\
\hspace*{\fill} difference 500 to 850 hPA &   & ensemble minimum of difference in mean,\\
\textbf{tdiff500700}: temperature         &   & ensemble maximum of difference in mean\\
\hspace*{\fill} difference 500 to 700 hPA &   & \qquad all over 6--30 UTC\\
\textbf{tdiff700850}: temperature         &   & \\
\hspace*{\fill} difference 700 to 850 hPA &   & \\
\hline
\textbf{msl{\_}diff}: mean sea level  & 1 variation: & msl{\_}mean{\_}max - msl{\_}mean{\_}min\\
\hspace{1.65cm} difference            &   & \qquad over 6--30 UTC\\
\hline
\end{tabular}
\caption[Table caption text]{Basic covariates together with the number and the type of variations.}
\label{covariates}
\end{minipage} 
%}
\end{table}



\subsection{Models}
Next to distributional forests three other methods are applied to this data set and their results are compared based on (RMSE, log-likelihood values and) CRPS values. One of the other models is an EMOS models as this is a common choice in this type of setting / research field and therefore provides a valueable reference. The other two models are both GAMs, once in a prespecified form in terms of variable selection for each distribution parameters (denoted as GAM (prespecified)) and once applying a boosting algorithm (GAM (boosted)). 
The chosen methods and their specifications are listed in Table \ref{model_specification}.
\begin{table}[t!]
%\resizebox{1.4\textwidth}{!}{
%\begin{minipage}{\textwidth}
\begin{center}
\begin{tabular}{ l  l   l   l }
\hline
 & \textbf{Type} & \textbf{Location ($\boldsymbol{\mu}$)} & \textbf{Scale ($\boldsymbol{\log(\sigma)}$)} \\
\hline
\textbf{Forest} & recursive    & all & all \\ 
                & partitioning &     &     \\ 
\hline
\textbf{GAM (prespecified)} & spline  & tp{\_}mean, & tppow{\_}sprd,\\
                            & in each & tp{\_}max,  & dswrf{\_}sprd{\_}mean,\\
 & & tp{\_}mean1218 $\cdot$ & tp{\_}sprd1218 $\cdot$\\ 
 & & \quad cape{\_}mean1218, & \quad  cape{\_}mean1218,\\
 & & dswrf{\_}mean{\_}mean, & tcolc{\_}sprd{\_}mean,\\
 & & tcolc{\_}mean{\_}mean, & tdiff500850{\_}mean\\
 & & pwat{\_}mean{\_}mean, & \\
 & & tdiff500850{\_}mean, & \\
 & & msl{\_}diff & \\
\hline
\textbf{GAM (boosted)} & spline  & all & all \\
                       & in each &     &     \\
\hline
\textbf{EMOS} & linear & tp{\_}mean & tp{\_}spread \\
\hline
\end{tabular}
\caption[Table caption text]{Table of applied methods together with their type (way of including the covariates) and the included covariates for each distribution parameter.}
\label{model_specification}
%}
\end{center}
\end{table}

For the prespecified GAM model the covariates were selected manually based on meteorological knowledge and experience. Additionally, interactions are specified in advance, once between \texttt{tp{\_}mean1218} and \texttt{cape{\_}mean1218} for the location parameter and once between \texttt{tp{\_}sprd1218} and \texttt{cape{\_}mean1218} for the scale parameter. By putting extra effort in the specification of the formulas the prespecified GAM model is provided with additional information and is therefore expected to profit from this advantage.\\
In the EMOS model the logarithmic function is chosen as the link function for the scale parameter. This choice has been made based on the results of \cite{Gebetsberger2017} where different link functions (identity, quadratic and logarithmic function) have been compared. Moreover, testing the identity, quadratic and logarithmic function as link functions in this setting supported this choice as the logarithmic function lead to the best performance of the EMOS model in this application.



\SweaveOpts{eval = TRUE}
\subsection{Application on one station}
First of all we focus on one observation station, Axams in Tyrol. This station is selected due to its geographical closeness to Innsbruck, the capital of Tyrol. As for all other stations, daily observations of the month of July of 28 years are available, starting in 1985 and ending in 2012, both years included.\\ 
The results of fitting one distributional forest to the first 25 years of the data set and predicting the total amount of precipitation for one day of each of the three successive years have been shown in the introduction in Figure \ref{axams_onestation} as a motivational example for a probabilistic forecast. Staying with this fitted model we are now going to analyze variable importance of this forest model and compare its results to those of the GAM models and the EMOS model based on PIT histograms.\\
\\
As seen in Table \ref{model_specification} the forest model includes all 82 covariates. In a single tree model it can easily be observed which covariates are used as split variables and hence have an impact on the response variable. In a forest model though, for each of the trees a subset of these covariates is chosen randomly as possible split variables which is one of the reasons leading to different splitting variables being chosen in each tree. To get an overview of which covariates have the most influential effect on the response variable a standard measurement of variable importance for forest models has been chosen. The 10 leading covariates of the above described forest model for the station Axams learned on the first 25 year are listed in Figure \ref{varimp}. As expected, the numerical forecast for total precipitation is the most frequently selected. In particular, five variations of this variable are among the top ten of the list of covariates ordered by variable importance.  
<<variabble_importance, echo=FALSE, eval=TRUE>>=
if(file.exists("vimp_crps.rda")){
  #load("~/svn/partykit/pkg/disttree/inst/draft/vimp_crps.rda")
  load("vimp_crps.rda")
} else {
  
  # locally redefine logLik.distforest to use crps in varimp()
  logLik.distforest <- function(object, newdata = NULL, ...) {
    if(is.null(newdata)) {
      newdata <- object$data
    } 
    
    # predict parameter
    pdf <- predict(object, newdata = newdata, type = "parameter")
    df_mu <- pdf$mu
    df_sigma <- pdf$sigma
    
    # calculate CRPS
    crps <- mean(crps_cnorm(newdata$robs, location = df_mu, scale = df_sigma, lower = 0, upper = Inf), na.rm = TRUE)
  
    return(structure(crps, df = NA, class = "logLik"))
  }
  
  vimp_crps <- varimp(res$df, nperm = 1L)
  #vimp10_crps <- varimp(res$df, nperm = 10L)
  #vimp100_crps <- varimp(res$df, nperm = 100L)
  #vimp1000_crps <- varimp(res$df, nperm = 1000L)
  
  save(vimp_crps, file = "~/svn/partykit/pkg/disttree/inst/draft/vimp_crps.rda")
  #save(vimp10_crps, file = "~/svn/partykit/pkg/disttree/inst/draft/vimp10_crps.rda")
  #save(vimp100_crps, file = "~/svn/partykit/pkg/disttree/inst/draft/vimp100_crps.rda")
  #save(vimp1000_crps, file = "~/svn/partykit/pkg/disttree/inst/draft/vimp1000_crps.rda")
  
  rm(logLik.distforest) 
}
    
@
  

\begin{figure}[t!]
\begin{center}
\begin{subfigure}{0.7\textwidth}
<<rain_axams_varim, fig=TRUE, echo=FALSE, width=6.5, height=4.5>>=
par(mar = c(2,10,2,2))
barplot(sort(vimp_crps, decreasing = FALSE)[(length(vimp_crps)-9):length(vimp_crps)], 
        horiz = TRUE, las = 1, cex.names = 1.2, axes = FALSE, 
        names.arg = gsub("pow", "", names(sort(vimp_crps, decreasing = FALSE)[(length(vimp_crps)-9):length(vimp_crps)])))
@
\end{subfigure}
%\hspace{-1cm}
%\begin{subfigure}{0.5\textwidth}
%<<rain_axams_varim_margeffects1_sigma, fig=TRUE, echo=FALSE>>=
%plot(x = sp1[,id_mi[1]], y = sp1[, "sigma"], type = "l", xlab = names_mi[1], ylab = %"sigma")
%@
%\end{subfigure}
\caption{\label{varimp} Variable importance: Top 10 list for the forest model.}
\end{center}
\end{figure}

To get another impression of the quality of the predictions made by the tested methods a probability integral transform (PIT) histograms is plotted in Figure \ref{pit_oosample} for out-of-sample predictions of each of the models. A perfectly fitting model would lead to a histogram with all bars being of height 1 as indicated by the red horizontal line. Overall, it can be stated that all four model provide well fitting models while the forest seems to be the one being closest to the ideal case.

  

%\begin{figure}[t!]
%\begin{subfigure}{0.5\textwidth}
%<<rain_axams_pit_insample, fig=TRUE, echo=FALSE>>=
%
%# in sample
%set.seed(7)
%par(mfrow = c(2,2))
%pithist(pit_df_l, nsim = 1000, breaks = seq(0, 1, length.out = 9))
%pithist(pit_g_l, nsim = 1000, breaks = seq(0, 1, length.out = 9))
%pithist(pit_gb_l, nsim = 1000, breaks = seq(0, 1, length.out = 9))
%pithist(pit_ml_l, nsim = 1000, breaks = seq(0, 1, length.out = 9))
%
%@
%\caption{\label{pit_insample} PIT histograms for station Axams (in sample)}
%\end{figure}

\begin{figure}[t!]
\begin{center}
\begin{subfigure}{0.8\textwidth}
<<rain_axams_pit_oosample, fig=TRUE, echo=FALSE, width = 6.5>>=
    
# out of sample
set.seed(7)

par(mfrow = c(2,2))
pithist(pit_df_t, nsim = 1000, breaks = seq(0, 1, length.out = 9), main = "Forest")
pithist(pit_g_t, nsim = 1000, breaks = seq(0, 1, length.out = 9), main = "GAM (prespecified)")
pithist(pit_gb_t, nsim = 1000, breaks = seq(0, 1, length.out = 9), main = "GAM (boosted)")
pithist(pit_ml_t, nsim = 1000, breaks = seq(0, 1, length.out = 9), main = "EMOS")
      
@
\end{subfigure}
\caption{\label{pit_oosample} PIT histograms for station Axams (out of sample).}
\end{center}
\end{figure}

%\begin{figure}[t!]
%\begin{subfigure}{0.5\textwidth}
%<<rain_axams_qqr_insample, fig=TRUE, echo=FALSE>>=
%# in sample
%set.seed(7)
%par(mfrow = c(2,2))
%set.seed(1)
%qqrplot(pit_df_l, nsim = 1, main = "distforest (in bag)")
%qqrplot(pit_g_l, nsim = 1, main = "gamlss (in bag)")
%qqrplot(pit_gb_l, nsim = 1, main = "gamboostLSS (in bag)")
%qqrplot(pit_ml_l, nsim = 1, main = "EMOS (in bag)")

%@
%\caption{\label{qqr_insample} QQR-Plot for station Axams (in sample)}
%\end{figure}

%\begin{figure}[t!]
%%\begin{subfigure}{0.5\textwidth}
%<<rain_axams_qqr_oosample, fig=TRUE, echo=FALSE>>=
%    
%# out of sample
%set.seed(7)
%par(mfrow = c(2,2))
%set.seed(1)
%qqrplot(pit_df_t, nsim = 1, main = "distforest (out of bag)")
%qqrplot(pit_g_t, nsim = 1, main = "gamlss (out of bag)")
%qqrplot(pit_gb_t, nsim = 1, main = "gamboostLSS (out of bag)")
%qqrplot(pit_ml_t, nsim = 1, main = "EMOS (out of bag)")

%@
%\caption{\label{qqr_oosample} QQR-Plot for station Axams (out of sample)}
%\end{figure}


  






\SweaveOpts{eval = TRUE}
%\subsubsection{Cross-Validation}
After analyzing the results of the models on one data set only where visualization is easier, we now want to get a more profound impression of the performances of the different methods. For that reason a cross validation framework is considered for the station Axams. The data set over all 28 years is split randomly into 10 roughly equally sized data sets. While 9 of them are used as learning data the 10th is the testing data for which predictions are made and evaluated. This is done by calculating (the RMSE, the log-likelihood value and) the CRPS value for each of the observations in the testing data and then averaging them. Each of the 10 parts is selected as testing data once which leads to 10 different settings over which, again, the average values are stored. This whole procedure is repeated 10 times. \\
As we are interested in comparing the performance of the distributional forest to those of other models, we choose one model as a reference and calculated the skills score with respect to the CRPS. In that way we obtain a measure of how much a method improves the model fit in terms of CRPS values compared to the reference method.
$$
\text{SkillsScore(method)} = 1 - \frac{\text{CRPS}_{\text{method}}}{\text{CRPS}_{\text{reference}}}
$$
The results regarding the CRPS values are illustrated by the boxplots in Figure \ref{boxplots_crps_axams} where the skills score of the CRPS is the chosen measurement scale with the EMOS model as the reference model represented by the horizontal line at height 0.\\
(The boxplots of the RMSE and the log-likelihood values can be found in the appendix.)
<<echo=FALSE, results=hide>>=
#### cross validation rain
if(file.exists("rain_Axams.rda")){
  #load("~/svn/partykit/pkg/disttree/inst/draft/rain_Axams.rda")
  load("rain_Axams.rda")
} else {
  #source("~/svn/partykit/pkg/disttree/inst/draft/rain_cross.R")
  source("rain_cross.R")
  library("gamlss.cens")
  gen.cens("NO", type = "left")
  rainres <- rain_cross(stationname = "Axams", 
                        seedconst = 7, ntree = 100,
                        forest_mtry = 27,
                        gamboost_cvr = TRUE,
                        frac = FALSE)
  
  save(rainres, file = "rain_Axams.rda")
}

rain_rmse <- matrix(0, ncol = ncol(rainres[[1]]$rmse), length(rainres)-1)
rain_ll <- matrix(0, ncol = ncol(rainres[[1]]$ll), length(rainres)-1)
rain_crps <- matrix(0, ncol = ncol(rainres[[1]]$crps), length(rainres)-1)

for(i in 1:(length(rainres)-1)){
  rain_rmse[i,] <- colMeans(rainres[[i]]$rmse, na.rm = TRUE)
  rain_ll[i,] <- colMeans(rainres[[i]]$ll, na.rm = TRUE)
  rain_crps[i,] <- colMeans(rainres[[i]]$crps, na.rm = TRUE)
}

colnames(rain_rmse) <- colnames(rain_ll) <- colnames(rain_crps) <-
  colnames(rainres[[1]]$rmse)

@


\begin{figure}[t!]
\begin{center}
\center
<<rain_cross_axams_crps_skills_score, fig=TRUE, echo=FALSE, height=4.5>>=
boxplot(1 - rain_crps[,c(2,3,4)] / rain_crps[,6], ylim = c(-0.005, 0.065),
        names = c("Forest", "GAM (prespecified)", "GAM (boosted)"),
        ylab = "CRPS skills score", col = "lightgray") 
abline(h = 0, col = pal["EMOS"], lwd = 2)
@
\center
\caption{\label{boxplots_crps_axams}CRPS skills score with reference model EMOS from the 10x10 cross validation at the station Axams.}
\end{center}
\end{figure}

The CRPS skills score boxplot shows that in this setting both GAM models and the forest perform better than the EMOS model. While the two GAM models lead to an improvement of around 4 percent with the boosted version being slightly ahead, the distforest even reaches an improvement rate of close to 6 percent.\\
From this result we can conclude that the distributional forest provides a framework where only little or almost no effort is required to build a model which performs even better than other methods that are commonly used in this kind of application.\\
For the specified GAM model the covariates needed to be selected and interactions needed to be defined in advance. To do so, meteorological experience and knowledge is required and a lot of thought has to be put in these decisions. Moreover, making such decisions is always equivalent to influencing the resulting model and comes along with the risk of having missed important facts or even imposing certain effects on the model which might not match / correspond to the truth.\\
This risk is avoided in the boosted GAM as all covariates are included and the selection of the influential covariates is done within a boosting framework. However, the trade off here is a remarkably higher computational effort.\\
To build a distributional forest the user does not have to put any additional effort in it and therefore is not required to provide any expert knowledge or additional information about the setting. Variable selection is done automatically and interactions are detected by the tree structure of the model.\\
In this particular setting, a very high number of covariates is available which intensifies / increases the impact of the above mentioned strength of the forest model.\\
Therefore, in comparing these methods one advantage of distributional forests is clearly outlined as they offer a very flexible framework which enables an easy use of the method and at the same time lead to an improvement in performance.

%\newpage
\subsection{Application on all stations}
Until now the focus has been on one observation station only. To show that the presented results do not depend on the choice of this station we are now going to apply the methods on all stations. At each of them, the first 25 years are used as learning data set and predictions are made for the last 3 years of available observations. For these out-of-sample predictions the CRPS skills score with respect to the EMOS model as reference is calculated for each method at each station. In Figure \ref{boxplot_crps_all} the boxplots of the CRPS skills score values are plotted and show that the distributional forest leads to the highest improvement averaged over all stations. The red line represents the results for the station Axams.\\
(Again, the corresponding boxplots of the RMSE and the log-likelihood values can be found in the appendix.)
%NAs in gamlss in 13 stations!!!\\
%for example in 13th station in complete{\_}stations\\
%\text{Error in glim.fit(f = mu.object, X = mu.X, y = y, w = w, fv = mu, os = mu.offset,  :}\\ 
%\text{NA's in the working vector or weights for parameter mu}
%\\
% sum(is.na(crps[,3]))
<<eval = TRUE, echo = FALSE>>=
#### prediction over all stations 25 - 3
if(file.exists("rain_pred.rda")){
  #load("~/svn/partykit/pkg/disttree/inst/draft/rain_pred.rda")
  load("rain_pred.rda")
} else {
  
  source("rain_pred.R")
  library("gamlss.cens")
  gen.cens("NO", type = "left")
  
  res <- rain_pred(seedconst = 7,
                   gamboost_cvr = TRUE,
                   frac = FALSE)
  
  save(res, file = "rain_pred.rda")
}
 
ll <- res[[1]]$results["ll",]
for(i in 2:(length(res)-1)) ll <- rbind(ll, res[[i]]$results["ll",])

rmse <- res[[1]]$results["rmse",]
for(i in 2:(length(res)-1)) rmse <- rbind(rmse, res[[i]]$results["rmse",])

crps <- res[[1]]$results["crps",]
for(i in 2:(length(res)-1))  crps <- rbind(crps, res[[i]]$results["crps",])

colnames(ll) <- colnames(rmse) <- colnames(crps) <- colnames(res[[1]]$results)

# skills score
s <- 1 - crps[, 2:4]/crps[,6]
colnames(s) <- c("Forest", "GAM (prespecified)", "GAM (boosted)")

## prepare date for map which shows where distforest performed better than gamlss or gamboostLSS
# based on the crps skills score (reference model EMOS)
  
library("raster") # dem (digital elevation model)
library("sp")     # gadm www.gadm.org/country

load("plot_map_rain/demo/tirol.gadm.rda")
load("plot_map_rain/demo/tirol.dem.rda")
load("plot_map_rain/demo/ehyd.statlist.rda")
  
# Create SpatialPointsDataFrame from station list
sp <- SpatialPointsDataFrame(subset(ehyd.statlist[res$complete_stations,],
                                    select=c(lon,lat)),
                             data = subset(ehyd.statlist[res$complete_stations,],
                                           select = -c(lon,lat)),
                             proj4string = crs(tirol.dem))

library("colorspace")
crps_ssc_df <- 1 - crps[,2]/crps[,6]
crps_ssc_gb <- 1 - crps[,4]/crps[,6]
crps_ssc_g <- 1 - crps[,3]/crps[,6]
crps_ssc_ml <- 1 - crps[,6]/crps[,6]    # <- rep(0, length(crps_ssc_g))
crps_ssc <-  1- crps/crps[,6]
# abs(crps_ssc_df - crps_ssc_g)
# boxplot(abs(crps_ssc_df - crps_ssc_g))
# hist(abs(crps_ssc_df - crps_ssc_g))

# set the colour according to whether distforest performed better than gamlss and
# set the size of the points on the map according to the difference to the forest model
pal0 <- hcl(c(10, 260, 290, 50), 100, 80)
pal2 <- hcl(c(10, 260, 290, 50), 100, 65)
pal5 <- hcl(c(10, 260, 290, 50), 100, 50)
#pie(rep(1, 3), col = c(pal0[1], pal2[1], pal5[1]))

col_crps_ssc <- numeric(length = length(crps_ssc_df))
crps_ssc_best <- numeric(length = length(crps_ssc_df))
diff_crps_ssc <- numeric(length = length(crps_ssc_df))
transp_col_crps_ssc <- numeric(length = length(crps_ssc_df))
level_col_crps_ssc <- numeric(length = length(crps_ssc_df))

for(i in 1: length(crps_ssc_df)){
  col_crps_ssc[i] <- c(pal["forest"], pal["gamlss"], pal["gamboostLSS"], pal["EMOS"])[which.max(c(crps_ssc_df[i], crps_ssc_g[i], crps_ssc_gb[i], crps_ssc_ml[i]))]
  crps_ssc_best[i] <- which.max(crps_ssc[i,])
  diff_crps_ssc[i] <- crps_ssc[i,crps_ssc_best[i]] - crps_ssc[i,2]
  if(crps_ssc_best[i] == 2){
    reference_crps_ssc <- max(c(crps_ssc_g[i], crps_ssc_gb[i], crps_ssc_ml[i]), na.rm = TRUE)
    diff_crps_df <- crps_ssc_df[i] - reference_crps_ssc
    level_col_crps_ssc[i] <- pal0[1]
    if(diff_crps_df > 0.02) level_col_crps_ssc[i] <- pal2[1]
    if(diff_crps_df > 0.05) level_col_crps_ssc[i] <- pal5[1]
  } else {
    level_col_crps_ssc[i] <- pal0[which.max(c(crps_ssc_df[i], crps_ssc_g[i], crps_ssc_gb[i], crps_ssc_ml[i]))]
    if(diff_crps_ssc[i] > 0.02) level_col_crps_ssc[i] <- pal2[which.max(c(crps_ssc_df[i], crps_ssc_g[i], crps_ssc_gb[i], crps_ssc_ml[i]))]
    if(diff_crps_ssc[i] > 0.05) level_col_crps_ssc[i] <- pal5[which.max(c(crps_ssc_df[i], crps_ssc_g[i], crps_ssc_gb[i], crps_ssc_ml[i]))]
  }
  
}

# ehyd.statlist[res$complete_stations,][col_crps_ssc == pal["gamlss"],]
# table(crps_ssc_best)
# Axams (Station 77) is the 70th complete station
# boxplot(diff_crps_ssc[diff_crps_ssc>0])
# abline(h = diff_crps_ssc[70], col = "red")

# define transperancy of points according to the difference to the forest model (for those points where the forest is not the best one) 
# alpha parameter between 0 and 1
alpha_crps_ssc <- (diff_crps_ssc*10) + 0.15
#alpha_crps_ssc <- (diff_crps_ssc*10/0.85) * 0.5 + 0.5
alpha_crps_ssc[diff_crps_ssc == 0] <- 1
add.alpha <- function(col, alpha=1){
  if(missing(col))
    stop("Please provide a vector of colours.")
  apply(sapply(col, col2rgb)/255, 2, 
                     function(x) 
                       rgb(x[1], x[2], x[3], alpha=alpha))  
}
for(i in 1:length(col_crps_ssc)) transp_col_crps_ssc[i] <- add.alpha(col_crps_ssc[i], alpha = alpha_crps_ssc[i])
@



\begin{figure}[h!]
\begin{center}
<<rain_all_crps_skills_score, fig=TRUE, echo=FALSE>>=
  matplot(t(s[,]), type = "l", lwd = 2, 
          col = c(rep(gray(0.5, alpha = 0.3),69),
                  #gray(0.1, alpha = 0.8), 
                  "brown",
                  rep(gray(0.5, alpha = 0.3),15)), 
          lty = 1, axes = FALSE, 
          xlab = "", ylab = "CRPS skills score", xlim = c(0.5, 3.5))
  # Station Axams is the 77th station which is the 70th in the list of complete stations
  boxplot(s, add = TRUE, col = "transparent")
  abline(h = 0, col = pal["EMOS"], lwd = 2)
  
@
\caption{\label{boxplot_crps_all}CRPS skills score with reference model EMOS for predictions over all stations.}
\end{center}
\end{figure}

On the map in Figure \ref{map} each observation station is represented by a point colored according to the method that performed best in terms of CRPS at this station. For those points where the distributional forest is not the best performing method the intensity of the color represents how much better (in terms of CRPS) the method is compared to the distributional forest. The more intense the color is the bigger the difference. In case of the forest being the best model, the second best method is chosen as reference.

\begin{figure}[h!]
\begin{center}
<<map, fig=TRUE, echo=FALSE, height=6, width=9>>=

  raster::image(tirol.dem, col = rev(gray.colors(100)),
                main="Stations in Tyrol", ylab = "Latitude", xlab = "Longitude", 
                xlim = c(9.8,13.2), 
                ylim = c(46.6, 47.8))
  plot(tirol.gadm, add = TRUE)
  points(sp, pch = 21, bg = level_col_crps_ssc, col = 1, cex = 1.5)
  #points(sp, pch = 21, bg = col_crps_ssc, col = 1)
  #points(sp, pch = 21, bg = transp_col_crps_ssc, col = 1)
  legend("topleft", fill=c(pal["forest"], pal["gamlss"], pal["gamboostLSS"], pal["EMOS"]), legend = c("Forest", "GAM (prespecified)", "GAM (boosted)", "EMOS"), cex = 1, bty = "n")
  
@
\caption{\label{map}Map of all observations with corresponding CRPS skills score for predictions.}
\end{center}
\end{figure}





\section{Discussion}



%\newpage
%\section{References}

%Hothorn T, Hornik K, Zeileis A (2006). 
%``Unbiased Recursive Partitioning: A Conditional Inference Framework.''
%\textit{Journal of Computational and Graphical Statistics}, 
%\textbf{15}(3), 651--674. 
%\doi{10.1198/106186006X133933}\\
%\\
%Zeileis A, Hothorn T, Hornik K (2008). 
%``Model-Based Recursive Partitioning.''
%\textit{Journal of Computational and Graphical Statistics}, 
%\textbf{17}(2), 492--514. 
%\doi{10.1198/106186008X319331}\\
%\\
%Hothorn T, Zeileis A (2015). 
%``partykit: A Modular Toolkit for Recursive Partytioning in R.''
%\textit{Journal of Machine Learning Research}, 
%\textbf{16}, 3905--3909. 
%\url{http://www.jmlr.org/papers/v16/hothorn15a.html}\\
%\\
%Stasinopoulos DM, Rigby RA (2007).
%``Generalized Additive Models for Location Scale and Shape (GAMLSS) in R.''
%\textit{Journal of Statistical Software}, 
%\textbf{23}(7), 1--46.
%\doi{10.18637/jss.v023.i07}\\
%\\
%Stasinopoulos DM, Rigby RA (2005).
%``Generalized Additive Models for Location Scale and Shape (with discussion)''
%\textit{Applied Statistics},
%\textbf{54}(3), 507--554.
%\doi{10.1111/j.1467-9876.2005.00510.x}\\
%\\
%Seibold H, Zeileis A, Hothorn T (2017).
%``Individual Treatment Effect Prediction for Amyotrophic Lateral Sclerosis Patients.''
%\textit{Statistical Methods in Medical Research}, 
%\textbf{12}(1), 45--63.
%\doi{10.1177/0962280217693034}\\
%\\
%Hothorn T, Zeileis A (2017).
%``Transformation Forests.''
%\emph{arXiv 1701.02110}, arXiv.org E-Print Archive.
%\url{http://arxiv.org/abs/1701.02110}\\
%\\
%Messner JW, Mayr GJ, Zeileis A (2016).
%``Heteroscedastic Censored and Truncated Regression with {crch}''
%\textit{The R Journal},
%\textbf{8}(1), 173--181.
%\url{https://journal.R-project.org/archive/2016-1/messner-mayr-zeileis.pdf}\\
%\\
%Zeileis A, Wiel MA, Hornik K, Hothorn T (2008).
%``Implementing a {Class} of {Permutation} {Tests}: {The} {Coin} {Package}''
%\textit{Journal of Statistical Software},
%\textbf{28}(8), 1--23.
%\doi{10.18637/jss.v028.i08}\\
%%publisher={American Statistical Association}
%\\
%Zeileis A, Hothorn T, Hornik K (2010).
%``Party with the {MOB}: {Model-Based} {Recursive} {Partitioning} in {R}''
%\textit{R package version 0.9-9999}.
%\url{https://cran.r-project.org/web/packages/party/vignettes/MOB.pdf}\\
%\\
%Breiman L (2001).
%``Random {Forests}''
%\textit{Machine Learning},
%\textbf{45}(1), 5--32
%%\doi{10.1023/A:1010933404324}
%%publisher={Springer}\\
%\\
%Loh WY (2011).
%``Classification and {Regression} {Trees}''
%\textit{Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
%\textbf{1}(1), 14--23.
%\doi{10.1002/widm.8}
%%publisher={Wiley Online Library}\\
%\\
%Hothorn T (Maintainer), Hornik K, Strobl C, Zeileis A (2015).
%``Package 'party' ''
%\textit{Package Reference Manual for Party Version 0.9--0.998},
%\textbf{16}, 37.\\
%%  \doi{}
%\\
%Yee TW (2010).
%``The {VGAM} {Package} for {Categorical} {Data} {Analysis}''
%\textit{Journal of Statistical Software},
%\textbf{32}(10), 1--34.
%\url{http://www.jstatsoft.org/v32/i10/}\\
%\\
%Umlauf N, Klein N, Zeileis A (2017).
%``{BAMLSS}: {B}ayesian Additive Models for Location, Scale
%  and Shape (and Beyond)''
%\textit{Working Papers in Economics and Statistics, Research
%  Platform Empirical and Experimental Economics, Universit\"at
%  Innsbruck},
%\textbf{2017-05},
%\url{http://EconPapers.RePEc.org/RePEc:inn:wpaper:2017-05}\\
%%type = {Working Paper},
%%number = {2017-05},
%%month = {February},
%\\
%Benjamin Hofner and Andreas Mayr and Nora Fenske and
%  Matthias Schmid (2017). 
%``{gamboostLSS}: Boosting Methods for {GAMLSS} Models''
%\textit{{R} package version 2.0-0}.
%\url{https://CRAN.R-project.org/package=gamboostLSS}\\
%\\
%Liaw A, Wiener M (2002).
%``Classification and Regression by randomForest''
%\textit{R News},
%\textbf{2}(3), 18--22.
%\url{http://CRAN.R-project.org/doc/Rnews/}
%\\
%Gneiting T, Raftery A E, Westveld III A H, Goldman T (2005).
%``Calibrated probabilistic forecasting using ensemble model output statistics and minimum CRPS estimation''
%\textit{Monthly Weather Review},
%\textbf{133}(5), 1098--1118.
%\doi{https://doi.org/10.1175/MWR2904.1}










%\newpage
\nocite{hothorn2006unbiased, zeileis2008model, hothorn2015partykit, Messner2016, loh2011classification, stasinopoulos2007generalized, Seibold2016, Seibold2017, breiman2001random, stasinopoulos2005, yee2010vgam}
% zeileis2008implementing, 
% zeileis2010party, 
% hothorn2015package,

\newpage
\newpage
\bibliography{ref.bib}

\newpage
\section*{Appendix}

\begin{figure}[h!]
\begin{center}
%\begin{subfigure}{0.5\textwidth}
<<rain_cross_axams_rmse, fig=TRUE, echo=FALSE, width=10, height=7>>=
boxplot(rain_rmse[,c(2,3,4,6)], ylab = "RMSE",
        names = c("Forest", "GAM (prespecified)", "GAM (boosted)", "EMOS")) 
@
%\end{subfigure}
\caption{\label{boxplot_rmse_cross}RMSE values of the 10x10 cross validation at the station Axams.}
\end{center}
\end{figure}
%\hspace{-1cm}
\begin{figure}[h!]
\begin{center}
%\begin{subfigure}{0.5\textwidth}
<<rain_cross_axams_ll, fig=TRUE, echo=FALSE, width=10, height=7>>=
boxplot(rain_ll[,c(2,3,4,6)], ylim = c(-2.8,-1.3), ylab = "Log-likelihood",
        names = c("Forest", "GAM (prespecified)", "GAM (boosted)", "EMOS")) 
#boxplot(rain_ll)
@
%\end{subfigure}
%\caption{\label{boxplots_rain}Results 10x10 cross validation at the station Axams\\
%left: RMSE, right: log-likelihood}
\caption{\label{boxplot_cross_ll}Log-likelihood values of the 10x10 cross validation at the station Axams (extracted interval [-2.8,-1.3]).}
\end{center}
\end{figure}


\begin{figure}[h!]
\begin{center}
<<rain_all_rmse, fig=TRUE, echo=FALSE, width=10, height=7.4>>=
  matplot(t(rmse[,c(2,3,4,6)]), type = "l", lwd = 1, 
          col = gray(0.5, alpha = 0.5), lty = 1, axes = FALSE, 
          xlab = "", ylab = "RMSE", xlim = c(0.5, 4.5))
  boxplot(rmse[,c(2,3,4,6)], add = TRUE,
        names = c("Forest", "GAM (prespecified)", "GAM (boosted)", "EMOS")) 
@
\caption{\label{boxplot_rmse_all}RMSE for predictions over all stations.}
\end{center}
\end{figure}


\begin{figure}[h!]
\begin{center}
<<rain_all_ll, fig=TRUE, echo=FALSE, width=10, height=7.4>>=
  matplot(t(ll[,c(2,3,4,6)]), type = "l", lwd = 1, col = gray(0.5, alpha = 0.5), 
          lty = 1, axes = FALSE, 
          xlab = "", ylab = "Log-likelihood", xlim = c(0.5, 4.5), ylim = c(-4,-1))
  boxplot(ll[,c(2,3,4,6)], add = TRUE,
        names = c("Forest", "GAM (prespecified)", "GAM (boosted)", "EMOS")) 
@
\caption{\label{boxplot_ll_all}Log-likelihood values for predictions over all stations (extracted interval [-4,-1]).}
\end{center}
\end{figure}

\end{document}