\documentclass[nojss, shortnames]{jss}

\usepackage{graphicx, natbib}
\pagestyle{empty}
\usepackage{Sweave}
\usepackage{amstext,amsfonts,amsmath,bm,thumbpdf,lmodern}

\title{Supplement A: Different Response Distributions}

\author{Lisa Schlosser\\Universit\"at Innsbruck
\And Torsten Hothorn\\Universit\"at Z\"urich
\And Reto Stauffer\\Universit\"at Innsbruck
\And Achim Zeileis\\Universit\"at Innsbruck}
\Plainauthor{Lisa Schlosser, Torsten Hothorn, Reto Stauffer, Achim Zeileis}


\Abstract{
  In the case study presented in the main manuscript ``Distributional Regression
  Forests for Probabilistic Precipitation Forecasting in Complex Terrain''
  the novel distributional regression forests are based on a Gaussian response distribution,
  left-censored at zero to accommodate both the non-negativity of precipitation and
  the point mass at zero. To assess the goodness of fit of this response distribution
  this supplement employs the same evaluations as in the main manuscript but based on
  two other distributional assumptions: A logistic distribution, left-censored at zero,
  is employed to potentially better capture heavy tails -- and a two-part hurdle model
  combining a binary model for zero vs.\ positive precipitation and a Gaussian model,
  truncated at zero, for the positive precipitation observations. The latter could
  potentially better discriminate determinants of the occurrence of precipitation
  vs.\ the amount of precipitation given that it occurs.
}

\Keywords{random forests, GAMLSS, hurdle model, logistic, Gaussian}



\Address{
Lisa Schlosser, Reto Stauffer, Achim Zeileis \\
Universit\"at Innsbruck \\
Department of Statistics \\
Faculty of Economics and Statistics \\
Universit\"atsstr.~15 \\
6020 Innsbruck, Austria \\
E-mail: \email{Lisa.Schlosser@uibk.ac.at}, \email{Reto.Stauffer@uibk.ac.at},\\
\email{Achim.Zeileis@R-project.org} \\
URL: \url{https://www.uibk.ac.at/statistics/personal/schlosser-lisa/},\\
\phantom{URL: }\url{https://retostauffer.org/},\\
\phantom{URL: }\url{https://eeecon.uibk.ac.at/~zeileis/}\\

Torsten Hothorn\\
Universit\"at Z\"urich \\
Institut f\"ur Epidemiologie, Biostatistik und Pr\"avention \\
Hirschengraben 84\\
CH-8001 Z\"urich, Switzerland \\
E-mail: \email{Torsten.Hothorn@R-project.org}\\
URL: \url{http://user.math.uzh.ch/hothorn/}\\
}


\begin{document}
\SweaveOpts{concordance=TRUE}

\section{Introduction}

This supplement extends the case study presented in Section~3.4 of the main
manuscript by carrying out the same evaluation at 15~stations using two
alternative response distributions for the power-transformed daily precipitation
sums.


To assess how appropriate the distributional assumption (left-censored Gaussian distribution
with censoring point zero; or zero-censored for short) is, two questions are considered:
(1)~Are the tails of the distribution captured appropriately by a Gaussian distribution or
are \emph{heavier tails} required?
(2)~Is the point mass at zero (= no rain) driven by the same determinants as the
positive observations (= rain) or should this be captured by a \emph{separate parameter}?
For (1) we compare the zero-censored Gaussian with a \emph{zero-censored logistic} distribution and
for (2) a Gaussian \emph{hurdle model} (or two-part model with binary and zero-truncated parts)
is employed instead of the zero-censored Gaussian.

Just as in the main manuscript the performance of the distributional forest is also
compared with other commonly-used distributional regression models using the same
data set. The models are learned on data from the first 24 years (1985--2008) and 
evaluated on the following 4 years (2009--2012). Table~2 of the manuscript provides 
an overview of all models considered and their specifications. As a reminder the 
approaches are briefly summarized below.
%
\begin{itemize}

\item \emph{Distributional forest:} All predictor variables are
considered for learning a forest where subsampling is employed 
for each tree. Parameters are estimated by
adaptive local likelihood based on forest weights.

\item \emph{EMOS} \citep{Gneiting+Raftery+Westveld:2005}: 
Ensemble model output statistics uses the ensemble 
mean of total precipitation as regressor in the location submodel 
and the corresponding ensemble standard deviation in the scale submodel.
The parameters are estimated by maximum likelihood.

\item \emph{Prespecified GAMLSS:} Smooth additive splines are selected for
the most relevant predictors based on meteorological expert knowledge
following \cite{Stauffer+Umlauf+Messner:2017}. The model is estimated by maximum
penalized likelihood using a backfitting algorithm \citep{Stasinopoulos+Rigby:2007}.

\item \emph{Boosted GAMLSS:} Smooth additive splines are selected
automatically from all available variables, using non-cyclic boosting
for parameter estimation \citep{Hofner+Mayr+Schmid:2016, Messner+Mayr+Zeileis:2017}.
The optimal stopping point for the iterations is determined based on 
a (computationally intensive) out-of-bag bootstrap estimate of the log-likelihood.

\end{itemize}


\section{Alternative response distributions}

The main manuscript employs a Gaussian distribution with probability
density function $f$ (PDF) and cumulative distribution function $F$ (CDF)
given by
%
\begin{align}
f_{\mathrm{Gaussian}}(Y; \mu, \sigma) &= 
\frac{1}{\sigma} \cdot \phi\left(\frac{Y-\mu}{\sigma}\right) \\
F_{\mathrm{Gaussian}}(Y; \mu, \sigma) &=
\Phi\left(\frac{Y-\mu}{\sigma}\right)
\end{align}
where $\phi$ and $\Phi$ are the PDF and the CDF of the standard normal
distribution $\mathcal{N}(0,1)$. 

An alternative distribution with several similar properties but with
heavier tails is the logistic distribution. The PDF and CDF are given by
%
\begin{align}
f_{\mathrm{logistic}}(Y; \mu, \sigma) &= 
\frac{\exp{\left(-\frac{Y - \mu}{\sigma}\right)}}
{\sigma \cdot \left(1 + \exp{\left(-\frac{Y - \mu}{\sigma}\right)}\right)^2} \\
F_{\mathrm{logistic}}(Y; \mu, \sigma) &=
\frac{1}{1+ \exp{\left(-\frac{Y - \mu}{\sigma}\right)}}.
\end{align}
%
Both distributions have to be adapted in our case study to accommodate that
precipitation is non-negative with a point mass at zero (= no rain).
Two common strategies for this are to employ either \emph{left-censoring at zero}
or a \emph{hurdle} (or two-part) approach, combining a binary (zero vs.\ greater) and
a truncated (positive) part (see \citealp{Long:1997} or \citealp{Winkelmann:2006}
for an overview of both techniques).
%
\begin{itemize}
\item \emph{Censored distribution:}
For a chosen distribution with PDF $f$ and CDF $F$ the log-likelihood function of the corresponding 
zero-censored distribution is
%
\begin{equation}
\ell_{\mathrm{cens}}(\mu, \sigma; Y) = 
\begin{cases}
\log\left\{f(Y; \mu, \sigma) \right\}, & \text{if } Y > 0 \\[0.2cm]
\log\left\{F(0; \mu, \sigma) \right\}, & \text{if } Y = 0
\end{cases}
\end{equation}
%
as stated in Equation~2.1.1 of the main manuscript for the Gaussian special case.
For positive values of $Y$ the censored version of the PDF
equals the original non-censored version. For $Y=0$ the censored density
function takes the values of the probability mass lying left of the 
censoring point. Therefore, the parameters $\mu$ and $\sigma$ control
both aspects, the point mass and the positive part of the distribution.

\item \emph{Hurdle model \citep[or two-part model,][]{Cragg:1971,Mullahy:1986}:}
An additional parameter~$\nu$ describing the probability that there is 
any precipitation at all, i.e., $\nu = \mathbb{P}(Y>0)$, is included in the model.
The three-parameter log-likelihood function for the full hurdle model 
with parameter vector $\bm{\theta} = (\mu, \sigma, \nu)$ is
\begin{equation} \label{eq:hurdle}
\ell_{\mathrm{hurdle}}(\mu, \sigma, \nu; Y) = 
\begin{cases}
\log\left\{\nu \cdot \frac{f(Y; \mu, \sigma)}{1 - F(0; \mu, \sigma)} \right\}, & \text{if } Y > 0 \\[0.2cm]
\log(1 - \nu), & \text{if } Y = 0.
\end{cases}
\end{equation}
For $Y=0$ the probability for no rain is $1-\nu$. 
For $Y>0$ the probability $\nu$ for no rain is multiplied with the 
PDF, left-truncated at zero. In contrast to the censored model, 
there is thus a dedicated parameter $\nu$ controlling the point mass while 
the other parameters $\mu$ and $\sigma$ only influence the distribution of
the positive observations.

\end{itemize}
%
Note that, due to the logs in the first part of Equation~\ref{eq:hurdle},
the log-likelihood of the hurdle model can be separated additively into
a part that depends only on $\nu$ (for all observations) and a part that
depends on $\mu$ and $\sigma$ (for only the positive observations). Therefore,
in a GAMLSS approach, the log-likelihood of the $\nu$ submodel can be
maximized separately from the log-likelihood of the $\mu$ and $\sigma$
submodels to maximize the overall log-likelihood. Hence in all GAMLSS
models considered subsequently a binary logit model is employed for
$Y = 0$ vs.\ $Y > 0$ and a zero-truncated Gaussian model in the case
of $Y > 0$. Similarly, for the distributional forest two separate forests
are employed with a binary and a zero-truncated Gaussian response,
respectively. For comparison with the zero-censored distributions
the combined three-parameter hurdle model as formulated in Equation~\ref{eq:hurdle}
is employed.

All models are evaluated using the continuous ranked probability 
score (CRPS). For the censored models the closed form formulas provided by
\cite{Jordan:2018} are used while a numerical approximation is 
employed for the hurdle models.



\section{Stations}

Out of the 95~observation stations 15 are considered for this 
supplemental study (see Figure~\ref{fig:map}). They have been selected to cover a wide range regarding
geographical location, altitude, and which of the competing models performed
best in the single-split setting reported in Section~3.4 of the main manuscript.

Selected stations: Axams, Lech, Zuers, See im Paznaun, Jungholz, Ladis-Neuegg, 
Oetz, Ochsengarten-Obergut, Ginzling, Rotholz, Walchsee, Koessen, 
Innervillgraten, Matrei in Osttirol, St.~Johann im Walde. See also Supplement~2
(``Stationwise Evaluation'') for more details.

\begin{figure}[t!]
\vspace*{-3.8cm}
\setkeys{Gin}{width=0.99\textwidth}
<<map_stationwise, fig=TRUE, echo=FALSE>>=
##  map
data("StationsTyrol", package = "RainTyrol")
data("MapTyrol", package = "RainTyrol")
library("sp")
sp <- SpatialPointsDataFrame(subset(StationsTyrol, select = c(lon, lat)),
                             data = subset(StationsTyrol, select = -c(lon, lat)),
                             proj4string = raster::crs(MapTyrol$RasterLayer))

plot(MapTyrol$SpatialPolygons)
points(sp, pch = 21, col = "darkgrey", las = 1, cex = 0.6)
points(sp[c(5, 6, 20, 23, 25, 32, 33, 46, 47, 56, 57, 70, 79, 82, 83),], 
       pch = 21, bg = hcl(325, 100, 70), cex = 1.5)
# stationname beneath
text(sp[c(25, 47, 57, 70),], 
     labels = StationsTyrol$name[c(25, 47, 57, 70)],
     pos=1, cex=0.75)
# stationname left
text(sp[c(20, 32, 46, 79, 83),], 
     labels = StationsTyrol$name[c(20, 32, 46, 79, 83)],
     pos=2, cex=0.75)
# stationname above
text(sp[c(6, 33, 82),], 
     labels = StationsTyrol$name[c(6, 33, 82)],
     pos=3, cex=0.75)
# stationname right
text(sp[c(5, 23, 56),], 
     labels = StationsTyrol$name[c(5, 23, 56)],
     pos=4, cex=0.75)
@
\vspace*{-3.8cm}
\caption{\label{fig:map}Map of Tyrol with all 95 observation stations and 
the 15 considered stations highlighted in pink.}
\end{figure}


\pagebreak

\section{Results}

\subsubsection*{CRPS for all distributions by model and station}

Each panel of Figure~\ref{fig:lattice} compares the CRPS across the three distributions
considered: \textit{cgaussian} (zero-censored Gaussian), \textit{hgaussian} (Gaussian hurdle model),
and \textit{clogistic} (zero-censored logistic). The four different models considered
are coded by both colors and symbols and connected by lines for easier comparison between
models. As the average CRPS (i.e., the difficulty of the prediction problem) varies
substantially by station, separate panels are employed for each station.
%
\begin{itemize}

\item For the distributional forest all three distributions perform very similarly for all
stations. Thus, it appears to be very robust against missspecifications of the distribution.

\item Also for the prespecified GAMLSS all three distributions lead to almost the same 
results and no clear advantage of one or the other distribution can be detected.

\item Similarly, for the EMOS model and the boosted GAMLSS all distributions 
lead to comparable CRPS for all stations. However, there is some more variation 
in CRPS across distributions compared to the distributional forest.

\end{itemize}
%
\textit{Note:} When applying the hurdle model at stations Oetz and 
Innvervillgraten the prespecified GAMLSS could not be estimated (using the \proglang{R}
package \pkg{gamlss}) due to numerical issues in the fitting algorithm.
Therefore, this method is not represented in the two corresponding panels 
of Figure~\ref{fig:lattice}.

\begin{figure}[p!]
\setkeys{Gin}{width=0.99\textwidth}
<<results_lattice, fig=TRUE, echo=FALSE, results=hide, height=10, width=8>>=
library("lattice")
##### 
# HCL palette
pal <- hcl(c(10, 128, 260, 290, 50), 100, 50)

load("Rain_distributions.rda")
levels(results$means$distribution) <- c("cgaussian", "hgaussian", "clogistic")
levels(results$means$station)[15] <- "St. Johann im Walde"

stations <- c("Axams", "Lech", "Zuers", "See im Paznaun", "Jungholz", 
              "Ladis-Neuegg", "Oetz", "Ochsengarten-Obergut",
              "Ginzling", "Rotholz", "Walchsee", "Koessen", 
              "Innervillgraten", "Matrei in Osttirol", 
              "St. Johann im Walde")

means <- results$means[(results$means$method != "disttree") & 
                         results$means$station %in% stations, ]
rownames(means) <- c(1:NROW(means))
means$station <- factor(means$station, levels = stations)
means$method <- factor(means$method, 
                       levels = c("distforest",
                                  "gamlss",
                                  "gamboostLSS",
                                  "EMOS"),
                       labels = c("Distributional forest",
                                  "Prespecified GAMLSS",
                                  "Boosted GAMLSS",
                                  "EMOS"))

strip.background.settings <- trellis.par.get("strip.background")
strip.background.settings$col <- "gray"
trellis.par.set("strip.background", strip.background.settings)

strip.border.settings <- trellis.par.get("strip.border")
strip.border.settings$col <- "black"
trellis.par.set("strip.border", strip.border.settings)

superpose.line.settings <- trellis.par.get("superpose.line")
superpose.line.settings$col <- hcl(c(128, 260, 290, 50), 100, 50)
trellis.par.set("superpose.line", superpose.line.settings)

superpose.symbol.settings <- trellis.par.get("superpose.symbol")
superpose.symbol.settings$pch <- c(16,24,25,15)
superpose.symbol.settings$col <- hcl(c(128, 260, 290, 50), 100, 50)
superpose.symbol.settings$fill <- hcl(c(128, 260, 290, 50), 100, 50)
trellis.par.set("superpose.symbol", superpose.symbol.settings)

xyplot(crps ~ distribution | station, groups = ~ method, 
       data = means, auto.key = TRUE, 
       type = "o", lwd = 2, lty = 1,  
       drop.unused.levels = TRUE, layout = c(3,5),
       as.table = TRUE, ylab = "CRPS", xlab = "Distribution")
@
\caption{\label{fig:lattice}CRPS values (lower = better) across the three
distributions for all four models and 15~stations.}
\end{figure}

\subsubsection*{CRPS skill score in reference to zero-censored Gaussian}

To aggregate the CRPS results across stations, CRPS skill scores are visualized
with boxplots in Figure~\ref{fig:box_crpss_gaussian}, using the zero-censored Gaussian distribution
as the reference distribution (orange horizontal line at zero). The underlying skill
scores for each station are also included in the graphic by gray lines.
%
\begin{itemize}

\item As before, there are only slight variations for the distributional forest, mostly
  between $\pm 1$\%. While the median CRPS skill score over the 15 stations for the 
  Gaussian hurdle model is slightly better than for the simpler zero-censored Gaussian
  model, this advantage is very small and not systematic across stations.
  
\item Again, for the prespecified GAMLSS the results are very similar as for the 
  distributional forest showing only a slight advantage of the Gaussian hurdle model, 
  however, not for all stations. 
  %The zero-censored logistic distribution performs on average equally
  %well as the the zero-censored Gaussian distribution.
  
\item For the boosted GAMLSS, the Gaussian hurdle is quite close to the zero-censored
  Gaussian reference but shows a high variation, while the zero-censored logistic 
  performs slightly better.

\item For EMOS, the median skill score for both Gaussian hurdle and zero-censored logistic
  are very close to the zero-censored Gaussian reference. However, as for the boosted GAMLSS
  the variation for the hurdle model is much higher than for the censored model.
  
\end{itemize}

\begin{figure}[t!]
\setkeys{Gin}{width=0.99\textwidth}
<<results_box_crpss_gaussian, fig=TRUE, echo=FALSE, height=3, width=7.2>>=
# crps skill score by distribution (reference: gaussian)
means$crps_ss_dist <- means$crps
means$crps_ss_dist[means$distribution == "cgaussian"] <- 
  1 - means$crps[means$distribution == "cgaussian"] / means$crps[means$distribution == "cgaussian"]
means$crps_ss_dist[means$distribution == "clogistic"] <- 
  1 - means$crps[means$distribution == "clogistic"] / means$crps[means$distribution == "cgaussian"]
means$crps_ss_dist[means$distribution == "hgaussian"] <- 
  1 - means$crps[means$distribution == "hgaussian"] / means$crps[means$distribution == "cgaussian"]

means_sel <- means[means$distribution %in% c("clogistic", "hgaussian"),]
means_sel$distribution <- factor(means_sel$distribution, 
                                 levels(means_sel$distribution)[c(2:3)])

# CRPS skill score for distributional forest
s_df <- matrix(ncol = 2, nrow = 15)
s_df[,1] <- means_sel[means_sel$method == "Distributional forest" & means_sel$distribution == "hgaussian", "crps_ss_dist"]
s_df[,2] <- means_sel[means_sel$method == "Distributional forest" & means_sel$distribution == "clogistic", "crps_ss_dist"]

# CRPS skill score for prespecified GAMLSS
s_g <- matrix(ncol = 2, nrow = 15)
s_g[,1] <- means_sel[means_sel$method == "Prespecified GAMLSS" & means_sel$distribution == "hgaussian", "crps_ss_dist"]
s_g[,2] <- means_sel[means_sel$method == "Prespecified GAMLSS" & means_sel$distribution == "clogistic", "crps_ss_dist"]

# CRPS skill score for boosted GAMLSS
s_gb <- matrix(ncol = 2, nrow = 15)
s_gb[,1] <- means_sel[means_sel$method == "Boosted GAMLSS" & means_sel$distribution == "hgaussian", "crps_ss_dist"]
s_gb[,2] <- means_sel[means_sel$method == "Boosted GAMLSS" & means_sel$distribution == "clogistic", "crps_ss_dist"]

# CRPS skill score for EMOS
s_emos <- matrix(ncol = 2, nrow = 15)
s_emos[,1] <- means_sel[means_sel$method == "EMOS" & means_sel$distribution == "hgaussian", "crps_ss_dist"]
s_emos[,2] <- means_sel[means_sel$method == "EMOS" & means_sel$distribution == "clogistic", "crps_ss_dist"]

colnames(s_df) <- colnames(s_g) <- colnames(s_gb) <- colnames(s_emos) <- c("hgaussian", "clogistic")


# boxplots with matplot
par(mfrow = c(1,4), mar = c(4,4,3,0.7))
matplot(t(s_df[,]), type = "l", lwd = 2, 
        col = gray(0.5, alpha = 0.2),
        lty = 1, axes = FALSE,  main = "Distributional forest",
        xlab = "", ylab = "CRPS skill score", xlim = c(0.5, 2.5),
        ylim = c(-0.05, 0.05))
boxplot(s_df, add = TRUE, col = "transparent")
abline(h = 0, col = pal[5], lwd = 2)
#boxplot(crps_ss_dist~distribution, data = means_sel, 
#        subset = (method == "Distributional forest"), main = "Distributional forest",
#        ylim = c(-0.12,0.07), las = 2, ylab = "CRPS skill score")
#abline(h = 0, col = pal[5], lwd = 2)
par(mar = c(4,2.9,3,1.8))
matplot(t(s_g[,]), type = "l", lwd = 2, 
        col = gray(0.5, alpha = 0.2),
        lty = 1, axes = FALSE, main = "Prespecified GAMLSS",
        xlab = "", ylab = "", xlim = c(0.5, 2.5),
        ylim = c(-0.05, 0.05))
boxplot(s_g, add = TRUE, col = "transparent", yaxt = 'n')
abline(h = 0, col = pal[5], lwd = 2)
#boxplot(crps_ss_dist~distribution, data = means_sel, 
#        subset = (method == "Prespecified GAMLSS"), main = "Prespecified GAMLSS",
#        ylim = c(-0.12,0.07), las = 2)
#abline(h = 0, col = pal[5], lwd = 2)
par(mar = c(4,1.8,3,2.9))
matplot(t(s_gb[,]), type = "l", lwd = 2, 
        col = gray(0.5, alpha = 0.2),
        lty = 1, axes = FALSE, main = "Boosted GAMLSS",
        xlab = "", ylab = "", xlim = c(0.5, 2.5),
        ylim = c(-0.05, 0.05))
boxplot(s_gb, add = TRUE, col = "transparent", yaxt = 'n')
abline(h = 0, col = pal[5], lwd = 2)
#boxplot(crps_ss_dist~distribution, data = means_sel, 
#        subset = (method == "Boosted GAMLSS"), main = "Boosted GAMLSS",
#        ylim = c(-0.12,0.07), las = 2)
#abline(h = 0, col = pal[5], lwd = 2)
par(mar = c(4,0.7,3,4))
matplot(t(s_emos[,]), type = "l", lwd = 2, 
        col = gray(0.5, alpha = 0.2),
        lty = 1, axes = FALSE,  main = "EMOS",
        xlab = "", ylab = "", xlim = c(0.5, 2.5),
        ylim = c(-0.05, 0.05))
boxplot(s_emos, add = TRUE, col = "transparent", yaxt = 'n')
abline(h = 0, col = pal[5], lwd = 2)
#boxplot(crps_ss_dist~distribution, data = means_sel, 
#        subset = (method == "EMOS"), main = "EMOS",
#        ylim = c(-0.12,0.07), las = 2)
#abline(h = 0, col = pal[5], lwd = 2)
@
\caption{\label{fig:box_crpss_gaussian} CRPS skill score (higher = better)
for hurdle Gaussian and censored logistic distributions in reference to the
censored Gaussian distribution (orange horizontal line). Gray lines
correspond to the 15~stations that are aggregated in the boxplot.}
\end{figure}

%\subsubsection*{FIXME: FURTHER GRAPHICS?}

%\emph{The same data already shown in Figures~\ref{fig:lattice} 
%and~\ref{fig:box_crpss_gaussian} can be displayed in further ways. However, these do 
%not really add much and hence should probably be omitted.}

%Figures~\ref{fig:box_crpss_EMOS} and \ref{fig:box_crps} compare the methods separately 
%for each distribution, once based on CRPS skill score values with reference model EMOS 
%and once based on CRPS values. Both confirm the conclusion that all three distributions
%lead to almost the same results for all methods.

Altogether it can be stated that for this application the zero-censored Gaussian
distribution seems to be an appropriate choice while the other two distributions are also
reasonable alternatives. However, they do not lead to a pronounced overall improvement in 
performance for this application.


%\begin{figure}[t!]
%\setkeys{Gin}{width=0.99\textwidth}
<<results_box_crpss_EMOS, fig=TRUE, echo=FALSE, height=3, width=7.2, eval=FALSE>>=
# crps skill score by method (reference: EMOS)
means$crps_ss_method <- means$crps
means$crps_ss_method[means$method == "Distributional forest"] <- 
  1 - means$crps[means$method == "Distributional forest"] / means$crps[means$method == "EMOS"]
means$crps_ss_method[means$method == "Prespecified GAMLSS"] <- 
  1 - means$crps[means$method == "Prespecified GAMLSS"] / means$crps[means$method == "EMOS"]
means$crps_ss_method[means$method == "Boosted GAMLSS"] <- 
  1 - means$crps[means$method == "Boosted GAMLSS"] / means$crps[means$method == "EMOS"]
means$crps_ss_method[means$method == "EMOS"] <- 
  1 - means$crps[means$method == "EMOS"] / means$crps[means$method == "EMOS"]

means_sel <- means[means$method %in% c("Distributional forest", "Prespecified GAMLSS", "Boosted GAMLSS"),]
means_sel$method <- factor(means_sel$method, levels(means_sel$method)[c(1:3)])



# CRPS skill score for gaussian
s_gaussian <- matrix(ncol = 3, nrow = 15)
s_gaussian[,1] <- means_sel[means_sel$method == "Distributional forest" & means_sel$distribution == "cgaussian", "crps_ss_method"]
s_gaussian[,2] <- means_sel[means_sel$method == "Prespecified GAMLSS" & means_sel$distribution == "cgaussian", "crps_ss_method"]
s_gaussian[,3] <- means_sel[means_sel$method == "Boosted GAMLSS" & means_sel$distribution == "cgaussian", "crps_ss_method"]

# CRPS skill score for boosted hgaussian
s_hgaussian <- matrix(ncol = 3, nrow = 15)
s_hgaussian[,1] <- means_sel[means_sel$method == "Distributional forest" & means_sel$distribution == "hgaussian", "crps_ss_method"]
s_hgaussian[,2] <- means_sel[means_sel$method == "Prespecified GAMLSS" & means_sel$distribution == "hgaussian", "crps_ss_method"]
s_hgaussian[,3] <- means_sel[means_sel$method == "Boosted GAMLSS" & means_sel$distribution == "hgaussian", "crps_ss_method"]

# CRPS skill score for prespecified logistic
s_logistic <- matrix(ncol = 3, nrow = 15)
s_logistic[,1] <- means_sel[means_sel$method == "Distributional forest" & means_sel$distribution == "clogistic", "crps_ss_method"]
s_logistic[,2] <- means_sel[means_sel$method == "Prespecified GAMLSS" & means_sel$distribution == "clogistic", "crps_ss_method"]
s_logistic[,3] <- means_sel[means_sel$method == "Boosted GAMLSS" & means_sel$distribution == "clogistic", "crps_ss_method"]

colnames(s_gaussian) <- colnames(s_hgaussian) <- colnames(s_logistic) <- c("Distributional forest", "Prespecified GAMLSS", "Boosted GAMLSS")


par(mfrow = c(1,3), mar = c(5,4,3,0))
matplot(t(s_gaussian[,]), type = "l", lwd = 2, 
        col = gray(0.5, alpha = 0.2),
        lty = 1, axes = FALSE,  main = "cgaussian", ylim = c(-0.12,0.23), 
        xlab = "", ylab = "CRPS skill score", xlim = c(0.5, 3.5))
boxplot(s_gaussian, add = TRUE, col = "transparent", axes = FALSE)
abline(h = 0, col = pal[5], lwd = 2)
#boxplot(crps_ss_method~method, data = means_sel, 
#        subset = (distribution == "cgaussian"), main = "cgaussian",
#        ylim = c(-0.12,0.23), axes = FALSE, ylab = "CRPS skill score")
axis(1, 0:4, c("",
               "Distr.
                 forest",
               "Presp.
                 GAMLSS",
               "Boosted
                 GAMLSS", ""), las=2)
axis(2, seq(-0.15, 0.25, 0.05), c(seq(-0.15, 0, 0.05), seq(0.05, 0.25, 0.05)), las = 0)
axis(3, 0:4, lwd.ticks = 0, labels = FALSE)
axis(4, seq(-0.15, 0.25, 0.05), lwd.ticks = 0, labels = FALSE)

par(mar = c(5,2,3,2))
matplot(t(s_hgaussian[,]), type = "l", lwd = 2, 
        col = gray(0.5, alpha = 0.2),
        lty = 1, axes = FALSE,  main = "hgaussian", ylim = c(-0.12,0.23), 
        xlab = "", ylab = "", xlim = c(0.5, 3.5))
boxplot(s_hgaussian, add = TRUE, col = "transparent", axes = FALSE)
abline(h = 0, col = pal[5], lwd = 2)
#boxplot(crps_ss_method~method, data = means_sel, 
#        subset = (distribution == "hgaussian"), main = "hgaussian",
#        ylim = c(-0.12,0.23), axes = FALSE)
axis(2, seq(-0.15, 0.25, 0.05), lwd.ticks = 0, labels = FALSE)
axis(1, 0:4, c("",
               "Distr.
                 forest",
               "Presp.
                 GAMLSS",
               "Boosted
                 GAMLSS", ""), las=2)
axis(3, 0:4, lwd.ticks = 0, labels = FALSE)
axis(4, seq(-0.15, 0.25, 0.05), lwd.ticks = 0, labels = FALSE)

par(mar = c(5,0,3,4))
matplot(t(s_logistic[,]), type = "l", lwd = 2, 
        col = gray(0.5, alpha = 0.2),
        lty = 1, axes = FALSE,  main = "clogistic", ylim = c(-0.12,0.23), 
        xlab = "", ylab = "", xlim = c(0.5, 3.5))
boxplot(s_logistic, add = TRUE, col = "transparent", axes = FALSE)
abline(h = 0, col = pal[5], lwd = 2)
#boxplot(crps_ss_method~method, data = means_sel, 
#        subset = (distribution == "clogistic"), main = "clogistic",
#        ylim = c(-0.12,0.23), axes = FALSE)
axis(2, seq(-0.15, 0.25, 0.05), lwd.ticks = 0, labels = FALSE)
axis(1, 0:4, c("",
               "Distr.
                 forest",
               "Presp.
                 GAMLSS",
               "Boosted
                 GAMLSS", ""), las=2)
axis(3, 0:4, lwd.ticks = 0, labels = FALSE)
axis(4, seq(-0.15, 0.25, 0.05), lwd.ticks = 0, labels = FALSE)

@
%\caption{\label{fig:box_crpss_EMOS}CRPS skill score values of each of 
%the 15 observation stations, separated by method and distribution. 
%EMOS is chosen as reference model.}
%\end{figure}


%\begin{figure}[t!]
%\setkeys{Gin}{width=0.99\textwidth}
<<results_box_crps, fig=TRUE, echo=FALSE, height=3, width=7.2, eval=FALSE>>=

# CRPS for gaussian
c_gaussian <- matrix(ncol = 4, nrow = 15)
c_gaussian[,1] <- means[means$method == "Distributional forest" & means$distribution == "cgaussian", "crps"]
c_gaussian[,2] <- means[means$method == "Prespecified GAMLSS" & means$distribution == "cgaussian", "crps"]
c_gaussian[,3] <- means[means$method == "Boosted GAMLSS" & means$distribution == "cgaussian", "crps"]
c_gaussian[,4] <- means[means$method == "EMOS" & means$distribution == "cgaussian", "crps"]

# CRPS for hgaussian
c_hgaussian <- matrix(ncol = 4, nrow = 15)
c_hgaussian[,1] <- means[means$method == "Distributional forest" & means$distribution == "hgaussian", "crps"]
c_hgaussian[,2] <- means[means$method == "Prespecified GAMLSS" & means$distribution == "hgaussian", "crps"]
c_hgaussian[,3] <- means[means$method == "Boosted GAMLSS" & means$distribution == "hgaussian", "crps"]
c_hgaussian[,4] <- means[means$method == "EMOS" & means$distribution == "hgaussian", "crps"]

# CRPS for logistic
c_logistic <- matrix(ncol = 4, nrow = 15)
c_logistic[,1] <- means[means$method == "Distributional forest" & means$distribution == "clogistic", "crps"]
c_logistic[,2] <- means[means$method == "Prespecified GAMLSS" & means$distribution == "clogistic", "crps"]
c_logistic[,3] <- means[means$method == "Boosted GAMLSS" & means$distribution == "clogistic", "crps"]
c_logistic[,4] <- means[means$method == "EMOS" & means$distribution == "clogistic", "crps"]

colnames(c_gaussian) <- colnames(c_hgaussian) <- colnames(c_logistic) <- c("Distributional forest", "Prespecified GAMLSS", "Boosted GAMLSS", "EMOS")


par(mfrow = c(1,3), mar = c(5,4,3,0))
matplot(t(c_gaussian[,]), type = "l", lwd = 2, 
        col = gray(0.5, alpha = 0.2),
        lty = 1, axes = FALSE,  main = "cgaussian", 
        xlab = "", ylab = "CRPS", xlim = c(0.5, 4.5),
        ylim = c(0.65,1.15))
boxplot(c_gaussian, add = TRUE, col = "transparent", axes = FALSE)
#boxplot(crps~method, data = means, subset = distribution == "cgaussian", main = "cgaussian",
#        ylim = c(0.65,1.15), axes = FALSE, ylab = "CRPS")
axis(2, seq(0.6, 1.2, 0.1), seq(0.6, 1.2, 0.1), las = 0)
axis(1, 0:5, c("",
               "Distr.
                 forest",
               "Presp.
                 GAMLSS",
               "Boosted
                 GAMLSS",
               "EMOS", ""), las=2)
axis(3, 0:5, lwd.ticks = 0, labels = FALSE)
axis(4, seq(0.6, 1.2, 0.1), lwd.ticks = 0, labels = FALSE)

par(mar = c(5,2,3,2))
matplot(t(c_hgaussian[,]), type = "l", lwd = 2, 
        col = gray(0.5, alpha = 0.2),
        lty = 1, axes = FALSE,  main = "hgaussian", 
        xlab = "", ylab = "", xlim = c(0.5, 4.5),
        ylim = c(0.65,1.15))
boxplot(c_hgaussian, add = TRUE, col = "transparent", axes = FALSE)
#boxplot(crps~method, data = means, subset = distribution == "hgaussian", main = "hgaussian",
#        ylim = c(0.65,1.15), las = 2, axes = FALSE)
axis(2, seq(0.6, 1.2, 0.1), lwd.ticks = 0, labels = FALSE)
axis(1, 0:5, c("",
               "Distr.
                 forest",
               "Presp.
                 GAMLSS",
               "Boosted
                 GAMLSS",
               "EMOS", ""), las=2)
axis(3, 0:5, lwd.ticks = 0, labels = FALSE)
axis(4, seq(0.6, 1.2, 0.1), lwd.ticks = 0, labels = FALSE)

par(mar = c(5,0,3,4))
matplot(t(c_logistic[,]), type = "l", lwd = 2, 
        col = gray(0.5, alpha = 0.2),
        lty = 1, axes = FALSE,  main = "clogistic", 
        xlab = "", ylab = "CRPS", xlim = c(0.5, 4.5),
        ylim = c(0.65,1.15))
boxplot(c_logistic, add = TRUE, col = "transparent", axes = FALSE)
#boxplot(crps~method, data = means, subset = distribution == "clogistic", main = "clogistic",
#        ylim = c(0.65,1.15), las = 2, axes = FALSE)
axis(2, seq(0.6, 1.2, 0.1), lwd.ticks = 0, labels = FALSE)
axis(1, 0:5, c("",
               "Distr.
                 forest",
               "Presp.
                 GAMLSS",
               "Boosted
                 GAMLSS",
               "EMOS", ""), las=2)
axis(3, 0:5, lwd.ticks = 0, labels = FALSE)
axis(4, seq(0.6, 1.2, 0.1), lwd.ticks = 0, labels = FALSE)

@
%\caption{\label{fig:box_crps}CRPS values of each of the 15 observation stations, 
%separated by method and distribution.}
%\end{figure}


\section{Discussion}

This supplement shows that the zero-censored Gaussian models from the main
manuscript and the alternative distributional specifications in this
supplement perform at least similarly. For some combinations of models and
stations, the Gaussian hurdle leads to small improvements on average but
typically with more variability (especially for EMOS and the boosted
GAMLSS). Furthermore, for the boosted GAMLSS the zero-censored logistic
distribution shows slightly better results, however, only within a range of
less than $2\%$ and not for all stations.
Therefore, the main manuscript focuses on the zero-censored
Gaussian model which is well-established in the probabilistic
precipitation forecasting literature. Moreover, the distributional forest
can be shown to be rather robust against distributional
(mis-)specifications compared to the other models.

% The comparison of the zero-censored Gaussian models from the main manuscript with the
% alternative distributional specifications in this supplement reveals that a Gaussian hurdle
% may lead to small improvements. However, especially for EMOS and the boosted GAMLSS model
% the outcome is also considerably more variable. Therefore, overall there is no convincing
% enough improvement over the zero-censored Gaussian model which is therefore the focus of
% the main manuscript. Also, the distributional forest appears to be more robust against
% distributional (mis-)specifications compared to the other models.

As a final consideration, we briefly remark that the distributional forest also allows
for a different hurdle model specification. Previously, we argued that for GAMLSS/EMOS
the log-likelihood of the hurdle model can be separated additively and hence the maximum
likelihood fit can be obtained by fitting two separate models for $\nu$ on one hand and
$\mu$ and $\sigma$ on the other. And therefore the same two-part approach was also adopted
for the distributional forests. However, an additional alternative is to fit a single
distributional forest based on the three-parameter log-likelihood from Equation~\ref{eq:hurdle}.
When all three parameters change simultaneously across the covariates, then this specification
can be expected to perform somewhat better. Whereas when different variables/effects drive
the different model parameters, then the two-part approach implemented in this supplement
should perform somewhat better. In this particular case study, though, neither approach
performs clearly better as Figure~\ref{fig:box_crpss_forests} reveals. The figure depicts
CRPS skill scores for the 15~stations of the one-part hurdle models with the two-part
hurdle model as the reference. Clearly, both hurdle specifications perform very
similarly with skill scores being mostly below 1\%.

\begin{figure}[t!]
\centering
\setkeys{Gin}{width=0.5\textwidth}
<<compare_forests, fig=TRUE, results=hide, echo=FALSE, width=4, height=4>>=
load("crps_forests.rda")
crps_ss_forest_2p <- data.frame(1 - crps_forests[,"forest_2p"] / crps_forests[,"forest_h"])
names(crps_ss_forest_2p) <- "Three-parametric forest"
boxplot(crps_ss_forest_2p, ylab = "CRPS skill score", names = c("Three-parametric forest"))
abline(h = 0, col = pal[5], lwd = 2)
axis(1, 0:2, c("","One-part vs. two-part hurdle forest", ""), las=1)
@
\caption{\label{fig:box_crpss_forests}CRPS skill score (higher = better) for the one-part
hurdle distributional forest in reference to the two part hurdle distributional forest
(orange horizontal line).
}
\end{figure}


\bibliography{ref_supplement.bib}


\end{document}
