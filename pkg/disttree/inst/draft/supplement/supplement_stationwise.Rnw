\documentclass[nojss, shortnames]{jss}

\usepackage{graphicx, natbib}
\pagestyle{empty}
\usepackage{Sweave}
\usepackage{amstext,amsfonts,amsmath,bm,thumbpdf,lmodern,pdfpages,hyperref,bookmark}

\title{Supplement 2: Stationwise Evaluation}

\author{Lisa Schlosser\\Universit\"at Innsbruck
   \And Torsten Hothorn\\Universit\"at Z\"urich
   \And Reto Stauffer\\Universit\"at Innsbruck
   \And Achim Zeileis\\Universit\"at Innsbruck}
\Plainauthor{Lisa Schlosser, Torsten Hothorn, Reto Stauffer, Achim Zeileis}


\Abstract{In the application discussed in the paper ''Distributional 
Regression Forests for Probabilistic Precipitation Forecasting in Complex Terrain''
the presented method is evaluated on 95 observation stations. More extensive
evaluations including a cross-validation are preformed for one station, Axams.
In this supplement this exact same analysis as for station Axams is
made for 14 further observation stations.}

\Keywords{random forests, GAMLSS, evaluation, meteorological stations}



\Address{
  Lisa Schlosser, Reto Stauffer, Achim Zeileis \\
  Universit\"at Innsbruck \\
  Department of Statistics \\
  Faculty of Economics and Statistics \\
  Universit\"atsstr.~15 \\
  6020 Innsbruck, Austria \\
  E-mail: \email{Lisa.Schlosser@uibk.ac.at}, \email{Reto.Stauffer@uibk.ac.at},\\
  \email{Achim.Zeileis@R-project.org} \\
  URL: \url{https://www.uibk.ac.at/statistics/personal/schlosser-lisa/},\\
  \phantom{URL: }\url{https://retostauffer.org/},\\
  \phantom{URL: }\url{https://eeecon.uibk.ac.at/~zeileis/}\\
  
  Torsten Hothorn\\
  Universit\"at Z\"urich \\
  Institut f\"ur Epidemiologie, Biostatistik und Pr\"avention \\
  Hirschengraben 84\\
  CH-8001 Z\"urich, Switzerland \\
  E-mail: \email{Torsten.Hothorn@R-project.org}\\
  URL: \url{http://user.math.uzh.ch/hothorn/}\\
  
}


\begin{document}
\SweaveOpts{concordance=TRUE}

\section{Introduction}
This application study is supplementary material to the application in Section~3 of 
the paper where the presented distributional forest model is employed to obtain 
probabilistic precipitation forecasts in a mountainous region.
Next to an evaluation over all 95 observation stations further investigations
including a cross-validation have been performed for one selected station, Axams.
Here, this more extensive evaluation is carried out, applying the exact same settings 
as in the paper, for Axams and 14 further observation stations. Additionally, computation
times are measured.


\section{Models and evaluation}
As in the paper a distributional forest applying a left-censored at zero 
Gaussian distribution is evaluated together with three other zero-censored 
Gaussian models. Further details on all four models can be found in Section~3.2. of
the paper. In particular, Table~2 gives an overview over the models and their 
specifications. As a brief reminder the approaches are summarized below.
%
\begin{itemize}

\item \emph{Distributional forest:} All predictor variables are
considered for learning a forest where subsampling is employed 
for each tree. Parameters are estimated by
adaptive local likelihood based on forest weights.

\item \emph{EMOS} \citep{Gneiting+Raftery+Westveld:2005}: 
Standard ensemble model output statistics models use the ensemble 
mean of total precipitation as regressor in the location submodel 
and the corresponding ensemble standard deviation in the scale submodel.
The parameters are estimated by maximum likelihood.

\item \emph{Prespecified GAMLSS:} Smooth additive splines are selected for
the most relevant predictors based on meteorological expert knowledge
following \cite{Stauffer+Umlauf+Messner:2017}. The model is estimated by maximum
penalized likelihood using a backfitting algorithm (\cite{Stasinopoulos+Rigby:2007}).

\item \emph{Boosted GAMLSS:} Smooth additive splines are selected
automatically from all available variables, using non-cyclic boosting
for parameter estimation (\cite{Hofner+Mayr+Schmid:2016, Messner+Mayr+Zeileis:2017}).
The optimal stopping point for the iteration is evaluated based on 
a (computationally intensive) out-of-bag bootstrap estimate of the log-likelihood.

\end{itemize}

%\subsection{Subset selection}
The models are always learned on data from 24 years and tested on the 
remaining 4 years. Splitting the data into these two subsets has been done
in two different ways leading to the two following evaluation settings.
%
\begin{itemize}

\item Single split: The data is time-ordered. The learning data consists of the first 24 years (1985--2008) and the testing data of the following 4 years (2009--2012). 

\item Cross-validation: In order to not just split the data at one splitpoint the 28 years are randomly split in 7 blocks of 4 years which are not necessarily successive. Each of the blocks is chosen once as testing data while the remaining 6 blocks are the learning data. This procedure is repeated 10 times resulting in a 10 times 7-fold cross-validation framework.

\end{itemize}

The performances of the models are compared based on numerical and graphical evaluations.

\textit{Numerical:}
For the single split setting one value is calculated for each observation in the test data and the mean of these values is considered. For the cross-validation this mean value over all observations in the test data is retrieved from all 70 evaluations and then again the mean of them is considered. This holds for the following measurements.
%
\begin{itemize}

\item CRPS: The continuous ranked probability score (CRPS) is calculated to assess the predictive performance in terms of full probabilistic forecasts.

\item Time: For all models fitting times and predictions times are measured. In particular, \textit{fit time} refers to the time needed to fit the model to the whole learn data. Depending on the setting \textit{predict time} refers to the time needed to make one prediction in the single split setting and to make predictions for the whole test data in the cross-validation.

\end{itemize}


\textit{Graphical:} The following methods of visualization are applied to provide further means of comparison of all models but also to investigate on the distributional forest in particular.
%
\begin{itemize}
%
\item Boxplot of the CRPS skill score: With the EMOS as reference model the CRPS skill scores are calculated for the cross-validation setting and illustrated in a boxplot.
%
\item Barplot of variable importance: For the forest model it is investigated which variables have the strongest influence on the model and its goodness of fit. For each variable this is measured based on mean decrease in CRPS after permutation. The 10 variables showing the highest values in the single split setting are listed in a barplot.
%
\item Residual QQ plots: For each of the models an out-of-sample residual QQ plot for the single split setting provides a visualization of goodness of fit. Randomized residual quantiles are compared to standard normal quantiles.
%
\item PIT histograms: As an additional visualization out-of-sample probability integral transform histograms illustrate how well-calibrated the distributional fits are for the single split setting.
%
\end{itemize}


\section{Stations}
Out of the 95 observation stations 15 are considered for this 
benchmark study. They have been selected based on the region
they are in, their altitude and the performance of the 
different models in the application in the paper such that
all of these features are represented equally.

Selected stations: \hyperlink{sumAxams.1}{Axams},
\hyperlink{sumLech.1}{Lech},
\hyperlink{sumZuers.1}{Zuers},
\hyperlink{sumSeeimPaznaun.1}{See im Paznaun},
\hyperlink{sumJungholz.1}{Jungholz},
\hyperlink{sumLadisNeuegg.1}{Ladis-Neuegg},
\hyperlink{sumOetz.1}{Oetz},
\hyperlink{sumOchsengartenObergut.1}{Ochsengarten-Obergut},
\hyperlink{sumGinzling.1}{Ginzling},
\hyperlink{sumRotholz.1}{Rotholz},
\hyperlink{sumWalchsee.1}{Walchsee},
\hyperlink{sumKoessen.1}{Koessen},
\hyperlink{sumInnervillgraten.1}{Innervillgraten},
\hyperlink{sumMatreiinOsttirol.1}{Matrei in Osttirol},
\hyperlink{sumStJohannimWalde.1}{St.Johann im Walde}.



\begin{figure}[h]
\setkeys{Gin}{width=0.99\textwidth}
\vspace*{-3cm}
<<map_stationwise, fig=TRUE, echo=FALSE>>=
##  map
data("StationsTyrol", package = "RainTyrol")
data("MapTyrol", package = "RainTyrol")
library("sp")
sp <- SpatialPointsDataFrame(subset(StationsTyrol, select = c(lon, lat)),
                             data = subset(StationsTyrol, select = -c(lon, lat)),
                             proj4string = raster::crs(MapTyrol$RasterLayer))

plot(MapTyrol$SpatialPolygons)
points(sp, pch = 21, col = "darkgray", las = 1, cex = 0.6)
points(sp[c(5, 6, 20, 23, 25, 32, 33, 46, 47, 56, 57, 70, 79, 82, 83),], pch = 21, bg = hcl(325, 100, 70), cex = 1.5)
# stationname beneath
text(sp[c(25, 47, 57, 70),], 
     labels = StationsTyrol$name[c(25, 47, 57, 70)],
     pos=1, cex=0.75)
# stationname left
text(sp[c(20, 32, 46, 79, 83),], 
     labels = StationsTyrol$name[c(20, 32, 46, 79, 83)],
     pos=2, cex=0.75)
# stationname above
text(sp[c(6, 33, 82),], 
     labels = StationsTyrol$name[c(6, 33, 82)],
     pos=3, cex=0.75)
# stationname right
text(sp[c(5, 23, 56),], 
     labels = StationsTyrol$name[c(5, 23, 56)],
     pos=4, cex=0.75)
@
\vspace*{-3.5cm}
\caption{\label{fig:map}Map of Tyrol with all 95 observation stations and the 15 considered stations highlighted in pink.}
\end{figure}



\section{Results}

\subsection{Forecasting skills} 
Comparing CRPS values of the single split setting it can be stated that the forest model works well over all stations. Often it is even the best performing method (e.g. at stations \hyperlink{sumLech.1}{Lech}, \hyperlink{sumSeeimPaznaun.1}{See im Paznaun} and \hyperlink{sumGinzling.1}{Ginzling}), but there are also stations where one of the other models leads to better results. For example at \hyperlink{sumOchsengartenObergut.1}{Ochsengarten-Obergut} the boosted GAMLSS performs best, at \hyperlink{sumRotholz.1}{Rotholz} the EMOS model and at \hyperlink{sumMatreiinOsttirol.1}{Matrei in Osttirol} the prespecified GAMLSS is the best model, however, they do not clearly outperform the forest model. Moreover, these advantages often even out or turn into disadvantages when looking at the CRPS values from the cross-validation. For example at station \hyperlink{sumInnervillgraten.1}{Innervillgraten}, the prespecified GAMLSS is clearly the best in the single split setting, however, in the cross-validation the forest model is slightly ahead. An exception is given by station \hyperlink{sumSt.JohannimWalde.1}{St.Johann im Walde} as it is the only station where the prespecified GAMLSS clearly outperforms all other models in both settings. But overall, the results from the cross validation also support the statement that the forest model performs well on all stations being either the best or among the best performing methods at the majority of stations.
Moreover, it can be concluded that Axams is an observation station without any unusual features that would lead to outstanding performances of any of the methods. Therefore, it is a reasonable choice as a representative for all 95 station. Examples for stations where the forest model performs better or worse can be found easily, however, in general the methods perform similarly as for station Axams.


%Comparing CRPS skill score values from the cross-validation the distributional forest
%and the two GAMLSS versions show clear improvements compared to the reference model EMOS.
%At many stations the forest and either the prespecified GAMLSS (e.g. at station Innervillgraten) or the boosted GAMLSS (e.g. at station Lech or Zuers) are ahead, almost on the same level, while the second GAMLSS is slightly behind. At stations Axams, See im Paznaun and Ginzling the forest clearly leads to the highest improvement in terms of CRPS. Station St.Johann im Walde stands out as it is the only of the 15 stations where the prespecified GAMLSS is clearly ahead of the other models. But overall it can be stated that all three models perform well with only small variations over stations. Even if one of the evaluated models shows clear advantages in the single split setting (such as the EMOS at station Rotholz or the prespecified GAMLSS at station Innervillgraten) this is evened out within the cross-validation. 


\subsection{Computation time}
The forest model and the two GAMLSS models achieve similar results in both time measurements, fit and predict time, while the EMOS model is clearly the fastest. However, for the boosted GAMLSS the cross-validation applied to find the optimal stopping value \code{mstop} is extremely time consuming. This additional time can not be neglected since the stopping value is crucial for the complexity of the model and the selection of covariates.

As to prediction times it has to be mentioned that the forest can keep up with the other models if only one prediction is made. However, to make predictions for a whole set of new observations the forest clearly needs more time. This is due to the fact that the forest has to computes each single prediction separately as the maximum-likelihood estimation is based on observation-specific weight matrix.

\subsection{Variable importance}
Concerning variable importance, two covariates based on total precipitation (\code{tp\_max} and \code{tp\_mean}) are at the top of the list for all stations, often clearly ahead of the other covariates. The other members of the top ten variables vary slightly over stations, however, further covariates concerning total precipitation, total column-integrated condensate and temperature differences are often among them.

\subsection{Calibration}
The PIT histograms and residual QQ plots show that overall all four models are reasonably well calibrated at the majority of stations while at some stations they all fit better (e.g. at station \hyperlink{sumOetz.1}{Oetz}) or worse (e.g. at station \hyperlink{sumMatreiinOsttirol.1}{Matrei in Osttirol}) than on average. One feature of the boosting method that can be detected in the residual QQ plots for many stations is that heavy upper tails are not captured very well. This is also supported by the PIT histogram.


\textit{Note:} As mentioned by \cite{Hofner+Mayr+Schmid:2016} the methods provided by GAMLSS ``can be unstable, especially when it comes to selecting possibly different sets of variables for multiple distribution parameters''. These problems have also occurred in a few situations for the 
prespecified GAMLSS model within the cross-validation. In case of this method failing to 
build a model it is represented by \code{NA}s in the evaluation.


\includepdf[pages=-, link, linkname=sumAxams]{Axams.pdf}
\bookmark[dest=sumAxams.1]{Axams}
\includepdf[pages=-, link, linkname=sumLech]{Lech.pdf}
\bookmark[dest=sumLech.1]{Lech}
\includepdf[pages=-, link, linkname=sumZuers]{Zuers.pdf}
\bookmark[dest=sumZuers.1]{Zuers}
\includepdf[pages=-, link, linkname=sumSeeimPaznaun]{SeeimPaznaun.pdf}
\bookmark[dest=sumSeeimPaznaun.1]{See im Paznaun}
\includepdf[pages=-, link, linkname=sumJungholz]{Jungholz.pdf}
\bookmark[dest=sumJungholz.1]{Jungholz}
\includepdf[pages=-, link, linkname=sumLadisNeuegg]{LadisNeuegg.pdf}
\bookmark[dest=sumLadisNeuegg.1]{Ladis-Neuegg}
\includepdf[pages=-, link, linkname=sumOetz]{Oetz.pdf}
\bookmark[dest=sumOetz.1]{Oetz}
\includepdf[pages=-, link, linkname=sumOchsengartenObergut]{OchsengartenObergut.pdf}
\bookmark[dest=sumOchsengartenObergut.1]{Ochsengarten-Obergut}
\includepdf[pages=-, link, linkname=sumGinzling]{Ginzling.pdf}
\bookmark[dest=sumGinzling.1]{Ginzling}
\includepdf[pages=-, link, linkname=sumRotholz]{Rotholz.pdf}
\bookmark[dest=sumRotholz.1]{Rotholz}
\includepdf[pages=-, link, linkname=sumWalchsee]{Walchsee.pdf}
\bookmark[dest=sumWalchsee.1]{Walchsee}
\includepdf[pages=-, link, linkname=sumKoessen]{Koessen.pdf}
\bookmark[dest=sumKoessen.1]{Koessen}
\includepdf[pages=-, link, linkname=sumInnervillgraten]{Innervillgraten.pdf}
\bookmark[dest=sumInnervillgraten.1]{Innervillgraten}
\includepdf[pages=-, link, linkname=sumMatreiinOsttirol]{MatreiinOsttirol.pdf}
\bookmark[dest=sumMatreiinOsttirol.1]{Matrei in Osttirol}
\includepdf[pages=-, link, linkname=sumStJohannimWalde]{StJohannimWalde.pdf}
\bookmark[dest=sumStJohannimWalde.1]{St.Johann im Walde}


%bibliographystyle{jss}
\bibliography{ref_supplement.bib}

\end{document}

%% https://cran.r-project.org/web/packages/gamboostLSS/vignettes/gamboostLSS_Tutorial.pdf
%% As a consequence, gamboostLSS is a convenient alternative to the AIC-based variable selection methods implemented in gamlss. The latter methods can be unstable, especially when it comes to selecting possibly different sets of variables for multiple distribution parameters.