%\documentclass[a4paper,11pt]{article}
\documentclass[nojss, shortnames]{jss}

\usepackage{graphicx, natbib}
\pagestyle{empty}
\usepackage{Sweave}
\usepackage{amstext,amsfonts,amsmath,bm,thumbpdf,lmodern,pdfpages,hyperref}

\title{Supplement 2: Stationwise Evaluation}

\author{Lisa Schlosser\\Universit\"at Innsbruck
   \And Torsten Hothorn\\Universit\"at Z\"urich
   \And Reto Stauffer\\Universit\"at Innsbruck
   \And Achim Zeileis\\Universit\"at Innsbruck}
\Plainauthor{Lisa Schlosser, Torsten Hothorn, Reto Stauffer, Achim Zeileis}


\Abstract{In the application discussed in the paper ''Distributional 
Regression Forests for Probabilistic Precipitation Forecasting in Complex Terrain''
the presented method is evaluated on 95 observation stations. More extensive
evaluations including a cross-validation are preformed for one station, Axams.
In this supplement this exact same analysis as for station Axams is
made for 14 further observation stations.}

\Keywords{parametric models, regression trees, random forests, recursive partitioning, probabilistic forecasting, GAMLSS}



\Address{
  Lisa Schlosser, Reto Stauffer, Achim Zeileis \\
  Universit\"at Innsbruck \\
  Department of Statistics \\
  Faculty of Economics and Statistics \\
  Universit\"atsstr.~15 \\
  6020 Innsbruck, Austria \\
  E-mail: \email{Lisa.Schlosser@uibk.ac.at}, \email{Reto.Stauffer@uibk.ac.at},\\
  \email{Achim.Zeileis@R-project.org} \\
  URL: \url{https://www.uibk.ac.at/statistics/personal/schlosser-lisa/},\\
  \phantom{URL: }\url{https://retostauffer.org/},\\
  \phantom{URL: }\url{https://eeecon.uibk.ac.at/~zeileis/}\\
  
  Torsten Hothorn\\
  Universit\"at Z\"urich \\
  Institut f\"ur Epidemiologie, Biostatistik und Pr\"avention \\
  Hirschengraben 84\\
  CH-8001 Z\"urich, Switzerland \\
  E-mail: \email{Torsten.Hothorn@R-project.org}\\
  URL: \url{http://user.math.uzh.ch/hothorn/}\\
  
}


\begin{document}
\SweaveOpts{concordance=TRUE}

\section{Introduction}
This application study is supplementary material to the paper ''Distributional 
Regression Forests for Probabilistic Precipitation Forecasting in Complex Terrain''. 
In Section~3 the presented distributional forest model is employed to obtain 
probabilistic precipitation forecasts in a mountainous region.
Next to an evaluation over all 95 observation stations further investigations
including a cross-validation have been performed for one selected station, Axams.
Here, this more extensive evaluation is carried out, applying the exact same settings 
as in the paper, for Axams and 14 further observation stations. Additionally, computation
times are measured.


\section{Models and evaluation}
As in the paper a distributional forest applying a left-censored at zero 
Gaussian distribution is evaluated together with three other zero-censored 
Gaussian models. Further details on all four models can be found in Section~3.2.. 
In particular, Table~2 gives an overview over the models and their specifications. 
As a brief reminder the approaches are summarized below.

\begin{itemize}

\item \emph{Distributional forest:} All predictor variables are
considered for learning a forest where bootstrap
sampling is employed for each tree. Parameters are estimated by
adaptive local likelihood based on forest weights.

\item \emph{EMOS} \citep{Gneiting+Raftery+Westveld:2005}: 
Standard ensemble model output statistics models use the ensemble 
mean of total precipitation as regressor in the location submodel 
and the corresponding ensemble standard deviation in the scale submodel.
The parameters are estimated by maximum likelihood.

\item \emph{Prespecified GAMLSS:} Smooth additive splines are selected for
the most relevant predictors based on meteorological expert knowledge
following \cite{Stauffer+Umlauf+Messner:2017}. The model is estimated by maximum
penalized likelihood using a backfitting algorithm (\cite{Stasinopoulos+Rigby:2007}).

\item \emph{Boosted GAMLSS:} Smooth additive splines are selected
automatically from all available variables, using non-cyclic boosting
for parameter estimation (\cite{Hofner+Mayr+Schmid:2016, Messner+Mayr+Zeileis:2017}).
The optimal stopping point for the iteration is evaluated based on 
a (computationally intensive) out-of-bag bootstrap estimate of the log-likelihood.

\end{itemize}

%\subsection{Subset selection}
The models are evaluated in two different scenarios.
\begin{itemize}
\item Ordered/Base model: The learning data consists of the first 24 years (1985--2008) and the testing data of the following 4 years (2009--2012). 
\item Cross-validation: The 28 years are randomly split in 7 blocks of 4 years which are not necessarily successive. Each of the blocks is chosen once as testing data while the remaining 6 blocks are the learning data. This procedure is repeated 10 times resulting in a 10 times 7-fold cross-validation framework.

\end{itemize}

The performance of the models are compared based on the following evaluations.

\begin{itemize}
\item CRPS: The continuous ranked probability score (CRPS) is calculated
for each observation in the testing data set and then averaged. For the cross-validation
setup the mean of the resulting 70 CRPS average values is listed and all 70 average values
are illustrated in a boxplot where the EMOS model is chosen as reference model, 
represented by the horizontal line at 0.
 
\item Fit time: It is measured how long it takes to fit the model to the learning data. 

\item Predict time: It is measured how long it takes the model to make predictions. For the ordered model the predict time refers to the time needed to make one single prediction. The mean predict time over all observations in the test data is considered.
For the cross-validation the predict time refers to the time needed to make all predictions for the test data. The mean predict time over all 70 evaluations in the cross-validation is considered.

\item Variable importance: For the forest model it is investigated which variables have the strongest influence on the model and its goodness of fit. For each variable this is measured based on mean decrease in CRPS after permutation. The 10 variables showing the highest values are listed in a barplot.

\item Residual QQ plots: For each of the models an out-of-sample residual QQ plot for the ordered model provides a visualization of goodness of fit. Randomized residual quantiles are compared to standard normal quantiles.

\item PIT histograms: As an additional visualization out-of-sample probability integral transform histograms illustrated how well-calibrated the distributional fits are for the ordered model.

\end{itemize}


\section{Stations}
Out of the 95 observation stations 15 are considered for this 
benchmark study. They have been selected based on the region
they are in, their altitude and the performance of the 
different models in the application in the paper such that
all of these features are represented equally.

Stations: \hyperlink{sumAxams.1}{Axams},
\hyperlink{sumLech.1}{Lech},
\hyperlink{sumZuers.1}{Zuers},
\hyperlink{sumSeeimPaznaun.1}{See im Paznaun},
\hyperlink{sumOetz.1}{Oetz},
\hyperlink{sumJungholz.1}{Jungholz},
\hyperlink{sumKoessen.1}{Koessen},
\hyperlink{sumWalchsee.1}{Walchsee},
\hyperlink{sumGinzling.1}{Ginzling},
\hyperlink{sumRotholz.1}{Rotholz},
\hyperlink{sumInnervillgraten.1}{Innervillgraten},
\hyperlink{sumMatreiinOsttirol.1}{Matrei in Osttirol},
\hyperlink{sumStJohannimWalde.1}{St.Johann im Walde},
\hyperlink{sumLadisNeuegg.1}{Ladis-Neuegg},
\hyperlink{sumOchsengartenObergut.1}{Ochsengarten-Obergut},




\begin{figure}[h]
\setkeys{Gin}{width=0.99\textwidth}
\vspace*{-3cm}
<<map_stationwise, fig=TRUE, echo=FALSE>>=
##  map
data("StationsTyrol", package = "RainTyrol")
data("MapTyrol", package = "RainTyrol")
library("sp")
sp <- SpatialPointsDataFrame(subset(StationsTyrol, select = c(lon, lat)),
                             data = subset(StationsTyrol, select = -c(lon, lat)),
                             proj4string = raster::crs(MapTyrol$RasterLayer))

plot(MapTyrol$SpatialPolygons)
points(sp, pch = 21, col = "darkgray", las = 1, cex = 0.6)
points(sp[c(5, 6, 20, 23, 25, 32, 33, 46, 47, 56, 57, 70, 79, 82, 83),], pch = 21, bg = hcl(325, 100, 70), cex = 1.5)
# stationname beneath
text(sp[c(25, 47, 57, 70),], 
     labels = StationsTyrol$name[c(25, 47, 57, 70)],
     pos=1, cex=0.75)
# stationname left
text(sp[c(20, 32, 46, 79, 83),], 
     labels = StationsTyrol$name[c(20, 32, 46, 79, 83)],
     pos=2, cex=0.75)
# stationname above
text(sp[c(6, 33, 82),], 
     labels = StationsTyrol$name[c(6, 33, 82)],
     pos=3, cex=0.75)
# stationname right
text(sp[c(5, 23, 56),], 
     labels = StationsTyrol$name[c(5, 23, 56)],
     pos=4, cex=0.75)
@
\vspace*{-3.5cm}
\caption{\label{fig:map}Map of Tyrol with all 95 observation stations and the 15 considered stations highlighted in pink.}
\end{figure}



\section{Results}
Comparing CRPS skill score values from the cross-validation the distributional forest
and the two GAMLSS versions show clear improvements compared to the reference model EMOS.
At many stations the forest and either the prespecified GAMLSS (e.g. at station Innervillgraten) or the boosted GAMLSS (e.g. at station Lech or Zuers) are ahead, almost on the same level, while the second GAMLSS is slightly behind. At stations Axams, See im Paznaun and Ginzling the forest clearly leads to the highest improvement in terms of CRPS. Station St.Johann im Walde stands out as it is the only of the 15 stations where the prespecified GAMLSS is clearly ahead of the other models. But overall it can be stated that all three models perform well with only small variations over stations. Even if one of the evaluated models shows clear advantages in the ordered model (such as the EMOS at station Rotholz or the prespecified GAMLSS at station Innervillgraten) this is evened out within the cross-validation. 

Concerning fitting times either the prespecified GAMLSS (e.g. St.Johann im Walde, Innervillgraten) or the distributional forest (e.g. Zuers, Walchsee, Jungholz, Oetz, Koessen, Ginzling) are the fastest while the boosted GAMLSS usually takes more time, especially if the time required for the selection of the optimal value for \code{mstop} a multiple of the times of the other models is needed.

As to prediction times the EMOS is by far the fastest due to its simple linear structure. The boosted GAMLSS is the second fastest followed by the prespecified GAMLSS and the distributional forest on the 4th place. This order holds for all stations and also for both subsample settings. However, while all four models are comparably fast making one single prediction the forest needs about ten times more time to make predictions for the whole learning data set. This is due to the maximum-likelihood estimation based on observation-specific weights matrices which have to be calculated separately for each observation.

Concerning variable importance three covariates based on total precipitation (\code{tp\_min}, \code{tp\_max}, \code{tp\_mean}) are at the top of the list for almost all stations (except for the two stations Matrei in Osttirol and Ochsengarten-Obergut where a covariate describing total column-integrated condensate is on the third place). This was to be expected since the aim is to predict amounts of precipitation. The other members of the top ten variables vary slightly over stations, however, further covariates concerning total precipitation, total column-integrated condensate and temperature differences are often among them.


\section{Discussion}
Overall, the results presented in this supplement show that Axams is an observation station without any unusual features that would lead to outstanding performances of any of the methods. Therefore, it is a reasonable choice as a representative for all 95 station. Examples for stations where the forest model performs better or worse can be found easily, however, in general the methods perform similarly as for station Axams.
Also at most observation stations in East Tyrol where the prespecified GAMLSS performed clearly better in the ordered subsample setting the cross-validation confirms that the forest model performs at almost equally well over varying subsample selections.

As to fit times, all models achieve similar results, however, the cross-validation applied to find the optimal stopping value \code{mstop} for the boosted GAMLSS is extremely time consuming. 
Looking at prediction times for one single observation, the distributional forest is on average
the slowest model, however, the difference to the predict times of the other models is not too big. This changes regarding predictions for a whole set of observation due to the fact that the forest computes each single prediction separately as it is based on an observation-specific
weight matrix. 

As mentioned by \cite{Hofner+Mayr+Schmid:2016} the methods provided by GAMLSS ``can be unstable, especially when it comes to selecting possibly different sets of variables for multiple distribution parameters''. These problems have also occured in a few situations for the 
prespecified GAMLSS model within the cross-validation. In case of this method failing to 
build a model it is represented by \code{NA}s in the evaluation.



%\bibliography{ref_supplement_stationwise.bib}

\includepdf[pages=-, link, linkname=sumAxams]{Axams.pdf}
\includepdf[pages=-, link, linkname=sumLech]{Lech.pdf}
\includepdf[pages=-, link, linkname=sumZuers]{Zuers.pdf}
\includepdf[pages=-, link, linkname=sumSeeimPaznaun]{SeeimPaznaun.pdf}
\includepdf[pages=-, link, linkname=sumOetz]{Oetz.pdf}
\includepdf[pages=-, link, linkname=sumJungholz]{Jungholz.pdf}
\includepdf[pages=-, link, linkname=sumKoessen]{Koessen.pdf}
\includepdf[pages=-, link, linkname=sumWalchsee]{Walchsee.pdf}
\includepdf[pages=-, link, linkname=sumGinzling]{Ginzling.pdf}
\includepdf[pages=-, link, linkname=sumRotholz]{Rotholz.pdf}
\includepdf[pages=-, link, linkname=sumInnervillgraten]{Innervillgraten.pdf}
\includepdf[pages=-, link, linkname=sumMatreiinOsttirol]{MatreiinOsttirol.pdf}
\includepdf[pages=-, link, linkname=sumStJohannimWalde]{StJohannimWalde.pdf}
\includepdf[pages=-, link, linkname=sumLadisNeuegg]{LadisNeuegg.pdf}
\includepdf[pages=-, link, linkname=sumOchsengartenObergut]{OchsengartenObergut.pdf}


%bibliographystyle{jss}
\bibliography{ref_supplement.bib}

\end{document}

%% https://cran.r-project.org/web/packages/gamboostLSS/vignettes/gamboostLSS_Tutorial.pdf
%% As a consequence, gamboostLSS is a convenient alternative to the AIC-based variable selection methods implemented in gamlss. The latter methods can be unstable, especially when it comes to selecting possibly different sets of variables for multiple distribution parameters.