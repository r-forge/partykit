\documentclass[nobf,man]{apa}
\usepackage{amssymb}
\usepackage{amsmath,amsthm,tabularx,curves,texdraw,psfrag}
\usepackage{apacite}
\usepackage[english]{babel}
\usepackage{lscape,dcolumn,hhline}
\usepackage{graphicx,epic,eepic,rotating}
\usepackage[normalem]{ulem}
\usepackage{fancyhdr}
\usepackage{eurosym,bbding}
\usepackage{verbatim}
\usepackage{ctable}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{lineno}
\usepackage{tikz}
\usepackage{lineno}
\pagewiselinenumbers

\graphicspath{{figures/}}

\usetikzlibrary{shapes,shadows,arrows,positioning}



\title{Detection of Treatment-Subgroup Interactions in Clustered Datasets: Combining Model-Based Recursive Partitioning and Random-Effects Estimation}
\shorttitle{Detection of treatment-subgroup interactions in clustered datasets}
\author{M. Fokkema, N. Smits, H. Kelderman}
\affiliation{Vrije Universiteit, Amsterdam}
\abstract{
Identification of subgroups of patients for which treatment A is more effective than treatment B, and vice versa, is of key importance to the development of personalized medicine. Several tree-based algorithms have been developed for detection of such treatment-subgroup interactions. In many instances, however, datasets may have a clustered structure, where observations are clustered within, for example, research centers or persons. In the current paper we propose a new algorithm, called clusterMOB, that allows for detection of treatment-subgroup interactions, as well as estimation of cluster-specific random effects. The clusterMOB algorithm uses model-based recursive partitioning (MOB; \citeNP{ZeilyHoth08}) to detect treatment-subgroup interactions, and a linear mixed-effects model \cite{PinhyBate00} for estimation of random-effects parameters. In a simulation study, we evaluate the performance of clusterMOB, in terms of recovery of treatment-subgroup interactions and prediction of treatment differences, and compare it with that of MOB without random effects. The clusterMOB algorithm recovered the true treatment-subgroup interactions in 91.2\% of the simulated datasets, whereas MOB without random effects recovered the true interactions in 63.5\% of the simulated datasets. Also, clusterMOB predicted treatment outcome differences with an average accuracy of .94, whereas MOB predicted treatment differences with an average accuracy of .88. We conclude that clusterMOB is a promising algorithm for the detection of treatment-subgroup interactions in clustered datasets, and discuss directions for future research.\\
}
%\acknowledgements{}
\rightheader{Detection of treatment-subgroup interactions in clustered datasets}
\leftheader{Detection of treatment-subgroup interactions in clustered datasets}

\begin{document}

\maketitle

\pagebreak






\section{Introduction}

In medicine-efficacy research, the one-size-fits-all paradigm is slowly losing ground, and personalized medicine is becoming increasingly important. Personalized medicine presents us with the challenge of finding which patients respond best to which treatments. This can be referred to as the detection of treatment-subgroup interactions \cite<e.g., >{DoovyDuss14}. In most cases, treatment-subgroup interactions are studied using linear models, such as factorial analysis of variance techniques, in which potential moderators have to be specified a-priori, have to be checked one at a time, and continuous moderator variables have to be discretized a-priori. This may hamper identification of which treatments work best for whom, especially when there are no a-priori hypothesis about treatment-subgroup interactions. As noted by \citeA{KreayFran06}, there is a need for methods that generate, instead of test, hypotheses and that are specifically directed at the detection of treatment interactions.

Tree-based methods are such hypothesis-generating methods, as they can automatically detect subgroups which differ on the expected outcomes for one or more treatments. Due to their flexibility, tree-based methods are preeminently suited to detect treatment-subgroup interactions: they can handle many potential predictor variables at once, and can automatically detect (higher order) interactions between predictor variables. Several promising tree-based algorithms and software packages have been developed to assist in the detection of treatment-subgroup interactions (\citeNP{DussyMech14, DussyMeul04, SuyTsai09, FostyTayl11, LipkyDmit11, ZeilyHoth08}; see \citeNP{DoovyDuss14} for an overview). Model-based recursive partitioning (MOB; \citeNP{ZeilyHoth08}) may be the most flexible method for detecting treatment-subgroup interactions, as it offers a very generic data-analytic framework for detecting partitions in a dataset, with different model parameter estimates. Earlier, MOB has been successfully applied by \citeA{DrieySmit14} in the detection of subgroups with differential treatment outcomes for two different psychotherapies.

Single randomized clinical trails may often be underpowered to detect treatment-subgroup interactions. Therefore, meta-analysis of individual-level patient data (IPD), in which datasets from several RCTs are pooled, is becoming increasingly popular \cite{KoopyHeij07}. In such analyses, the clustered structure of the dataset should be taken into account by including study-specific effects in the model, prompting the need for modeling random effects \cite<e.g., >{Frie93,DersyLair86,HiggyWhit01}. Likewise, analyses of datasets from single clinical trials conducted in multiple research centers, and longitudinal datasets may also require modeling of random effects.

However, none of the aforementioned tree-based algorithms allow for estimation of random effects. Ignoring the clustered structure of datasets may lead to biased inference, due to underestimated standard errors \cite<e.g., >{BrykyRaud92}. More importantly, when the interest is in subgroup detection, ignoring random effects on the outcome variable may result in the detection of spurious subgroups.

In the current paper, we present a tree-based algorithm for treatment-subgroup interaction detection, which takes the clustered nature of datasets into account. The algorithm combines MOB with the estimation of random effects, thus allowing for the detection of treatment-subgroup interactions, as well as accounting for variation between clusters (e.g., trials).

In what follows, we will first discuss existing frameworks for estimating treatment effects: the linear fixed-effects model, model-based recursive partitioning of the linear fixed-effects model, and the linear mixed-effects model. Then, we present a new algorithm, which combines model-based recursive partitioning and random-effects estimation. In the methods and results section, we present a simulation study evaluating the comparative accuracy of the algorithm.

\section{General modeling framework}

\subsection{Linear fixed-effects model}

In a clinical trial, where the outcomes of two treatments are compared, an overall linear regression model may be used to estimate treatment effects. Let $N$ be the total number of observations in the dataset\footnote{In the Appendix, an overview of notation used in the current paper is provided}. The following linear regression model can be used to estimate the effects of the treatments:

\begin{equation}
\label{eq:fixedeffects}
    y = X \beta + \epsilon
\end{equation}

Where $y$ is an $N \times 1$ column vector of outcome variable values, $X$ is the $N \times 2$ matrix of predictor variable values (the first column consisting of a vector of ones, and the second column consisting of a dummy variable indicating treatment type); $\beta$ is the $2 \times 1$ column vector of regression coefficients (the first element of $\beta$ representing the intercept, or mean value of the outcome variable for treatment 1, and the second element of $\beta$ representing the difference in mean values of the outcome variables for treatments 1 and 2). $\epsilon$ is an $N \times 1$ column vector of residuals, which is assumed to be normally distributed with mean 0 and variance $\sigma^{2}_{\epsilon}$, and independent of all other variables in the model. 

In Figure \ref{fig:fixed_effects}, a graphical representation of the model in Equation \ref{eq:fixedeffects}, estimated for a simulated dataset of 150 observations, is provided. The boxplots show the distribution of the outcome variable (posttreatment depression score) among 150 participants, who were randomly assigned to treatment 1 and treatment 2. Figure \ref{fig:fixed_effects} suggests little overall difference between the outcomes of treatment 1 and treatment 2, as the slope of the regression line is nearly zero. We shall see that this does not necessarily mean that posttreatment depression score and treatment type are unrelated, as the effect of treatment may be moderated by other variables. Conditional on other variables, the relationship between $X$ and $Y$ may be stronger (Simpson's paradox; \citeNP{Simp51}).




\subsection{Model-based recursive partitioning}

The rationale behind MOB is that a global model for all observations, like that in Equation \ref{eq:fixedeffects}, may not describe the data well, and when additional covariates $U$ are available, it may be possible to partition the dataset with respect to these covariates, and find a better model in each cell of the partition \cite{ZeilyHoth08}. This is reminiscent of the classification and regression tree (CART) algorithm of \citeA{BreiyFrie84}, which splits the dataset into subsets, of which the distributions of the outcome variable are most different. Whereas CART trees have constant fits in the terminal nodes, MOB trees have parametric models with one or more predictor variables in the terminal nodes.

To find partitions and better-fitting local linear models, the MOB algorithm tests for parameter instability: differences in $\hat{\beta}$ across partitions of the dataset, which are defined by one or more variables in $U$. To this end, the MOB algorithm cycles iteratively through the following steps \cite{ZeilyHoth08}: (1) fit the parametric model $y=X \beta + \epsilon$ to the data set, (2) test for parameter instability over a set of partitioning variables $U_1$ through $U_r$, (3) if there is some overall parameter instability, split the data set with respect to the variable associated with the highest instability, (4) repeat the procedure in each of the resulting subsamples.

More specifically, in step (2), to test for parameter instability, so-called \textit{scores} are computed, using the score function. The expected value of the scores over all observations in a data set is zero, by definition. Under the null hypothesis of parameter stability, the scores do not systematically deviate from the expected value of zero, when the observations are ordered by the value of a potential partitioning variable $U_k$. To statistically test whether there are systematic deviations of the scores from zero with respect to variable $U_k$, the class of generalized M-fluctuation tests is used in MOB \cite{Zeil05,ZeilyHorn07}.  

If the null hypothesis of parameter stability in step (2) can be rejected, that is, if at least one of the partitioning variables $U_{k}$ has a p-value for the M-fluctuation test below the pre-specified significance level $\alpha$, the data set is partitioned into two subsets in step (3). In step (3), a binary partition is created using $U_{k*}$, the variable with the minimal p-value in step (2). This split point for $U_{k*}$ is selected, that minimizes the sum of the RSSs in both partitions \cite{ZeilyHoth08}. In step (4), steps (1) through (3) are repeated in each partition, until the null hypothesis of parameter stability can no longer be rejected.

Due to the binary recursive nature of MOB, the resulting partitions can be represented as a tree, with a local fixed effects linear regression model $y = X \beta + \epsilon$ in all $j = 1,...,t$ terminal nodes of the tree. As a result, the model in Equation \ref{eq:fixedeffects} keeps the same formulation in each endnode, yielding a $\beta$ that is a $2t \times 1$ column vector ${\left[ {\beta_1}^T,...,{\beta_j}^T,...,{\beta_t}^T \right]}^T$, where each $\beta_j$ relates to a subsample of the observations in terminal node $j$. $X$ becomes an $N \times 2t$ (block diagonal) design matrix, with the $N_{j} \times 2$ matrices of predictor variable values in terminal node $j$ on the diagonal. Note that when the null hypothesis of parameter stability can not be rejected in the first iteration, no partitions are created by MOB, and the resulting tree has a single node with the dimensions of $X$ remaining $N \times 2$ and the dimensions of $\beta$ remaining $2 \times 1$.




Figure \ref{fig:example_mobtree} provides an example of a graphical representation of a model-based partition, based on the same simulated data of Figure \ref{fig:fixed_effects}. By using several additional covariates (i.e., anxiety questionnaire score, duration of depressive symptoms at baseline, age), MOB separated the observations into four partitions, each with a different estimate for $\beta$ (i.e., different mean values for treatment 1 and/or 2). Figure \ref{fig:example_mobtree} shows a subgroup with low duration and low anxiety, for which treatment 1 is much more beneficial (i.e., lower posttreatment depression scores); a subgroup of patients with low duration and anxiety $\leq 12$, for whom treatment 1 is somewhat more beneficial; a subgroup of patients with low duration and anxiety $> 12$, for whom treatment 2 is slightly more beneficial; and a subgroup of patients with duration of depressive symptoms $> 6$ months, for whom treatment 2 is much more beneficial. Age did not have a main or moderating effect on treatment outcomes, and therefore does not appear as a splitting variable in the tree.




\subsection{Linear mixed-effects models}

When the dataset consists of observations from multiple clusters, the model in Equation \ref{eq:fixedeffects} can be extended to include cluster-specific, or random, effects. With the inclusion of random effects, the model becomes:

\begin{equation}
\label{eq:mixedeffects}
	y = X \beta + Z b + \epsilon
\end{equation}

where $X$ is the $N \times 2$ matrix of fixed-effects predictor variable values, of which the first column is a vector of ones, and the second column is a dummy indicator for treatment type. $\beta$ is the $2 \times 1$ column vector of fixed-effects coefficients. $Z$ is the $N \times n$ (block diagonal) design matrix of random-effects predictor variables, with $n$ being the number of clusters. $Z$ has $N_i \times 1$ vectors of ones on the diagonal, with $N_i$ being the number of observations in cluster $i$; $b$ is the $n \times 1$ column vector ${\left[ {b_1},...,{b_i},...,{b_n} \right]}^{T}$ of random-effects coefficients, and it is assumed that that $b_{i}$ is normally distributed with mean zero and (scalar) variance $\sigma^{2}_{b_i}$ (note that in the current paper, for simplicity, we incorporate random intercepts only, but random slopes can easily be included in the model). $\epsilon$ is the $N \times 1$ column vector of residuals, which are normally distributed with mean zero and variance $\sigma^{2}_{\epsilon}$. Further, it is assumed that $b_{i}$ and $\epsilon$ are independent from all other variables in the model. The parameters of the linear mixed-effects model $\beta$,  $\sigma^{2}_{\epsilon}$ and $\sigma^{2}_{b_i}$ can be estimated with, for example, maximum likelihood (ML) and restricted ML \cite{LairyWare82}.


\subsection{Combining model-based recursive partitioning and random-effects estimation}

As noted earlier, ordinary linear fixed-effects models (Equation \ref{eq:fixedeffects}) are not well suited for the detection of treatment-subgroup interactions, whereas the MOB algorithm is, but it does not allow for estimation of random effects. Therefore, we propose an extension of the MOB algorithm, which incorporates estimation of random effects. 

For this extensions, we replace the fixed-effects part of Equation \ref{eq:mixedeffects} by a MOB tree, so $\beta$ becomes the $2t \times 2$ column vector ${\left[{\beta_1}^T,...,{\beta_j}^T,...,{\beta_t}^T \right]}^T$, and $X$ becomes the $N \times 2t$ (block diagonal) design matrix, with the $N_{j} \times 2$ matrices of predictor variable values in terminal node $j$ on the diagonal. This tree is estimated as described above, in the subsection \textit{Model-based recursive partitioning}. The random-effects coefficients $b$ in Equation \ref{eq:mixedeffects} are estimated globally, that is, the random-effects model is invariant over the nodes of the MOB tree.

To estimate $\beta$ and $b$ for this model, we take an iterative approach, alternating between assuming random effects $b$ known, allowing for estimation of $\beta = {\left[ {\beta_l}^T,...,{\beta_j}^T,...,{\beta_t}^T \right]}^T$; and assuming $\beta$ known, allowing for estimation of the random-effects vector $b$. The algorithm initializes by setting $b$ to zero, since both the random and the fixed-effects parts are initially unknown. In every iteration, the MOB tree (i.e., fixed effects $\beta$) and random effects $b$ are re-estimated: the MOB tree is estimated, given the estimated value of $b$ from the last iteration, and $b$ is estimated, given the estimated value of $\beta$ from the current iteration. Iterations are continued until convergence, which is monitored by computing the log-likelihood criterion of the mixed-effects model in Equation \ref{eq:mixedeffects}.     

A similar approach has been taken by \citeA{HajjyBell11} and \citeA{SelaySimo12}, who added random-effects estimation to CART trees with constant fits, instead of linear models, in the terminal nodes. Because the proposed algorithm allows for model-based recursive partitioning of linear fixed-effects models, as well as estimation of cluster-specific random effects, we will refer to it as clusterMOB. 

In what follows, we present a simulation study in which we assess the performance of clusterMOB in recovering treatment-subgroup interactions, and predicting differences between the outcomes of two treatments. Furthermore, we will compare the performance of clusterMOB with that of MOB. We will generate a large number of datasets, varying seven parameters: total sample size, number of clusters, variance of cluster-specific random intercepts, number of potential partitioning variables, intercorrelations between potential partitioning variables, (in)dependence between cluster-specific random intercepts and potential partitioning variables, and magnitude of the mean difference in treatment outcomes.

For clusterMOB, we expect the accuracy of recovered trees and predictions to improve with increasing sample size and/or magnitude of the difference in treatment outcomes. For MOB, we have the same expectations, when cluster-specific random effects are absent in the dataset. When cluster-specific random effects are present, we expect clusterMOB to perform better than MOB, in terms of accuracy of recovered trees and predictions, especially when the variance of cluster-specific random intercepts and the dependence between cluster-specific random intercepts and potential partitioning variables increases, and/or when the magnitude of the mean difference in treatment outcomes decreases. In the absence of cluster-specific random effects, we expect clusterMOB not to perform better than MOB.


\section{Method}

\subsection{Software}

\verb|R| \cite{R10} was used for generation and analysis of all datasets. Default settings were used for all functions, with exception of the maximum tree depth, which was set to four (i.e., eight terminal nodes) for both clusterMOB and MOB.

\subsubsection{Estimation of MOB and clusterMOB trees}

For estimating MOB trees, the \verb|lmtree| function from the \verb|R| package \verb|partykit| \cite{HothyZeil14,ZeilyHoth08} was used. The \verb|lmtree| function builds a linear model tree: a model-based recursive partition based on least squares regression (Equation \ref{eq:fixedeffects}).  

For building clusterMOB trees, we build a function following the design of the \verb|REEMtree| function from the \verb|R| package \verb|REEMtree| \cite{SelaySimo11}. The \verb|clusterMOB| function is used to estimate the partitions and parameters of the MOB-tree extended model (Equation \ref{eq:mixedeffects}, with $\beta = {\left[{\beta_1}^T,...,{\beta_j}^T,...,{\beta_t}^T \right]}^T$, and $X$ is the $N \times 2t$ block diagonal design matrix of predictor variable values). The \verb|clusterMOB| function makes use of the \verb|lmtree| function, and the \verb|lme| function from the \verb|R| package \verb|nlme| (linear and non-linear mixed-effects models; \citeNP{PinhyBate00}). 

The \verb|clusterMOB| function iterates through the following two steps: (1) building a linear model tree using the \verb|lmtree| function, and (2) calculating the random-effects coefficient vector $b$ using the \verb|lme| function. More specifically, in step (1) of the first iteration, the random effects are assumed to be zero. In each iteration $>1$, the random effects as estimated by the \verb|lme| function in step (2) of the last iteration are included as an offset (a variable to be included in the model, with a known, pre-specified coefficient of one) in the \verb|lmtree| function. In step (2), indicators for terminal node of the linear model tree, and the interaction between terminal node and treatment type are included as fixed-effects factors, to account for the fixed effects in estimation of the random-effects parameters. Convergence of \verb|clusterMOB| is monitored using the log-likelihood value of the linear mixed-effects model estimated in step (2). When the difference in the log-likelihoods of two consecutive iterations is less than a prespecified value (.001 by default), \verb|clusterMOB| has converged.  
 



\subsection{Evaluation of performance}

\subsubsection{Tree size} 

For each dataset, we calculate the tree size of the MOB and clusterMOB trees, and compare it with the true tree size. To detect predictors of tree size for MOB and clusterMOB, a linear model tree was built using the \verb|lmtree| function. The outcome variable in this tree is the number of nodes of the generated trees. The predictor variable for the linear model is algorithm type (MOB or clusterMOB), and the (potential) partitioning variables are the parameters of the data-generating process, which will be described in the \textit{Simulation design} subsection. The maximum depth will be set to four, as this yields a visual representation which is easy to interpret.  


\subsubsection{Splitting points and variables} 

We compared the performance of MOB and clusterMOB in recovering the true splitting variables and splitting values, for trees matching the true trees size and trees not matching the true tree sizes, separately.

\subsubsection{Predictive accuracy} 

We evaluated the predictive accuracy of MOB and clusterMOB by calculating correlations between the predicted treatment differences and true treatment-effect differences ($\beta_1$, Figure \ref{fig:modelC}) of test observations. Evaluation using the same data for testing and for training model results in overly optimistic estimates of predictive accuracy \cite{HastyTibs09}, so the MOB and clusterMOB trees were used for prediction of new observations in test datasets. These test datasets were generated from the same population as the training datasets. Because the cluster specific intercepts $b_i$ were randomly generated for training and testing, the observations in the test datasets were from 'new' clusters. As a result, only the linear model tree was used for prediction with clusterMOB.

For every dataset, two correlation coefficients were calculated, representing the linear association between the true and predicted treatment differences: one for clusterMOB, and one for MOB. To detect predictors of predictive accuracy of both algorithms, a linear model tree was built using the \verb|lmtree| function (with maximum tree depth set to four). The outcome variable in this tree is the correlation between the true and predicted treatment differences. The predictor variable for the linear model is algorithm type (MOB or clusterMOB), and the parameters of the data-generating process are the (potential) partitioning variables, which will be described next.


\subsection{Simulation design}

\subsubsection{Training datasets}
For generating treatment-subgroup interactions, we used a treatment-subgroup interaction design from \citeA{DussyMech14}, which is also depicted in Figure \ref{fig:modelC}. Figure \ref{fig:modelC} shows two subgroups with a treatment interaction, and two subgroups without a treatment interaction. The four subgroups are characterized by their values on the partitioning variables $U_2$, and $U_1$ or $U_5$. In other words, $U_1$, $U_2$ and $U_5$ are true partitioning variables, whereas the other potential partitioning variables in $U$ (e.g., $U_3$, $U_4$) are noise variables.


In generating the datasets, we varied seven parameters of the data-generating process. For each cell, 50 datasets were generated, resulting in a total of 50 x 3 x 2 x 2 x 3 x 3 x 3 x 2 = 32,400 training datasets:

\begin{enumerate}
\item Three levels for the total number of observations: $N=200$, $N=500$, $N=1000$.
\item Two levels for the number of potential partitioning covariates $U_1$ through $U_r$: $r=5$, $r=15$ (where only $U_1$, $U_2$ and $U_5$ are true partitioning variables and the other $U_k$s are not).
\item Two levels of intercorrelations between the covariates $U_1$ through $U_r$: $\rho_{U_{k},U_{k'}}=0.0$, $\rho_{U_{k},U_{k'}}=0.3$.
\item Three levels for the number of clusters: $n=5$, $n=10$, $n=25$.
\item Three levels for the population standard deviation of the normal distribution from which the cluster specific intercepts are drawn: $\sigma_{b_i}=0$, $\sigma_{b_i}=5$, $\sigma_{b_i}=10$.
\item Three levels for the intercorrelations between $b_i$ and one of the $U_k$s: $b_i$ and $U_k$ uncorrelated, $b_i$ correlated with a true partitioning covariate (i.e., $U_2$, $U_1$, or $U_5$, introducing a correlation between the correlated $U_k$ and $b_i$ of about 0.42), $b_i$ correlated with a non-partitioning covariate (i.e., $U_3$ or $U_4$, introducing a correlation between the correlated $U_k$ and $b_i$ of about 0.42). 
\item Two different levels for $\beta_1$, the unstandardized mean difference in treatment outcomes, in subgroups with differential effects for treatment 1 ($X_{1}=0$) and treatment 2 ($X_{1}=1$). The levels for mean differences in subgroups with differential treatment effect were $|\beta_1| = 2.5$ (corresponding to a medium effect size, Cohen's $d = 0.5$; \citeNP{Cohe92}) and $|\beta_1| = 5.0$ (corresponding to a large effect size; Cohen's $d = 1.0$).
\end{enumerate}

We expect the last four parameters to be most influential in determining the difference in performance between MOB and clusterMOB. We expect the first four parameters to have little effect on the difference in performance between MOB and clusterMOB.

As in \citeA{DussyMech14}, all covariates $U_1$ through $U_{15}$ were drawn from a multivariate normal distribution with ${\mu_U}_1$, ${\mu_U}_2$, ${\mu_U}_4$, and ${\mu_U}_5$ fixed at 10, 30, -40 and 70, respectively. The means for all other covariates (i.e., ${\mu_U}_3$, and ${\mu_U}_6$ through ${\mu_U}_{15}$) were drawn from a discrete uniform distribution on the interval $[-70,70]$. All covariates $U_1$ through $U_{15}$ have the same standard deviation: ${\sigma_U}_k = 10$. Correlations between the variables in $U$ vary according to the third facet of the simulation design described above.

To generate the random error term $\epsilon$, for every observation we drew a value from a normal distribution with $\mu_{\epsilon} = 0$ and $\sigma_{\epsilon} = 5$. 

To generate the cluster-specific intercepts $b_i$, we partitioned the sample into equally sized clusters, conditional on one of the variables $U_1$ through $U_5$, producing the correlations in the sixth facet of the simualtion design. For each cluster we drew a single $b_i$ from a normal distribution with $\mu_{b_i} = 0$ and the value of $\sigma_{b_i}$ given by the fifth facet of the simulation design. When $b_i$ was correlated with one of the potential partitioning variables $U$, the partitioning or non-partitioning covariate correlated with $b_i$ was randomly selected.

To generate the node-specific fixed-effects, we partitioned the sample according to the endnodes of the tree in Figure 4.3. In combination with the seventh facet of the simulation design, this determines the values of $\beta_j$. For every observation, we generated a binomial variable (with $p =.5$) as an indicator for treatment type (i.e., the second column of $X$). 

Finally, outcome variable Y was calculated according to the model in Equation \ref{eq:fixedeffects}. 


\section{Results}

\subsection{Tree structure recovery}

\subsection{Tree size}

Overall, smaller trees were created by clusterMOB than by MOB. The average tree size for clusterMOB, across the 32,400 training datasets, was 7.15 (SD=0.61). For MOB, average tree size was 8.11 (SD=2.05). The true tree size was 7 (4 terminal nodes and 3 inner nodes; Figure \ref{fig:modelC}). As Table \ref{tab:treesize} shows, 29,556 out of 32,400 (91.22\%) clusterMOB trees matched the true tree size, and 20,578 out of 32,400 (63.51\%) MOB trees matched the true tree size.




The linear model tree depicting the relationship between the various data-generating parameters and tree size is presented in Figure \ref{fig:treesizetree}. The endnotes of Figure \ref{fig:treesizetree} indicate that in many situations MOB and clusterMOB build about equally sized trees, with two notable exceptions: nodes 7 and 14. 

Node 14 represents a large majority (59.26\%) of the total number of simulated datasets, and shows that clusterMOB recovered the true tree size in nearly all of these datasets, whereas MOB provided trees with more than 7 nodes in the majority of these datasets. The treesize distributions in node 14 indicate that the increased power provided by a larger sample size ($N=500$ or $N=1000$) results in spurious splits by MOB, when $\sigma_{b_i}$ is non-zero and when $b_i$ is correlated to one of the variables in $U$. Because clusterMOB can more adequately deal with the additional variance in $Y$ caused by non-zero values of $\sigma_{b_i}$, the size of clusterMOB trees seems not to be influenced by values of $\sigma_{b_i}$.

Node 7 indicates that with low sample size ($N=200$), when $b_i$ is not correlated to one of the $U$ variables, and the variance of $b_i$ is large, MOB tends to grow smaller trees. Because MOB cannot account for the variance in $Y$ due to cluster-specific effects, MOB has difficulty to detect partitions when $\sigma_{b_i}$ is large and sample size is low. At the same time, the upper whisker of the boxplot of MOB in node 7 reaches a value of 8, indicating that in many cases, MOB may still find spurious splits in small datasets, when $\sigma_{b_i}$ is large. 



\subsubsection{Recovery of partitioning variables and values}

Overall, in recovering the true partitioning variables and values, clusterMOB performed better than MOB. However, for the first split, both MOB and clusterMOB always selected the true partitioning variable ($U_2$). The true splitting value for $U_2$ was 30 (Figure \ref{fig:modelC}), and the mean splitting value selected for the first split was 29.94 for both clusterMOB and MOB. However, MOB showed higher variability in recovering the splitting value than clusterMOB (SD=0.155 and SD=0.126, respectively).

In Table \ref{tab:splitstats}, summary statistics for the splitting values recovered by MOB and clusterMOB are presented, for trees with the true tree size (i.e., total number of nodes $= 7$). As Table \ref{tab:splitstats} shows, for the right sized trees produced by clusterMOB, 22 out of 88,668 splits (0.025\%) involved the wrong variables. For the right sized trees produced by MOB, 361 out of 61,734 splits (0.585\%) involved the wrong variables. This makes the odds for selecting the wrong splitting variable 23.7 times higher for MOB than for clusterMOB. In recovering the true splitting values, the right-sized clusterMOB trees performed slightly better than the right-sized MOB trees: average splitting values were very close to the true splitting values for both algorithms, but standard deviations were 25 to 100\% higher for MOB, compared to clusterMOB (Table \ref{tab:splitstats}).



In Table \ref{tab:splitstats_wrongsizetrees}, summary statistics for the wrong-sized trees (i.e., total number of nodes $\neq 7$) are presented. For both MOB and clusterMOB, splitting values for the true partitioning variables were close to the true values, although the splitting values recovered by MOB had higher standard deviations. Out of a total number of 10,908 splits in wrong-sized trees created by clusterMOB, 1,522 (13.95\%) involved a wrong variable. For MOB, 11,582 (21.64\%) out of a total number of 53,524 splits in wrong-sized trees involved a wrong variable.









\subsection{Predictive accuracy on test data}

Overall, the treatment differences as predicted by clusterMOB were closer to the true differences than the predicted treatment differences of MOB. The average correlation between the true and predicted treatment differences over all 32,400 datasets was .88 (SD=0.20) for the trees generated by MOB, and .94 (SD=0.10) for the trees generated by clusterMOB.

To check whether correlations between true and predicted treatment differences are appropriate indicators of predictive accuracy, we compared the means and variances of the true and predicted treatment differences. The mean true treatment difference was 0 (Figure \ref{fig:modelC}). The mean treatment difference, averaged over all test datasets, as estimated by clusterMOB was -0.034 (SD=0.63), and as estimated by MOB was -0.031 (SD=0.95). This was only a small deviation from the expected value of 0, when compared to the spread of the distribution of treatment differences: The standard deviation of the true treatment differences, averaged over all test datasets, was 3.34 (SD=1.12). The average standard deviation of the predicted treatment differences for clusterMOB was 3.48 (SD=1.21) and 3.65 (SD=1.36) for MOB. These values indicate that true and predicted treatment differences had similar scales.


The linear model tree depicting the relationship between the various data-generating parameters and predictive accuracy of MOB and clusterMOB is presented in Figure \ref{fig:correlationtree}, showing that clusterMOB generally outperforms MOB. The dependent variable in this tree is the correlation between the true treatment differences, and the predictions of MOB and clusterMOB. Figure \ref{fig:correlationtree} shows clear main effects of sample size $N$, treatment difference size, and value of $\sigma_{b_i}$.

With larger sample sizes ($N =$ 500 or 1,000), both MOB and clusterMOB perform better than with smaller sample sizes ($N = 200$). With larger treatment differences ($d=5$ v.s. $d=2.5$), the difference between the performance of MOB and clusterMOB becomes smaller, and the performance of MOB shows less variation. When the random intercepts are sampled from a population distribution with large variance (i.e., $\sigma_{b_i} = 10$), the difference in accuracy between clusterMOB and MOB is more pronounced, than when there are no random intercept differences, or when the differences are small (i.e., $\sigma_{b_i} = $ 0 or 5).

As the boxplots in Figure \ref{fig:correlationtree} show, for some simulated datasets, correlations between predicted and true differences were obtained that were clear outliers. LIT should be noted that low or negative correlations between the true and predicted treatment differences were much more often found for MOB than for clusterMOB: a correlation $<.40$ was obtained in 930 out of 32,400 datasets for MOB, and in 175 out of 32,400 datasets for clusterMOB. 


\subsection{Example trees}

To illustrate the results of the application of clusterMOB and MOB, we selected a dataset from the simulation where the results of MOB are confounded by random effects, whereas clusterMOB recovered the right tree. The total number of observations in the dataset was 1,000, the correlation between partitioning variables $U$ was 0.30, the number of potential partitioning variables in $U$ was 5, the treatment effect difference was 5, the value of the random intercept $b_i$ was correlated to the value of $U_4$, the number of clusters was 10, and the variance of the distribution from which the random intercepts were drawn ($\sigma_{b_{i}}$) was 10. The resulting clusterMOB and MOB trees are presented in Figure \ref{fig:exampleREEMob} and \ref{fig:exampleMob}, to illustrate the differences between the results of both algorithms. 



Figure \ref{fig:exampleREEMob} shows that clusterMOB approximated the true tree structure as represented in Figure \ref{fig:modelC} quite well. The MOB tree in Figure \ref{fig:exampleMob} shows that the first partitioning variable ($U_2$) and splitting value ($\approx 30$) was correctly identified. However, after the first split, MOB incorrectly identified $U_4$ as a partitioning variable for observations with values $U_2 \leq 30$. Further, spurious partitions were made for these observations, involving variables $U_1$ and $U_4$. Although the splitting values were close to the true values for $U_1$ ($\approx 17$), node 5 wrongly indicates a lack of treatment effect differences, and node 10 wrongly indicates a treatment effect difference in favor of treatment 2. The right side of the tree also shows an additional spurious partition after node 11, involving $U_4$.



\section{Discussion}

The results of our simulation study show that clusterMOB performed very well in recovering treatment-subgroup interactions. clusterMOB recovered the right tree structure in 91.2\% of the simulated datasets. MOB performed less accurate than clusterMOB in recovering treatment-subgroup interactions, recovering the right tree structure in 63.5\% of the simulated datasets. The better performance of clusterMOB was mostly observed when random effects in the datasets were sizable, and random intercepts were correlated with potential partitioning variables. In these instances, the random effects gave rise to spurious subgroups (splits), in the trees resulting from application of MOB. 

The accuracies of both MOB and clusterMOB in predicting the difference in treatment outcomes were high, with accuracies of .88 and .94, respectively. clusterMOB clearly outperformed MOB when random effects in the datasets were sizable, and random intercepts were correlated to potential partitioning variables.

As expected, when simulated datasets contained no random effects, MOB and clusterMOB performed equally well. This finding indicates that clusterMOB can be applied, whenever cluster-specific random effects are expected: In the absence of random effects, MOB and clusterMOB are expected to perform equally well, and in the presence of random effects, clusterMOB will outperform MOB. Not surprisingly, accuracy of predicted treatment of both algorithms deteriorated when sample size was low (i.e., $N=200$). Sample size influenced performance of MOB and clusterMOB similarly, suggesting that the larger number of estimated parameters for clusterMOB did adversely influence accuracy with low sample sizes. However, our simulation results do warrant caution for the detection of treatment-subgroup interactions or treatment moderators in small datasets (e.g., single RCTs), irrespective of the algorithm used.

Although these findings are encouraging for the use of clusterMOB in the detection of treatment-subgroup interactions in datasets with clustered structures, some limitations of our study and challenges for future research should be noted.

As noted in the Introduction, simulations in the current study were confined to random-intercept models. clusterMOB allows for the estimation of random slopes as well, but estimation of random treatment effects is currently not possible, as treatment effects are estimated with local linear fixed-effects models. Our simulations also did not include models with multiple fixed-effects predictor variables in $X$. Multiple fixed-effects predictor variables can be easily included in MOB and clusterMOB, but it should be noted that the parameters corresponding to these variables will then be included in tests for parameter instability as well, which may be undesirable. For example, in RCTs, an ANCOVA will often be used, to control for the linear effects of pretreatment values on the treatment outcome variable. Whether or not such parameters should be included in parameter stability tests, or should be allowed to vary over partitions, should be decided by the researcher.     

One challenge for further research is the development of parameter stability tests for random-effects parameters. In the current study, random-effects parameters were estimated globally, using all observations in the dataset, and fixed-effects parameters were estimated locally, using the observations in a single node. This would allow, for example, for estimation of random treatment effects, instead of fixed treatment effects.

A second challenge is the development of more adequate ways to deal with missing data in treatment-subgroup interaction detection. MOB, like all tree-based algorithms for treatment-subgroup interaction detection, handles missing data by listwise deletion. However, missing data commonly occurs in clinical trails, and listwise deletion is obviously not the preferred method for dealing with missing data \cite<e.g., >{WoodyWhit04}.

In conclusion, clusterMOB provided highly accurate recovery of treatment-subgroup interactions and predictions of treatments differences in the presence and absence of cluster-specific random effects. Therefore, clusterMOB is a promising algorithm for the detection of treatment-subgroup interactions in datasets with a clustered structure, like for example in multi-center trials, individual-level patient data meta-analyses, and longitudinal studies.


\bibliographystyle{apacite}
\bibliography{refs}


\section{Appendix: Notation}

\begin{table}
\begin{tabular}{ll}
$1,...,i,...,n$	&	denotes cluster number \\
$1,...,j,...,t$ &	denotes terminal node number in a tree \\
$1,...,k,...,r$ &   denotes partitioning variable number \\
$\beta$ 		&	$2t \times 1$ column vector ${\left[ {\beta_l}^T,...,{\beta_j}^T,...,{\beta_t}^T \right]}^T$ of fixed-effects coefficients \\
$\beta_{j}$		& 	$2 \times 1$ column vector of fixed-effects coefficients in terminal node $j$ \\
$b$ 			&	$n \times 1$ column vector ${\left[ {b_l},...,{b_i},...,{b_n} \right]}^T$ of random intercepts \\
$b_{i}$ 		&	random intercept in cluster $i$\\
$d$				&	$\beta_1 / \sigma_{\epsilon}$; effect size of expected difference between treatment 1 and 2 in outcome $Y$\\
				&	with $X_1 = 0$ and $X_1 = 1$\\
$\epsilon$ 		&	$N \times 1$ column vector of residuals \\
$N$ 			&	total number of observations \\
$N_{i}$ 		&	number of observations in cluster $i$ \\
$N_{j}$			&	number of observations in terminal node $j$ \\
$\sigma_{b_i}$	&	square root of variance of $b_i$\\
$\sigma_{\epsilon}$&square root of the variance of $\epsilon$\\
$U$ 			&	$N \times r$ matrix of (potential) partitioning variables \\
$X$ 			&	$N \times 2t$ (block diagonal) design matrix of treatment indicators\\
$y$ 			&	$N \times 1$ column vector of response variable values \\
$Z$ 			&	$N \times n$ (block diagonal) design matrix of random effects predictor variables, \\
				&	with $N_i \times 1$ column vectors of ones on the diagonal \\
\end{tabular}
\end{table} 


\begin{figure}
    \includegraphics[width=8cm]{Interaction_ex;treedepth1.pdf}
	\caption{Example of a linear fixed-effects model for treatment outcomes (N=150). The dot for treatment 1 represents $\beta_0$, the dot for treatment 2 representes $\beta_1$.}
	\label{fig:fixed_effects}
\end{figure}

\begin{figure}
    \includegraphics[width=15cm]{Interaction_ex;treedepth3.pdf}
    \caption{Example of tree representation of model-based recursive partitions, based on the same data as Figure \ref{fig:fixed_effects}. Three additional covariates (anxiety questionnaire score, duration of depressive symptoms at baseline in months and age) were used as potential splitting variables.}
    \label{fig:example_mobtree}
\end{figure}


\begin{figure}
    \tikzstyle{decision} = [diamond, draw]
    \tikzstyle{line} = [draw, -stealth, thick]
    \tikzstyle{elli}=[draw, ellipse, minimum height=8mm, text width=6em, text centered]
   \tikzstyle{block} = [draw, rectangle, text width=7em, text centered, minimum height=15mm, node distance=6em]
    \begin{tikzpicture}[paths/.style={->, thick, >=stealth'}]
        \node [elli] (node1) { };
        \node [elli, below of =node1, xshift=-9em] (node2) { };
        \node [elli, below of =node1, xshift=9em] (node3) { };
        \node [block, below of=node2, xshift=-4em, yshift=-1em] (block1){\small$\beta_0=17.5$\\$\beta_1=-5.0$\\$d=-1.0$};
        \node [block, below of=node2, xshift=4em, yshift=-1em] (block2){\small$\beta_0=30.0$\\$\beta_1=0.0$\\$d=0.0$};
        \node [block, below of=node3, xshift=-4em, yshift=-1em] (block3){\small$\beta_0=30.0$\\$\beta_1=0.0$\\$d=0.0$};
        \node [block, below of=node3, xshift=5em, yshift=-1em, text width=8em] (block4){\small$\beta_0=42.5$\\$\beta_1=5.0$\\$d=1.0$};
        \draw [paths] (node1) to node [above left]{\small$U_2\leq30$} (node2);
        \draw [paths] (node1) to node [above right]{\small$U_2>30$} (node3);
        \draw [paths] (node2) to node [ left]{\small$U_1\leq17$} (block1);
        \draw [paths] (node2) to node [ right]{\small$U_1>17$} (block2);
        \draw [paths] (node3) to node [ left]{\small$U_5\leq63$} (block3);
        \draw [paths] (node3) to node [ right]{\small$U_5>63$} (block4);
    \end{tikzpicture}
	\caption{Data generating model for treatment-subgroup interactions. $d$ denotes the standardized mean difference between the outcomes of treatment 1 and 2 (i.e., $\beta_1 / \sigma_{\epsilon}$).}
	\label{fig:modelC}
\end{figure}

\begin{table}
\caption{Tree size distributions for MOB and clusterMOB.}
\small
\begin{tabular}{lrrrrrrrrr}
  \thickline
                    &       &   \multicolumn{7}{c}{MOB tree size}\\
                    \cline{3-9}
                    &       &   3   &   5   &   7   &   9   &  11  &   13   &   15  & total \\
  \hline
  clusterMOB  &   3   &   2   &   1   &    0  &   0   &   0  &    0   &    0  &    3  \\
  tree size                    &   5   &  13   & 228   &   33  &   3   &   0  &    0   &    0  &  277  \\
                    &   7   & 125   & 743  &19,936 &3,266  &3,731  &1,427   &  328 &29,556  \\
                    &   9   &   5   &  30   &  593 &1,161   & 404  &  220   &   59 & 2,472  \\
                    &  11   &   0   &   0   &   16  &  13   &  43  &   15   &    2  &   89  \\
                    &  13   &   0   &   0   &    1  &   0   &   0  &    3   &    0  &    3  \\
                    \cline{3-10}
                    & total &  145  &1002  &20,578	&4,443	&4,178 &1,665   &  389  &32,400  \\
  \hline
  \multicolumn{9}{l}{\textit{Note. }Tree sizes are expressed as the total number of nodes in the tree. A tree}\\
  \multicolumn{9}{l}{with a total of $k$ nodes has has $(k+1)/2$ terminal nodes.}\\
\end{tabular}
\label{tab:treesize}
\end{table}


\begin{sidewaysfigure}
    \includegraphics[width=20cm]{treesize_mob_maxdepth=4.pdf}
	\caption{Linear model tree of tree sizes for MOB and clusterMOB. The y-axes of the boxplots represent the total number of nodes in a tree (a tree with a total of $k$ nodes has $(k+1)/2$ terminal nodes). Circles represent outliers (values below $Q_1 - 1.5 \times IQR$ or above $Q3 + 1.5 \times IQR$). N = total sample size; corUbi = correlation between $b_i$ and one of the $U$ variables; sigmabi = $\sigma_{b_i}$; np = number of potential partitioning variables; rho = $\rho$; the correlation between the potential partitioning variables.}
	\label{fig:treesizetree}
\end{sidewaysfigure}



\begin{table}
\caption{Summary statistics for splitting values and variables of right-sized trees for MOB and clusterMOB}
\small
\begin{tabular}{cccrrrrrrr}
	\thickline
	 splitting & true && \multicolumn{3}{c}{MOB} && \multicolumn{3}{c}{clusterMOB}\\	
	\cline{4-6} \cline{8-10}
	variable &  value && \multicolumn{1}{c}{no. of splits} &  \multicolumn{1}{c}{M} & \multicolumn{1}{c}{SD} && \multicolumn{1}{c}{no. of splits}& \multicolumn{1}{c}{M} & \multicolumn{1}{c}{SD} \\
	\hline
	$U_1$ &  17 &  &  20,367&   16.49 & 1.07 & &  29,542& 16.57 & 0.84 \\
	$U_2$ &  30 &  &  20,602&   29.94 & 0.35 & &  29,562& 29.94 & 0.16 \\
	$U_3$-$U_4$& -&&    310&       - &   - & &      12&    - &  - \\
	$U_5$ &  63 &  &  20,404&   63.21 & 1.09 & &  29,542& 63.14 & 0.82 \\
	$U_6$-$U_{15}$&-&&   51& 	     - &  - & &	    10&     - &  - \\
	\cline{1-2} \cline{4-4} \cline{8-8} 
	\multicolumn{3}{l}{total no. of splits}& 61,734 & &&& 88,668 \\
\hline
\multicolumn{10}{l}{\textit{Note.} The total number of right-sized trees was 20,578 for MOB and 29,556 for}\\
\multicolumn{10}{l}{clusterMOB, out of a total of 32,400 datasets. The no. of splits refers to the number}\\
\multicolumn{10}{l}{of splits involving a variable; M and SD refer to the mean and the standard deviation}\\
\multicolumn{10}{l}{of the recovered splitting values for a variable. True splitting variables are $U_1$, $U_2$ and}\\
\multicolumn{10}{l}{$U_5$ (Figure \ref{fig:modelC}); therefore other variables do not have true splitting values. Note that}\\
\multicolumn{10}{l}{splitting variables may appear multiple times in the same tree, therefore a variable}\\
\multicolumn{10}{l}{may be involved in more splits than there are trees.}\\
\end{tabular}
\label{tab:splitstats}
\end{table}

\begin{table}
\caption{Summary statistics for splitting variables and values of wrong-sized trees for MOB and clusterMOB}
\small
\begin{tabular}{cccrrrrrrr}
	\thickline
	 & true && \multicolumn{3}{c}{MOB} && \multicolumn{3}{c}{clusterMOB}\\	
	\cline{4-6} \cline{8-10}
	variable &  value && \multicolumn{1}{c}{no. of splits} &  \multicolumn{1}{c}{M} & \multicolumn{1}{c}{SD} && \multicolumn{1}{c}{no. of splits}& \multicolumn{1}{c}{M} & \multicolumn{1}{c}{SD} \\
	\hline

	$U_1$ &  17 &   &  14,255&   15.00 & 4.57 & &    3,071&   16.05 & 3.43 \\
	$U_2$ &  30 &   &  13,592&   29.95 & 3.49 & &    3,243&   29.94 & 2.98 \\
	$U_3$-$U_4$&-& &   10,889&      - &   - & &     738&      -  &  - \\
	$U_5$ &  63 &   &  14,092&   64.77 & 4.53 & &    3,072&   63.81 & 3.32 \\
	$U_6$-$U_{15}$&-&&   696&	  - &   - & &      784	&   - &	 - \\
	\cline{1-2} \cline{4-4} \cline{8-8} 
	\multicolumn{3}{l}{total no. of splits} & 53,524 & &&& 10,908 \\
	\hline
\multicolumn{10}{l}{\textit{Note.} The total number of wrong-sized trees was 11,822 for MOB and 2,844 for}\\
\multicolumn{10}{l}{clusterMOB, out of a total of 32,400 datasets. The number of splits in a wrong-}\\ 
\multicolumn{10}{l}{sized tree is unequal to 3, and can be any other number from 1 through 7. The}\\
\multicolumn{10}{l}{no. of splits refers to the number of splits involving a variable; M and SD refer}\\
\multicolumn{10}{l}{to the mean and standard deviation of the recovered splitting value for a variable.}\\
\multicolumn{10}{l}{True splitting variables are $U_1$, $U_2$ and $U_5$ (Figure \ref{fig:modelC}); therefore other variables}\\
\multicolumn{10}{l}{do not have true splitting values. Note that splitting variables may appear multiple}\\ 
\multicolumn{10}{l}{times in the same tree, therefore a variable may be involved in more splits than}\\ 
\multicolumn{10}{l}{there are trees.}\\
\label{tab:splitstats_wrongsizetrees}
\end{tabular}
\end{table}

\begin{sidewaysfigure}
	\includegraphics[width=20cm]{correlation_mob_maxdepth=4.pdf}
	\caption{Linear model tree of correlations with true treatment differences for clusterMOB and MOB trees. Circles represent outliers (values below $Q_1 - 1.5 \times IQR$ or above $Q3 + 1.5 \times IQR$). N = total sample size; treatdiff = $\beta_1$, the unstandardized mean difference in treatment outcomes in subgroups with differential effects for treatment 1 and treatment 2; sigmabi = $\sigma_{b_i}$.}
	\label{fig:correlationtree}
\end{sidewaysfigure}


\begin{sidewaysfigure}
    \includegraphics[width=20cm]{Mobtree1218.pdf}
	\caption{Example tree build by MOB.}
	\label{fig:exampleMob}
\end{sidewaysfigure}


\begin{figure}
    \includegraphics[width=12cm]{REEMobtree1218.pdf}
	\caption{Example tree build by clusterMOB.}
	\label{fig:exampleREEMob}
\end{figure}


\end{document} 
