\documentclass[nobf,doc]{apa}
\usepackage{amssymb}
\usepackage{amsmath,amsthm,tabularx,curves,texdraw,psfrag}
\usepackage{apacite}
\usepackage[english]{babel}
\usepackage{lscape,dcolumn,hhline}
\usepackage{graphicx,epic,eepic,rotating}
\usepackage[normalem]{ulem}
\usepackage{fancyhdr}
\usepackage{eurosym,bbding}
\usepackage{verbatim}
\usepackage{bigfoot}
\usepackage{ctable}
\usepackage{rotating}
\usepackage{lineno}
\usepackage{placeins}
\usepackage{listings}
\linespread{1.2}
\usepackage{tabto}

%% for \usepackage{Sweave}
\SweaveOpts{echo=FALSE, results=hide}
\setkeys{Gin}{width=0.8\textwidth}

<<preliminaries>>=
library("partykit")
@

\setkeys{Gin}{width=0.8\textwidth}



\title{Prediction rule ensembles in R}
\shorttitle{Prediction rule ensembles in R}
\leftheader{Prediction rule ensembles in R}
\rightheader{Prediction rule ensembles in R}
\author{M. Fokkema$^1$}
\affiliation{$^1$Universiteit Leiden}

%\acknowledgements{}

\begin{document}
	
\maketitle
\pagewiselinenumbers

\section{Abstract}
Prediction rule ensembles are a highly interpretable method for regression and classification. Prediction rule ensembles consist of a small set of simple decision rules, which are easy to evaluate and apply in practice. The current paper discusses rule ensemble methodology, and introduces the unbiased rule prediction ensemble (upre) algorithm. The upre algorithm and its software implementation are largely based on the prediction rule ensemble methodology of \citeA{FrieyPope08}, with some added features and improvement. To illustrate the algorithm and the \verb|R| package \verb|upre|, upre will be applied to real datasets and the results will presented. The software is well documented and freely available. It is hoped that readers, after reproducing the examples, will wave their hands in the air like they just don't care and exclaim 'Upre? I think it's super!'.


\section{Introduction}

Prediction rule ensembles provide accurate and interpretable methods for regression and classification. Prediction rules are logical statements of the form if [conditions] then [prediction], which are highly usable in clinical decision making. The rules in prediction rule ensembles can be seen as very simple decision trees \cite{FokkySmit15a, KatsyPach08}. Decision trees can be derived by, for example, the classification and regression trees (CART; \citeNP{BreiyFrie84}) algorithm, and such trees are well known for their interpretability and applicability in practical in decision making. However, single trees are also known for their instability: small changes in the training data may cause large changes in the resulting tree \cite<e.g., >{Dann00}.  

A powerful solution to this problem of instability is combining the predictions of many single trees, with techniques like bagging, boosting and random forests. Ensembling trees has been shown to substantially improve predictive accuracy \cite<e.g., >{Brei96, Diet00, StroyMall09}. However, the tree ensembles generally consist of a large number of trees, and are therefore difficult to interpret, let alone apply without the help of statistical software. A trade-off between accuracy and interpretability seems to apply: single trees provide high interpretability but lower accuracy, whereas ensembles provide high accuracy but lower interpretability. 

Rule-based ensembles aim to balance accuracy and interpretability: accuracy is improved by ensembling predictions and interpretability is improved by using prediction rules instead of full trees. Several algorithms for deriving rule-based ensembles have been developed. Most are aimed exclusively at classification, like ROCCER, \cite{PratyFlac05}, SLIPPER \cite{CoheySing99}, Lightweight Rule Induction \cite{WeisyIndu00} and C4.5rules \cite{Quin14}. In contrast, the Rulefit \cite{FrieyPope08}, ENDER \cite{DembyKotl10} and Node Harvest \cite{Mein10} algorithms can be applied to both classification and regression problems. Among these three, Rulefit is most explicitly aimed at balancing accuracy and interpretability, as it employs regularized regression to select only a small set of prediction rules for the final ensemble. In contrast, ENDER and Node Harvest produce ensembles with a large number of rules, limiting interpretability of the ensemble. 

The aim of the current paper is to introduce the unbiased prediction rule ensemble (upre) algorithm and its software implementation. The upre algorithm is largely based on Rulefit, as described by \cite{FrieyPope08}, and derives a prediction rule ensembles in two stages. In the first stage, a large initial ensemble of rules is generated by taking the nodes from trees that are grown on a large number of samples drawn from the training dataset. This will be further described in the subsection Rule generation. In the second stage, a small final ensemble of prediction rules is selected by means of penalized regression, which will be further described in the subsection Rule selection. In the subsection Comparison with Rulefit, the differences and similarities between upre and Rulefit will be discussed. In the subsection Illustration, the upre algorithm will be applied to several real datasets, to illustrate the algorithm and its \verb|R| implementation. In the Simulation section, the predictive accuracy of upre will be assessed and compared with that of Rulefit and Node Harvest, using simulated datasets. In the Discussion, results will be summarized, and further research and future developments will be discussed.   




\subsection{Rule generation}

The upre algorithm derives prediction rules from an ensemble of trees, which are sequentially grown on random samples of the training data. The trees grown by upre are conditional inference trees or ctrees \cite{HothyHorn06, ZeilHoth08}. Although tree-based ensemble algorithms generally use CART for growing trees, CART uses biased variable selection criteria: given two equally informative input variables, the variable with a larger number of unique and/or missing values has a higher probability of being selected as a splitting variable \cite<e.g., >{KimyLoh01, StroyBoul08}. The ctree algorithm provides unbiased variable selection, by using statistical tests for parameter instability to select splitting variables. For further details on the parameter instability tests and split selection, readers are referred to \citeNP{HothyZeil06}. 

An example ctree is presented in Figure \ref{fig:regression_tree}. The first node of a tree does not represent an informative rule, as it applies to all observations in the sample. Therefore, although the tree in Figure \ref{fig:regression_tree} consists of nine nodes, the following eight prediction rules are derived from it:

\begin{multline*}
	\\
	r_1(\mathbf{x}) = I (x_4 \leq 82)\\
	r_2(\mathbf{x}) = I (x_4 \leq 82 \cdot x_3 \in \{a,c\})\\
	r_3(\mathbf{x}) = I (x_4 \leq 82 \cdot x_3 \in \{b,d,e\})\\
	r_4(\mathbf{x}) = I (x_4 \leq 82 \cdot x_3 \in \{b,d,e\} \cdot x_4 \leq 77)\\
	r_5(\mathbf{x}) = I (x_4 \leq 82 \cdot x_3 \in \{b,d,e\} \cdot x_4 > 77)\\
	r_6(\mathbf{x}) = I (x_4 > 82)\\
	r_7(\mathbf{x}) = I (x_4 > 82 \cdot x_5 \leq \textrm{seldom})\\
	r_8(\mathbf{x}) = I (x_4 > 82 \cdot x_5 > \textrm{seldom})\\
\end{multline*}

Where $\mathbf{x}$ is random vector of $n$ input variables, $I$ is an indicator of the truth of its argument, and $r_m(\mathbf{x})$ denotes prediction rule $m$, taking a value of 0 when its conditions do not apply, and a value of 1 when its conditions apply. Note that the tree in Figure \ref{fig:regression_tree} shows that input variables used for growing trees may be continuous (like $x_4$), unordered categorical (like $x_3$) or ordered categorical (like $x_5$). Also, that the same input variable may appear multiple times in the same rule (like $x_4$ in $r_4(\mathbf{x}) or r_5(\mathbf{x})$).


\begin{figure}[t!]
\setkeys{Gin}{width=\textwidth}
<<example-tree, fig=TRUE, width= 5, height= 5>>=
ex_tree <- party(
# node 0:
partynode(0L,
split = partysplit(2L, breaks = 82),
kids = list(
# node 1:
partynode(1L,
split = partysplit(1L, breaks = 2),
kids = list(
# node 2:
partynode(2L, info = ""), 
# node 3:
partynode(3L, 
split = partysplit(2L, breaks = 77),
kids = list(
# node 4:
partynode(4L, info = ""),
# node 5:
partynode(5L, info = ""))))),
# node 6:
partynode(6L,
split = partysplit(3L, breaks = 1),
kids = list(
# node 7:
partynode(7L, info = " "),
# node 8:
partynode(8L, info = " "))))), 
# dataset:
data.frame(x3 = factor(character(0), levels = c("a", "c", "b", "d", "e"), ordered = FALSE), 
x4 = numeric(0), 
x5 = factor(character(0), levels = c("seldom", "somtimes", "regularly"), ordered = TRUE))
)
plot(ex_tree, inner_panel = node_inner(ex_tree, id = FALSE), terminal_panel = node_terminal(ex_tree, id = FALSE, fill = "white"))
@
\label{fig:regression_tree}
\caption{Example conditional inference tree. Note that a conditional inference tree would normally depict the distribution of the outcome variable $y$ in each of the terminal nodes. For generating the initial ensemble of prediction rules, only the tree structure, not the distribution of the outcome variable is used, which is therefore omitted here.}
\end{figure}

To derive the ensemble of trees, approaches similar to bagging \cite{Brei96bagging}, boosting \cite{FreuyShap95}, random forests \cite{Brei01RandFor}, or a combination can be used. In either appraoch, a user-specified number of random samples (500, by default) of size $\eta$ are drawn from the training data with or without replacement. Setting $\eta = 1$ will result in sampling with replacement, or bootstrap sampling. Setting $\eta < 1$ will result in sampling without replacement, or subsampling. %In some circumstances, subsampling may lead to better results: Strobl psymeth paper and some other reference. 
Trees are grown on the random samples in a sequential fashion, with learning rate $\nu = 0, \dots, 1$. The learning rate $\nu$ corresponds to the shrinkage parameter in boosting, and controls the influence of earlier trees in the induction of new trees. Let $m = 1, \dots, M$ denote the random sample of observations drawn from the training data, and $f_m(\mathbf{x})$ the predictive function from the tree grown on subsample $m$. Then every tree $m > 1$ is grown using outcome variable $u_m$, instead of the original outcome variable $y$:

\begin{equation}
	u_m  = y - \sum_{k=1}^{m-1}{\nu \cdot f_k(\mathbf{x})} 
	% some kind of notation for residualized y should be used
	% first, the mean of y should be substracted, which is $f_0(\mathbf{x})$
\end{equation}
	
Note that setting $\nu = 0$ removes the influence of earlier trees, with each tree being grown using the original values of the outcome variable $y$. Setting $\nu = 1$ maximizes the influence of earlier trees. \citeA{FrieyPope03} have found small, nonzero values of $\nu$ (e.g., .01) to perform best for ensembles of small decision trees. In addition, they found their results to be fairly insensitive to the value selected for $\eta$, provided it was small (i.e., $\eta < N/2$ for $N \leq 500$ and $\eta < 10 \cdot \sqrt{N}$ for $N > 500$). The prediction rules in a upre ensemble can be based on a bagged ensemble by setting $\nu = 0$ and $\eta = 1$. The rules can be based on a random forest by setting $\nu = 0$, $\eta = 1$ and the number of input variables to be selected as candidates for each split to values $< n$. Note that $\nu$, $\eta$ and the number of input variables to be selected as candidates for each split can be varied independently from each other, to create a hybrid of bagging, boosting, and random forests.

Note that the original bagging and random forest algorithms make use of unpruned CART trees, which can be mimicked in the upre software implementation by setting the maximum tree depth to infinity. However, in situations with a large number of observations and few variables, \citeA{LinyJeon06} have found predictive accuracy of random forests to improve when tree size is limited. In addition, smaller trees provide shorter prediction rules, which are easier to interpret. Therefore, by default upre restricts maximal tree depth to 3, thereby limiting the maximum number of conditions of rules to 3. In effect, this limits the interactions that can be captured by the rule ensemble to second-order and three-variable interactions. Users may provide any other integer value for maximum tree depth, as well.   

After the ensemble of trees is generated, every node from every tree is included in the initial rule ensemble, after removing perfectly collinear rules (i.e., rules with conditions or support identical to that of earlier rules). 

In addition, users may choose the original input variables to be added as linear terms to the initial ensemble. Although non-linear functions of input variables are relatively easy for rules to approximate, linear functions are not. Therefore, addition of the input variables may improve sparsity and predictive accuracy of the final ensemble. To reduce the effect of possible outliers, input variables may be winsorized before entering the predictive model. The linear terms are given by: 

\begin{equation}
	l_j(x_j) = \min(\delta_j^+, max(\delta_j^-, x_j))
	\label{eq:linear_terms}	
\end{equation}

where $j$ is an indicator for the input variable, and $\delta_j$ and $\delta_j$ are the $\beta$ and $(1-\beta)$ quantiles of the distribution of $x_j$ in the training data. The value chosen for $\beta$ reflect user's prior suspicion of the fraction of outliers. By default $\beta$ is set to .025. Setting $\beta = 0$ will result in the input variables not being winsorized. 


\subsection{Selection of the final ensemble}

The initial ensemble consists of a large number of rules and/or linear functions, only a few of which may contribute to predictive accuracy on future observations. Therefore, coefficients for the rules and/or linear terms for the final predictive model are derived using penalized regression. When both rules and linear terms are included, the predictive model becomes:

\begin{equation}
	F(\mathbf{x}) = \hat{a}_0 + \sum_{k=1}^K{\hat{a}_k r_k(\mathbf{x}) + \sum_{j=1}^n{\hat{b}_j l_j(x_j)}} 
	\label{eq:predictive_model}
\end{equation} 

Where $1 \dots j \dots n$ is an index for the input variable and $1 \dots k \dots K$ is an index for the rule in the initial ensemble. The coefficient values are estimated by means of penalized regression. When the final model is selected by lasso regression \cite{Tibs96}, the estimates satisfy the following minimization criterion: 

\begin{equation}
	( \{ \hat{a}_k \}^K_0 , \{ \hat{b}_j \}^n_1  ) = 
	\arg\min_{ \{ \hat{a}_k \}^K_0 , \{ \hat{b}_j \}^n_1 } \sum_{i=1}^N{L \left( 
		y_i, \hat{a}_0 + \sum_{k=1}^K{\hat{a}_k r_k(\mathbf{x}_i) + \sum_{j=1}^n{\hat{b}_j l_j(x_ij)}}  \right)} +
	\lambda \left( \sum_{k=1}^K{| \hat{a}_k |} + \sum_{j=1}^n{| \hat{b}_j |} \right) 
	\label{eq:lasso_regression}
\end{equation}

The first term is the prediction risk over the $N$ training observations, and the second term is a penalty for non-zero coefficients of rules and linear terms. The influence of the penalty term is determined by the value of $\lambda \geq 0$. The value of $\lambda$ is taken to be that which minimizes the prediction risk on a separate validation dataset, or by multi-fold cross validation on the training data. The penalty term in Equation \ref{eq:lasso_regression} forces many coefficients to be set to zero, providing a final ensemble consisting of a small set of rules and/or linear terms. Alternatively, ridge or elastic net penalties \cite{ZouyHast05} may be applied in Equation \ref{eq:lasso_regression}, instead of the lasso penalty, but this will generally yield less sparse models.

The penalty term in \ref{eq:lasso_regression} is sensitive to linear transformations of the input variables: generally, terms with smaller variance are more heavily penalized. Users can give a linear terms the same weight in the penalty term as a typical rule, by selecting its normalized values to be used in minimizing Equation \ref{eq:lasso_regression}:

\begin{equation} 
	l_j(x_j) \leftarrow 0.4*l_j(x_j) / \mathit{std}(l_j(x_j))
\end{equation}

Alternatively, all rules and linear functions may be standardized prior to estimating Equation \ref{eq:lasso_regression}, giving each rule and linear term equal a-priori influence. However, rules with support close to zero or one are defined by a correspondingly small number of training observations, and reducing penalization of these terms may be unwanted. 

% In the 2010 version of RuleFit, Huber loss was an option for estimating the final model. In the 2012 version, Huber loss is no longer used (it seems).


\subsection{Interpreting the final ensemble: variable importances and interactions}

\subsubsection{Importances}

The estimated coefficients for rules and/or linear terms in the final ensemble provide an unstandardized global measure of their contribution to the predictive model. Importances are provide standardized measures of their contribution to the predictive model, and can be calculated for rules, linear terms, as well as input variables (as input variables may appear in the final model as part of one or more rules, as well as a linear term). Variable importances provide a measure of the extent to which individual input variables influence the predictions of the final rule ensemble. Importances can be calculated globally (averaged over the whole training dataset) or locally (over a selected subregion of the input variable space). The global importance of a rule $r_k$ is given by 

\begin{equation}
	I_k = |\hat{a}_k| \cdot \sqrt{s_k(1-s_k)} % equation 28 in F\&P
	\label{eq:global_rule_imp}
\end{equation}

where $s_k$ is the support of rule $r_k$ on the training data, given by 

\begin{equation}
	s_k = \frac{1}{N} \sum^{N}_{i=1}{r_k(x_i)}
	\label{eq:global_support}
\end{equation}

The global importance of linear term $l_j(x_j)$ is given by

\begin{equation}
	I_j = |\hat{b}_j| \cdot \mathit{std}(l_j(x_j)) % equation 29 in F\&P
	\label{eq:global_linear_imp}
\end{equation}

where $std(l_j(x_j))$ is the standard deviation of the linear term $l_j(x_j)$ (Equation \ref{eq:linear_terms}) in the training data. The importance of an input variable $J_j$ is given by the sum of the importance of its linear term $I_j$, and a weighted sum of the importances of the rules $I_k$ in which the variable appears:

\begin{equation}
	J_j = I_j + \sum_{x_j \in r_k}{I_k / m_k}
	\label{eq:glob_inputvar_imp}
\end{equation}

where $m_k$ is the number of variables that define rule $k$. The second term in Equation \ref{eq:glob_inputvar_imp} shows that the importance of a rule is distributed equally over the input variables appearing in the rule\footnote{Note that when $x_j$ appears multiple times in the conditions of $r_k$, Equation \ref{eq:glob_inputvar_imp} is not completely accurate. In those instances, the importance of $r_k$ that is distributed to the importance $J_j$ equals $\frac{\textrm{the number of times } x_j \textrm{appears in the conditions of} r_k \times I_k}{\textrm{the total number of conditions in} r_k}$}.

Local importances for a selected subregion $S$ of the input variable space can be calculated by replacing the global support $\s_k$ in Equation \ref{eq:global_rule_imp} by the support in the selected subregion, which is given by

\begin{equation}
	s_k(S) = \frac{1}{|S|} \sum_{x_i \in S}{r_k(x_i)}
	\label{eq:local_support}
\end{equation}

where $|S|$ is the cardinality (number of training set observations) in subregion $S$. Similarly, the local importance of a linear term over a selected subregion can be calculated by replacing the global standard deviation $\mathit{std}(l_j(x_j))$ in Equation \ref{eq:global_linear_imp} by its standard deviation in the selected subregion:

\begin{equation}
	\mathit{std}(l_j(x_j), S) = \sqrt{ \frac{1}{|S| - 1} \sum_{x_i \in S}{(l_j(x_ij) - \bar{l}_j(x_j, S))^2} }
	\label{eq:local_sd}
\end{equation}

where $\bar{l}_j(x_j, S)$ is the mean of $l(x_j)$ in subregion $S$. 

Local importances can also be calculated for a single point $\mathbf{x}$. The local importance of rule $r_k$ at a single point $\mathbf{x}$ is given by

\begin{equation}
	I_k(\mathbf{x}) = |\hat{a_k}| \cdot |r_k(\mathbf{x}) - s_k |)% equation 30 in F\&P
	\label{eq:local_rule_imp}
\end{equation}

The local importance of linear term $l_j(x_j)$ at a single value $x_j$ is given by

\begin{equation}
	I_j(x_j) = |\hat{b_j}| \cdot |l_j(x_j) - \bar{l}_j |)% equation 31 in F\&P
	\label{local_linear_imp}
\end{equation}

where $\bar{l}_j$ is the mean of $l_j(x_j)$ over the training data. The quantities in Equations \ref{eq:global_rule_imp}, \ref{eq:global_linear_imp}, \ref{eq:local_rule_imp} and \ref{eq:local_linear_imp} represent the absolute change in the (average) prediction $F(\mathbf{x})$ when the corresponding rule or linear term is dropped from the predictive model (Equation \ref{eq:predictive_model}), when the intercept would be adjusted accordingly (i.e., $\hat{a}_0 \leftarrow \hat{a}_0 + \hat{a}_k s_k$ for rules and $\hat{a}_0 \leftarrow \hat{a}_0 + \hat{b}_j \bar{l}_j$ for linear terms). The local importance of input variable $x_j$ at each point $\mathbf{x}$ is given by

\begin{equation}
	J_j(\mathbf{x}) = I_j(\mathbf{x}) + \sum{x_j \in r_k}{I_k(\mathbf{x}) / m_k} 
\end{equation} 


\subsection{Partial dependence plots}

The influence of individual or pairs of predictor variables on the predictions of the final rule ensemble can be also assessed visually by plotting the partial dependence of $F(\mathbf{x})$ on individual or pairs of input variables. The partial dependence of $F(\mathbf{x})$ on a subset of input variable $s \subset \{1, ..., n\}$ is the expected value of $F(\mathbf{x})$ over the marginal joint distribution of variables $\mathbf{x}_{\backslash s}$ not in $\mathbf{x}_s$. It is defined as

\begin{equation}
	F_s(\mathbf{x}_s) = E_{\mathbf{x}_{\backslash s}} [F(\mathbf{x}_s, \mathbf{x}_{\backslash s})] % equation 39 in F/&P
\end{equation}

and can be estimated from the data by

\begin{equation}
	\hat{F}_s(\mathbf{x}_s) = \frac{1}{N} \sum^N_{i=1}{F(\mathbf{x}_s, \mathbf{x}_{i \backslash s})} % equation 40 in F/&P
\end{equation}

where $\{ \mathbf{x}_{i \backslash s} \}^N_1$ are the observations in the training dataset. The subset of input variables $s$ must consist of single or a pair of input variables to allow for plotting the partial dependence of $F(\mathbf{x})$ on $\mathbf{x}_s$.




\subsubsection{Interaction effects}

Prediction rules are well suited to capturing interaction effects of input variables. However, the presence of prediction rules involving multiple variables in the final ensemble are a necessary but not sufficient condition for the presence of interaction effects. Different rules involving the same sets of variables may eliminate the interaction effect of these variable in the final predictive model $F(\mathbf{x})$. Therefore, the presence of interaction effects should be assessed by analyzing the full predictive model $F(\mathbf{x})$, for example by means of partial dependence plots and computation of interaction statistics, which will be discusses in this section. 

If an input variable $x_j$ does not interact with any of the other input variables, then the effect of $x_j$ and $x_{\backslash j}$ on $F(\mathbf{x})$ are additive, and therefore can be expressed as

\begin{equation}
	F(\mathbf{x}) = F_j(x_j) + F(\mathbf{x}_{\backslash j}) % equation 42
\end{equation}

where $F_j(x_j)$ is the partial dependence of $F(\mathbf{x})$ on $x_j$ and $F(\mathbf{x}_{\backslash j})$ is the partial dependence of $F(\mathbf{x})$ on $\mathbf{x}_{\backslash j}$. A statistics for testing whether $x_j$ interacts with any other input variable is given by

\begin{equation}
	H^2_j = \frac{\sum^N_{i=1}{ [F(\mathbf{x}_i) - \hat{F}_j(x_{ij}) - \hat{F}_{\backslash j}(x_{i \backslash j})]^2 }}{{\sum^N_{i=1}{ F^2(\mathbf{x}_i)}
	\label{eq:interaction_statistic}
\end{equation}

$H^2_j$ will differ from zero to the extent that $x_j$ is involved in interactions with other input variables. To assess whether $H^2_j$ differs significantly from zero, a null distribution for the statistic has to be derived. \cite{FrieyPope08} suggest the use of a variant of the parametric bootstrap \cite{EfroyTibs94} to derive a null distribution of statistic $ H_j^2$. The statistic $H^2_j$ is repeatedly computed on a series of artificial datasets from which interactions involving $x_j$ are absent. The distribution of test statistics thus obtained can be used as a reference distribution, to assess significance of the value of  $H^2_j$ based on the training data.  


%To disencourage spurious interactions entering the model, F\&P (section 8.2) use a strategy to put an incentive on variables entering a tree, that do not appear earlier in the tree. I currently do not know of a way to do that for ctrees.


\subsection{Comparison with Rulefit}

- ctree instead of CART trees
- Rulefit uses an approach that can be best described as a combination of bagging and boosting: it takes subsamples\footnote{With subsampling, observations are drawn without replacement from the full dataset, so strictly speaking, Rulefit does not use a bagging approach, as this would involve bootstrap samples of the dataset.} of size $\eta$ of the training data, and grows trees in a sequential fashion with learning rate $\nu = 0, \dots, 1$.
- Rulefit allows for inducing trees in a manner that can be best described as a hybrid of bagging \cite{Brei96bagging} and boosting \cite{Frie01}. In some situations, bagged ensembles of trees may be outperformed by random forests in terms of predictive accuracy \cite{Diet00bbrf}. Also, Node Harvest derives prediction rules from the nodes of random forest trees. Therefore, upre allows for inducing trees in a bagging, boosting or random forest type manner, or a hybrid of the three.    
- Rulefit also provides forward stagewise regression
- Current implementation of rulefit does not provide user-friendly interface for rules inspection, does not provide value of lambda or intercept, and does not allow user to select a different values for lambda (e.g., lamba min or lambda 1se).
- In the pairplot function, rulefit does not seem to cross all possible values of x1 and x2, it may just take all observed combinations. Then, correlated x1 and x2 may result in a plot that looks like an interaction, but it is not. Check if my plotting function gives a different results, although it may be slower (but then, it is immediately clear why))

\subsection{Miscellaneous}

RuleFit returns the cross-validated model selection criterion value with standard error (when test.reps > 1), and number of terms in the resulting model, in the command line. 
The penalized regression is performed on the whole set of training observations. However, the value of the penalty parameter(s) is chosen to be the value that minimizes future prediction risk, estimated in a separate sample not used in training, or by full multi-fold cross validation.

"3.2. Missing values. An interesting property of NH is its natural ability to
cope with missing values. Once a fit is obtained, predictions for new data can be
obtained without use of imputation techniques or surrogate splits. To fit the node
harvest estimator with missing data, we replace missing values in the matrix X
by the imputation technique described in ...."

Users are adviced to not use the lambda path determined by \verb|glmnet|, include a way to determine own path? 

Future implementations may support multivariate outcomes. ctree() allows for multivariate responses. If glmnet also allows for that, multivariate rule ensembles can be created.


\subsection{Extensions to random-effect models}

Schelldorder, Meier \& Buhlmann (2014) propose a two-step approach for high-dimensional GLMMs: In the first step, variable screening is performed, selecting a set of candidate active variables. In the second step, refitting by maximum likelihood (ML) estimation is performed to get accurate parameter estimates.
Groll \& Tutz (2010) propose a method that works by combining gradient ascent optimization with the Fisher scoring algorithm and is based on the approach of Goeman (2010).
 



\subsection{Alternative rule ensemble methods}

Meinshausen's Node harvest algorithm derives an initial ensemble Q which is very large (thousands of rules). The rules in the initial ensemble can be generated at random from the training data, without using a response variable. However, Meinshausen (2010) suggests to derive the rules from the trees in a random forest (Breiman, 2001). However, the trees are fitted on subsamples of the data (of size n/10), rather than on bootstrap samples. Next, the nodes that satisfy maximal interaction order and minimal node size constraints are added to the initial ensemble, provided that they are not already present in the set. If two or more nodes in the initial ensemble contain the exact same set of training observations, only one is randomly selected and included in the ensemble. 

Node Harvest uses only the nodes (rules) in which a new observation `falls' for prediction, requiring only a small number of rules for a single observation. However, future observations may `fall' into any of the nodes of the initial ensemble, so all rules from the initial (and large) ensemble have to be retained to allow for prediction of future observations. 

ROCCER \cite{PratyFlac05}, SLIPPER \cite{CoheySing99}, LRI \cite{WeisyIndu00}, and C4.5rules \cite{Quin14} are for classification problems only. \cite{DembyKotl10} can be used for both classification and regression. None of these methods are implemented in statistical software like R of SPSS. 

Lightweight Rule Induction \cite{WeisyIndu00}: "A light weight rule induction method is described that generates compact Disjunctive Normal Form (DNF) rules. Each class has an equal number of unweighted rules. A new example is classified by applying all rules and assigning the example to the class with the most satisfied rules. The induction method attempts to minimize the training error with no pruning. An overall design is specified by setting limits on the size and number of rules. During training, cases are adaptively weighted using a simple cumulative error method. The induction method is nearly linear in time relative to an increase in the number of induced rules or the number of cases. Experimental results on large bench-mark datasets demonstrate that predictive performance can rival the best reported results in the literature."

SLIPPER \cite{CoheySing99}: "We describe SLIPPER - a new rule learner that generates rulesets by repeatedly boosting a simple, greedy rule builder. Like the rulesets built by other rule learners, the ensemble of rules created by SLIPPER is compact and comprehensible. This is made possible by imposing appropriate constraints on the rule-builder, and by use of a recently proposed generalization of Ad-aboost called confidence-rated boosting. In spite of its relative simplicity, SLIPPER is highly scalable, and an effective learner. Experimentally, SLIPPER scales no worse than O(n log n), where n is the number of examples, and on a set of 32 benchmark problems, SLIPPER achieves lower error rates than RIPPER 20 times, and lower error rates than C4.5rules 22 times."

\cite{Quin14} C4.5rules is for classification only

SLIPPER and ENDER take a boosting approach to rule induction. 

ENDER \cite{DembyKotl10}: "We believe that ENDER can still be used for interpretation purposes. One way is to follow the approach given in Friedman and Popescu (2008) that relies on a postprocessing phase in which the rules are refitted by using the lasso regularization."

 
 





\bibliography{bib}

\end{document}