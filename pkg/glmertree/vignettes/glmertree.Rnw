\documentclass[nojss]{jss}

%\VignetteIndexEntry{Detecting Treatment-Subgroup Interactions with Generalized Linear Mixed-Effects Model Trees}
%\VignetteDepends{glmertree}
%\VignetteKeywords{mixed-effects model trees, recursive partitioning, decision trees}
%\VignettePackage{glmertree}

%% packages
\usepackage{thumbpdf,lmodern}

\title{Fitting Generalized Linear Mixed-Effects Model Trees}
\Shorttitle{Generalized Linear Mixed-Effects Model Trees}
\author{Marjolein Fokkema\\Universiteit Leiden \And Achim Zeileis\\Universit\"at Innsbruck}
\Plainauthor{Marjolein Fokkema, Achim Zeileis}

\Abstract{
This vignette briefly introduces the \pkg{glmertree} package for fitting a wide range of generalized linear mixed-effects model trees (GLMM trees or glmertrees). In hands-on examples baed on artificial datasets, emphasis is given to the special cases of fitting regression trees with constant fits in the terminal nodes to clustered data (Section~\hyperref[sec:regression_trees]{\ref{sec:regression_trees}}), detecting treatment-subgroup interactions in clustered data (Section~\hyperref[sec:treatment_subgroups]{\ref{sec:treatment_subgroups}}), and detecting subgroups in linear growth curve models (Section~\hyperref[sec:growth_curves]{\ref{sec:growth_curves}}). 
}

\Keywords{recursive partitioning, mixed-effects model trees, decision trees}

\Address{
  Marjolein Fokkema\\
  Department of Methods \& Statistics, Intitute of Psychology\\
  Universiteit Leiden\\
  Wassenaarseweg 52\\
  2333 AK Leiden, The Netherlands\\
  E-mail: \email{m.fokkema@fsw.leidenuniv.nl}\\
  URL: \url{http://www.marjoleinfokkema.nl/}\\

  Achim Zeileis \\
  Department of Statistics \\
  Faculty of Economics and Statistics \\
  Universit\"at Innsbruck \\
  Universit\"atsstr.~15 \\
  6020 Innsbruck, Austria \\
  E-mail: \email{Achim.Zeileis@R-project.org} \\
  URL: \url{https://eeecon.uibk.ac.at/~zeileis/} \\
}


\begin{document}
\SweaveOpts{concordance=TRUE}

\vspace*{-0.35cm}

\SweaveOpts{engine=R, keep.source=TRUE}
<<preliminaries, echo=FALSE, results=hide>>=
library("glmertree")
options(width = 70, prompt = "R> ", continue = "+  ")
@

\section{Introduction}

Generalized linear mixed-effects model trees (GLMM trees or glmertrees) have recently been proposed by \cite{FokkySmit18} for detecting treatment-subgroup interactions in clustered datasets. Using a hands-on (artificial) example, this vignette describes how to fit such GLMM trees: Section 3 will describe how to assess main and interaction effects of a categorical variable (treatment) on a continuous response (treatment outcome). But first, Section 2 will describe how to fit (G)LMM trees with constant fits in the terminal nodes. The \proglang{R} package \pkg{glmertree} can be used to detect predictors and moderators in a wide range of generalized linear mixed-effects models.    

GLMM trees estimate a global random-effects model, using all training observations. The fixed-effects model is estimated locally: the dataset is partitoned with respect to additional covariates or partitioning variables and a fixed-effects model is estimated in each cell of the partition. The \pkg{glmertree} package makes use of the \pkg{partykit} package \citep{HothyZeil15} to find the partition and the \pkg{lme4} package \citep{BateyMeac15} to fit the mixed-effects model.

The current stable release version of the package from the Comprehensive \proglang{R} Archive Network (CRAN) can be installed via:
%
<<eval = FALSE, echo = TRUE>>=
install.packages("glmertree")
@
%
Alternatively, the current development version can be installed from \proglang{R}-Forge:
%
<<eval = FALSE, echo = TRUE>>=
install.packages("glmertree", repos = "http://R-Forge.R-project.org")
@
%
After installation, the package can be loaded as follows:
%
<<eval = FALSE, echo = TRUE>>=
library("glmertree")
@

The main functions in the \pkg{glmertree} package are \code{lmertree()}, for continuous outcome variables, and \code{glmertree()}, for binary or count outcome variables.


\section{Fitting regression trees to clustered data}
\label{sec:regression_trees}

For this example, we will make use of the artificially generated \code{MHserviceDemo} dataset, containing data on $N = 350$ young people receiving treatment at one of 13 mental-health service providers. The response variable is (\code{outcome}), a continuous variable representing treatment outcome, as measured by a mental-health difficulties score at follow-up, corrected for the baseline assessment, where higher values reflect poorer treatment outcome. Potential predictor variables are demographic variables and case characteristics: two continuous (\code{age} and \code{impact}) and four binary covariates (\code{gender}, \code{emotional}, \code{autism} and \code{conduct}). The cluster indicator (\code{cluster\_id}) is an indicator for mental-health service provider. The data can be loaded as follows:
%
<<echo = TRUE, eval = TRUE>>=
data("MHserviceDemo", package = "glmertree")
summary(MHserviceDemo)
@

The main functions in the \pkg{glmertree} package are \code{lmertree()}, for continuous outcome variables, and \code{glmertree()}, for binary or count responses. Both functions require the user to specify at least two arguments: \code{formula} and \code{data}. 
%
<<echo = TRUE, eval = TRUE>>=
lmmt <- lmertree(outcome ~ 1 | cluster_id | age + gender + emotional + 
                 autism + impact + conduct, data = MHserviceDemo)
@
%
The first argument specified the model formula. The left hand side (preceding the tilde), specifies the response variable, which is \code{outcome} in the current example. The right-hand side of the model formula (following the tile) consists of three parts, separated by vertical bars: The first part specifies the subgroup-specific fixed-effect model, consisting only of an intercept (\code{1}) in the current example. The second part specifies the random effects, consisting of only a single variable, resulting in estimation of a random intercept with respect to \code{cluster_id}. Finally, the third part specifies the potential partitioning variables: \code{age}, \code{gender}, \code{emotional}, \code{autism}, \code{impact} and \code{conduct}.
%
A more complex random-effects structures could also be specified. For example, specifying the model formula as:
%
<<echo = TRUE, eval = FALSE>>=
outcome ~ 1 | (1 + age | cluster_id) | age + gender + emotional + 
  autism + impact + conduct
@
%
would yield a model in which a random intercept and slope for \code{age} would be estimated with respect to \code{cluster_id}. Note that the parentheses are necessary to protect the vertical bars, which separate the random effects from the other parts of the model.

Alternatively, using the \code{glmertree()} function, a tree may be fitted to binary (\code{family = binomial}, default) or count response variables (\code{family = poisson}). Therefore, a binomial GLMM tree for a dichotomized response could be obained by:
%
<<eval = FALSE, echo = TRUE>>=
MHserviceDemo$outcome_bin <- factor(outcome > 0)
glmmt <- glmertree(outcome_bin ~ 1 | cluster_id | age + gender + 
                  emotional + autism + impact + conduct, 
                data = MHserviceDemo, family = "binomial")
@
%
Using the \code{plot} method, we can plot the resulting tree and random effects:
%
<<eval = FALSE, echo = TRUE>>=
plot(lmmt)
@
%
\begin{figure}[t!]
\centering
\setkeys{Gin}{width=0.8\textwidth}
<<eval = TRUE, echo = FALSE, fig = TRUE, width = 9, height = 7>>=
plot(lmmt, which = "tree")
@
\caption{\label{fig:tree_1} Linear mixed-effects model tree with constant fits in the terminal nodes.}
\end{figure}
%
\begin{figure}[t!]
\centering
\setkeys{Gin}{width=0.5\textwidth}
<<echo = FALSE, results = hide, fig = TRUE>>=
plot(lmmt, which = "ranef")
@
\caption{\label{fig:ranef_1} Random effects for the linear mixed-effects model tree in Figure~\ref{fig:tree_1}.}
\end{figure}
%
Using the argument \code{which}, we can also specify which part of the model should be plotted: \code{which = "tree"} plots only the tree, \code{which = "ranef"} plots only the predicted random effects and \code{which = "all"} (the default) plots the tree as well as the random effects. 

The plotted tree is depicted in Figure~\ref{fig:tree_1}. In every inner node of the plotted tree, the splitting variable and corresponding $p$-value from the parameter stability test is reported. To control for multiple testing, the $p$-values are Bonferroni corrected, by default. This can be turned off by adding \code{bonferroni = FALSE} to the function call, yielding a less conservative criterion for the parameter stability tests, but note that this will increase the likelihood of overfitting. The significance level $\alpha$ equals .05 by default, but a different value, say for example .01, can be specified by including \code{alpha = .01} in the function call.  

The plotted tree shows that there are four subgroups: node 3 indicates that for female patients with lower age, somewhat higher values for the response are observed and thus for these patients, poorer treatment outcomes are predicted. Somewhat better treatment outcomes are observed for those in node 4 (with female gender and higher age) and node 6 (male gender and no emotional disorder). The best treatment outcomes (lowest response variable values) are observed among those in node 7 (male gender and presence of an emotional disorder).  

The predicted random effects are plotted in Figure~\ref{fig:ranef_1}. On average, patients at service provider 3 appear to have higher response variable values (poorer outcomes), while patients at service provider 10 appear to have more favorable outcomes.

To obtain numerical results, \code{print}, \code{coef}, code{fixef}, \code{ranef} and \code{VarCorr} methods are available (results omitted for space considerations):
%
<<echo = TRUE, eval = FALSE>>=
print(lmmt)
coef(lmmt)
fixef(lmmt)
ranef(lmmt)
VarCorr(lmmt)
@
%
To obtain predicted values, the \code{predict} method can be used: 
%
<<echo = TRUE, eval = TRUE>>=
predict(lmmt, newdata = MHserviceDemo[1:10,])
@
%
When \code{newdata} is not specified, predictions for the training observations are returned, by default. Also by default, the predictions are based on both random- and fixed-effects (tree) predictions. Random effects can be excluded from the predictions by adding \code{re.form = NA}. This is useful, for example, when \code{newdata} is specified, but the new observations do not have a cluster indicator or are from new clusters:
%
<<echo = TRUE, eval = TRUE>>=
predict(lmmt, newdata = MHserviceDemo[1:10, -7], re.form = NA)
@


\subsection{Inspecting residuals}

Residuals of the fitted mixed-effects tree can be obtained with the \code{residuals} method. This can be useful for assessing potential misspecification of the model (e.g., heteroscedasticity):
%
<<eval = FALSE, echo = TRUE>>=
resids <- residuals(lmmt)
preds <- predict(lmmt)
plot(MHserviceDemo$cluster_id, resids)
scatter.smooth(preds, resids)
@
%
\begin{figure}[b!]
\centering
\setkeys{Gin}{width=0.49\textwidth}
<<eval = TRUE, echo = FALSE, fig = TRUE>>=
resids <- residuals(lmmt)
preds <- predict(lmmt)
plot(MHserviceDemo$cluster_id, resids, xlab = "Cluster", ylab = "Residuals")
@
<<eval = TRUE, echo = FALSE, fig = TRUE>>=
scatter.smooth(preds, resids, xlab = "Predicted values", ylab = "Residuals")
@
\caption{\label{fig:residuals_1} Residuals of the fitted linear mixed-effects model tree in Figure~\ref{fig:tree_1}.}
\end{figure}
%
The plotted residuals are depicted in Figure~\ref{fig:residuals_1}. The left panel indicates there may be some differences in the error variances across the levels of \code{cluster_id}. The right panel indicates no association between fitted values and residuals.



\section{Detecting treatment-subgroup interactions in clustered data}
\label{sec:treatment_subgroups}

The (generalized) linear model specified for the terminal nodes can easily be extended to accomodate additional predictor variables. This may be particularly helpful when the interest is in the detection of moderators. For example, in the detection of treatment-subgroup interactions, where the effect of treatment on the response variable may be moderated by one or more additional covariates. To illustrate, we will use an artificial motivating dataset from \cite{FokkySmit18}, which can be recreated using the code provided in Appendix~\ref{app}, or can be loaded as follows: 
%
<<echo = TRUE, eval = TRUE>>=
data("DepressionDemo", package = "glmertree")
summary(DepressionDemo)
@
%
The dataset includes seven variables: A continuous response variable (\code{depression}), a predictor variable for the linear model (\code{treatment}), three potential partitioning variables (\code{age}, \code{anxiety}, \code{duration}), an indicator for cluster (\code{cluster}) and a binarized response variable (\code{depression\_bin}).

We fit the model as follows: 
%
<<eval = TRUE, echo = TRUE>>=
lmm_tree <- lmertree(depression ~ treatment | cluster |
  age + duration + anxiety, data = DepressionDemo)
@
%
The left hand side of the model formula (preceding the tilde symbol) specifies the response variable (\code{depression}). The right hand side of the model formula consists of three parts, separated by vertical bars: The first part specifies the predictor variable(s) of the (generalized) linear model (\code{treatment}, in this example). The second part specifies the random effects and the third part specifies the potential partitioning variables. All partitioning variables are continuous in this example, but note that (ordered) categorical partitioning variables may also be specified. Also, we specified a single variable in the random-effects part, resulting in estimation of a random intercept with respect to \code{cluster}. More complex random effects can also be specified; for example, specifying the model formula as 
%
<<eval = FALSE, echo = TRUE>>=
depression ~ treatment | ( 1 + age | cluster) | age + duration + anxiety
@
%
would yield a model with a random intercept and slope for \code{age} estimated with respect to \code{cluster}. The brackets are necessary to protect the vertical bars in the formulation of the random effects.

Alternatively, using the \code{glmertree()} function, a tree may be fitted to binary (\code{family = binomial}, default) or count response variables (\code{family = poisson}). Therefore, a binomial GLMM tree for the dichotomized response \code{depression\_bin} could be obained by:
%
<<eval = TRUE, echo = TRUE>>=
glmm_tree <- glmertree(depression_bin ~ treatment | cluster |
  age + duration + anxiety, data = DepressionDemo, family = binomial)
@
%
Using the \code{plot} method, we can plot the resulting tree and random effects:
%
<<eval = FALSE, echo = TRUE>>=
plot(lmm_tree)
@

\begin{figure}[t!]
\centering
\setkeys{Gin}{width=0.8\textwidth}
<<eval = TRUE, echo = FALSE, fig = TRUE, width = 9, height = 7>>=
plot(lmm_tree, which = "tree")
@
\caption{\label{fig:tree} Linear mixed-effects model tree with treatment-subgroup interactions.}
\end{figure}

\begin{figure}[t!]
\centering
\setkeys{Gin}{width=0.5\textwidth}
<<echo = FALSE, results = hide, fig = TRUE>>=
plot(lmm_tree, which = "ranef")
@
\caption{\label{fig:ranef} Random effects for the linear mixed-effects model tree in Figure~\ref{fig:tree}.}
\end{figure}

Using the argument \code{which}, we can also specify which part of the model should be plotted: \code{which = "tree"} plots only the tree, \code{which = "ranef"} plots only the predicted random effects and \code{which = "all"} (the default) plots the tree as well as the random effects. 

The plotted tree is depicted in Figure~\ref{fig:tree}. In every inner node of the plotted tree, the splitting variable and corresponding $p$-value from the parameter stability test is reported. To control for multiple testing, the $p$-values are Bonferroni corrected, by default. This can be turned off by adding \code{bonferroni = FALSE} to the function call, yielding a less conservative criterion for the parameter stability tests, but note that this will increase the likelihood of overfitting. The significance level $\alpha$ equals .05 by default, but a different value, say for example .01, can be specified by including \code{alpha = .01} in the function call.  

The plotted tree in Figure~\ref{fig:tree} shows that there are three subgroups with differential treatment effectiveness: node 3 indicates that for patients with lower duration and lower anxiety, Treatment 1 leads to lower post-treatment depression. Node 4 indicates that for patients with lower duration and higher anxiety, both treatments yield more or less the same expected outcome. Node 5 indicates, that for patients with higher duration, Treatment 2 leads to lower post-treatment depression.

The predicted random effects are plotted in Figure~\ref{fig:ranef}. On average, patients from cluster 10 have somewhat higher expected post-treatment depression scores, whereas patients from cluster 4 have somewhat lower expected post-treatment depression scores.

To obtain numerical results, \code{print}, \code{coef}, \code{fixef}, \code{ranef}, and \code{VarCorr} methods are available (results omitted for space considerations):
%
<<echo = TRUE, eval = FALSE>>=
print(lmm_tree)
coef(lmm_tree)
fixef(lmm_tree)
ranef(lmm_tree)
VarCorr(lmm_tree)
@
%
To obtain predicted values, the \code{predict} method can be used: 
%
<<echo = TRUE, eval = TRUE>>=
predict(lmm_tree, newdata = DepressionDemo[1:7,])
@
%
When \code{newdata} is not specified, predictions for the training observations are returned, by default. Random effects can be excluded from the predictions by adding \code{re.form = NA}. This is useful, for example, when \code{newdata} is specified, but the new observations do not have a cluster indicator or are from new clusters:
%
<<echo = TRUE, eval = TRUE>>=
predict(lmm_tree, newdata = DepressionDemo[1:7, -3], re.form = NA)
@


\subsection{Inspecting residuals}

Residuals of the fitted GLMM tree can be obtained with the \code{residuals} method. This can be useful for assessing potential misspecification of the model (e.g., heteroscedasticity):
%
<<eval = FALSE, echo = TRUE>>=
resids <- residuals(lmm_tree)
preds <- predict(lmm_tree)
plot(factor(DepressionDemo$cluster), resids)
scatter.smooth(preds, resids)
@
%
\begin{figure}[b!]
\centering
\setkeys{Gin}{width=0.49\textwidth}
<<eval = TRUE, echo = FALSE, fig = TRUE>>=
resids <- residuals(lmm_tree)
preds <- predict(lmm_tree)
plot(factor(DepressionDemo$cluster), resids, xlab = "Cluster", ylab = "Residuals")
@
<<eval = TRUE, echo = FALSE, fig = TRUE>>=
scatter.smooth(preds, resids, xlab = "Predicted values", ylab = "Residuals")
@
\caption{\label{fig:residuals} Residuals of the fitted linear mixed-effects model tree in Figure~\ref{fig:tree}.}
\end{figure}
%
The plotted residuals are depicted in Figure~\ref{fig:residuals}. The left panel does not indicate substantial variation in error variances across levels of the random effects. The right panel show fitted values plotted against residuals and also does not reveal a pattern indicating model misspecification.





\section{Detecting subgroups in linear growth curve models}
\label{sec:growth_curves}

For partitioning longitudinal data, function \code{lmertree()} requires data to be in the long format. An artificially generated longitudinal dataset is included in the package and can be loaded as follows: 
%
<<echo = TRUE, eval = TRUE>>=
data("GrowthCurveDemo", package = "glmertree")
dim(GrowthCurveDemo)
names(GrowthCurveDemo)
@
%
The dataset contains 1250 repeated measurements from 250 individuals. The response was measured at five timepoints for each individual. The dataset contains 31 variables: A continuous response variable (\code{y}), a predictor variable for the linear model (\code{time}, taking values 0 through 4), 28 potential partitioning variables (\code{x1} through \code{x28}), and an indicator for person (\code{person}). 

The data were generated so that \code{x1}, \code{x2} and \code{x3} are true partitioning variables. Furthermore, \code{x1} is a binary variable, while all other potential partitioning variables follow a normal distribution with $\mu = 0$ and $\sigma^2 = 25$. Potential partitioning variables were generated so as to be uncorrelated. Random intercepts and slopes were generated so that the intercept and slope values for persons vary around their node-specific means, following a normal distribution with $\mu = 0$ and $\sigma^2 = 2$ for the intercept and $\sigma^2 = .4$ for the slope. Errors were uncorrelated and followed a normal distribution with $\mu = 0$ and $\sigma^2 = 5$.

Because we have a relatively large amount of potential partitioning variables, we first construct the model formula as follows:
%
<<eval = TRUE, echo = TRUE>>=
form <- formula(paste0("y ~ time | person | ", 
                       paste0("x", 1:28, collapse = " + ")))
form
@
%
The first part of the formula (\code{y ~  time}) regresses the response on time. The second part (\code{| person |}) specifies that a random intercept should be estimated with respect to \code{person}. The third part (\code{x1 + ... + x28}) specifies the potential partitioning variables. 

The default fitting procedure as employed in \code{lmertree()} will assumes potential predictor variables are measured on the observation level. With longitudinal data in the long format, and with potential partitioning variables measured on the cluster level (i.e., time-invariant covariates), the observation-level stability tests will likely have inflated type-I error. This can be accounted for through specification of the \code{cluster} argument. As a result, parameter stability tests will be performed on the cluster instead of the observation level:

<<eval=TRUE, echo=TRUE>>=
gcm_tree <- lmertree(form, cluster = person, data = GrowthCurveDemo)
@
%
Using the cluster-level stability tests yields a tree with 4 subgroups (terminal nodes):
%
<<eval=TRUE, echo=TRUE>>=
width(gcm_tree$tree)
@
%
Employing the default observation-level stability tests would have yielded a tree with many spurious splits and subgroups:
%
<<eval=TRUE, echo=TRUE>>=
gcm_obs_tree <- lmertree(form, data = GrowthCurveDemo)
width(gcm_obs_tree$tree)
@
%
We plot the growth-curve tree using the \code{plot} method:
%
<<eval=FALSE, echo=TRUE>>=
plot(gcm_tree, which = "tree")
@
%
%
\begin{figure}[t!]
\centering
\setkeys{Gin}{width=0.8\textwidth}
<<eval = TRUE, echo = FALSE, fig = TRUE, width = 9, height = 7>>=
plot(gcm_tree, which = "tree")
@
\caption{\label{fig:gcm_tree} Linear mixed-effects model tree with growth curve models in the terminal nodes.}
\end{figure}
%
By default, the fixed effect of the predictor variable in the linear model (in this case, \code{time}) is plotted in the terminal nodes. The dots represent the observed data values.

The plot reveals that the true partitioning variables (\code{x1}, \code{x2} and \code{x3}) were selected for splitting. The fitted models in the terminal nodes (red lines) reveal a decrease in the response variable over time for the left-most subgroup, and an increase for the right-most subgroup. The curves in the two middle subgroups are rather flat, indicating no change over time. 

The observed data points indicate that the individual observations show substantial variation around the estimated fixed effects. To obtain an estimate of the random effects and residual variances, we can use the \code{VarCorr} method:
%
<<eval=TRUE, echo=TRUE>>=
varcor <- VarCorr(gcm_tree)
varcor
@
%
To obtain an estimate of the intraclass correlation (ICC), we could divide the variance of the random intercept by the variance of the residuals and that of the random intercept:
%
<<eval=TRUE, echo=TRUE>>=
res_var <- attr(varcor, "sc")^2
int_var <- as.numeric(varcor$person)
ICC <- int_var / (res_var + int_var)
ICC
@
%

\subsection{Random slopes}

Earlier, we specified a model formula with only a random intercept and thus did not account for possible variation between persons in the effect of time, within terminal nodes. To account for such differences we can incorporate a random slope of time into the model formula:
%
<<eval = TRUE, echo = TRUE>>=
form_s <- formula(paste0("y ~ time | (1 + time | person) | ", 
                         paste0("x", 1:28, collapse = " + ")))
form_s
@
%
Again, we fit the tree:
%
<<eval=TRUE, echo=TRUE>>=
gcm_tree_s <- lmertree(form_s, cluster = person, data = GrowthCurveDemo)
@
%
In this case, we obtained the same tree structure with or without estimating random slopes (Figure~\ref{fig:gcm_tree}). This need not necessarily be the case with other datasets. At the very least, the estimated random effects can provide us with additional information about variation due to between-person differences in initial levels and growth over time:
%
<<>>=
VarCorr(gcm_tree_s)
@
%
Compared to the fitted model with random intercepts only, we see that the residual variance decreased somewhat.


\newpage

\bibliography{glmertree}

\begin{appendix}

\section[R code for generating artificial motivating dataset]{\proglang{R} code for generating artificial motivating dataset}
\label{app}

Generate the predictor variables and error term:
%
<<appendix1, echo=TRUE>>=
set.seed(123)
treatment <- rbinom(n = 150, size = 1, prob = .5)
duration <- round(rnorm(150, mean = 7, sd = 3))
anxiety <- round(rnorm(150, mean = 10, sd = 3))
age <- round(rnorm(150, mean = 45, sd = 10))
error <- rnorm(150, 0, 2)
@
%
Generate the random intercepts:
<<appendix2, echo=TRUE>>=
cluster <- error + rnorm(150, 0, 6)
rand_int <- sort(rep(rnorm(10, 0, 1), each = 15))
rand_int[order(cluster)] <- rand_int 
error <- error - rand_int
cluster[order(cluster)] <- rep(1:10, each = 15)
@
%
Generate treatment subgroups:
%
<<appendix3, echo=TRUE>>=
node3t1 <- ifelse(duration <= 8 & anxiety <= 10 & treatment == 0, -2, 0)
node3t2 <- ifelse(duration <= 8 & anxiety <= 10 & treatment == 1, 2, 0)
node5t1 <- ifelse(duration > 8 & treatment == 0, 2.5, 0)
node5t2 <- ifelse(duration > 8 & treatment == 1, -2.5, 0)
@
%
Generate the continuous and dichotomized outcome variable:
%
<<appendix4, echo=TRUE>>=
depression <- round(9 + node3t1 + node3t2 + node5t1 + node5t2 +
  .4 * treatment + error + rand_int)
depression_bin <- factor(as.numeric(depression > 9))
@
%
Make treatment indicator a factor and collect everything in a data frame:
%
<<appendix5, echo=TRUE>>=
treatment <- factor(treatment, labels = c("Treatment 1", "Treatment 2"))
DepressionDemo <- data.frame(depression, treatment, cluster,
  age, anxiety, duration, depression_bin)
@

\end{appendix}

\end{document}
