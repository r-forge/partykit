\documentclass[nobf,doc]{apa}
\usepackage{amssymb}
\usepackage{amsmath,amsthm,tabularx,curves,texdraw,psfrag}
\usepackage{apacite}
\usepackage[english]{babel}
\usepackage{lscape,dcolumn,hhline}
\usepackage{graphicx,epic,eepic,rotating}
\usepackage[normalem]{ulem}
\usepackage{fancyhdr}
\usepackage{eurosym,bbding}
\usepackage{verbatim}
\usepackage{ctable}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{lineno}
\usepackage{tikz}
\linespread{1.2}
\usepackage{tabto}


\usetikzlibrary{shapes,shadows,arrows,positioning}
\title{Detection of Treatment-Subgroup Interactions in Clustered Datasets with Generalized Linear Mixed-effects Model Trees (GLMM tree)}
\shorttitle{GLMM Tree}
\leftheader{GLMM Tree}
\rightheader{GLMM Tree}
\author{M. Fokkema$^1$, N. Smits$^2$, A. Zeileis$^3$, T. Hothorn$^4$, H. Kelderman$^5$}
\affiliation{$^1$Universiteit Leiden, $^2$Universiteit van Amsterdam, $^3$Universit\"{a}t Innsbruck, $^4$Universit\"{a}t Z\"{u}rich, $^5$Universiteit Leiden and Vrije Universiteit, Amsterdam}

\acknowledgements{The authors would like to thank Prof. Pim Cuijpers, Prof. Jeanne Miranda, Dr. Boadie Dunlop, Prof. Rob DeRubeis, Prof. Zindel Segal, Dr. Sona Dimidjian, Prof. Steve Hollon and Erica Weitz for granting access to the dataset for the application. The work for this paper was partially done while MF, AZ and TH were visiting the Institute for Mathematical Sciences, National University of Singapore in 2013. The visit was supported by the Institute.}

\begin{document}

\maketitle
\pagewiselinenumbers

\section{Abstract}
Identification of subgroups of patients for which treatment A is more effective than treatment B, and vice versa, is of key importance to the development of personalized medicine. Several tree-based algorithms have been developed for the detection of such treatment-subgroup interactions. In many instances, however, datasets may have a clustered structure, where observations are clustered within, for example, research centers, studies or persons. In the current paper we propose a new algorithm, GLMM tree, that allows for detection of treatment-subgroup interactions, as well as estimation of cluster-specific random effects. The algorithm uses model-based recursive partitioning (MOB) to detect treatment-subgroup interactions, and a generalized linear mixed-effects model for estimation of random-effects parameters. In a simulation study, we evaluate the performance of GLMM tree and compare it with that of MOB trees without random-effects estimation. In datasets without treatment-subgroup interactions, GLMM tree was found to have a much lower Type I error rate than MOB trees without random effects (4 and 33\%, respectively). Furthermore, in datasets with treatment-subgroup interactions, GLMM tree recovered the true treatment subgroups much more often than MOB without random effects (90\% and 61\% of the datasets, respectively). Also, GLMM tree predicted treatment outcome differences more accurately than MOB trees without random effects (average accuracy of .94 and .88, respectively). We illustrate the application of GLMM tree on a patient-level dataset of a meta-analysis on the effects of psycho- and pharmacotherapy for depression. We conclude that GLMM tree is a promising algorithm for the detection of treatment-subgroup interactions in clustered datasets, and discuss some directions for future research.\\
\\
\textit{Keywords}: model-based recursive partitioning, treatment-subgroup interactions, random effects, generalized linear mixed-effects model, classification and regression trees

%\section{Citation stuff for journal publication}
%
%Potential Psychological Methods references: only \citeA{StroyMall09} (not added yet). Maybe force in: 
%
%''Hedges, L. V., \& Pigott, T. D. (2004). The power of statistical tests for moderators in meta-analysis. 
%
%''Cooper, H., \& Patall, E. A. (2009). The relative benefits of meta-analysis conducted with individual participant data versus aggregated data. Psychological methods, 14(2), 165.'' (they write in abstract ''Regardless of whether a meta-analysis is conducted with IPD or AD, synthesists must remain vigilant in how they interpret their results. They must avoid ecological fallacies, Simpsonâ€™s paradox, and interpretation of synthesis-generated evidence as supporting causal inferences.'')
%
%''Hedges, L. V., \& Vevea, J. L. (1998). Fixed-and random-effects models in meta-analysis. Psychological methods, 3(4), 486.''


\section{Introduction}

In research assessing the efficacy of treatments for somatic and psychological disorders, the one-size-fits-all paradigm is slowly losing ground, and personalized medicine is becoming increasingly important. Personalized medicine presents the challenge of finding which patients respond best to which treatments. This can be referred to as the detection of treatment-subgroup interactions \cite<e.g., >{DoovyDuss14}. In most cases, treatment-subgroup interactions are studied using linear models, such as factorial analysis of variance techniques, in which potential moderators have to be specified a-priori, have to be checked one at a time, and continuous moderator variables have to be discretized a-priori. This may hamper identification of which treatments work best for whom, especially when there are no a-priori hypotheses about treatment-subgroup interactions. As noted by \citeA{KreayFran06}, there is a need for methods that generate, instead of test, hypotheses and that are specifically directed at the detection of treatment interactions.

Tree-based methods are such hypothesis-generating methods, as they can automatically detect subgroups which differ on the expected outcomes for one or more treatments. Due to their flexibility, tree-based methods are preeminently suited to the detection of treatment-subgroup interactions: they can handle many potential predictor variables at once and can automatically detect (higher order) interactions between predictor variables \cite{StroyMall09}. Several promising tree-based algorithms for the detection of treatment-subgroup interactions have been developed (e.g., \citeNP{DussyMech14, DussyMeul04, SuyTsai09, FostyTayl11, LipkyDmit11, ZeilyHoth08}; see \citeNP{DoovyDuss14} for an overview). Among these methods, model-based recursive partitioning (MOB; \citeNP{ZeilyHoth08}) seems to be the most flexible tool for detecting treatment-subgroup interactions, as it offers a very generic data-analytic framework for detecting partitions in a dataset, with different model parameter estimates. The recursive partitioning in MOB can be based on a broad class of parametric models that can be fitted using M-type estimators \cite{ZeilyHoth08}, the most well-known example being the generalized linear model (GLM). Earlier, GLM-based MOB has been successfully applied by \citeA{DrieySmit14} in the detection of subgroups with differential treatment outcomes for two different psychotherapies. 

However, none of the aforementioned tree-based algorithms allow for taking into account the clustered structure of datasets. In many cases, researchers may want to detect treatment-subgroup interactions in datasets with a clustered structure \cite<e.g., >{KoopyHeij07}. For example, in individual-level patient data meta-analyses, in which datasets of multiple trials evaluating the effects of the same treatments are pooled. In such analyses, the clustered structure of the dataset should be taken into account by including study-specific effects in the model, prompting the need for modeling random effects \cite<e.g., >{CoopyPatt09,HiggyWhit01}. Likewise, longitudinal datasets, and datasets from multi-center trials also require modeling of random effects. Ignoring the clustered structure of datasets may lead to biased inference, due to underestimated standard errors \cite<e.g., >{BrykyRaud92,NooryOpde05}. More specifically, when the interest is in subgroup detection, ignoring random effects on the outcome variable may result in the detection of spurious subgroups \cite<e.g., >{SelaySimo12}.

In the current paper, we present a tree-based algorithm for detecting treatment-subgroup interactions, which takes the clustered nature of datasets into account. The algorithm combines MOB with random-effects estimates, thus allowing for the detection of treatment-subgroup interactions, as well as accounting for variation between clusters (e.g., trials). In what follows, we will introduce the existing frameworks for estimating treatment effects: the GLM, model-based recursive partitioning, and the generalized linear mixed-effects model (GLMM). Then, we intoduce a new algorithm, which combines MOB and the GLMM: GLMM tree. 

Before we discuss these methods for estimating treatment effects, we will introduce an artificial motivating data set, with which the methods will be illustrated. After we introduce the GLMM tree algorithm, we present a simulation study, in which we evaluate GLMM trees comparative accuracy. Finally, in the application, we use GLMM tree to detect treatment-subgroup interactions in an existing dataset on the effects of treatments for depression.  

\subsection{Artificial motivating dataset} 

To illustrate the application of the methods to be discussed, we will use a simulated dataset of 150 observations, which were randomly assigned to Treatment 1 (78 observations) or Treatment 2 (72 observations). Every observation has a value on the response variable, with which the effect of treatment is assessed: the posttreatment total score on a depression inventory. Further, all observations have values on three covariates: duration of depressive symptoms prior to treatment in months (range 0-15); age in years (range 18-75); anxiety inventory total score (range 3-18). 

The simulated dataset has 3 subgroups with treatment interactions. The first subgroup consists of observations with duration $\leq 6$ and anxiety $\leq 10$. In this subgroup, the mean of the response variable for Treatment 1 is 7, and the mean for the response variable for Treatment 2 is 11. The second subgroup consists of observations with duration $\leq 6$ and anxiety $> 10$. In this subgroup, the mean value of the response variable for Treatment 1 and 2 is 9. The third subgroup consists of observations with duration $>6$. In this subgroup, the mean value of the response variable for Treatment 1 is 12, and the mean of the response variable for Treatment 2 is 7. 

Observations were drawn from one of ten clusters, each with a different, cluster-specific (i..e, random) intercept. Data was generated such, that covariates and cluster-specific intercepts were uncorrelated. Also, 43\% of variance in posttreatment depression scores was due to treatment-subgroup interactions, and 8\% of variance was due to cluster-specific variation. 




\section{General modeling framework}

\subsection{GLM}
In a clinical trial, where the outcomes of two or more treatments are compared, an overall GLM may be used to estimate treatment effects. The goal is to estimate a model for predicting the value of a treatment outcome, which follows, for example, a normal, binomial or Poisson distribution \footnote{ an overview of notation used is provided in the Appendix}:

\begin{equation}
\label{eq:expected_value}
	E[y_i | x_i] = \mu_i 
\end{equation}

\begin{equation}
\label{eq:fixedeffects}
	g(\mu_{i}) = x_{i}^{\top}\beta 
\end{equation}

Where $y_i$ is the value of response variable for observation $i$, and $g$ is the link function, characterizing the relationship between the linear predictor $x_{i}^{\top}\beta$ and the mean of the response distribution function. In case of a continuous response variable, $g$ is often as taken as the identity function, and $\mu_{i}$ as the mean of a Gaussian distribution with variance $\sigma_{\epsilon}$. Further, $x_{i}^{\top}$ is a vector of fixed-effects predictor variable values for observation $i$, of which the first element takes a value of 1 for the intercept, and the second element takes the value of a dummy indicator for treatment type. $\beta$ is a vector of fixed-effects regression coefficients, the first element representing the intercept, which is the mean value of the linear predictor in the first treatment group, and the second element representing the slope, which is the mean difference in the linear predictor between the first and second treatment groups.  

To keep notation and examples simple, we assume $x_{i}^{\top}$ and $\beta$ to have length 2. That is, the effects of only two treatment conditions are estimated and no additional covariates are included in the GLM. However, additional treatment conditions and covariates can easily be included. In addition, examples and datasets in the current paper will focus on continuous response variables with normally distributed errors, such as posttreatment severity of a disorder. But the models and algorithms to be discussed can also be applied with discrete outcomes, such as remission of a disorder (yes/no).

To illustrate, the GLM estimated for the artificial motivating dataset is graphically represented in Figure \ref{fig:fixedeffects}. The boxplots in Figure \ref{fig:fixedeffects} show the distribution of the posttreatment depression scores in both treatment groups. There seems to be little overall difference in effects of both treatments, as the slope of the regression line is nearly zero. We shall see that this does not necessarily mean that posttreatment depression score and treatment type are unrelated, as the effect of treatment may be moderated by variables not yet included in the model.

\vspace{20pt}
\begin{figure}[!htbp]
    \includegraphics[width=6cm]{glm_example.pdf}
	\caption{Example of a linear fixed-effects model for treatment outcomes, based on the artificial motivating dataset (N=150). The dot for Treatment 1 represents the first, and the slope of the regression line represents the second element of $\beta$.}
	\label{fig:fixedeffects}
\end{figure}


\subsection{Model-based recursive partitioning}

The rationale behind MOB is that a global model for all observations, like the GLM in Equation \ref{eq:expected_value} and \ref{eq:fixedeffects}, may not describe the data well, and when additional covariates are available it may be possible to partition the dataset with respect to these covariates, and find a better model in each cell of the partition \cite{ZeilyHoth08}. This is reminiscent of the classification and regression tree (CART) algorithm of \citeA{BreiyFrie84}, which splits the dataset into subsets, for which the distributions of the outcome variable are most different. However, CART trees detect differences in constant fits across terminal nodes, whereas MOB trees detect differences in parametric models across terminal nodes.

To find partitions and better-fitting local GLMs, the MOB algorithm tests for parameter instability. When the partitioning is based on a GLM, instabilities are differences in $\hat{\beta}$ across partitions of the dataset, which are defined by one or more auxiliary covariates not included in the linear predictor. To find partitions, the MOB algorithm cycles iteratively through the following steps \cite{ZeilyHoth08}: (1) fit the parametric model to the dataset, (2) test for parameter instability over a set of partitioning variables, (3) if there is some overall parameter instability, split the dataset with respect to the variable associated with the highest instability, (4) repeat the procedure in each of the resulting subgroups.

More specifically, in step (2), to test for parameter instability, the so-called \textit{scores} are computed, using the score function. By definition, the empirical scores of all observations in a dataset sum to zero, and when the model is correctly specified, the expected value of the score for each observation is also zero. Under the null hypothesis of parameter stability, the scores do not systematically deviate from the expected value of zero, when the observations are ordered by the values of a potential partitioning variable $U_k$ \cite<c.f., >{MerkyZeil13}. To statistically test whether the scores systematically deviate from zero with respect to variable $U_k$, the class of generalized M-fluctuation tests is used \cite{Zeil05,ZeilyHorn07}. 

If the null hypothesis of parameter stability in step (2) can be rejected, that is, if at least one of the partitioning variables $U_{k}$ has a p-value for the M-fluctuation test below the pre-specified significance level $\alpha$, the dataset is partitioned into two subsets in step (3). In step (3), a binary partition is created using $U_{k*}$, the variable with the minimal p-value in step (2). The split point for $U_{k*}$ is selected, by taking the value that minimizes the sum of the values of the objective function in both partitions \cite{ZeilyHoth08}. In step (4), steps (1) through (3) are repeated in each partition, until the null hypothesis of parameter stability can no longer be rejected.

Due to the binary recursive nature of MOB, the resulting partition can be represented as a binary tree. If the partitioning is based on the GLM, the result is a GLM tree, which has a local fixed-effects regression model in every $j$th ($j = 1,...,J$) terminal node of the tree. As a result, in the GLM-tree model, the value for $\beta$ depends on terminal node $j$ in which observation $i$ `falls':

\begin{equation}
\label{eq:fixedeffects_MOB}
	g(\mu_{i}) = x_{i}^{\top}\beta_{j}
\end{equation}

Alternatively, if the recursive subgroup structure (i.e., the partition) were known, the tree could be estimated as a single GLM. The model could then be written: $g(\mu_{i}) = x_{i}^{* \top}\beta^{*}$, where $x_{i}^{*\top}$ are the values of the $2J$ interactions between the subgroups from the tree, and the elements of $x_{i}$. $\beta^{* \top}$ would also have length $2J$, and contain the subgroup-specific fixed-effects coefficients.

Figure \ref{fig:example_mobtree} provides an example of the GLM-tree model in Equation \ref{eq:fixedeffects_MOB}, based on the artificial motivating dataset. By using the three additional covariates (anxiety, duration and age), MOB partitioned the observations into four subgroups, each with a different estimate for $\beta_j$. Age was correctly not detected as a partitioning variable, and the left- and rightmost subgroups are in accordance with the treatment-subgroup interactions as described above. However, the two subgroups in the middle result from a spurious split.

\begin{figure}[!h]
    \includegraphics[width=15cm]{glmtree_example.pdf}
    \caption{Example of a tree representation of model-based recursive partition, based on the artificial motivating dataset. Three additional covariates (anxiety questionnaire score, duration of depressive symptoms at baseline in months and age) were used as potential splitting variables.}
    \label{fig:example_mobtree}
\end{figure}



\subsection{GLMM}
When a dataset contains observations from multiple clusters (e.g., trials, research centers, or individuals in longitudinal datasets), the GLM in Equation \ref{eq:fixedeffects} may be extended to include cluster-specific, or random effects, and the model becomes a GLMM:

\begin{equation}
\label{eq:mixedeffects}
	g(\mu_{i}) = x_{i}^{\top}\beta + z_{i}^{\top}b_{m}
\end{equation}

Where $z_{i}$ is a vector of random-effects predictor variables values for observation $i$, and $b_{m}$ is the vector of random-effects regression coefficients in cluster $m$ ($m=1,...,M$), of which observation $i$ is part. Within the GLMM, it is assumed that $b$ is normally distributed, with mean zero and variance $\sigma^{2}_{b}$. The parameters of the GLMM can be estimated with, for example, maximum likelihood (ML) and restricted ML (REML), as described in \citeA{BrykyRaud92}, for example.  

For simplicity, we assume $z_{i}$ and $b_{m}$ to have length 1 in the current paper; that is, only cluster-specific intercepts are included in the models. However, random-effects covariates and coefficients can easily be included. Note that, alternatively, if the random-effects coefficients were known, values of $z_{i}^{\top}b_{m}$ could be included as an offset (i.e., a variable with a fixed coefficient of 1) in the linear predictor of a GLM.



\subsection{GLMM tree}

As noted earlier, ordinary GLM(M)s are not well suited for the detection of treatment-subgroup interactions, whereas the MOB algorithm is, but does not allow for estimation of random effects. Therefore, we propose the GLMM tree, which combines the GLMM from Equation \ref{eq:mixedeffects} with the tree from Equation \ref{eq:fixedeffects_MOB}: 

\begin{equation}
\label{eq:glimmertree}
	g(\mu_{i}) = x_{i}^{\top}\beta_{j} + z_{i}^{\top}b_{m}
\end{equation}

To estimate the parameters of this model, we take an approach similar to that of \citeA{HajjyBell11} and \citeA{SelaySimo12}. \citeA{HajjyBell11} and \citeA{SelaySimo12} developed a method for estimation of mixed-effects regression trees (MERTs), which are somewhat similar to GLMM trees. In the MERT approach, the fixed-effects part of a GLMM is replaced by a CART regression tree, and the random-effects part is estimated as usual. To estimate a MERT, an iterative approach is taken, alternating between (1) assuming random effects known, allowing for estimation of the regression tree, and (2) assuming the regression tree known, allowing for estimation of the random effects. 

For estimating GLMM trees, we take the MERT approach a step further, by using a GLM tree instead of a regression tree with constant fits. This allows not only for detection of differences in main effects,but also for detection of differences in regression effects (e.g., of treatment type) across terminal nodes. In addition, GLMM trees can be estimated for continuous, as well as binary and count variables. The GLMM-tree algorithm takes the following steps to estimate the model in Equation \ref{eq:glimmertree}:

\vspace{5pt}
\noindent Step 0: Initialize by setting $r$ and all values $\hat{b}_{m(r)}$ to 0.

\vspace{5pt}
\noindent \hangindent=4em Step 1: Set $r = r+1$. Estimate GLM tree ($x_{i}^{\top}\hat{\beta}_{j(r)}$) using responses $y_i - z_{i}^{\top}\hat{b}_{m(r-1)}$.

\vspace{5pt}
\noindent Step 2: Estimate random effects $z_{i}^{\top}\hat{b}_{m(r)}$ using responses $y_i - x_{i}^{\top}\hat{\beta}_{j(r)}$.

\vspace{5pt}
\noindent Step 3: Repeat steps 1 and 2 until convergence.

\vspace{5pt}
The algorithm initializes by setting all values $b_m$ to $0$, since the random-effects (and also the fixed-effects) parts are initially unknown. In every iteration, the GLM tree (i.e., the partition and corresponding fixed-effects coefficients $\beta_j$) and random-effects coefficients $b_m$ are re-estimated. The GLM tree is estimated, given the estimated $b_m$ values from the last iteration, and the $b_m$ values are estimated, given the estimated GLM tree from the current iteration. Iterations are continued until convergence, which is monitored by computing the log-likelihood criterion of the mixed-effects model in Equation \ref{eq:mixedeffects}. 

In Figure \ref{fig:example_glimmertree}, the GLMM tree that was grown on the artificial motivating dataset is presented. As can be seen, by taking into account the clustering of observations by estimating random intercepts, the spurious split involving the anxiety variable no longer appears in the tree. 

\begin{figure}[!h]
    \includegraphics[width=12cm]{glimmertree_example.pdf}
    \caption{Generalized Linear Mixed Model tree of the motivating example dataset. Three covariates (anxiety questionnaire score, duration of depressive symptoms at baseline in months and age) were used as potential splitting variables, and the clustering structure was taken into account by estimating random intercepts.}
    \label{fig:example_glimmertree}
\end{figure}


\section{Simulation}

We will asses the performance of GLMM tree in recovering treatment-subgroup interactions, and predicting differences between the outcomes of two treatments, in simulated datasets with continuous outcomes. In addition, we will compare the performance of GLMM tree with that of GLM tree. In the simulation study, the main interest will be in the effects of sample size, and the presence and magnitude of treatment-subgroup interactions and random effects, but other parameters will be varied, as well. 

For GLMM tree, we expect the accuracy of recovered trees and predictions to improve with increasing sample size, and magnitude of the differences in treatment outcomes. For GLM tree, we have the same expectation, when random effects are absent; that is, when the variance of the random coefficients is zero, we expect GLM tree and GLMM tree to perform equally well. When random effects are present, we expect GLMM tree to perform better than GLM tree, and more so when the variance of random-effects coefficients is larger.



\subsection{Simulation design}

\subsubsection{Datasets with treatment-subgroup interactions}
For generating datasets with treatment-subgroup interactions, we used a treatment-subgroup interaction design from \citeA{DussyMech14}, which is also depicted in Figure \ref{fig:modelC}. Figure \ref{fig:modelC} shows two subgroups with mean differences in treatment outcomes, and two subgroups without mean differences in treatment outcomes. The four subgroups are characterized by their values on the partitioning variables $U_2$, and $U_1$ or $U_5$. In other words, $U_1$, $U_2$ and $U_5$ are true partitioning variables, whereas the other potential partitioning variables ($U_3$, $U_4$, $U_6$ through $U_{15}$) are noise variables.

\begin{figure}[!h]
    \tikzstyle{decision} = [diamond, draw]
    \tikzstyle{line} = [draw, -stealth, thick]
    \tikzstyle{elli}=[draw, ellipse, minimum height=8mm, text width=6em, text centered]
   \tikzstyle{block} = [draw, rectangle, text width=7em, text centered, minimum height=15mm, node distance=6em]
    \begin{tikzpicture}[paths/.style={->, thick, >=stealth'}]
        \node [elli] (node1) { };
        \node [elli, below of =node1, xshift=-9em] (node2) { };
        \node [elli, below of =node1, xshift=9em] (node3) { };
        \node [block, below of=node2, xshift=-4em, yshift=-1em] (block1){\small$\beta_{j0}=17.5$\\$\beta_{j1}=-5.0$\\$d_j=-1.0$};
        \node [block, below of=node2, xshift=4em, yshift=-1em] (block2){\small$\beta_{j0}=30.0$\\$\beta_{j1}=0.0$\\$d_j=0.0$};
        \node [block, below of=node3, xshift=-4em, yshift=-1em] (block3){\small$\beta_{j0}=30.0$\\$\beta_{j1}=0.0$\\$d_j=0.0$};
        \node [block, below of=node3, xshift=5em, yshift=-1em, text width=8em] (block4){\small$\beta_{j0}=42.5$\\$\beta_{j1}=5.0$\\$d_j=1.0$};
        \draw [paths] (node1) to node [above left]{\small$U_2\leq30$} (node2);
        \draw [paths] (node1) to node [above right]{\small$U_2>30$} (node3);
        \draw [paths] (node2) to node [ left]{\small$U_1\leq17$} (block1);
        \draw [paths] (node2) to node [ right]{\small$U_1>17$} (block2);
        \draw [paths] (node3) to node [ left]{\small$U_5\leq63$} (block3);
        \draw [paths] (node3) to node [ right]{\small$U_5>63$} (block4);
    \end{tikzpicture}
	\caption{Data-generating model for treatment-subgroup interactions. Parameter $d$ denotes the standardized mean difference between the outcomes of Treatment 1 and 2 (i.e., $\beta_{j1} / \sigma_{\epsilon}$).}
	\label{fig:modelC}
\end{figure}


\subsubsection{Datasets without treatment-subgroup interactions}
For generating datasets without treatment-subgroup interactions, we used a design in which there is only a main effect of treatment in the population. Put differently, the number of subgroups or terminal nodes in these datasets was $J=1$, and there was only a single value of $\beta_j = \beta$ in every dataset. The mean of the outcome variable in the datasets without treatment-subgroup interactions was 30, which is the same value as in the datasets with treatment-subgroup interactions. As a result, $\beta = (27.5, 32.5)$ for all observations when $d=1$.

\subsubsection{Parameters of the data-generating process}

In generating datasets, we varied seven parameters of the data-generating process:
 
\begin{enumerate} 
\item Three levels for the total number of observations: $N=200$, $N=500$, $N=1000$.
\item Two levels for the number of potential partitioning covariates $U_1$ through $U_K$: $K=5$, $K=15$ (where only $U_1$, $U_2$ and $U_5$ are true partitioning variables).
\item Two levels of intercorrelations between the covariates $U_1$ through $U_K$: $\rho_{U_{k},U_{k'}}=0.0$, $\rho_{U_{k},U_{k'}}=0.3$.
\item Three levels for the number of clusters: $M=5$, $M=10$, $M=25$.
\item Three levels for the population standard deviation of the normal distribution from which the cluster specific intercepts are drawn: $\sigma_{b}=0$, $\sigma_{b}=5$, $\sigma_{b}=10$.
\item Three levels for the intercorrelations between $b$ and one of the $U_k$ variables: $b$ and $U_k$ uncorrelated, $b$ correlated with a true partitioning covariate (i.e., $U_2$, $U_1$, or $U_5$, introducing a correlation of about 0.42), $b$ correlated with a non-partitioning covariate (i.e., $U_3$ or $U_4$, introducing a correlation of about 0.42). 
\item Two different levels for $\beta_1$, the unstandardized mean difference in treatment outcomes, in subgroups with differential effects for Treatment 1 ($x_{1}=0$) and Treatment 2 ($x_{1}=1$). The levels for mean differences in subgroups with differential treatment effect were $|\beta_1| = 2.5$ (corresponding to a medium effect size, Cohen's $d = 0.5$; \citeNP{Cohe92}) and $|\beta_1| = 5.0$ (corresponding to a large effect size; Cohen's $d = 1.0$).
\end{enumerate}

For each cell, 50 datasets with treatment-subgroup interactions were generated, resulting in 50 x 3 x 2 x 2 x 3 x 3 x 3 x 2 = 32,400 training datasets. For the datasets without treatment-subgroup interactions, the 6th parameter of the data-generating process had only two levels ($b$ correlated with one of the $U_k$ variables, and $b$ not correlated with any of the $U_k$ variables). Therefore, 50 x 3 x 2 x 2 x 3 x 3 x 2 x 2 = 21,600 datasets without treatment-subgroup interactions were generated.



\subsubsection{Variable distributions}

As in \citeA{DussyMech14}, all covariates $U_1$ through $U_{K}$ were drawn from a multivariate normal distribution with means ${\mu_U}_1$, ${\mu_U}_2$, ${\mu_U}_4$, and ${\mu_U}_5$ fixed at 10, 30, -40 and 70, respectively. The means for all other covariates (i.e., ${\mu_U}_3$, and ${\mu_U}_6$ through ${\mu_U}_{15}$) were drawn from a discrete uniform distribution on the interval $[-70,70]$. All covariates $U_1$ through $U_{15}$ have the same standard deviation: ${\sigma_U}_k = 10$. Correlations between the $U_k$ variables vary according to the third facet of the simulation design described above.

To generate the random error term $\epsilon$, for every observation we drew a value from a normal distribution with $\mu_{\epsilon} = 0$ and $\sigma_{\epsilon} = 5$. 

To generate the cluster-specific intercepts $b_m$, we partitioned the sample into equally-sized clusters, conditional on one of the variables $U_1$ through $U_5$, producing the correlations in the sixth facet of the simulation design. For each cluster we drew a single $b_m$ from a normal distribution with mean 0 and the value of $\sigma_{b}$ given by the fifth facet of the simulation design. When $b$ was correlated with one of the potential partitioning variables, the correlated potential partitioning variable was randomly selected.

To generate node-specific fixed effects, we partitioned the sample according to the terminal nodes of the tree in Figure 4.3. In combination with the seventh facet of the simulation design, this determines the values of $\beta_j$. For every observation, we generated a binomial variable (with $p =.5$) as an indicator for treatment type. 

Finally, the response variable was calculated as the sum of the (node-specific) fixed effects, random effects and the error term: $y_{i} = x_{i}^{\top} \beta_{j} + z_{i}^{\top} b_{m} + \epsilon_{i}$.




\subsection{Evaluation of performance}

\subsubsection{Tree size and accuracy} 

For every dataset, accuracy and size of the GLM and GLMM tree was evaluated. We calculated the total number of nodes in every tree, and compared it with the true tree size. For datasets without treatment-subgroup interactions, this allowed us to assess tree accuracy in terms of Type I error: the probability that the dataset is erroneously partitioned. For datasets with treatment-subgroup interactions, this allowed us to assess the probability that the dataset is erroneously not partitioned, and the extent to which the algorithms may detect spurious subgroups.

For datasets with treatment-subgroup interactions, we assessed the accuracy of the trees created by GLM and GLMM tree. An accurately recovered tree was defined as a tree with (1) the true tree size (i.e., total number of nodes equals 7), (2) the first split in the tree involving variable $U_2$ and a value of $30 \pm 5$, (3) the next split on the left involving variable $U_1$ and a value of $17 \pm 5$, and (4) the next split on the right involving variable $U_5$ and a value of $63 \pm 5$. Note that the allowance of $\pm 5$ equals an allowance of plus or minus half the population standard deviation of the partitioning variable ($\sigma_{U_k}$). 

To detect predictors of tree size for both algorithms, we performed ANOVAs with algorithm type and the parameters of the data-generating process as independent variables. In addition, interactions between algorithm type and each of the data-generating parameters were also entered as independent variables. The effects of predictors with main and/or interaction effects with $\eta^2 > .01$ were further investigated using graphical displays.

To detect predictors of tree accuracy for both algorithms in datasets with treatment-subgroup, we used a GLM with algorithm type and the parameters of the data-generating process as independent variables. In addition, interactions between algorithm type and each of the data-generating parameters were also entered as independent variables. The effects of predictors with main and/or interaction effects with unstandardized coefficients $|\beta| > .2$ were further investigated using graphical displays.

\subsubsection{Predictive accuracy} 

We evaluated predictive accuracy of GLM and GLMM trees by calculating correlations between true and predicted treatment-effect differences ($\beta_{j1}$ in Figure \ref{fig:modelC}) for test observations. Note that this correlation was only assessed for datasets with treatment-subgroup interactions, as the true treatment differences have a constant value in datasets without treatment-subgroup interactions.

Using the same data for training and evaluation of a model results in overly optimistic estimates of predictive accuracy \cite{HastyTibs09}. Therefore, GLM and GLMM trees were used for prediction of new observations from test datasets. Test datasets were generated from the same population as the training datasets. Because the cluster-specific intercepts $b_m$ were randomly generated for training as well as test datasets, test observations were from 'new' clusters. As a result, a model without random effects was used for prediction with GLMM tree.

For every dataset, two correlation coefficients were calculated, representing the linear association between the true and predicted treatment-effect differences: one for GLM tree, and one for GLMM tree. To detect predictors of predictive accuracy, we performed ANOVAs with algorithm type and the parameters of the data-generating process as independent variables. In addition, interactions between algorithm type and each of the data-generating parameters were also entered as independent variables. The effects of predictors with main and/or interaction effects with $\eta^2 > .01$ were further investigated using graphical displays.


\subsection{Software}

\verb|R| \cite{R14} was used for generation and analysis of all datasets. Two additional R packages were used: \verb|partykit| (version 0.8-3; \citeNP{HothyZeil14,ZeilyHoth08}) for the estimation of GLM trees, and \verb|lme4| (version 1.1-7 \citeNP{BateyMeac12}) for the estimation of random-effects coefficients. 


\subsubsection{Estimation of GLM and GLMM trees}

For estimating GLM trees, the \verb|lmtree| function from the \verb|partykit| package was used. The \verb|lmtree| function builds a linear model tree: a GLM-based recursive partition (Equation \ref{eq:fixedeffects_MOB}) for a real-valued response variable with normally distributed errors. Models in the nodes of the tree are estimated with ordinary least squares (OLS). The $\alpha$-level for assessing parameter instability was set to .05, a Bonferroni correction for multiple testing was applied, and the minimum number of observations in a node was set to 20. Maximum tree depth was set to four (i.e., maximum of eight terminal nodes), as this yields a model tree which is easy to interpret. 


\subsubsection{Estimation of GLMM trees}  

For estimating GLMM trees, we implemented the GLMM tree algorithm in a function that iterates between (1) estimation of a generalized linear model tree using the \verb|lmtree| function, and (2) estimation of the random-effects coefficients $b_m$ using the \verb|lmer| function. Convergence of \verb|glmmtree| is monitored using the log-likelihood value of the generalized linear mixed-effects model estimated in step (2) of the algorithm. When the difference in the log-likelihoods of two consecutive iterations is less than a prespecified value (.001 by default), \verb|glmmtree| has converged. 

For building the GLM trees in step (1) of the GLMM tree algorithm, the same settings as described above were used. That is, models in the nodes of the tree were estimated with OLS, the $\alpha$-level for assessing parameter instability was set to .05, the Bonferroni correction for multiple testing was applied, the minimum number of observations in a node was set to 20, and maximum tree depth was set to four.

For estimating the $b_m$ values in step (2), the \verb|lmer| function from the \verb|lme4| package was used. This function estimates a linear mixed-effects model using maximum likelihood (ML) or restricted ML (REML). In the current paper, REML estimation was used.





\subsection{Tree size and accuracy in datasets without treatment-subgroup interactions}

In Table \ref{tab:treesize_main}, tree sizes for GLM and GLMM trees for datasets without treatment-subgroup interactions are presented. Overall, smaller trees were created by GLMM tree: the average tree size was 1.09 (SD=0.44) for GLMM tree, and 2.02 (SD=1.68) for GLM tree. The estimated probability that a dataset was erroneously partitioned was very small for GLMM tree (.04; Table \ref{tab:treesize_main}), and much larger for GLM tree (.33; Table \ref{tab:treesize_main}). 

\begin{table}[!htbp]
\caption{Tree size distributions for GLM and GLMM tree for datasets without treatment-subgroup interactions.}
\small
\begin{tabular}{lrrrrrrr}
	\thickline
	&\multicolumn{7}{c}{tree size}\\
	\cline{2-7}
	&   1    &   3   &   5   &   7   &   9   &  11  &   total \\
	\hline
	GLMM tree	& 20625 & 932 & 43 &0&0&0& 21,600\\
		  		& (.96) & (.04) & ($<.01$) &(.00)&(.00)&(.00)&(1.00) \\
   	GLM tree		& 14501  & 4202 & 2013 & 802 & 79 & 3 & 21,600 \\ 
    			& (.67)  & (.20) & (.09) & (.04) & ($<.01$) & ($<.01$) & (1.00)\\ 
	\hline
  	\multicolumn{8}{l}{\textit{Note. }Bracketed values are proportions. Tree sizes are expressed as the total}\\
  	\multicolumn{8}{l}{number of nodes in a tree. A tree with a total of $J$ nodes has $(J+1)/2$ }\\
  	\multicolumn{8}{l}{terminal nodes; the true tree size in datasets without treatment-subgroup}\\	  	\multicolumn{8}{l}{interactions was 1.}\\
\end{tabular}
\label{tab:treesize_main}
\end{table}


\begin{figure}[!htbp]
	\includegraphics[width=12cm]{xy_treesizes_maineff.pdf}
	\caption{Average tree size of GLM and GLMM trees for datasets without treatment-subgroup interactions. Values ``correlated'' and ``uncorrelated'' refer to whether random intercept values are correlated to one of the $U_k$ variables; values $200$, $500$ and $1000$ refer to sample size. Reference line at $y=1$ represents the true tree size.}
	\label{fig:xyplot_treesize_nointeract}
\end{figure}


A graphical display was used to asses the effects of sample size, $\sigma_b$ and the correlation between $b$ and one of the $U_k$ variables, on tree size (Figure \ref{fig:xyplot_treesize_nointeract}). When random effects were absent (i.e., $\sigma_{b}=0$), both GLM and GLMM tree tend to create trees of size 1. In the presence of random effects, GLMM tree also tends to create trees of size 1, but GLM tree created much larger trees, when $b$ was correlated to one of the $U_k$ variables. This effect was stronger when sample size was larger. 







\subsection{Tree size in datasets with treatment-subgroup interactions}

In datasets with treatment-subgroup interactions, GLMM trees were also smaller than GLM trees. For these datasets, the true tree size was 7 (4 terminal nodes and 3 inner nodes; Figure \ref{fig:modelC}). Th distribution of tree sizes for GLM and GLMM tree in datasets with treatment-subgroup interactions are presented in Table \ref{tab:treesize_interactions}. The average size of GLMM trees was 7.15 (SD=0.61), and the average size of GLM trees was 8.11 (SD=2.05). The estimated probability that a datasets was erroneously not partitioned was 0, for both GLM and GLMM tree. However, Table \ref{tab:treesize_interactions} shows that a proportion of .91 of GLMM trees matched the true tree size, whereas a proportion of only .64 of GLM trees matched the true tree size (Table \ref{tab:treesize_interactions}).


\begin{table}[!htbp]
\caption{Tree size distributions for GLM and GLMM tree for datasets with treatment-subgroup interactions.}
\small
\begin{tabular}{lrrrrrrrr}
	\thickline
	&&\multicolumn{7}{c}{tree size}\\
	\cline{2-8}
	&     3   &   5   &   7   &   9   &  11  &   13   &   15  & total \\
	\hline
  	GLMM tree	& 3 & 227& 29556 & 2472 & 89 & 3 &0& 32,400 \\
				&  ($<.01$)  & ($<.01$)& (.91) & ($<.01$) & ($<.01$) & ($<.01$) &(.00)& (1.00) \\

  	GLM tree		& 145 & 1002 & 20578 & 4443 & 4178 & 1665 & 389 & 32,400\\ 
 			 	& ($<.01$) & (.03) & (.64) & (.14) & (.13) & (.05) & (.01) & (1.00) \\ 
	\hline
  	\multicolumn{9}{l}{\textit{Note. } Bracketed values are proportions. Tree sizes are expressed as the total number of}\\
  	\multicolumn{9}{l}{nodes in the tree. A tree with a total of $J$ nodes has $(J+1)/2$ terminal nodes; the true}\\	
  	\multicolumn{8}{l}{tree size in datasets with treatment-subgroup interactions was 7.}\\
\end{tabular}
\label{tab:treesize_interactions}
\end{table}
 
A graphical display was used to assess the effects of sample size, $\sigma_b$ and the correlation between $b$ and one of the $U_k$ variables, on tree size (Figure \ref{fig:xyplot_treesize_interact}). When random effects were absent (i.e., $\sigma_{b}=0$), both GLM and GLMM tree created trees of size 7, on average. 

Clear differences in performance between GLM and GLMMtree were observed when $\sigma_b > 0$. When $b$ is not correlated with one of the $U_k$ variables, when sample size is small (i.e., 200) and when $\sigma_b$ is large (i.e., 10), GLM tree has difficulty detecting splits and grows trees that are too small, on average. When $b$ is not correlated with one of the $U_k$ variables and when sample size is larger (i.e., 500 or 1000), GLM and GLMM trees are about the same size (i.e., $\approx 7$). When $b$ is correlated with one of the $U_k$ variables, GLM starts creating spurious splits, especially when sample size is larger (i.e., 500 or 1000) and when $\sigma_b$ is large (i.e., 10).  


\begin{figure}[!htbp]
	\includegraphics[width=12cm]{xy_treesizes_treatsubs.pdf}
	\caption{Average tree size of GLM and GLMM trees for datasets with treatment-subgroup interactions. Values ``correlated'' and ``uncorrelated'' refer to whether random intercept values are correlated to one of the $U_k$ variables; values $200$, $500$ and $1000$ refer to sample size. Reference line at $y=7$ represents true tree size.}
	\label{fig:xyplot_treesize_interact}
\end{figure}
	


 
\subsection{Tree accuracy in datasets with treatment-subgroup interactions}

To assess the accuracy of the trees created by GLM and GLMM tree, we inspected the variables and values that were selected for partitioning in every dataset. For the first split, both GLM tree and GLMM tree always selected the true partitioning variable ($U_2$). The true splitting value for $U_2$ was 30 (Figure \ref{fig:modelC}), and the mean splitting value selected for the first split was 29.94, for both GLM and GLMM tree. However, GLM tree showed somewhat higher variability in recovering the splitting value for the first split, than did GLMM tree (SD=0.155 and SD=0.126, respectively).

Overall, GLMM tree performed well in recovering treatment-subgroup interactions, accurately recovering the tree in 90.19\% of datasets. GLM tree accurately recovered the treatment-subgroup interactions in only 61.44\% of datasets. 

A graphical display was used to assess the effects of sample size, $\sigma_b$ and the correlation between $b$ and one of the $U_k$ variables, on the probability of accurate tree recovery for both algorithms (Figure \ref{fig:xyplot_treeaccuracy}). When random effects were absent from the datasets (i.e., $\sigma_b = 0$), the trees recovered by GLM and GLMM tree were equally accurate, on average. In the presence of random effects, GLM trees were much less accurate than GLMM trees. This was found for all sample sizes, when $b$ was correlated to one of the $U_k$ variables. When $b$ was not correlated to one of the $U_k$ variables, GLMM tree clearly outperformed GLM tree only when sample size was small (i.e., 200).



\begin{figure}[!htbp]
	\includegraphics[width=12cm]{xy_accuracy.pdf}
	\caption{Average accuracy of GLM and GLMM trees. Accuracy of trees is defined as the proportion of datasets in which the true tree was accurately recovered. Values ``correlated'' and ``uncorrelated'' refer to whether random intercept values are correlated to one of the $U_k$ variables; values $200$, $500$ and $1000$ refer to sample size.}
	\label{fig:xyplot_treeaccuracy}
\end{figure}





\subsection{Predictive accuracy on test data}

To assess predictive accuracy of both algorithms, correlation between the true and predicted treatment-effect differences of both algorithms were calculated for every dataset. Overall, treatment-effect differences predicted by GLMM tree were closer to the true differences than those predicted by GLM tree. The average correlation between the true and predicted treatment-effect differences over all 32,400 datasets was .88 (SD=0.20) for GLM tree, and .94 (SD=0.10) for GLMM tree.

A graphical display was used to assess the effects of sample size, $\sigma_b$ and the correlation between $b$ and one of the $U_k$ variables, on the predictive accuracy of both algorithms (Figure \ref{fig:xyplot_correlations}). Both algorithms showed higher predictive accuracy when sample size was larger, and when treatment-effect differences were larger. When random effects were absent from the datasets (i.e., $\sigma_b = 0$), predictions of GLM and GLMM tree were equally accurate. In the presence of random effects, GLM tree predictions were always much less accurate than those of GLMM tree. This effect was stronger when $\sigma_b$ was larger, sample size was larger, and/or treatment-effect differences were larger.

\begin{figure}[!htbp]
	\includegraphics[width=12cm]{xy_correlations.pdf}
	\caption{Average predictive accuracy of GLM and GLMM trees. Predictive accuracy of trees is defined as the correlation between the true and predicted differences between Treatment 1 and 2. Values $5$ and $2.5$ refer to the absolute value of the unstandardized treatment-effect difference in subgroups with treatment-effect differences; values $200$, $500$ and $1000$ refer to sample size.}
	\label{fig:xyplot_correlations}
\end{figure}



\section{Application to real data}

\subsection{Method}
To illustrate the application of, and differences in the results of GLM tree and GLMM tree, we applied both algorithms to a dataset from a meta-analytic study of \citeA{CuijyWeit14}. This meta-analysis was based on individual-patient data from 14 RCTs, comparing the effects of psychotherapy (cognitive behavioral therapy; CBT) and pharmacotherapy (PHA) in the treatment of depression. The study of \citeA{CuijyWeit14} was aimed at establishing whether gender is a predictor or moderator of the outcomes of psychological and pharmacological treatments for depression. Treatment outcomes were assessed by means of the 17-item Hamilton Rating Scale for Depression (HAM-D; \citeNP{Hami60}). \citeA{CuijyWeit14} found no indication that gender either predicted or moderated treatment outcomes. Further details on the dataset are provided in \citeA{CuijyWeit14}.

In our analyses, posttreatment HAM-D score was the outcome variable, and potential partitioning variables were age, gender, level of education, presence of a comorbid anxiety disorder at baseline, and pretreatment HAM-D score. The predictor variable in the linear model was treatment type (0=CBT and 1=PHA). An indicator for study was used as the cluster indicator. 

In RCTs, treatment effects are often estimated after controlling posttreatment values on the outcome measure for the linear effect of pretreatment values on the same measure. Therefore, we included the predictions of a linear regression of HAM-D posttreatment on HAM-D pretreatment scores, as an offset variable in all models. An offset variable is a linear predictor with an a-priori determined coefficient of one. Including the linear regression predictions as an offset has the same effect as statistically controlling for the linear effects of pretreatment scores, as is often done in ANCOVA. 

The \verb|lmtree| function deals with missing data by listwise deletion. Therefore, we build all trees using data of a subset of 694 patients from 7 studies, as complete observations (i.e., observations with non-missing values for potential partitioning variables, and pre- and posttreatment HAM-D score) for these patients were available. Results of our analysis may therefore not be representative of the complete dataset of the meta-analysis by \cite{CuijyWeit14}. 

Predictive accuracy of GLM and GLMM tree was assessed by calculating the average correlations between observed and predicted HAM-D scores, based on 50-fold cross validation.

\subsection{Results}

The trees resulting from appplication of GLM and GLMM tree to the dataset are presented in Figure \ref{fig:lmtree_C&W} and \ref{fig:lmertree_C&W}, respectively. Note that the GLM tree in Figure \ref{fig:lmtree_C&W} is also the tree that is created in the first iteration of the GLMM-tree algorithm. 

\begin{figure}[!ht]
    \includegraphics[width=12cm]{app_lmtree.pdf}
	\caption{GLM tree for prediction of posttreatment total scores on the Hamilton Rating Scale for Depression (HAM-D). The y-axes of the boxplots represent posttreatment HAM-D scores, and the x-axes represent treatment levels: cognitive behavior therapy (CBT) vs. pharmacotherapy (PHA).}
	\label{fig:lmtree_C&W}
\end{figure}

The GLM tree (Figure \ref{fig:lmtree_C&W}) selected level of education as the first partitioning variable, and presence of a comorbid anxiety disorder as a second partitioning variable, for observations with a higher level of education. Node 2 of Figure \ref{fig:lmtree_C&W} indicates that for patients with a low level of education, antidepressant medication provides the greatest reduction in HAM-D scores. Node 4 indicates that for patients with a higher level of education, and no comorbid anxiety disorder, the reduction in HAM-D scores is about the same for CBT and antidepressant mediation. Node 5 indicates, that for patients with a higher level of education and a comorbid anxiety disorder, the reduction in HAM-D scores is greatest for pharmacotherapy.


\begin{figure}[!ht]
    \includegraphics[width=10cm]{app_lmertree.pdf}
	\caption{GLMM tree for prediction of posttreatment total scores on the Hamilton Rating Scale for Depression (HAM-D). The y-axes of the boxplots represent posttreatment HAM-D scores, and the x-axes represent treatment levels: cognitive behavior therapy (CBT) vs. pharmacotherapy (PHA).}
	\label{fig:lmertree_C&W}
\end{figure}

By taking into account the study-specific intercepts, the final GLMM tree (Figure \ref{fig:lmertree_C&W}) indicates that the first split made by GLM tree is a spurious split. The GLMM tree selected only presence of a comorbid anxiety disorder as a partitioning variable. The terminal nodes of Figure \ref{fig:lmertree_C&W} show only a single treatment-subgroup interaction: for patients without a comorbid anxiety disorder, CBT and antidepressant medication provide more or less the same reduction in HAM-D scores, whereas for patients with a comorbid anxiety disorder, antidepressant medication provides a greater reduction in HAM-D scores. The estimated variance of the random intercept term was 2.12, with an estimated intraclass correlation coefficient of .05. 

Assessment of predictive accuracy by means of 50-fold cross validation showed that the GLMM tree had higher predictive accuracy than the GLM tree. The correlation between true and predicted posttreatment HAM-D total scores, averaged over the 50 folds, was .39 (SD=.20) for GLMM tree, and .31 (SD=.24) for GLM tree. This indicates that GLMM tree not only provided higher predictive accuracy, on average, but also had somewhat lower variability of predictive accuracy than GLM tree. 




\section{Discussion}

The results of our simulation study show that GLMM tree performed very well in recovering treatment-subgroup interactions, by recovering the true tree structure in 90\% of the simulated datasets with treatment-subgroup interactions. In the absence of treatment-subgroup interactions, GLMM tree erroneously detected subgroups in only 4\% of the datasets. GLM tree performed less accurate than GLMM tree: in datasets with treatment-subgroup interactions, GLM tree recovered the true tree structure in 61\% of the simulated datasets. In datasets without treatment-subgroup interactions, GLM tree erroneously detected subgroups in 33\% of the datasets. 

The better performance of GLMM tree was mostly observed when random effects in the datasets were sizable, and random intercepts were correlated with potential partitioning variables. In these instances, the random effects gave rise to spurious subgroup detection (spurious splits) by GLM tree, both in datasets with and without treatment-subgroup interactions. 

Also, predictive accuracy of GLMM tree was higher than that of GLM tree. The average correlation between the true treatment differences and those predicted by GLMM tree was .94.  The average correlation between the true treatment differences and those predicted by GLM tree was .88. In terms of predictive accuracy, GLMM tree clearly outperformed GLM tree when random effects in the datasets were sizable, and the differences in treatment effects were relatively small (i.e., $d=.5$).

As expected, when random effects were absent from the simulated datasets, GLM tree and GLMM tree showed high and equal predictive accuracy. This finding indicates that GLMM tree can be applied, whenever cluster-specific random effects are expected. In the absence of random effects, GLM tree and GLMM tree are expected to perform equally well, and in the presence of random effects, GLMM tree will outperform GLM tree. This is especially the case with large sample sizes ($N > 200$), as the increased power will likely cause GLM tree to create spurious splits in the presence of random effects. 

Not surprisingly, for both algorithms, accuracy of predicted treatment differences was less when sample size was low (i.e., $N=200$). Sample size influenced performance of GLM tree and GLMM tree similarly, suggesting that a larger number of estimated parameters for GLMM tree does not adversely influences accuracy at low sample sizes. Our simulation results do warrant some caution for the detection of treatment-subgroup interactions or treatment moderators in small datasets (e.g., single RCTs), but irrespective of the algorithm used. 

Although these findings are encouraging for the use of GLMM tree in the detection of treatment-subgroup interactions in datasets with clustered structures, some limitations and challenges for future research should be noted.

The simulations show that GLMM tree performs very well, if the model is correctly specified. That is, if there are subgroups with respect to the partitioning variables, so that there are different parameters of the GLM in each of these subgroups, then the algorithm will accurately recover those subgroups. However, misspecification of the model can reduce performance. One source of misspecification would be, when relevant variables are not included in the GLM or as partitioning variables. If there are actual subgroups, but the variables describing them are not entered as partitioning variables, the algorithm can only approximate the subgroups using the partitioning variables that are available. Or, if the coefficients of other variables vary across subgroups, then those variables should also be included in the GLM. Another source of misspecification would be the inclusion of irrelevant variables in the GLM or as partitioning variables, which may reduce the power to detect the actual subgroups. However, it should be note that in our simulations, the number of partitioning variables did not substantially influence performance of the algorithm. 
  
A challenge for future research is the development of more adequate ways to deal with missing data. GLM tree, and therefore also GLMM tree, handle missing data by listwise deletion, like most tree-based algorithms for treatment-subgroup interaction detection. However, missing data commonly occurs in clinical trails, and listwise deletion is not an optimal approach for dealing with missing data.

In conclusion, GLMM tree provided highly accurate recovery of treatment-subgroup interactions and predictions of treatment effect differences, both in the presence and absence of cluster-specific random effects. Therefore, GLMM tree is a promising algorithm for the detection of treatment-subgroup interactions in datasets with a clustered structure, like for example in multi-center trials, individual-level patient data meta-analyses, and longitudinal studies.

\nolinenumbers
\bibliographystyle{apacite}
\bibliography{bib}

\newpage
\section{Appendix: Notation}

\NumTabs{12}
\noindent $1,...,i,...,N$ \tab{observation number}\\
$1,...,j,...,J$ \tab{terminal node number in a tree}\\
$1,...,k,...,K$ \tab{partitioning variable number}\\
$1,...,m,...,M$	\tab{cluster number}\\
$\beta_{j}$		\tab\tab{column vector of fixed-effects coefficients in terminal node $j$}\\
$b_{m}$ 		\tab\tab{column vector of random-effects coefficients in cluster $m$}\\
$d_{j}$			\tab\tab{$\beta_{j1} / \sigma_{\epsilon}$; effect size of treatment-effect differences between Treatment 1 and}\\
\tab\tab\tab\tab{Treatment 2 in terminal node $j$}\\
$\epsilon$		\tab\tab{deviation of observed treatment outcome $y$ from its expected value}\\
$\sigma_{b}$	\tab\tab{square root of variance of $b$}\\
$\sigma_{\epsilon}$\tab\tab{square root of the variance of $\epsilon$}\\
$U_k$			\tab\tab{(potential) partitioning variable $k$}\\
$x_i$ 			\tab\tab{column vector of fixed-effects predictor variable values for observation $i$}\\
%$y_i$ 			\tab\tab{treatment outcome for observation $i$}\\
%$z_i$ 			\tab\tab{column vector of random-effects predictor variable values for observation $i$}\\

\end{document}