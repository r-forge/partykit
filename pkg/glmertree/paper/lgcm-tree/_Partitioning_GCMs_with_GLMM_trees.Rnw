\documentclass[doc,floatsintext,natbib]{apa7}
%\usepackage[man,floatsintext]{apa6}
%\usepackage[doc,floatsintext]{apa6}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{doi}
\usepackage{booktabs}
%\usepackage[dvipsnames]{xcolor}
\usepackage{placeins}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{setspace}
\usepackage{hyperref}
\renewcommand{\doitext}{} % prevents superfluous addition of "doi:" in references
\renewcommand{\thefootnote}{\fnsymbol{footnote}} % uses symbols instead of numbers for footnotes

%\usepackage[colorinlistoftodos]{todonotes}
%%\todo[inline, color=green!40]{This is an inline comment.}

\linespread{1.3}


\title{Subgroup detection in linear growth curve models with generalized linear mixed model (GLMM) trees}
\shorttitle{Recursive partitioning of linear growth curve models}
\authorsnames{Marjolein Fokkema$^1$, Achim Zeileis$^2$}
\authorsaffiliations{{$^1$Leiden University}, {$^2$Universit\"at Innsbruck}}

\abstract{Growth curve models are popular tools for studying the development of a response variable within subjects over time. Heterogeneity between subjects is common in such models, and researchers are typically interested in explaining or predicting this heterogeneity. We show how generalized linear mixed effects model (GLMM) trees can be used to identify subgroups with differently shaped trajectories in linear growth curve models. Originally developed for clustered cross-sectional data, GLMM trees are extended here to longitudinal data. The resulting extended GLMM trees are directly applicable to growth curve models as an important special case. In simulated and real-world data, we assess the performance of the extensions and compare against other partitioning methods for growth curve models. Extended GLMM trees perform more accurately than the original algorithm and LongCART, and similarly accurate as structural equation model (SEM) trees. In addition, GLMM trees allow for modeling both discrete and continuous time series, are less sensitive to (mis-)specification of the random-effects structure and are much faster to compute.\\}

\begin{document}
\SweaveOpts{concordance=TRUE}
\maketitle
\setlength{\tabcolsep}{3pt}



\section{Introduction}
\label{sec:Introduction}

<<eval=FALSE,echo=FALSE>>=
## If .Rnw will not compile bibliography, run tinytex on .tex file:
tinytex::latexmk("Partitioning_GCMs_with_GLMM_trees.tex")
@

<<echo=FALSE>>=
library("glmertree")
load("bag_ids, N = 250, Science.Rda")
load("Science ability data.Rdata")
names(sciedata) <- c("CHILDID", "GENDER", "RACE", "SES", "GMOTOR", "FMOTOR", 
                 "INTERN", "EXTERN", "INTERP", "CONTRO", "FIRKDG", 
                 "AGEBASELINE", "science", "score", "asmtmm", "months")
data <- sciedata[sciedata$CHILDID %in% bag_ids[ , 39], ]
data$CHILDID <- factor(data$CHILDID)
data$GENDER <- factor(data$GENDER)
data$asmtmm <- factor(data$asmtmm)
library("lmerTest")
LMM <- lmer(score ~ months + (1|CHILDID), data = data)
icc <- data.frame(VarCorr(LMM))
@


Development over time is of prime interest in psychological research. For example, in educational studies researchers may want to model student's academic development over time; in clinical studies researchers may want to model patients' symptoms over time. Mixed-effects or latent-variable models can be used to model such trajectories and allow for explaining heterogeneity with covariates of a-priori known relevance \citep[e.g., ][]{NeisyMatt18}. However, when these covariates are not known in advance, methods for identifying them are needed. 

As an example, trajectories of science knowledge and skills from a sample of 250 children are depicted in Figure~\ref{fig:global_trajectories}. The children were assessed at three timepoints across grades 3 through 8\footnote{Further details on the source of these data are provided in Study~III.}. The red line depicts the estimated average trajectory, while the gray lines depict individual trajectories. The gray lines reveal substantial variability between the children, both in initial levels and growth over time. An obvious research aim would be to identify covariates that can explain or predict this heterogeneity. 

\begin{figure}[b]%
\caption{Growth curves of science ability.}
\begin{subfigure}{0.65\textwidth}%
<<fig=TRUE, echo=FALSE, results=hide, fig.width=7, fig.height=7>>=
y <- t(sapply(unique(data$CHILDID), function(x) data$score[data$CHILDID == x]))
x <- t(sapply(unique(data$CHILDID), function(x) data$months[data$CHILDID == x]))
plot(data$months, data$score, type = "n", xlab = "Months after baseline", 
     ylab = "Science ability", xaxt = "n", cex.lab = 1, cex.axis=1,
     xlim = c(0.5, 57^(2/3)), main = "Full sample (N = 250)")
axis(1, at=c(0, 12, 36, 60)^(2/3), labels=c(0, 12, 36, 60), cex.axis = .8)
sapply(1:nrow(x), function(row) lines(x[row, ], y[row, ], col = gray(0.2, alpha = 1/6), lwd = 2))
lines(x = c(min(data$months), max(data$months)), 
     y = c(fixef(LMM)[1], fixef(LMM)[1] + max(data$months)*fixef(LMM)[2]),
     lwd = 3, col = 2)
@
\\\footnotesize{\textit{Note.} Gray lines depict observed individual trajectories, red line depicts average growth curve as estimated with a linear mixed-effect model, comprising a fixed effect of time and a random intercept with respect to individuals. The $x$-axis is not linear in the number of months, because time was scaled as $(\text{\# of months})^\frac{2}{3}$ in order to obtain approximately linear trajectories.}
\end{subfigure}%
\label{fig:global_trajectories}
\end{figure}%


%\FloatBarrier
\subsection{Recursive Partitioning Methods for Growth Curve Models}

Recursive partitioning methods (RPMs), also known as ``trees``, allow for identifying relevant predictors from a potentially (very) large number of covariates. Figure~\ref{fig:LMM_tree_r} shows an example tree, which identified socio-economic status (SES), gross motor skills (GMOTOR) and internalizing problems (INTERN) from a set of 11 socio-demographic and behavioral characteristics of the children, assessed at baseline. Five subgroups were identified, corresponding to the terminal nodes of the tree, each with a different estimate of the fixed intercept and slope %(presented in Table~\ref{tab:local_coefs}). 
Groups of children with lower (higher) SES also have a lower (higher) intercept, indicating lower (higher) average science ability. For children with lower SES (node~2), there is also an effect of gross motor skills: The group with lower motor skills has a lower intercept. For children with intermediate levels of SES (node~6), there is also an effect of internalizing problems: The group with lower internalizing problems has a higher intercept. The two groups (or nodes) with higher intercepts also have higher slopes, indicating that children with higher ability also gain more ability over time.   




\begin{figure}%
\caption{Partitioned growth curves of science ability.}
\begin{subfigure}{0.9\textwidth}%
<<fig=TRUE, height=9, width=13.5, echo=FALSE>>=
LMM_form <- score ~ months | CHILDID | GENDER + RACE + SES + GMOTOR + FMOTOR + 
  INTERN + EXTERN + INTERP + CONTRO + FIRKDG + AGEBASELINE
testdat <- sciedata[!sciedata$CHILDID %in% bag_ids[ , 39], ]
testdat$CHILDID <- factor(testdat$CHILDID)
testdat$GENDER <- factor(testdat$GENDER)
testdat$asmtmm <- factor(testdat$asmtmm)
## Remove obs with levels of race in test that are not in train
levs <- unique(testdat$RACE) %in% unique(data$RACE)
if (!all(levs)) {
  testdat <- testdat[-which(!testdat$RACE %in% unique(testdat$RACE)[levs]), ]
}

## Fit models
lmm_tree <- lmertree(LMM_form, data = data, maxdepth = 6)
lmm_tree_c <- lmertree(LMM_form, data = data, cluster = CHILDID, maxdepth = 6)
lmm_tree_r <- lmertree(LMM_form, data = data, maxdepth = 6, ranefstart = TRUE)
icc_r <- as.data.frame(VarCorr(lmm_tree_r))
lmm_r_preds <- predict(lmm_tree_r, newdata = testdat, re.form=NA)
icc_c <- as.data.frame(VarCorr(lmm_tree_c))
lmm_c_preds <- predict(lmm_tree_c, newdata = testdat, re.form=NA)
lmm_preds <- predict(LMM, newdata = testdat, re.form=NA)

## Plot tree
plot(lmm_tree_r, which = "growth", ip_args = list(pval = FALSE), 
     nodesize_level = "CHILDID", tp_args = 
       list(xscale = c(0, 60^(2/3)), xaxis.at = c(0, 12, 36, 60)^(2/3),
            xaxis.labs = c(0, 12, 36, 60)), gp = gpar(cex = 1.5))
@
\label{subfig:tree}
\end{subfigure}%
~
\begin{subfigure}{0.25\textwidth}%
\vspace{-2.95cm}
\hspace{-2.5cm}
\scriptsize
<<echo=FALSE, results=tex, fig.pos='b'>>=
library("kableExtra")
coefs <- coef(lmm_tree_r)
coefs <- data.frame(cbind(Node = as.numeric(rownames(coefs)), coefs))
colnames(coefs) <- c("Node", "Intercept", "Slope")
coefs$Slope <- paste0("$", formatC(coefs$Slope, digits = 3, format = "f"), "$")
coefs$Intercept <- paste0("$", formatC(coefs$Intercept, digits = 3, format = "f"), "$")
coefs$Intercept[!grepl("-", coefs$Intercept)] <- paste("$\\:\\:$",
                                                       coefs$Intercept[!grepl("-", coefs$Intercept)])
kable(coefs, format = "latex", linesep = "", # linesep command suppressess addlinesep every 5 rows
        row.names = FALSE, digits = 3L,escape=FALSE, align = c("ccc"), booktabs = TRUE)
@
\label{subfig:coefs}
\end{subfigure}
\\\footnotesize{\textit{Note.} The $x$-axes represent the number of months after the baseline assessment, $y$-axes represent science ability. Gray lines depict observed individual trajectories, red lines depict average growth curve within each terminal node, as estimated with a linear mixed-effect model comprising node-specific fixed effects of time and a random intercept with respect to individuals. The table presents numerical estimates of fixed intercepts and slopes.}
\label{fig:LMM_tree_r}
\end{figure}%

<<echo=FALSE, results=tex, fig.pos='b'>>=
# library("kableExtra")
# coefs <- coef(lmm_tree_r)
# coefs <- data.frame(cbind(Node = as.numeric(rownames(coefs)), coefs))
# colnames(coefs) <- c("Node", "Intercept", "Slope")
# coefs$Slope <- paste0("$", formatC(coefs$Slope, digits = 3, format = "f"), "$")
# coefs$Intercept <- paste0("$", formatC(coefs$Intercept, digits = 3, format = "f"), "$")
# coefs$Intercept[!grepl("-", coefs$Intercept)] <- paste("$\\:\\:$",
#                                                        coefs$Intercept[!grepl("-", coefs$Intercept)])
# kable_styling(kable(coefs, format = "latex", booktabs = TRUE, label = "local_coefs",
#                              align = "lccc", escape=FALSE, linesep = "", # linesep command suppressess addlinesep every 5 rows
#         caption = "Fixed-effects coefficient estimates from the terminal nodes of Figure 2.",
#         row.names = FALSE, digits = 3L),
#   font_size = 11, full_width=FALSE)
@

The tree in Figure~\ref{fig:LMM_tree_r} has been estimated with generalized linear mixed-effects model trees \citep[GLMM trees; ][]{FokkySmit18}. GLMM trees were originally proposed for subgroup detection in clustered cross-sectional studies, where subjects are nested in treatment centers, classrooms and/or geographical areas, for example. In the current paper we extend GLMM trees, so that they can be applied to partitioning LGCMs. The general idea of GLMM trees is appealing for subgroup detection in almost any type of mixed-effects model. Compared to clustered cross-sectional data, however, longitudinal data may require a different estimation approach: The variance of random effects tends to be higher with longitudinal data, and the predictors of interest tend to be measured at higher levels (e.g., time-invariant covariates). In this paper, we propose and test two extensions of GLMM trees that account for these characteristics. We focus on the specific use case of partitioning LGCMs, but the extensions are critical for a wider range of settings where covariates are measured at higher levels and/or where the random effects have substantial variance.

There are several alternative RPMs that can be used to partition linear growth-curve models (LGCMs): GUIDE \citep{Loh02}, longRPart \citep{AbdoyLeBl02}, GEE-based decision trees \citep{Lee05}, longitudinal interaction trees \cite[IT; ][]{SuyMene11}, SEM trees \citep{BranyOert13}, mixed-effects longitudinal trees \citep[MELT; ][]{EoyCho14} and LongCART \cite{KundyHare19}. Further, the longRPart2 \citep{StegyJaco18} and IT-LT \citep{WeiyLiu20} methods allow for subgroup detection in non-linear growth curve models\footnote{Both IT methods specifically target subgroups with different time-by-treatment interactions, so are not generally applicable for partitioning growth curve models.}.

The main characteristic that sets GLMM trees apart from other methods for partitioning LGCMs is its local-global estimation approach: GLMM trees do not fit a full parametric model in each of the subgroups defined by the terminal nodes of the tree. Instead, fixed-effects parameters are estimated \textit{locally}, using the observations within a terminal node, and the random-effects parameters are estimated \textit{globally}, using all observations. This local-global estimation approach was first proposed by \cite{HajjyBell11} and \cite{SelaySimo12} for trees with constant fits (i.e., intercepts only) in the terminal nodes. With GLMM trees, the approach was generalized to allow for GLMs with any number of parameters in the terminal nodes, thus allowing for non-Gaussian responses and targeted detection of a wide range of possible interaction effects in mixed-effects models \citep{FokkySmit18}.

Other methods for partitioning LGCMs take a fully local estimation approach: Within every node or subgroup defined by the terminal nodes, a full parametric model is estimated based on the observations in that subgroup only. This fully local estimation approach provides more flexibility, but yields higher computational burden and model complexity. In contrast, GLMM trees estimate a (much) lower number of random-effects parameters, which likely reduces overfitting and improves stability and generalizability of the results. Furthermore, the fully local estimation requires possible partitioning variables to be measured at the highest level of nesting, while GLMM trees' local-global estimation approach allows partitioning variables to be measured at any level.

The computational advantage of GLMM trees is strongest compared to longRPart, longRPart2, IT-LT and LRT-based SEM trees. These methods employ an exhaustive split detection procedure, where for every possible split point in the current node, the full parametric model needs to be re-estimated in the two resulting nodes. To choose the optimal split, the splitting criterion (such as a $p$-value from a likelihood-ratio test) is derived from these two models. Not only does this bring a heavy computational load, it also introduces a selection bias towards covariates with a larger number of possible cutpoints \citep{ShihyTsai04, Shih04}. LongCART, MELT, GEE-based decision trees and score-based SEM trees also fit full parametric models in each of the nodes, but do not require model refitting for cutpoint selection; they employ the predictions or residuals from the fitted model in the current node for selecting the best split. This reduces computational load, while it also allows for separating variable and cutpoint selection, thus preventing selection bias. The GLMM tree algorithm shares these advantages, because it also employs a two-step approach to split selection.

Given their unbiased variable selection, lower model complexity and computational burden, GLMM trees might be particularly useful for subgroup detection in LGCMs. The next section explains how GLMM trees are estimated and propose adjustments for partitioning longitudinal trajectories. Next, the performance of the proposed adjustments is evaluated: In Study~I, we assess performance in simulated datasets, in Study~II, we compare performance of GLMM trees with that of two other methods: SEM trees and LongCART. In Study~III, we assess performance of the proposed adjustments in existing datasets on children's development of reading, math and science abilities. In the Discussion, we summarize our findings and discuss implications.

% Compared to RPMs that employ likelihood-ratio tests as a splitting criterion \citep{AbdoyLeBl02, StegyJaco18, BranyOert13, WeiyLiu20}, GLMM trees require less computation, do not present with a variable selection bias, and do not require partitioning variables to be measured at the highest level.

% Compared to GEE-based RPMs of \cite{Lee05, SuyMene11, CalhyLevi20}, GLMM trees also allow for a GEE-type approach to split selection, while explicitly modeling inter-individual variation through the estimation of random effects.

% Compared to RPMs which compute splitting criteria based on only the sign of residuals \cite{ChauyLo95, Loh02, Lee05}, the score-based tests employed by GLMM trees allow for capturing both sign and magnitude of misfit.


\FloatBarrier
\section{Estimation of GLMM trees and Adaptations for Longitudinal Data}
\label{sec:estimation}

In the GLMM tree model \citep{FokkySmit18}, expectation $\mu_i$ of outcome vector $y_i$ is modeled through a linear predictor and suitable link function:
%
\begin{eqnarray}
\label{eq:expected_value}
E[y_i | X_i] & = & \mu_i, \\
\label{eq:GLMMtree}
g(\mu_{i}) & = & X_{i} \beta_{j} + Z_{i} b_{i}
\end{eqnarray}
%
Throughout this paper we focus on the case with a continuous, normally-distributed response $y_i$ with constant variance $\sigma_\epsilon$. Therefore, the identity function can be used for the link $g$ but generalizations to other response variable types within the GLM are straightforward. In the general notation above, $X_i$ is the $n_i \times (p+1)$ fixed-effects design matrix for subject~$i$ ($i = {1, \dots, N}$), comprising $p$ regressors plus one column of 1s for the intercept. In the following, we assume that time is the predictor variable of interest (i.e., $p = 1$), where the number $n_i$ and spacing of observed timepoints may differ between subjects. The fixed-effects parameters $\beta_j$ (here, intercept and time slope) in GLMM trees are node-specific, i.e., their value depends on the subgroup/node~$j$ into which subject~$i$ falls. As in a traditional GLMM, $Z_i$ is the random-effects design matrix for subject $i$, comprising a subset of columns of $X_i$, and $b_i$ is the corresponding vector of random effects for subject $i$. Finally, $b_i$ is assumed to follow a (possibly multivariate) normal distribution with mean zero and (co)variance $\Sigma$.

The parameters of a traditional GLMM can be estimated, among other techniques, by maximum likelihood (ML) or restricted ML (REML). Thus, when it is known into which node~$j$ each subject~$i$ falls, the GLMM specified by Equation~\ref{eq:GLMMtree} can be fitted ``as usual'', yielding \emph{local} subgroup-specific fixed-effect estimates~$\hat \beta_j$ and \emph{global} random-effect estimates~$\hat b_i$. To infer the subgroup membership for all observations~$i$, the random-effect estimate is treated as a known offset and a GLM tree is estimated using the model-based (MOB) recursive partitioning algorithm of \cite{ZeilyHoth08}. The overall GLMM tree model is then estimated by alternating between estimating the partition (i.e., subgroups or terminal nodes $j$), and estimating the random- and fixed-effects parameters, as per the following algorithm:
%
\begin{enumerate}
\setlength\itemsep{0em}
	\setcounter{enumi}{-1}
	\item Initialize by setting step $r = 0$ and all random-effect estimates $\hat{b}_{i,(r)} = 0$.
	\item Set $r = r+1$. Fit a GLM tree using $Z_{i} \hat{b}_{i,(r-1)}$ as an offset, yielding the partition $j_{(r)}$.
	\item Fit the mixed-effects model $g(\mu_{i}) = X_{i} \beta_{j, (r)} + Z_{i} b_{i, (r)}$ with the partition $j_{(r)}$ from Step~1. Extract the random-effect estimates~$\hat{b}_{i,(r)}$ from the fitted model.
	\item Repeat Steps~1 and~2 until convergence.
\end{enumerate}
%
This initialization simply assumes zero random effects. Convergence of the algorithm is monitored through the log-likelihood of the mixed-effects model fitted in Step~3. Typically, this converges when the partition~$j_{(r)}$ from the GLM tree is the same as $j_{(r-1)}$ from the previous step.

The following two subsections describe alternative approaches for the initialization in Step~0 and for fitting the GLM tree in Step~1. Each subsection first reviews the well-established methods and then proceeds to discuss modifications that may improve performance when partitioning longitudinal data.


\subsection{Initialization}

Previous publications on mixed-effects recursive partitioning find that initializing the random-effect estimates with zero yields accurate estimates of subgroup memberships and the final models \citep{HajjyBell11, HajjyBell14, HajjyLaro17, SelaySimo12, FuySimo15, FokkySmit18}. \cite{SelaySimo12} assessed the impact of different initialization values and found only minor differences that decreased with increasing sample size. In \cite{FokkySmit18}, we found initializing estimation of GLMM trees with zero random effects performed well in cross-sectional clustered data. With longitudinal data, however, random effects tend to be more pronounced: Repeated measures on the same subjects tend to be correlated more strongly than observations nested within the same unit in cross-sectional data. If random effects are sizable, the initial assumption of zero random effects could provide an unrealistic starting point that may be difficult to overcome in subsequent iterations. Our expectation is that for partitioning LGCMs, initializing estimation with the random effects instead of the subgroup structure may improve subgroup recovery. Specifically, that means the algorithm starts by estimating the classic version of the mixed-effects model from Equation~\ref{eq:GLMMtree} with just one set of fixed-effects coefficients $\beta$ and all subjects in a single group.

The alternative initialization step is thus:
%
\begin{enumerate}
\setlength\itemsep{0em}
\renewcommand{\labelenumi}{\arabic{enumi}$'$.}
	\setcounter{enumi}{-1}
	\item Initialize by setting step $r = 0$ and fit the mixed-effects model $g(\mu_{i}) = X_{i} \beta + Z_{i} b_{i, (r)}$ to the full sample. Extract the random-effect estimates~$\hat{b}_{i,(r)}$ from the fitted model.
\end{enumerate}


\begin{figure}%
\caption{GLMM tree estimated by initializing with zero random effects.}
\begin{subfigure}{1.2\textwidth}
<<fig=TRUE, height=9, width=16, echo=FALSE>>=
lmm_tree <- lmertree(LMM_form, data = data, maxdepth = 6)
plot(lmm_tree, which = "growth", ip_args = list(pval = FALSE), 
     nodesize_level = "CHILDID", tp_args = 
       list(xscale = c(0, 60^(2/3)), xaxis.at = c(0, 12, 36, 60)^(2/3),
            xaxis.labs = c(0, 12, 36, 60)), gp = gpar(cex = 1))
icc <- as.data.frame(VarCorr(lmm_tree))
lmm_preds <- predict(lmm_tree, newdata = testdat, re.form=NA)
@
\end{subfigure}
\\\footnotesize{\textit{Note.} The $x$-axes represent the number of months after the baseline assessment, the $y$-axes represent science ability. Gray lines depict observed individual trajectories. Red lines depict the average growth curve within each terminal node, as estimated with a linear mixed-effect model with a node-specific fixed effect of time and random intercepts estimated with respect to individuals.}
\label{fig:LMM_tree}
\end{figure}%


To illustrate, we applied both initialization approaches to the dataset from Figure~\ref{fig:global_trajectories}. In fact, the tree presented in Figure~\ref{fig:LMM_tree_r} was estimated by initializing with the random effects. Initializing estimation assuming zero random effects resulted in the tree in Figure~\ref{fig:LMM_tree}. The split based on gross motor skills in Figure~\ref{fig:LMM_tree_r} was not implemented in  Figure~\ref{fig:LMM_tree}, while additional splits were implemented based on gender, race, internalizing problems and fine motor skills. Considering the relatively large number of subgroups in  Figure~\ref{fig:LMM_tree_r}, some with relatively small sample sizes, this tree may overfit and not generalize well to other samples. 



\FloatBarrier
\subsection{Partitioning}

\begin{figure}%
\caption{GLMM tree estimated with cluster-level parameter stability tests.}
<<fig=TRUE, height=8, width=12, echo=FALSE>>=
plot(lmm_tree_c, which = "growth", ip_args = list(pval = FALSE), 
     nodesize_level = 2, tp_args = list(xscale = c(0, 60^(2/3)), 
                                        xaxis.at = c(0, 12, 36, 60)^(2/3),
                                        xaxis.labs = c(0, 12, 36, 60)),
     gp = gpar(cex = 1.5))
@
\\\footnotesize{\textit{Note.} The $x$-axes represent the number of months after the baseline assessment, $y$-axes represent science ability. Gray lines depict observed individual trajectories. Red lines depict the average growth curve within each terminal node, as estimated with a linear mixed-effect model with a node-specific fixed effect of time and random intercepts estimated with respect to individuals.}
\label{fig:LMM_tree_c}
\end{figure}%

The subgroup structure in Step~1 is estimated by a GLM tree using the general model-based recursive partitioning (MOB) algorithm of \cite{ZeilyHoth08}. Here, we give a general overview and then comment on the aspects of the algorithm that may be particularly relevant for LGCMs. In the case of GLMs (with random effects held constant in an offset), the MOB algorithm cycles iteratively through the following steps: 
%
\begin{enumerate}
\setlength\itemsep{0em}
\renewcommand{\labelenumi}{(\alph{enumi})}
	\item Fit the GLM to all observations in the current subgroup. 
	\item Test for instability of the GLM parameters with respect to each of the partitioning variables.
	\item If there is some overall parameter instability, split the subgroup with respect to the partitioning variable associated with the highest instability.
	\item Repeat Steps (a) through (c) in each of the resulting subgroups.
\end{enumerate}
%
Parameter stability in Step~(b) is tested using the the \textit{scores} (gradient contributions) from the GLM fitted in Step~(a). Under correct specification of the model and mild regularity conditions, the scores have an expected value of 0. The parameter stability tests evaluate whether the scores fluctuate randomly around this mean of 0, or exhibit systematic deviations when ordered by the values of a covariate available for partitioning. For continuous covariates $u_k$ (or ordered covariates with a large enough number of unique values), this involves computing the following cumulative score process $W_k(t)$ with respect to each potential partitioning variable \citep{ZeilyHoth08}:
%
\begin{eqnarray}
\label{eq:efp}
W_{k}(t) = \hat{J}^{-1/2} n_{j}^{-1/2} \sum^{[n_jt]}_{i=1}{\hat{\psi}}_{\sigma(u_{ik})}
\end{eqnarray}
%
where $\hat{J}$ is a suitable estimate of the covariance matrix of the parameter estimates, and $n_j$ gives the number of observations in the current subgroup. Further, $\hat{\psi}_{\sigma(u_{ik})}$ denotes the scores evaluated at the parameter estimates, with subscript {$\sigma(u_{ik})$} denoting their ordering by the values of partitioning covariate $u_k$. Note that $0 \leq t \leq 1$, thus $n_jt = 1$ for an observation associated with a unique minimum on the partitioning variable, and $n_jt = n_j$ for an observation with a unique maximum. 

From the cumulative score process $W_k(t)$, a range of test statistics can be derived which capture increased fluctuations (beyond the random fluctuation under parameter stability). For numerical partitioning variables, a maximum Lagrange multiplier test statistic can be computed, which takes the maximum of the squared Euclidean norm of $W_k(t)$, weighted by its variance \citep{ZeilyHorn07}. This statistic is referred to as the \textit{supLM} statistic, and is asymptotically equivalent to the maximum of likelihood-ratio statistics. Approximate asymptotic $p$-values for the \textit{supLM} statistic can be computed with the method of \cite{Hans97}. Categorical covariates do not provide an implicit ordering and scores are therefore binned at each level of the covariate. From these, a test statistic is computed that does not depend on the ordering of the levels \citep{MerkyFan14}. 

When partitioning longitudinal data, covariates will often be measured at the subject level (i.e., time-invariant covariates), which should be accounted for in computing the estimated covariance matrix $\hat{J}$. In general, this computation makes use of the scores. By summing the scores within clusters prior to computation of the covariances, so-called \textit{clustered} covariances are obtained, which account for dependence between observations within the same cluster \citep{ZeilyKoll20}. This resembles a GEE-type approach with an independence correlation structure. Our expectation is that in partitioning LGCMs, use of clustered covariances in the parameter stability tests will improve subgroup recovery.

The tree in Figure~\ref{fig:LMM_tree_c} was estimated using cluster-level parameter stability tests. Application of GLMM trees with both cluster-level parameter stability tests and random-effects initialization yielded the exact same tree structure and parameter estimates. Compared to Figures~\ref{fig:LMM_tree_r} and \ref{fig:LMM_tree}, the cluster-level parameter stability tests provided the most parsimonious tree structure thus far. The estimated variances of the random intercept were \Sexpr{formatC(data.frame(VarCorr(lmm_tree_r$lmer))$vcov[1], digits = 2, format = "f")} (Figure~\ref{fig:LMM_tree_r}, \Sexpr{formatC(data.frame(VarCorr(lmm_tree$lmer))$vcov[1], digits = 2, format = "f")} (Figure~\ref{fig:LMM_tree}) and \Sexpr{formatC(data.frame(VarCorr(lmm_tree_c$lmer))$vcov[1], digits = 2, format = "f")} (Figure~\ref{fig:LMM_tree_c}, indicating that the more parsimonious the tree structure, the more variance will be captured by the random effects.

In the next sections, we assess performance of the original algorithm and proposed adaptations through more extensive empirical evaluations.




% $\textrm{Var} \hat{\beta}$ = (X^\top X)^{-1} X^\top \Omega X (X^\top X)^{-1}$

% The $X^\top \Omega X$ is the meat, and the $(X^\top X)^{-1}$ is the bread. For computing robust standard errors, we compute a new kind of meat. $\Omega = \sigma^2 I_n$, a matrix with $\sigma^2$ on the diagonal and zeros elsewhere. $n$ can be the number of observations, or the number of clusters.









\FloatBarrier
\section{Study~I: Assessment of Subgroup Recovery}

\subsection{Method}

\subsubsection{Data generation}

<<echo=FALSE>>=
library("partykit")
set.seed(42)
u1 <- sample(0:1, size = 1250, replace = TRUE) 
u2 <- round(rnorm(1250))
u3 <- round(rnorm(1250))
time <- rep(0:4, each = 250)
## left-most node: x1 == 0 & x2 <= 0
## intercept = -1, slope = -1
node3 <- as.numeric(u1 == 0 & u2 <= 0)
## left-middle node: x1 == 0 & x2 > 0
## intercept = -1, slope = 0
node4 <- as.numeric(u1 == 0 & u2 > 0)
## right-middle node: x1 == 1 & x3 =< 0
## intercept = 1, slope = 0
node6 <- as.numeric(u1 == 1 & u3 <= 0)
## right-most node: x1 == 1 & x3 > 0
## intercept = 1, slope = 1
node7 <- as.numeric(u1 == 1 & u3 > 0)
y <- -(node3 + node3*time) - node4 + node6 + (node7 + node7*time) 
data <- data.frame(u1 = as.factor(u1), u2, u3, time, y)   
tree <- lmtree(y ~ time | u1 + u2 + u3, data = data, maxdepth = 3)

fig <- party(
  partynode(1L,
            split = partysplit(1L, breaks = 1),
            kids = list(
              partynode(2L,
                        split = partysplit(2L, breaks = 0),
                        kids = list(partynode(3L, info = c(
                            expression(beta[j0] == '-1.0'),
                            expression(''),
                            expression(beta[j1] == '-1.0'))),
                          partynode(4L, info = c(
                            expression(beta[j0] == '-1.0'),
                            expression(''),
                            expression(beta[j1] == '0.0'))))),
              partynode(5L,
                        split = partysplit(3L, breaks = 0),
                        kids = list(
                          partynode(6L, info = c(
                            expression(beta[j0] == '1.0'),
                            expression(''),
                            expression(beta[j1] == '0.0'))),
                          partynode(7L, info = c(
                            expression(beta[j0] == '1.0'),
                            expression(''),
                            expression(beta[j1] == '1.0'))))))),
  data.frame(data)
)

## Panel-combining function:
combine_panels <- function(panel1, panel2, party2 = NULL) {
  function(node) {
    nid <- id_node(node)
    pushViewport(viewport(
      layout = grid.layout(nrow = 2, ncol = 1, heights = c(1, 1)),
      name = paste("node_mob_mypanel", nid, sep = "")))
    grid.rect(gp = gpar(fill = "white", col = 0))
    pushViewport(viewport(layout.pos.col = 1, layout.pos.row = 1))
    panel1(node)
    popViewport()
    pushViewport(viewport(layout.pos.col = 1, layout.pos.row = 2))
    node2 <- if(is.null(party2)) node else node_party(party2[nid])
    panel2(node2)
    popViewport(2)
  }
}
@


\begin{figure}[tb]
\caption{Design of subgroups and fixed effects.}
<<fig=TRUE, height=5.5, width=5.5, echo=FALSE>>=
plot(tree, tnex = 3.5, terminal_panel = combine_panels(
    panel1 = glmertree:::node_growthplot(tree, xaxis.at = c(0, 4), xscal = c(0, 4), ylines = 2),
    panel2 = node_terminal(fig, FUN = identity, height = 4, width = 11, id = FALSE, fill = "white",
                           align = "right", just = "top", top = 0.85, gp = gpar(lty = 0, fill = "white")),
    party2 = fig), 
  gp = gpar(cex = .7), ip_args = list(pval = FALSE))
@
\label{fig:design_tree}
\end{figure}

We simulated data according to the subgroup structure depicted in Figure~\ref{fig:design_tree}. Every dataset comprised four non-overlapping subgroups, corresponding to the terminal nodes of the tree in Figure~\ref{fig:design_tree}. The subgroups are defined by the three true partitioning variables: $u_1$, $u_2$ and $u_3$. All partitioning variables were generated from a standard normal distribution with $\mu = 0$ and $\sigma^2 = 25$. To allow for assessing possible selection bias toward partitioning variables with a larger number of possible cutpoints, variable $u_1$ was transformed to a binary factor, with values below the mean set to 0 and values above the mean set to 1. The response was computed as:
%
$$y_{i} =  X_{i} \beta_{j} + Z_{i} b_i + \epsilon_{i},$$
%
where $\beta_j$ corresponds to the fixed effects in terminal node $j$ of which subject $i$ is part. $\beta_j$ values are reported below the terminal node panels in Figure~\ref{fig:design_tree}. The fixed- and random-effects design matrices $X_i$ and $Z_i$ are identical, each comprising two columns: a vector of 1s for the intercept, and a vector of timepoints. The same set of timepoints was generated for all subjects: ${0, 1, 2, 3, 4}$\footnote{Although GLMM trees are not restricted to have the same set of timepoints for each subject, this design allowed for comparison with SEM trees in Study~II.}. Values of $b_i$ (random intercepts and slopes) were generated from a multivariate normal distribution with mean zero and a $2 \times 2$ diagonal covariance matrix $\Sigma$, the diagonal entries determined by the level of the data-generating design described below. Values of $\epsilon_{i}$ were independently generated from a normal distribution with $\mu = 0$ and $\sigma^2 = 5$.   

We varied the following five data-generating characteristics:
%
\begin{itemize}
\setlength\itemsep{0em}
\setlength{\itemindent}{0.2in}
\item Number of subjects: small ($N = 100$) or large ($N = 250$).
\item Variance of the random intercept: small ($\sigma_{b_0}^2 = 1$) or large ($\sigma_{b_0}^2 = 4$).
\item Variance of the random slope: small ($\sigma_{b_1}^2 = 0.1$) or large ($\sigma_{b_1}^2 = 0.4$).
\item Number of noise variables: small ($p = 5$) or large ($p = 25$).
\item Intercorrelation between partitioning variables: absent ($\rho = 0$) or present ($\rho = 0.3$).
\end{itemize}
%
A full factorial design was employed, yielding $2^5 = 32$ cells of the design; 100 repetitions were performed per cell. All data generation and analysis was performed in R \citep[version 4.1.2; ][]{R22}. 



\subsubsection{Model fitting}

We applied ten different fitting approaches to every generated dataset. Each variation combines one of three \emph{random-effects specifications} (none vs. intercepts vs. intercepts+slopes) with one of two \emph{random-effect initializations} (if any; all zero vs.\ full sample estimates) and one of two \emph{covariance specifications in the parameter instability tests} (classic vs.\ clustered):
%
\begin{itemize}
  \item None: $\hat{\sigma}_{b_0} = \hat{\sigma}_{b_1} = 0$. Random effects are not estimated and their variances thus fixed to 0, yielding linear model (LM) trees with fixed effects only and the following variations of covariance specifications:
  \begin{itemize}
      \item Default: Classic observation-level covariances.
      \item Alternative: Clustered covariances. 
  \end{itemize}
  
  \item Intercepts: $\hat{\sigma}_{b_0} > 0; \hat{\sigma}_{b_1} = 0$. This yields linear mixed-effects model (LMM) trees in which only the variance of the random intercept was freely estimated and the variance of the random slope was fixed to 0. The four variations considered are the following:
  \begin{itemize}
    \item Default: Classic observation-level covariances in the parameter stability tests and random-effect initialization with all zeros (original step 0.). 
    \item Alternative: Clustered covariances in the parameter stability tests.
    \item Alternative: Random-effect initialization with the full-sample estimates (alternative step 0$'$.). 
    \item Alternative: Clustered covariances and random-effect initialization with the full-sample estimates.
  \end{itemize}
  
  \item Intercepts and slopes: $\hat{\sigma}_{b_0} > 0; \hat{\sigma}_{b_1} > 0$. This yields LMM trees in which the variance of both the random intercept and slope were freely estimated. The four variations considered are the same as for the intercept-only LMM trees.
\end{itemize}
%
To fit LM trees, we used package \textbf{partykit} \citep[version 1.2-15; ][]{HothyZeil15}. To fit LMMs, we used package \textbf{lme4} \cite[version 1.1-29; ][]{BateyMach15}. To fit LMM trees we used package \textbf{glmertree} \citep[version 0.2-0; ][]{FokkySmit18}. To compute clustered covariances, we used package \textbf{sandwich} \citep[version 3.0-1; ][]{ZeilyKoll20}. We employed the outer product of gradients method to compute covariances, thus employing only the meat of the sandwich estimator for the covariances.


\subsubsection{Evaluation of performance}

We evaluated tree accuracy by counting the number of splits in each tree, and computing the standard (SD) and mean absolute deviation (MAD) from the true tree size (3 splits). Trees with $> 3$ splits are indicative of Type-I error, while trees with $< 3$ splits are indicative of Type-II errors (i.e., power too low to detect the true partitioning variables). We also assessed whether the variable selected for the first split in every tree was the true first splitting variable. 

\FloatBarrier
\subsection{Results}


<<echo=FALSE>>=
###########################
##
## Load tree size data
##

load("tree_sizes.Rda")
tree_size <- data.frame(sizes, true = 7)
N <- c(100, 250)
sigma_int <- c(1, 2)
sigma_slope <- c(sqrt(.1), sqrt(.4))
p_noise <- c(5, 25)
rho <- c(0, .3)
design_matrix <- expand.grid(N = N, sigma_int = sigma_int,
                             sigma_slope = sigma_slope,
                             p_noise = p_noise, rho = rho)
design_matrix[] <- lapply(design_matrix, factor)
tree_sizes_long <- data.frame(stack(tree_size),
                              dataset_id = factor(rep(1:nrow(tree_size), 
                                                      times = ncol(tree_size))))
names(tree_sizes_long)[1:2] <- c("tree_size", "method_id") 
tree_sizes_long <- cbind(tree_sizes_long, design_matrix)
tree_sizes_long$method_id <- relevel(tree_sizes_long$method_id, ref = "true")
LMM_ids <- which(tree_sizes_long$method_id %in% c("true", "lm", "lm_c", "lmer", "lmer_c", 
                                                  "lmer_cr", "lmer_r", "lmer_s",
                                                  "lmer_s_c", "lmer_s_cr", "lmer_s_r"))
stdCoef <- function(object) {
  sdy <- sd(getME(object,"y"))
  sdx <- apply(getME(object,"X"), 2, sd)
  sc <- fixef(object)*sdx/sdy
  se.fixef <- coef(summary(object))[,"Std. Error"]
  se <- se.fixef*sdx/sdy
  return(data.frame(stdcoef=sc, stdse=se))
}
@

<<echo=FALSE, eval=FALSE>>=
library("lmerTest")
## LM(M) trees only
mod_LMM <- lmer(tree_size ~ (1 | dataset_id) + method_id*(
  N + sigma_int + sigma_slope + p_noise + rho), 
  data = tree_sizes_long[LMM_ids, ])
#summary(mod_LMM)
#fixef(mod_LMM)[order(abs(fixef(mod_LMM)))]
coefs <- stdCoef(mod_LMM)[,1, drop = FALSE]^2

## "Variance explained" by N, sigma_n0, sigma_b1, p_noise and rho:
sum(coefs[grep("sigma_int", rownames(coefs)), ])
sum(coefs[grep("N", rownames(coefs)), ])
sum(coefs[grep("p_noise", rownames(coefs)), ])
sum(coefs[grep("sigma_slope", rownames(coefs)), ])
sum(coefs[grep("rho", rownames(coefs)), ])

## Note, above is not really variance explained, but some measure 
## of influence that does not add up to 1

## Variance explained by main effects:
coefs["N250", ] 
coefs["sigma_int2", ]
coefs["p_noise", ]
coefs["sigma_slope0.632455532033676", ]
coefs["rho0.3", ]

round(coefs[order(coefs$stdcoef, decreasing = TRUE), , drop = FALSE], digits = 3)

## Variance explained by random effects:
varco <- as.data.frame(VarCorr(mod_LMM))$vcov /
  var(tree_sizes_long[LMM_ids, "tree_size"])
varco
@

<<echo=FALSE>>=
library("lmerTest")
## Compare means between settings
tmp <-  tree_sizes_long[LMM_ids, ]
tmp$tree_size <- (tmp$tree_size-1)/2
tmp$default_ids <- tmp$method_id %in% c("lm", "lmer", "lmer_s")
tmp$cluster_ids <- tmp$method_id %in% c("lm_c", "lmer_c", "lmer_s_c")
tmp$ranefstart_ids <- tmp$method_id %in% c("lmer_r", "lmer_s_r")
tmp$both_ids <- tmp$method_id %in% c("lmer_cr", "lmer_s_cr")

M_default <- mean(tmp$tree_size[tmp$default_ids])
M_cluster <- mean(tmp$tree_size[tmp$cluster_ids])
M_ranefstart <- mean(tmp$tree_size[tmp$ranefstart_ids])
M_both <- mean(tmp$tree_size[tmp$both_ids])

means <- tapply(tmp$tree_size, tmp$method_id, mean)
sds <- tapply(tmp$tree_size, tmp$method_id, sd)
mads <- tapply(tmp$tree_size, tmp$method_id, function(x) mean(abs(x-3)))
props_more_splits <- tapply(tmp$tree_size, tmp$method_id, function(x) mean(x > 3L))
props_less_splits <- tapply(tmp$tree_size, tmp$method_id, function(x) mean(x < 3L))
props_corr_splits <- tapply(tmp$tree_size, tmp$method_id, function(x) mean(x == 3L))
@

<<eval=FALSE, echo=FALSE>>=
## t-tests for comparing no of splits with true number of splits
round(apply((sizes-1)/2, 2, function(x) t.test(x, mu = 3)$p.value), digits = 3)

## pairwise difference tests
pairwise.t.test(tmp$tree_size, tmp$method_id, 
               alternative = "two.sided", p.adjust = "bonferroni")
@




\begin{figure}[b]
\caption{Tree size distributions for LM trees (left panel) and LMM trees (middle and right panel).}
\begin{subfigure}{1.25\textwidth}
<<echo=FALSE, fig=TRUE, height=4>>=
library("dplyr")

## Prepare data
tmp <-  tree_sizes_long
tmp$tree_size <- (tmp$tree_size-1)/2 ## count splits instead of nodes
tmp <- tmp[-which(tmp$method_id == "true") , ]
tmp$method_id <- factor(tmp$method_id)

## Create indicator for algorithm and random effects specification
tmp$ranef <- NA
tmp$ranef[tmp$method_id %in% c("lm", "lm_c")] <- "{hat(sigma)[b[0]] == hat(sigma)[b[1]]} == 0"
tmp$ranef[tmp$method_id %in% c("lmer", "lmer_c", "lmer_r", "lmer_cr")] <- "list(hat(sigma)[b[0]] > 0, hat(sigma)[b[1]] == 0)"
tmp$ranef[tmp$method_id %in% c("lmer_s", "lmer_s_c", "lmer_s_r", "lmer_s_cr")] <- "list(hat(sigma)[b[0]] > 0, hat(sigma)[b[1]] > 0)"
tmp$ranef[tmp$method_id %in% c("LongCART")] <- "LongCART"
tmp$ranef[tmp$method_id %in% c("s_sem", "f_sem")] <- "SEM trees \n(no random effects)"
tmp$ranef[tmp$method_id %in% c("s_sem_i", "f_sem_i")] <- "SEM tree \n(random intercept)"
tmp$ranef[tmp$method_id %in% c("s_sem_s", "f_sem_s")] <- "SEM tree \n(random intercept + slope)"

## Create indicator for additional settings
tmp$setting <- NA
tmp$setting[tmp$method_id %in% c("lmer", "lmer_s", "lm")] <- "default"
tmp$setting[tmp$method_id %in% c("lmer_c", "lmer_s_c", "lm_c")] <- "cl.cov."
tmp$setting[tmp$method_id %in% c("lmer_r", "lmer_s_r")] <- "ran.eff."
tmp$setting[tmp$method_id %in% c("lmer_cr", "lmer_s_cr")] <- "cl.cov.+\nran.eff."
tmp$setting[tmp$method_id %in% c("LongCART")] <- "default"
tmp$setting[tmp$method_id %in% c("s_sem", "s_sem_i", "s_sem_s")] <- "score"
tmp$setting[tmp$method_id %in% c("f_sem", "f_sem_i", "f_sem_s")] <- "fair"
tmp$setting <- factor(tmp$setting, levels = c("cl.cov.+\nran.eff.", "cl.cov.", 
                                              "default", "fair",
                                              "ran.eff.", "score"))
levels(tmp$sigma_int) <- c("1", "4")
levels(tmp$sigma_slope) <- c("0.1", "0.4")
tmp$setting <- factor(tmp$setting, levels = c("default", "cl.cov.",  "ran.eff.", 
                       "cl.cov.+\nran.eff.", "fair", "score"))

## Create the plot
library("ggplot2")
library("gridExtra")
library("colorspace")
library("scales")

## Set up data.frame of summary stats
summ_stats <- data.frame(M = round(means[c("lm", "lm_c", "lmer", "lmer_c", "lmer_cr", "lmer_r", 
                   "lmer_s", "lmer_s_c", "lmer_s_cr", "lmer_s_r")], digits = 2), 
                   SD = format(sds[c("lm", "lm_c", "lmer", "lmer_c", "lmer_cr", "lmer_r", 
                   "lmer_s", "lmer_s_c", "lmer_s_cr", "lmer_s_r")], digits = 2),
                   MAD = format(mads[c("lm", "lm_c", "lmer", "lmer_c", "lmer_cr", "lmer_r", 
                   "lmer_s", "lmer_s_c", "lmer_s_cr", "lmer_s_r")], digits = 2))
summ_stats$method <- rownames(summ_stats)
summ_stats$y <- 50
## Note!!! Labels of ran.eff. and cl.cov.+ran.eff. have been switched to have text labels
## above the right boxplot / countplot. Setting it as a factor and releveling does not help. 
summ_stats$setting <- c("default", "cl.cov.", 
                        "default", "cl.cov.", "cl.cov.+\nran.eff.", "ran.eff.", 
                        "default", "cl.cov.", "cl.cov.+\nran.eff.", "ran.eff.")

summ_stats$ranef <- NA
summ_stats$ranef[summ_stats$method %in% c("lm", "lm_c")] <- "{hat(sigma)[b[0]] == hat(sigma)[b[1]]} == 0"
summ_stats$ranef[summ_stats$method %in% c("lmer", "lmer_c", "lmer_r", "lmer_cr")] <- "list(hat(sigma)[b[0]] > 0, hat(sigma)[b[1]] == 0)"
summ_stats$ranef[summ_stats$method %in% c("lmer_s", "lmer_s_c", "lmer_s_r", "lmer_s_cr")] <- "list(hat(sigma)[b[0]] > 0, hat(sigma)[b[1]] > 0)"
summ_stats$all <- with(summ_stats, paste(M, SD, MAD, sep = "\n"))

## LM(M)
LMM_ids <- which(tmp$method_id %in% c("lm", "lm_c", "lmer", "lmer_c", "lmer_cr", 
                                      "lmer_r", "lmer_s", "lmer_s_c", 
                                      "lmer_s_cr", "lmer_s_r"))
breaks <- c(0, 1, 3, 7, 20, 38, 50, 64)
labels <- c("0", "1", "3", "7", "20", "MAD", "SD", "M")
lim <- c(0, 60)

## Plot
theme_set(theme_bw())
plot0 <- ggplot(tmp[LMM_ids, ]) +
  geom_boxplot(aes(x=setting, y=tree_size), 
               position=position_dodge(1), show.legend = FALSE, 
               alpha = 0.5, width = .6, outlier.shape = NA, coef = NULL) + 
  geom_count(aes(x=setting, y=tree_size), col = "gray", alpha = .75) +
  scale_y_continuous(trans = pseudo_log_trans(sigma = 1, base = exp(1)), 
                     breaks = breaks, labels = labels, lim = lim) +
  geom_text(data = summ_stats, aes(x=setting, y=y, label = all),
            size = 3, col = "gray44") +
  labs(x = "", y = "no. of splits") + theme(legend.position = "none") +
  geom_hline(yintercept=3, col = "darkgray") + 
  facet_grid(~ranef, scales = "free", space = "free", 
             labeller = label_parsed)  +
  coord_cartesian(clip = 'off') 
plot0
@
\end{subfigure}%
\vspace{-0.5cm}%

{\footnotesize \textit{Note.} M~= mean number of splits; SD~= standard deviation of the number of splits; MAD~= mean absolute deviation from true tree size. Gray circles represent counts, dark gray horizontal lines represent true number of splits~3. Panel labels indicate whether variances of the random intercept and slope were fixed to 0 or freely estimated. Distances on $y$-axis are on the log scale. cl.cov.~= cluster-level covariances employed in parameter stability tests; ran.eff.~= estimation initialized with the random effects on the full sample. }
\label{fig:LMM_sizes}
\end{figure}



Figure~\ref{fig:LMM_sizes} depicts the number of splits implemented by each partitioning approach. The default fitting approach tends to overfit, irrespective of the random-effects specification, implementing $> 3$ splits in \Sexpr{format(100*mean(c(props_more_splits["lm"], props_more_splits["lmer"], props_more_splits["lmer_s"])), digits = 2L)}\% datasets, on average. The use of cluster-level covariances in the parameter stability tests successfully mitigated overfitting: LMM trees with clustered covariances with (middle panel) or without (right panel) random slopes showed an average number of splits closest to the true tree size. 

In terms of MAD, however, LMM trees initializing estimation with the random effects performed best, but only if the random slope was not estimated. Initializing estimation with the random effects may only be useful if the random-effects specification is kept relatively simple. With more complex random-effects specifications, initializing estimation seems to result in too few groups being detected; subgroup differences are then more likely to be captured by the random effects than by the tree structure. The second-lowest MAD was observed for LM trees with clustered covariances (left panel). Combined use of random-effects initialization and cluster-level covariances was not very effective, irrespective of the random-effects specification.

Distributions of the number of splits, separated according to the levels of the data-generating design are presented in Figure~\ref{fig:LMM_sizes_interact} and discussed in Appendix~\ref{sec:AppendixA}. The results show a pattern very similar to Figure~\ref{fig:LMM_sizes}; no substantial interactions between data-generating and model-specification parameters were observed. Main effects of the data-generating parameters were as expected: The strongest effects were for $\sigma_{b_1}$ and $N$, with higher values resulting in a higher number of splits. The number of noise variables and $\sigma^2_{b_1}$ had smaller effects, while the correlation between partitioning variables hardly affected the number of implemented splits.

Table~\ref{tab:first_splits} shows the variables selected for the first split, and indicates high accuracy for recovery of the first split: Only very rarely is $u_1$ not selected for the first split.




<<echo=FALSE, results=tex, message=FALSE, warning=FALSE>>=
library("kableExtra")
load(file = "splitvars.Rda")
splitvar <- apply(splits, 2, unlist)
rownames(splitvar) <- NULL

## LongCART registers best splitting variable if no split was implemented, overwrite
#table(splitvar[tree_size$Lcrt == 1, "Lcrt"])
# x1 x14 x16  x2 x28  x5  x9 
#607   1   1   1   2   1   1 
#
## Note: variable selection bias may not be a problem; power seems to be a problem
Lcrt_splitvars <- splitvar[, "Lcrt"] ## save for later use
splitvar[tree_size$Lcrt == 1, "Lcrt"] <- NA

## Get splitting proportions
func <- function(x) table(x, useNA = "ifany") / sum(table(x, useNA = "ifany"))
probs <- apply(splitvar, 2, func)
probs <- sapply(probs, function(x) {
  names(x)[is.na(names(x))] <- "No split"
  return(x)
})

## Have column for u1, u2, u3, u4-up, no split
splits <- matrix(sapply(probs, function(x) x["x1"]), ncol = 1,
                 dimnames = list(names(probs), "x1"))
splits[is.na(splits)] <- 0
for (u_set in list("x2","x3", paste0("x", 4:30), "No split")) {
  splits <- cbind(splits, sapply(probs, function(x) sum(x[names(x) %in% u_set])))
}
splits <- data.frame(splits[c(9:10, 1:8, 11:17), ]) ## reorder rows so LM tree is 1st
rows <- cbind(c("LM tree", " ", 
                "LMM tree", " ", " ", " ", " ", " ", " ", " ", 
                "SEM tree", " ", " ", " ", " ", " ", 
                "LongCART"),
                c("$\\hat{\\sigma}_{b_0} = \\hat{\\sigma}_{b_1} = 0$",
                  "$\\hat{\\sigma}_{b_0} = \\hat{\\sigma}_{b_1} = 0$",
                  "$\\hat{\\sigma}_{b_0} > 0$, $\\hat{\\sigma}_{b_1} = 0$",
                  "$\\hat{\\sigma}_{b_0} > 0$, $\\hat{\\sigma}_{b_1} = 0$",
                  "$\\hat{\\sigma}_{b_0} > 0$, $\\hat{\\sigma}_{b_1} = 0$",
                  "$\\hat{\\sigma}_{b_0} > 0$, $\\hat{\\sigma}_{b_1} = 0$",
                  "$\\hat{\\sigma}_{b_0} > 0$, $\\hat{\\sigma}_{b_1} > 0$",
                  "$\\hat{\\sigma}_{b_0} > 0$, $\\hat{\\sigma}_{b_1} > 0$",
                  "$\\hat{\\sigma}_{b_0} > 0$, $\\hat{\\sigma}_{b_1} > 0$",
                  "$\\hat{\\sigma}_{b_0} > 0$, $\\hat{\\sigma}_{b_1} > 0$",
                  "$\\hat{\\sigma}_{b_0} = \\hat{\\sigma}_{b_1} = 0$",
                  "$\\hat{\\sigma}_{b_0} > 0$, $\\hat{\\sigma}_{b_1} = 0$",
                  "$\\hat{\\sigma}_{b_0} > 0$, $\\hat{\\sigma}_{b_1} > 0$",
                  "$\\hat{\\sigma}_{b_0} = \\hat{\\sigma}_{b_1} = 0$",
                  "$\\hat{\\sigma}_{b_0} > 0$, $\\hat{\\sigma}_{b_1} = 0$",
                  "$\\hat{\\sigma}_{b_0} > 0$, $\\hat{\\sigma}_{b_1} > 0$",
                  "$\\hat{\\sigma}_{b_0} > 0$, $\\hat{\\sigma}_{b_1} = 0$"),
             c("default", "cl.cov.", "default", "cl.cov.", 
               "ran.eff.", "cl.cov. + ran.eff.", "default", "cl.cov.", 
               "ran.eff.", "cl.cov. + ran.eff.",
               "LRT", "LRT", "LRT", "score-based", "score-based", "score-based", ""))
splits <- cbind(rows, splits)
colnames(splits) <- c("Algorithm", "Random effects", "Fitting approach", "$u_1$", 
                      "$u_2$", "$u_3$", "$u_4$--$u_{25}$", "No split")

## Print the table
kable_styling(add_footnote(row_spec(kable(splits[1:10, -1], format = "latex", booktabs = TRUE, 
                                 label = "first_splits", align = "llccccc", 
        escape=FALSE, linesep = "", # linesep command suppressess addlinesep every 5 rows
        caption = "Variables selected for the first split by each LM tree (top two rows) and LMM tree (bottom eight rows) estimation approach.", row.names = FALSE, digits = 3L), row = c(2, 6), hline_after =  TRUE),
  "\\footnotesize \\\\ \\textit{Note.} $u_1$ is the true first splitting variable and is binary; all other partitioning variables are continuous, with $u_2$ and $u_3$ being true splitting variables (nodes 2 and 3). $\\hat{\\sigma}_{b_0}$ and $\\hat{\\sigma}_{b_1}$ are the estimated standard deviations of the random intercept and slope, respectively.", 
  notation="none", threeparttable = TRUE, escape = FALSE),
  font_size = 11, full_width=FALSE)
@

\FloatBarrier



\section{Study~II: Comparison with Other Partitioning Methods}

Next, we compared the performance of LM(M) trees with that of SEM trees and LongCART. This allowed for evaluating the possible (dis)advantages of global versus local estimation of random-effects parameters, as well as the performance of the different splitting criteria employed by each method. The same data-generating design as in Simulation Study~I was employed. To reduce the number of comparisons, we only include performance of LM(M) trees fitted using clustered covariances, because they showed good performance in Simulation Study~I.

\subsection{Method}

We fitted a total of six SEM trees to every dataset. We used two different splitting criteria: 

\begin{itemize} 

\item The default ``naive'' splitting approach which employs likelihood ratio tests (LRTs) as the splitting criterion \citep{BranyOert13}. That is, for each candidate split, the log-likelihood of the SEM fitted to the observations in the current node is compared against the sum of the log-likelihoods of a two-group SEM, in which the two groups are defined by the candidate split. An LRT can thus be computed for each candidate split, which quantifies the improvement in fit that would result from implementing this split. In each step, the candidate split  yielding the highest LRT is selected for splitting, and splitting is continued as long as a candidate split yields a $p$-value of the LRT above a pre-specified $\alpha$ level (0.05, by default). 

\item The score-based splitting approach of \cite{ArnoyVoel21}. This approach uses the MOB algorithm described in the Introduction, where the parametric model fitted in step (a) is a SEM. While for GLMM trees, parameter stability tests are computed for the fixed-effects parameters only, score-based SEM trees compute parameter stability tests based on both fixed- and random-effects parameters. 

\end{itemize}

For each splitting criterion, three different random-effects specifications were employed:
%
\begin{itemize}
  \item None: $\hat{\sigma}_{b_0} = \hat{\sigma}_{b_1} = 0$. Random effects were not estimated and their variances were thus fixed to 0.
  
  \item Intercepts: $\hat{\sigma}_{b_0} > 0; \hat{\sigma}_{b_1} = 0$. In every node, the variance of the random intercept was freely estimated; the variance of the random slope was fixed to 0. 
  
  \item Intercepts and slopes: $\hat{\sigma}_{b_0} > 0; \hat{\sigma}_{b_1} > 0$. In every node, the variances of the random intercept and slope, as well as their correlation, were freely estimated. 
\end{itemize}
%
To specify the node-specific models for SEM trees, we employed an LGCM specification with the response at each timepoint regressed on a latent intercept and slope. Intercept loadings were fixed to 1; slope loadings were fixed to ${0, 1, 2, 3, 4}$, respectively. Errors were assumed uncorrelated between timepoints and an error variance was freely estimated for each timepoint. We used package \textbf{lavaan} \citep[version 0.6-11; ][]{Ross12} to fit the SEMs and we used package \textbf{semtree} \citep[version 0.9.17; ][]{BranyOert13} to fit the SEM trees.    

We fitted a single LongCART tree to each dataset. The LongCART function estimates node-specific models comprising a random intercept term; this default cannot be changed. A fixed-effects model was specified with the response regressed on time and a subject-specific random intercept. We used package \textbf{LongCART} \citep[version 3.1][]{Kund21} to fit LongCART trees.    


\subsection{Results}


\subsubsection{Tree size}

<<echo=FALSE>>=
tmp2 <- tmp
LMM_c_ids <- which(tmp2$method_id %in% c("lm_c", "lmer_c", "lmer_s_c"))
SEM_ids <- which(tmp2$method_id %in% c("f_sem", "f_sem_i", "f_sem_s",
                                      "s_sem", "s_sem_i", "s_sem_s"))
Lcrt_ids <- which(tmp2$method_id %in% "Lcrt")
tmp2 <-  tmp2[c(LMM_c_ids, SEM_ids, Lcrt_ids), ]
tmp2$method_id <- factor(tmp2$method_id)
@


<<echo=FALSE, eval=FALSE>>=
## SEM trees
SEM_ids <- which(tmp2$method_id %in% c("f_sem", "f_sem_i", "f_sem_s",
                                      "s_sem", "s_sem_i", "s_sem_s"))
mod_SEM <- lmer(tree_size ~ (1 | dataset_id) + method_id*(
  N + sigma_int + sigma_slope + p_noise + rho), 
  data = tmp2[SEM_ids, ])
summary(mod_SEM)
coefs <- stdCoef(mod_SEM)[,1, drop = FALSE]^2
r2_N <- sum(coefs[grep("N", rownames(coefs)), ])
r2_si <- sum(coefs[grep("sigma_int", rownames(coefs)), ])
r2_ss <- sum(coefs[grep("sigma_slope", rownames(coefs)), ])
r2_p <- sum(coefs[grep("p_noise", rownames(coefs)), ])
r2_rho <- sum(coefs[grep("rho", rownames(coefs)), ])


## LongCART
Lcrt_ids <- which(tmp2$method_id %in% "Lcrt")
mod <- lm(tree_size ~ N + sigma_int + sigma_slope + p_noise + rho, 
  data = tmp2[Lcrt_ids, ])
summary(mod)
coefs <- stdCoef(mod_LMM)[,1, drop = FALSE]^2
r2_N <- sum(coefs[grep("N", rownames(coefs)), ])
r2_si <- sum(coefs[grep("sigma_int", rownames(coefs)), ])
r2_ss <- sum(coefs[grep("sigma_slope", rownames(coefs)), ])
r2_p <- sum(coefs[grep("p_noise", rownames(coefs)), ])
r2_rho <- sum(coefs[grep("rho", rownames(coefs)), ])
@



<<eval=FALSE, echo=FALSE>>=
## LM(M) trees only
mod_all <- lmer(tree_size ~ (1 | dataset_id) + method_id*(
  N + sigma_int + sigma_slope + p_noise + rho), data = tmp2)
#summary(mod_LMM)
#fixef(mod_LMM)[order(abs(fixef(mod_LMM)))]
coefs <- stdCoef(mod_all)[,1, drop = FALSE]^2

## "Variance explained" by N, sigma_n0, sigma_b1, p_noise and rho:
sum(coefs[grep("N", rownames(coefs)), ])
sum(coefs[grep("sigma_int", rownames(coefs)), ])
sum(coefs[grep("p_noise", rownames(coefs)), ])
sum(coefs[grep("rho", rownames(coefs)), ])
sum(coefs[grep("sigma_slope", rownames(coefs)), ])

## Note, above is not really variance explained, but some measure 
## of influence that does not add up to 1

## Variance explained by main effects:
coefs["N250", ] 
coefs["sigma_int2", ]
coefs["p_noise", ]
coefs["sigma_slope0.632455532033676", ]
coefs["rho0.3", ]

round(coefs[order(coefs$stdcoef, decreasing = TRUE), , drop = FALSE], digits = 3)

## Variance explained by random effects:
varco <- as.data.frame(VarCorr(mod_all))$vcov /
  var(tmp2[, "tree_size"])
varco
@

\begin{figure}[tb]
\caption{Tree size distributions for LM(M) trees with clustered covariances, SEM trees and LongCART.}
\begin{subfigure}{1.25\textwidth}
<<echo=FALSE, fig=TRUE, height=4>>=
## Note!!! White spaces added to LM(M) and SEM tree, b/c with use of geom_text panels get ordered
## alphabetically
tmp2$method <- ifelse(
  tmp2$method_id %in% c("lm_c", "lmer_c", "lmer_s_c"), " LM(M) tree ", 
      ifelse(tmp2$method_id %in% c("f_sem", "f_sem_i", "f_sem_s"), " SEM tree (LRT-based) ", 
             ifelse(tmp2$method_id %in% c("s_sem", "s_sem_i", "s_sem_s"),
                             " SEM tree (score-based) ", "LongCART")))
tmp2$method <- factor(tmp2$method, levels = c(" LM(M) tree ", " SEM tree (LRT-based) ", 
                                              " SEM tree (score-based) ", "LongCART"))

tmp2$random <- ifelse(tmp2$method_id %in% c("lm_c", "f_sem", "s_sem"), 
                      "atop(hat(sigma)[b[0]] == 0, hat(sigma)[b[1]] == 0)", 
                      ifelse(tmp2$method_id %in% c("lmer_c", "f_sem_i", "s_sem_i", "Lcrt"),
                             "atop(hat(sigma)[b[0]] > 0, hat(sigma)[b[1]] == 0)", 
                              "atop(hat(sigma)[b[0]] > 0, hat(sigma)[b[1]] > 0)"))

## Set up data.frame of summary stats
means <- tapply(tmp2$tree_size, tmp2$method_id, mean)
sds <- tapply(tmp2$tree_size, tmp2$method_id, sd)
mads <- tapply(tmp2$tree_size, tmp2$method_id, function(x) mean(abs(x - 3)))
summ_stats <- data.frame(M = round(means[c("lm_c", "lmer_c", "lmer_s_c", 
                                            "f_sem", "f_sem_i", "f_sem_s",
                                            "s_sem", "s_sem_i", "s_sem_s", "Lcrt")], digits = 2), 
                   SD = round(sds[c("lm_c", "lmer_c", "lmer_s_c", 
                                            "f_sem", "f_sem_i", "f_sem_s",
                                            "s_sem", "s_sem_i", "s_sem_s", "Lcrt")], digits = 2),
                   MAD = sprintf(mads[c("lm_c", "lmer_c", "lmer_s_c", 
                                            "f_sem", "f_sem_i", "f_sem_s",
                                            "s_sem", "s_sem_i", "s_sem_s", "Lcrt")],  fmt = '%#.2f'))
summ_stats$method_id <- factor(rownames(summ_stats))

summ_stats$method <- ifelse(
  summ_stats$method_id %in% c("lm_c", "lmer_c", "lmer_s_c"), " LM(M) tree ", 
      ifelse(summ_stats$method_id %in% c("f_sem", "f_sem_i", "f_sem_s"), " SEM tree (LRT-based) ", 
             ifelse(summ_stats$method_id %in% c("s_sem", "s_sem_i", "s_sem_s"),
                             " SEM tree (score-based) ", "LongCART")))
tmp2$method <- factor(tmp2$method, levels = c(" LM(M) tree ", " SEM tree (LRT-based) ", 
                                              " SEM tree (score-based) ", "LongCART"))
summ_stats$y <- 25.5
summ_stats$random <- ifelse(summ_stats$method_id %in% c("lm_c", "f_sem", "s_sem"), 
                      "atop(hat(sigma)[b[0]] == 0, hat(sigma)[b[1]] == 0)", 
                      ifelse(summ_stats$method_id %in% c("lmer_c", "f_sem_i", "s_sem_i", "Lcrt"),
                             "atop(hat(sigma)[b[0]] > 0, hat(sigma)[b[1]] == 0)", 
                              "atop(hat(sigma)[b[0]] > 0, hat(sigma)[b[1]] > 0)"))
summ_stats$all <- with(summ_stats, paste(M, SD, MAD, sep = "\n"))

breaks <- c(0, 1, 3, 7, 15, 20, 25.5, 32)
labels <- c("0", "1", "3", "7", "15", "MAD", "SD", "M")
lim <- c(0, 30)
plot1 <- ggplot(tmp2) +
  geom_boxplot(aes(x=random, y=tree_size), alpha = .5, width = .6, coef = NULL) +
  geom_count(aes(x=random, y=tree_size), col = "gray", alpha = .75) +
  geom_hline(yintercept=3, col = "darkgray") +
  geom_text(data = summ_stats, aes(x=random, y=y, label = all),
            size = 3, col = "gray44") +
  scale_y_continuous(trans = pseudo_log_trans(), 
                     breaks = breaks, lim = lim, labels = labels) +
  facet_grid(~ method, scales = "free", space = "free") +
  labs(x = "", y = "no. of splits") + theme(legend.position = "none",
                       text=element_text(size=9)) +
  scale_x_discrete(labels = scales::parse_format()) +
  coord_cartesian(clip = 'off')
plot1
@
\end{subfigure}%
\vspace{-0.5cm}%
{\footnotesize \textit{Note.} M~= mean number of splits; SD~= standard deviation of the number of splits; MAD~= mean absolute deviation from true tree size. Gray circles represent counts, dark gray horizontal lines represent true number of splits~3. Distances on the $y$-axis are on the log scale; $x$-axis labels indicate whether variances of the random intercept and slope were fixed to 0 or freely estimated.}
\label{fig:tree_sizes}
\end{figure}

Figure~\ref{fig:tree_sizes} depicts tree size distributions for the different algorithms. SEM trees (middle two panels) performed well, exhibiting overfitting only when no random effects were specified. LRT-based SEM trees tended to implement more splits than score-based SEM trees. Both LRT- and score-based SEM trees yielded lower tree size with increasing complexity of the random-effect specification. With the correct random-effects specification (i.e., both random intercept and slope were estimated), LRT-based SEM trees showed average tree size very close to the true tree size, while under-specification (i.e., specification of only a random intercept) increased tree size by 0.2 splits on average. Score-based SEM trees perform best when only random intercepts are estimated, implementing too few splits when random slopes were (correctly) specified. LongCART trees seem to suffer from a lack of power, implementing only two splits on average. Overall, LM(M) trees with clustered covariances showed best performance, but are very closely followed by LRT-based SEM trees, which seem to be affected more strongly by mis-specification of the random effects. 



<<eval=FALSE, echo=FALSE>>=
pairwise.t.test(tmp2$tree_size, tmp2$method_id, 
               alternative = "two.sided", p.adjust = "bonferroni")
# 	Pairwise comparisons using t tests with pooled SD 
# 
# data:  tmp2$tree_size and tmp2$method_id 
# 
#          lmer_c  lmer_s_c lm_c    f_sem   f_sem_i f_sem_s s_sem   s_sem_i s_sem_s
# lmer_s_c 1.00000 -        -       -       -       -       -       -       -      
# lm_c     < 2e-16 < 2e-16  -       -       -       -       -       -       -      
# f_sem    < 2e-16 < 2e-16  < 2e-16 -       -       -       -       -       -      
# f_sem_i  0.00074 2.5e-08  < 2e-16 < 2e-16 -       -       -       -       -      
# f_sem_s  1.00000 1.00000  < 2e-16 < 2e-16 9.6e-08 -       -       -       -      
# s_sem    < 2e-16 < 2e-16  < 2e-16 < 2e-16 < 2e-16 < 2e-16 -       -       -      
# s_sem_i  1.00000 1.00000  < 2e-16 < 2e-16 8.2e-09 1.00000 < 2e-16 -       -      
# s_sem_s  < 2e-16 < 2e-16  < 2e-16 < 2e-16 < 2e-16 < 2e-16 < 2e-16 < 2e-16 -      
# Lcrt     < 2e-16 < 2e-16  < 2e-16 < 2e-16 < 2e-16 < 2e-16 < 2e-16 < 2e-16 0.03511

## t-tests for comparing no. of splits with true number of splits
t_tests <- round(apply((sizes-1)/2, 2, function(x) t.test(x, mu = 3)$p.value), digits = 3)
t_tests
@


Distributions of the number of splits, separated according to the levels of the data-generating parameters are depicted and discussed in Appendix~\ref{sec:AppendixA} and Figure~\ref{fig:tree_sizes_interact}. They are omitted here, as they show a pattern very similar to Figure~\ref{fig:tree_sizes}. Of the four data-generating parameters, $N$ and $\sigma_{b_0}^2$ showed the strongest effects, with higher values resulting in a higher number of splits, as expected. The number of splits implemented by SEM trees was most strongly affected by the data-generating parameters when the random effects were mis-specified (i.e., random intercepts and/or slope fixed to 0).



<<echo=FALSE, eval=FALSE>>=
tapply(tmp2$tree_size, list(tmp2$method_id, tmp2$N), mean)
tapply(tmp2$tree_size, list(tmp2$method_id, tmp2$sigma_int), mean)
tapply(tmp2$tree_size, list(tmp2$method_id, tmp2$p_noise), mean)
tapply(tmp2$tree_size, list(tmp2$method_id, tmp2$rho), mean)
tapply(tmp2$tree_size, list(tmp2$method_id, tmp2$sigma_slope), mean)
@







\FloatBarrier


\subsubsection{Split selection}


<<echo=FALSE, results=tex, message=FALSE, warning=FALSE>>=
library("kableExtra")
splits$Algorithm[2] <- "LM(M) tree"
splits$Algorithm[4] <- "  (cl.cov.)"
splits$Algorithm[8] <- " "
splits$Algorithm[12] <- "  (LRT-based)"
splits$Algorithm[14] <- "SEM tree"
splits$Algorithm[15] <- "  (score-based)"

## Print table
kable_styling(add_footnote(
  kable(splits[c(2, 4, 8, 11:17), -3], format = "latex", booktabs = TRUE, 
        label = "first_splits2", align = "llccccc", row.names = FALSE, digits = 3,
        escape=FALSE, linesep = "", # linesep command suppressess addlinesep every 5 rows
        caption = "Variables selected for the first splits by each of the partitioning approaches."),
  "\\footnotesize \\\\ \\textit{Note.} $u_1$ is the true first splitting variable and is a binary factor; all other partitioning variables are continuous, with $u_2$ and $u_3$ being true splitting variables (nodes 2 and 3). The first column indicates wheter the random intercept and/or slope were estimated or not.", 
  notation="none", threeparttable = TRUE, escape = FALSE),
  font_size = 11, full_width=FALSE)
@


Table~\ref{tab:first_splits2} presents variable selection frequencies for the first split in the fitted trees. For SEM trees, the LRT criterion yields almost perfect accuracy for the first split. The score-based criterion for SEM trees provides near-perfect accuracy when random effects were correctly specified. When random effects were mis-specified, score-based SEM trees selected the wrong variable for the first split in about \Sexpr{round(100*(1 - mean(splits$`$u_1$`[grep("s_sem", rownames(splits))])))}\% of datasets. Closer inspection of stability tests for individual models and parameters suggested that the the score-based tests for SEMs are more sensitive to instability in the fixed slope than in the fixed intercept, explaining why $u_2$ or $u_3$ were often selected for the first split.

LongCART trees exhibit low accuracy for recovering the first split, selecting the wrong variable in all datasets where at least one split was implemented. LongCART showed a strong tendency to select $u_2$ or $u_3$ for the first split. Closer inspection of the fitted LongCART trees revealed that in \Sexpr{round(100*prop.table(table(Lcrt_splitvars[tree_size$Lcrt == 1L] == "x1"))["TRUE"])}\% percent of datasets in which no splits were implemented, $u_1$ was the strongest splitting candidate, but the parameter stability tests did not reach significance. This suggests the tests proposed by \cite{KundyHare19} are less sensitive to instability of the intercept (compared to instability of the slope), or less sensitive to instability with respect to categorical covariates (compared to instability with respect to continuous covariates). 


<<echo=FALSE, eval=FALSE>>=
## Check: How come that score-based SEM trees do not pick up x1 for splitting?
## Which dataset yields the worst SEM tree(s)? Lots of nodes, wrong first splitting variable?
which.max(tree_size$f_sem)
which.max(tree_size$s_sem) ## seems worst

splitvar[which.max(tree_size$s_sem),]
which(!splitvar[ , "s_sem"] %in% c("x1", "x2", "x3"))
## Size of score-based semtrees where it picks up noise for 1st split
which(!splitvar[ , "s_sem"] %in% c("x1", "x2", "x3"))
tree_size$s_sem[which(!splitvar[ , "s_sem"] %in% c("x1", "x2", "x3"))]
## Size of score-based semtrees where it picks up x2 or x3 for 1st split
which(splitvar[ , "s_sem"] %in% c("x2", "x3"))
tree_size$s_sem[which(splitvar[ , "s_sem"] %in% c("x2", "x3"))]

## Could it be that tests w.r.t. residuals are overpowered with misspecification?

## It seems noise variables are mostly picked up when sample size is larger:
table(which(!splitvar[ , "s_sem"] %in% c("x1", "x2", "x3")) %% 2 == 0)
table(which(splitvar[ , "s_sem"] %in% c("x2", "x3")) %% 2 == 0)
head(tmp$N)

for (i in which(splitvar[ , "s_sem"] %in% c("x2", "x3"))[1:7]) {
  print(table(which(splitvar[ , "s_sem"] %in% c("x2", "x3")) %% 32 == i))
}
## best chances at 1st and 4th value, that's rows 18 and 26 of design matrix
tmp[18,]
tmp[26,]


## 1) Fit lavaan model
## 2) Extract parameter stability tests using strucchange
load("datasets1")
p <- if (ncol(datasets[[18]]) > 12L) 28L else 8L

data <- reshape(data = datasets[[18]], v.names = "y",
                timevar = "time", idvar = "cluster_id",
                direction = "wide")

# data <- reshape(data = datasets[[26]], v.names = "y",
#                 timevar = "time", idvar = "cluster_id",
#                 direction = "wide")


mod <- '
        i =~ 1*y.0 + 1*y.1 + 1*y.2 + 1*y.3 + 1*y.4
        s =~ 1*y.1 + 2*y.2 + 3*y.3 + 4*y.4
        s ~~ 0*s
        i ~~ 0*i
        i ~~ 0*s
      '
fit <- growth(mod, data = data, do.fit = FALSE)
  
mod.i <- '
        i =~ 1*y.0 + 1*y.1 + 1*y.2 + 1*y.3 + 1*y.4
        s =~ 1*y.1 + 2*y.2 + 3*y.3 + 4*y.4
        s ~~ 0*s
        i ~~ 0*s
      '
fit.i <- growth(mod.i, data = data, do.fit = FALSE)
  
mod.s <- '
        i =~ 1*y.0 + 1*y.1 + 1*y.2 + 1*y.3 + 1*y.4
        s =~ 1*y.1 + 2*y.2 + 3*y.3 + 4*y.4
      '
fit.s <- growth(mod.s, data = data, do.fit = FALSE)

st <- semtree(fit, predictors = paste0("x", 1L:p), data = data,
              control = semtree.control(method = "score", 
                                        bonferroni = TRUE))
st_i <- semtree(fit.i, predictors = paste0("x", 1L:p), data = data,
              control = semtree.control(method = "score", 
                                        bonferroni = TRUE))
st_s <- semtree(fit.s, predictors = paste0("x", 1L:p), data = data,
              control = semtree.control(method = "score", 
                                        bonferroni = TRUE))

for (parm in 1:7) {
for (i in c("x2")){#}, "x2", "x3", "x4", "x5")) {
  efp <- gefp(st$model, ## lavaan model in first node
              fit = NULL, 
              scores = function(fm) estfun(fm), 
              vcov = function(fm, order.by = order.by, data = data) fm@vcov$vcov, 
              data = data, 
              order.by = data[, i],
              sandwich=FALSE,
              parm = parm)
  if (inherits(data[ , i], "factor")) {
    LMuo <- catL2BB(efp)
    plot(efp, functional = LMuo, 
         xlab = paste(i, "(categorical partitioning variable)"))
    print(sctest(efp, functional = LMuo))    
  } else if (inherits(data[ , i], c("integer", "numeric"))) { 
    maxLM <- supLM(0.1)
    plot(efp, xlab = paste(i, "(numeric partitioning variable)"),
         functional = maxLM)
    print(sctest(efp, functional = maxLM))
  }
}
}
## Model without intercept and slope variances: Plots suggest all variables have very high test stats.
## But x2 and x3 win by far, much larger test stats. x1 also has significant test stat, but:
## Interestingly, the test with respect to the mean slope is much higher for 
##   the test w.r.t. x1, compared to that of the intercept

## Model with intercept but no slope variance: Again, all variables have very high stats but x2 and x3 win by far.
## Test w.r.t. x1 and intercept mean is now much smaller compared to test w.r.t. slope mean.

## Model with intercept and slope variance: Again, all variables have very high stats but x2 and x3 win by far.
## Test w.r.t. x1 and intercept mean is now much smaller compared to test w.r.t. slope mean.

## It a appears that the parameter test w.r.t. slope mean is much more sensitive than w.r.t. intercept mean
## You can see this when you fill in: st, order.by = data[ , "x2"], parm = 7 versus parm = 6
## Error variance tests behave well. But test w.r.t. slope mean is much higher than w.r.t. intercept mean.
## Then again, this should be the case for x2, but NOT for x1.
@



\subsubsection{Computation time}



\begin{figure}[!ht]%
\caption{Computation time distributions for the different partitioning methods.}
\begin{subfigure}{.8\textwidth}%
<<fig=TRUE, echo=FALSE, width=4, height=3.25>>=
load("comp_times.Rda")
colnames(times) <- c(rep("LMM tree", times = 8),
                     rep(" LM tree", times = 2), 
                     rep("SEM tree\n(LRT)", times = 3),
                     rep("SEM tree\n(score)", times = 3),
                     "LongCART")
times <- reshape2::melt(times)
mean_times <- tapply(times$value, times$Var2, mean)
summ_stats <- data.frame(M = sprintf(mean_times, fmt = '%#.2f'),
                         Var2 = levels(times$Var2),
                         y = 100000)

ggplot(times, aes(Var2, value)) + 
  geom_boxplot(, alpha = .5, fill = "gray", width=.5) +
  labs(x = "", y = "Computation time (in seconds)") +
  geom_text(data = summ_stats, aes(x=Var2, y=y, label = M),
            size = 3, col = "gray44") +
  theme(text = element_text(size = 10)) +
  scale_y_continuous(trans = "log", limits = c(0.01, 100000), 
                     breaks = c(.01, .1, 1, 10, 100, 1000, 10000, 100000),
                     labels = c("1e-02", "1e-01", "1e+00", "1e+01", "1e+02", "1e+03", "1e+04", "M"))
@
\end{subfigure}%
\\ {\footnotesize \textit{Note.} $y$-axis is on the log scale; M = mean computation time in seconds. }
\label{fig:comp_times}
\end{figure}%


Figure~\ref{fig:comp_times} presents computation time distributions for the partitioning algorithms. A clear computational advantage is observed for LM trees. LMM trees require longer computation times because of the estimation of random effects. Yet, this increase is minor compared to the computation times required by LongCART and SEM trees: LRT-based SEM trees required striking computation times, while score-based SEM trees were computationally more efficient, as expected.


\FloatBarrier


\section{Study~III: Partitioning Academic Trajectories}


\subsection{Method} 

\subsubsection{Dataset}

We analyzed trajectories on children's reading, math and science abilities from the Early Childhood Longitudinal Study-Kindergarten class of 1998--1999 \citep[ECLS-K; ][]{NCES10}. Data were collected from 21,304 children from 1,018 schools across the USA. Assessments took place from kindergarten in 1998 through 8th grade in 2007, here we focus on assessments from kindergarten, 1st, 3rd, 5th and 8th grade.

Response variables are reading, math, and science abilities, which were assessed using multi-item cognitive tests. Latent ability estimates were computed with a mean of zero and variance of one. Reading and math abilities were assessed in all five rounds of data collection, science knowledge was assessed in 3rd, 5th and 8th grade. We analyzed data from children who completed all assessments yielding $N = 6,277$ for reading; $N = 6,512$ for math; $N = 6,625$ for science.

Time was measured as the number of months since the baseline assessment. In order to obtain approximately linear trajectories, we chose the timing metric based on visual inspection of the observed data: $\mathrm{months}^{1/2}$ was used as the timing metric for reading and math trajectories, and $\mathrm{months}^{2/3}$ for science trajectories.

We used 11 time-invariant covariates as potential partitioning variables, all assessed at baseline: Gender (51.1\% male); Age in months (range 53 to 96; M = 6.14 years); Race (8 categories); First time in kindergarten (yes/no); Socio-economic status (range $-$5 to 3); Fine motor skills (e.g., drawing figures; range 0 to 9); Gross motor skills (e.g., ability to hop, skip and jump; range 0 to 8); Interpersonal skills (range 1 to 4); Self-control (range 1 to 4); Internalizing problem behavior (range 1 to 4); Externalizing problem behavior (range 1 to 4).





\subsubsection{Fitting approaches}


We applied five LM(M) trees to the data, focusing on the original GLMM tree approach and the approaches that performed well in the simulations:
% 
\begin{itemize}
  \item An LM tree ($\hat{\sigma}_{b_0} = \hat{\sigma}_{b_1} = 0$) using clustered covariances in the parameter-stability tests.
  \item An LMM tree using observation-level covariances, with a random intercept freely estimated ($\hat{\sigma}_{b_0} > 0$, $\hat{\sigma}_{b_1} = 0$) and initialization assuming zero random effects.
  \item An LMM tree using clustered covariances, with a random intercept freely estimated ($\hat{\sigma}_{b_0} > 0$, $\hat{\sigma}_{b_1} = 0$) and initialization assuming zero random effects.
  \item An LMM tree using clustered covariances, with both random intercept and slope freely estimated ($\hat{\sigma}_{b_0} > 0$, $\hat{\sigma}_{b_1} > 0$) and initialization assuming zero random effects . 
  \item An LMM tree uwith a random intercept freely estimated ($\hat{\sigma}_{b_0} > 0$, $\hat{\sigma}_{b_1} = 0$) and random-effect initialization with the full-sample estimate. 
\end{itemize}
%
Although LRT-based SEM trees performed very well in the simulations, they could not be used in this study because growth curve SEMs do not allow for incorporating continuous time. 

%We therefore opted for using longRPart \citep[version 1.0; ][]{AbdoyLeBl02}, which employs the exact same LRT-based splitting criterion as LRT-based SEM trees. It however fits a mixed-effects model in every node, instead of a SEM, allowing for the incorporation of time as a continuous variable. We fitted two longRPart trees with different random effects specifications:
% 
%\begin{itemize}
%  \item A longRPart tree which freely estimates a random intercept ($\hat{\sigma}_{b_0} > 0$, $\hat{\sigma}_{b_1} = 0$).
%  \item A longRPart tree which freely estimates both a random intercept and slope ($\hat{\sigma}_{b_0} > 0$, $\hat{\sigma}_{b_1} > 0$).
%\end{itemize}
%
%The default longRPart settings yielded a very large number of splits in the current analyses and poor predictive accuracy. We therefore restricted maximum tree depth to 5, yielding a maximum of $2^5$ = \Sexpr{2^5} terminal nodes, or \Sexpr{(2^5)-1} splits. We created a custom function for computing predictions from fitted longRPart trees, as this functionality was not included in the package itself. 

%As benchmarks, we included two LMMs with a fixed effect of time: 
%
%\begin{itemize}
%  \item An LMM with a random intercept freely estimated.
%  \item An LMM with both random intercept and slope freely estimated.
%\end{itemize}
%
%Besides time and the child identifier, the LMMs did not include additional predictors in the model.


\subsubsection{Evaluation of performance}

The ECLS-K datasets have exceptionally large sample sizes, so we employed random sampling to obtain training samples of $N=250$ children, likely more representative of real-world studies in psychology. We performed 100 repetitions for each response variable (math, reading, or science). We evaluated predictive accuracy by computing the mean squared difference between predicted and observed response variable values (MSE) for all children not included in the training sample in the current repetition. This separation of train and test observations does not allow for using the random effects in computing predictions; the cross-validated MSEs only quantify accuracy of the fixed-effects parameters. Tree size was measured by counting the number of splits in each tree.




\FloatBarrier
\subsection{Results}

Figure~\ref{fig:application_MSEs} and Table~\ref{tab:application_MSEs} present MSE distributions. Differences in predictive performance are small, all $R^2$ differences are smaller than 0.01. Bonferroni-adjusted pairwise $t$-tests indicated no difference in performance between default LMM trees and LMM trees with random-effects initialization for any of the three outcomes. Incontract, the three LM(M) trees using clustered covariances performed significantly better for the math and reading outcomes. For the science outcomes, no significant differences were observed. Thus, cluster-level covariances seem to provide the most robust improvement of performance. 

Figure~\ref{fig:application_sizes} presents tree size distributions. Similar to the simulation study's results, default LMM trees implement the largest number of splits. Cluster-level covariances provide a robust reduction in the number of splits. Given their non-distinguishable predictive performance, LM trees with clustered covariances may be preferred if a obtaining a sparse result is critical. LMM trees with clustered covariances may provide less sparsity but better predictive accuracy, irrespective of whether random intercepts and/or slopes have been specified. Random-effects initialization yields tree sizes similar to the default fitting approach. 


\begin{figure}[!bt]
\caption{Mean squared errors for trees fitted to math, reading and science ability trajectories.}
\begin{subfigure}{\textwidth}
<<echo=FALSE, fig=TRUE, height=2, width=5>>=
## Load datasets for computing variance of response variables
load("Reading ability data.Rdata")
var_read <- var(readdata$score)
load("Math ability data.Rdata")
var_math <- var(mathdata$score)
load("Science ability data.Rdata")
var_scie <- var(sciedata$score)

## MSE
load("MSEs, N = 250, Math.Rda")
math_MSEs <- cbind(MSEs, N = 250)
#load("MSEs, N = 1000, Math.Rda")
#math_MSEs <- rbind(math_MSEs, cbind(MSEs, N = 1000))
#sapply(math_MSEs, function(x) table(is.na(x)))$longRPart
#sapply(math_MSEs, function(x) table(is.na(x)))$longRPart_s
#sapply(math_MSEs, function(x) tapply(x, math_MSEs$N, mean, na.rm =TRUE))

load("MSEs, N = 250, Reading.Rda")
read_MSEs <- cbind(MSEs, N = 250)
#load("MSEs, N = 1000, Reading.Rda")
#read_MSEs <- rbind(read_MSEs, cbind(MSEs, N = 1000))
#sapply(read_MSEs, function(x) table(is.na(x)))
#sapply(read_MSEs, function(x) tapply(x, read_MSEs$N, mean))

load("MSEs, N = 250, Science.Rda")
scie_MSEs <- cbind(MSEs, N = 250)
#load("MSEs, N = 1000, Science.Rda")
#scie_MSEs <- rbind(scie_MSEs, cbind(MSEs, N = 1000))
#sapply(scie_MSEs, function(x) table(is.na(x)))$longRPart
#sapply(scie_MSEs, function(x) table(is.na(x)))$longRPart_s
#sapply(scie_MSEs, function(x) tapply(x, scie_MSEs$N, mean))


## Math
MSE_df <- data.frame(sapply(math_MSEs[ , c(1:4, 6)], 
                              function(x) sprintf("%.4f", tapply(x, math_MSEs$N, mean, na.rm = TRUE))))
MSE_df <- cbind(MSE_df, sapply(math_MSEs[ , c(1:4, 6)], 
                               function(x) sprintf("%.3f", tapply(x, math_MSEs$N, sd, na.rm = TRUE))))
MSE_df <- cbind(MSE_df, sapply(math_MSEs[ , c(1:4, 6)], 
                               function(x) sprintf("%.3f", tapply(x, math_MSEs$N, 
                                                                  function(x) 1 - mean(x, na.rm = TRUE)/var_math))))
## Reading
MSE_df <- cbind(MSE_df, sapply(read_MSEs[ , c(1:4, 6)], 
                               function(x) sprintf("%.4f", tapply(x, read_MSEs$N, mean, na.rm = TRUE))))
MSE_df <- cbind(MSE_df, sapply(read_MSEs[ , c(1:4, 6)], 
                               function(x) sprintf("%.3f", tapply(x, read_MSEs$N, sd, na.rm = TRUE))))
MSE_df <- cbind(MSE_df, sapply(read_MSEs[ , c(1:4, 6)], 
                               function(x) sprintf("%.3f", tapply(x, read_MSEs$N, 
                                                                  function(x) 1 - mean(x, na.rm = TRUE)/var_read))))

## Science 
MSE_df <- cbind(MSE_df, sapply(scie_MSEs[ , c(1:4, 6)], 
                               function(x) sprintf("%.4f", tapply(x, scie_MSEs$N, mean, na.rm = TRUE))))
MSE_df <- cbind(MSE_df, sapply(scie_MSEs[ , c(1:4, 6)], 
                               function(x) sprintf("%.3f", tapply(x, scie_MSEs$N, sd, na.rm = TRUE))))
MSE_df <- cbind(MSE_df, sapply(scie_MSEs[ , c(1:4, 6)], 
                               function(x) sprintf("%.3f", tapply(x, scie_MSEs$N, 
                                                                  function(x) 1 - mean(x, na.rm = TRUE)/var_scie))))

## tmp3 is math
tmp3 <- data.frame(stack(math_MSEs[ , c(1:4, 6)]),
                    dataset_id = factor(rep(1:nrow(math_MSEs), times = 5)),
                   N = math_MSEs$N)
names(tmp3)[1:2] <- c("MSE", "method") 
tmp3$method2 <- as.character(tmp3$method)

tmp3$method2[tmp3$method2 == "LMtree_c"] <- "LM tree\n\n(cl.cov.)"
tmp3$method2[tmp3$method2 == "LMMtree"] <- "LMM tree"
tmp3$method2[tmp3$method2 %in% c("LMMtree_c", "LMMtree_sc")] <- "LMM tree\n\n(cl.cov.)"
tmp3$method2[tmp3$method2 == "LMMtree_r"] <- "LMM tree\n\n(ran.eff.)"
tmp3$random <- ifelse(tmp3$method == "LMtree_c", "atop(hat(sigma)[b[0]] == 0, hat(sigma)[b[1]] == 0)", 
                      ifelse(tmp3$method %in% c("LMMtree", "LMMtree_c", "LMMtree_r"), 
                             "atop(hat(sigma)[b[0]] > 0, hat(sigma)[b[1]] == 0)", 
                             ifelse(tmp3$method %in% c("LMMtree_sc"), 
                                "atop(hat(sigma)[b[0]] > 0, hat(sigma)[b[1]] > 0)", NA)))
tmp3$N <- factor(tmp3$N)
theme_set(theme_bw(base_size = 12))
ggplot(tmp3) +
  geom_boxplot(aes(x=random, y=MSE), 
               position=position_dodge(1), alpha = .5, width = .45, fill = "gray") +
  scale_y_continuous(#trans = log_trans(),
                     sec.axis = sec_axis(trans=~1-(./var_math))) +
  facet_grid(~method2, scales = "free", space = "free") +
  theme(axis.title.x=element_blank(), axis.text.x=element_blank(),
        legend.position="none", axis.text.y = element_text(size = 8),
        axis.title.y = element_text(size = 10)) +
  labs(x = "", y = "MSE")
@
\end{subfigure}
\begin{subfigure}{\textwidth}
<<echo=FALSE, fig=TRUE, height=1.6, width=5>>=
## tmp4 is reading
tmp4 <- data.frame(stack(read_MSEs[ , c(1:4,6)]),
                    dataset_id = factor(rep(1:nrow(read_MSEs), times = 5)),
                   N = read_MSEs$N)
names(tmp4)[1:2] <- c("MSE", "method") 
tmp4$method2 <- as.character(tmp4$method)
tmp4$method2[grepl("LMtree", tmp4$method2)] <- "LM tree\n\n(cluster)"
tmp4$method2[tmp4$method2 %in% c("LMMtree_c", "LMMtree_sc")] <- "LMM tree\n\n(cluster)"
tmp4$method2[tmp4$method2 == "LMMtree_r"] <- "LMM tree\n\n(ranef)"
tmp4$method2[tmp4$method2 %in% c("LMMtree_cr", "LMMtree_scr")] <- "LMM tree\n\n(cluster + ranef)"
tmp4$method2[grepl("longR", tmp4$method2)] <- "longRPart"
tmp4$method2[tmp4$method2 == "LMtree_c"] <- "LM tree\n\n(cl.cov.)"
tmp4$method2[tmp4$method2 == "LMMtree"] <- "LMM tree"
tmp4$method2[tmp4$method2 %in% c("LMMtree_c", "LMMtree_sc")] <- "LMM tree\n\n(cl.cov.)"
tmp4$method2[tmp4$method2 == "LMMtree_r"] <- "LMM tree\n\n(ran.eff.)"
tmp4$random <- ifelse(tmp4$method == "LMtree_c", "atop(hat(sigma)[b[0]] == 0, hat(sigma)[b[1]] == 0)", 
                      ifelse(tmp4$method %in% c("LMMtree", "LMMtree_c", "LMMtree_r"), 
                             "atop(hat(sigma)[b[0]] > 0, hat(sigma)[b[1]] == 0)", 
                             ifelse(tmp4$method %in% c("LMMtree_sc"), 
                                "atop(hat(sigma)[b[0]] > 0, hat(sigma)[b[1]] > 0)", NA)))
tmp4$N <- factor(tmp4$N)
ggplot(tmp4) +
  geom_boxplot(aes(x=random, y=MSE), 
               position=position_dodge(1), alpha = .5, width = .45, fill = "gray") +
  scale_y_continuous(#trans = log_trans(),
                       sec.axis = sec_axis(trans=~1-(./var_read))) +
  facet_grid(~method2, scales = "free", space = "free") +
  theme(strip.background = element_blank(), strip.text.x = element_blank(),
        axis.title.x=element_blank(), axis.text.x=element_blank(),
        legend.position="none", axis.text.y = element_text(size = 8),
        axis.title.y = element_text(size = 10)) +
  labs(x = "", y = "MSE")
@
\end{subfigure}
\begin{subfigure}{\textwidth}
<<echo=FALSE, fig=TRUE, height=2, width=5>>=
## tmp5 is science
tmp5 <- data.frame(stack(scie_MSEs[ , c(1:4,6)]), ## omit longRPart with 7:10
                    dataset_id = factor(rep(1:nrow(scie_MSEs), times = 5)),
                   N = scie_MSEs$N)
names(tmp5)[1:2] <- c("MSE", "method") 
tmp5$method2 <- as.character(tmp5$method)
tmp5$method2[tmp5$method2 == "LMtree_c"] <- "LM tree\n\n(cl.cov.)"
tmp5$method2[tmp5$method2 == "LMMtree"] <- "LMM tree"
tmp5$method2[tmp5$method2 %in% c("LMMtree_c", "LMMtree_sc")] <- "LMM tree\n\n(cl.cov.)"
tmp5$method2[tmp5$method2 == "LMMtree_r"] <- "LMM tree\n\n(ran.eff.)"
tmp5$random <- ifelse(tmp5$method == "LMtree_c", "atop(hat(sigma)[b[0]] == 0, hat(sigma)[b[1]] == 0)", 
                      ifelse(tmp5$method %in% c("LMMtree", "LMMtree_c", "LMMtree_r"), 
                             "atop(hat(sigma)[b[0]] > 0, hat(sigma)[b[1]] == 0)", 
                             ifelse(tmp5$method %in% c("LMMtree_sc"), 
                                "atop(hat(sigma)[b[0]] > 0, hat(sigma)[b[1]] > 0)", NA)))
tmp5$N <- factor(tmp5$N)
library("ggforce")
ggplot(tmp5, aes(x=random, y=MSE)) +
  geom_boxplot(position=position_dodge(1), alpha = .5, width = .45, fill = "gray") +
  scale_y_continuous(#trans = log_trans(),
                       sec.axis = sec_axis(trans=~1-(./var_scie))) +
  facet_grid(~method2, scales = "free", space = "free") +
  theme(strip.background = element_blank(), strip.text.x = element_blank(),
        legend.position="none", axis.text.y = element_text(size = 9),
        axis.text.x = element_text(size = 9), axis.title.y = element_text(size = 10)) +
  labs(x = "", y = "MSE") +
  scale_x_discrete(labels = scales::parse_format()) +
  coord_cartesian(clip = 'off') 
@
\end{subfigure}%
\vspace{-0.5cm}%
{\footnotesize \\ \textit{Note. } Top, middle and bottom panels depict math, reading and science ability trajectories, respectively. The secondary $y$-axis on the right quantifies the proportion of variance explained, computed as $1 - \frac{\text{mean(MSE)}}{\text{var}(y)}$.}
\label{fig:application_MSEs}
\end{figure}


<<echo=FALSE, eval=FALSE>>=
## Pairwise difference tests of MSEs
pairwise.t.test(tmp3$MSE, tmp3$method, alternative = "two.sided", 
                p.adjust = "bonferroni") ## math
#            LMtree_c LMMtree LMMtree_c LMMtree_r
# LMMtree    3.0e-05  -       -         -        
# LMMtree_c  1        3.5e-08 -         -        
# LMMtree_r  2.9e-05  1       3.3e-08   -        
# LMMtree_sc 1        1.1e-06 1         1.0e-06  
pairwise.t.test(tmp4$MSE, tmp4$method, alternative = "two.sided", 
                p.adjust = "bonferroni") ## reading
#            LMtree_c LMMtree LMMtree_c LMMtree_r
# LMMtree    2.0e-06  -       -         -        
# LMMtree_c  1        7.2e-07 -         -        
# LMMtree_r  2.1e-06  1       7.6e-07   -        
# LMMtree_sc 1        7.8e-09 1         8.2e-09  
pairwise.t.test(tmp5$MSE, tmp5$method, alternative = "two.sided", 
                p.adjust = "bonferroni") ## science
#           LMtree_c LMMtree LMMtree_c LMMtree_r
# LMMtree    0.49     -       -         -        
# LMMtree_c  1.00     1.00    -         -        
# LMMtree_r  0.31     1.00    1.00      -        
# LMMtree_sc 0.61     1.00    1.00      1.00     
@




<<application_MSE, echo=FALSE, results=tex>>=
library("kableExtra")
rownames(MSE_df) <- c("LM tree$^c$", "LMM tree$^i$", "LMM tree$^{i,c}$", 
                      "LMM tree$^{i,r}$", "LMM tree$^{i,s,c}$")
colnames(MSE_df) <- rep(c("M", "SD", "$R^2$"), times = 3)

## Add benchmark LMMs
#load("LMM MSEs, N = 250, Math.RDa")
#MSE_df[c("LMM$^i$", "LMM$^{i,s}$"), 1] <- format(colMeans(MSEs), digits = 4)
#MSE_df[c("LMM$^i$", "LMM$^{i,s}$"), 2] <- round(sapply(MSEs, sd), digits = 3)
#MSE_df[c("LMM$^i$", "LMM$^{i,s}$"), 3] <- sprintf("%.3f", 1 - colMeans(MSEs)/var_math)

#load("LMM MSEs, N = 250, Reading.RDa")
#MSE_df[c("LMM$^i$", "LMM$^{i,s}$"), 4] <- format(colMeans(MSEs), digits = 4)
#MSE_df[c("LMM$^i$", "LMM$^{i,s}$"), 5] <- round(sapply(MSEs, sd), digits = 3)
#MSE_df[c("LMM$^i$", "LMM$^{i,s}$"), 6] <- sprintf("%.3f", 1 - colMeans(MSEs)/var_read)

#load("LMM MSEs, N = 250, Science.RDa")
#MSE_df[c("LMM$^i$", "LMM$^{i,s}$"), 7] <- format(colMeans(MSEs), digits = 4)
#MSE_df[c("LMM$^i$", "LMM$^{i,s}$"), 8] <- round(sapply(MSEs, sd), digits = 3)
#MSE_df[c("LMM$^i$", "LMM$^{i,s}$"), 9] <- sprintf("%.3f", 1 - colMeans(MSEs)/var_scie)

## boldface highest R2 values
best_ids <- apply(MSE_df[ , c(1,4,7)], 2, which.min)
MSE_df[best_ids[1] , 1:3] <- paste0("\\textbf{", MSE_df[best_ids[1] , 1:3], "}")
MSE_df[best_ids[2] , 4:6] <- paste0("\\textbf{", MSE_df[best_ids[2] , 4:6], "}")
MSE_df[best_ids[3] , 7:9] <- paste0("\\textbf{", MSE_df[best_ids[3] , 7:9], "}")

kable_styling(add_header_above(#add_header_above(
  add_footnote(
  kable(MSE_df[c(1:3, 5, 4), ], format = "latex", booktabs = TRUE, label = "application_MSEs", align = "c", 
        escape=FALSE, linesep = "", # linesep command suppresses addlinesep every 5 rows
        caption = "Cross-validated mean squared errors for each of the response variables."),
  "\\footnotesize \\\\ \\textit{Note.} Means and standard deviations computed over 100 cross-validation repetitions. Boldfaced values indicate the best-performing method for each outcome. $R^2$ was computed as $\\frac{\\text{mean(MSE)}}{\\text{var}(y)}$. $^c$~cluster-level covariances; $^r$~estimation initialized with random effects; $^i$~random-intercept variance freely estimated; $^s$~random-slope variance freely estimated.", 
  notation="none", threeparttable = TRUE, escape = FALSE),
  #c(" ", "N = 250"=1, "N = 1,000"=1, "N = 250"=1, "N = 1,000"=1,
  #  "N = 250"=1, "N = 1,000"=1), align = "c"),
  c(" ", "Math" = 3, "Reading" = 3, "Science" = 3), align = "c"),
  font_size = 11, full_width=FALSE)
@

<<eval=FALSE, echo=FALSE>>=
math_LMM <- lmer(score ~ months + (1 + months|CHILDID), data = mathdata)
(as.data.frame(VarCorr(math_LMM))$vcov / var_math)
(as.data.frame(VarCorr(math_LMM))$vcov / as.data.frame(VarCorr(math_LMM))$vcov[4])

read_LMM <- lmer(score ~ months + (1 + months|CHILDID), data = readdata)
(as.data.frame(VarCorr(read_LMM))$vcov / var_read)
(as.data.frame(VarCorr(read_LMM))$vcov / as.data.frame(VarCorr(read_LMM))$vcov[4])

scie_LMM <- lmer(score ~ months + (1 + months|CHILDID), data = sciedata)
(as.data.frame(VarCorr(scie_LMM))$vcov / var_scie)
(as.data.frame(VarCorr(scie_LMM))$vcov / as.data.frame(VarCorr(scie_LMM))$vcov[4])
@


\begin{figure}[!bt]%
\caption{Sizes of trees fitted to math, reading, and science ability trajectories.}
\begin{subfigure}{\textwidth}%
<<echo=FALSE, fig=TRUE, height=2, width=5>>=
## Tree size
load("sizes, N = 250, Math.Rda")
math_sizes <- cbind(sizes, N = 250)
#load("sizes, N = 1000, Math.Rda")
#math_sizes <- rbind(math_sizes, cbind(sizes, N = 1000))
#sapply(math_sizes, function(x) table(is.na(x)))
#sapply(math_sizes, function(x) tapply(x, math_sizes$N, mean, na.rm =TRUE))

load("sizes, N = 250, Reading.Rda")
read_sizes <- cbind(sizes, N = 250)
#load("sizes, N = 1000, Reading.Rda")
#read_sizes <- rbind(read_sizes, cbind(sizes, N = 1000))
#sapply(read_sizes, function(x) table(is.na(x)))
#sapply(read_sizes, function(x) tapply(x, read_sizes$N, mean, na.rm =TRUE))

load("sizes, N = 250, Science.Rda")
scie_sizes <- cbind(sizes, N = 250)
#load("sizes, N = 1000, Science.Rda")
#scie_sizes <- rbind(scie_sizes, cbind(sizes, N = 1000))
#sapply(scie_sizes, function(x) table(is.na(x)))
#sapply(scie_sizes, function(x) tapply(x, scie_sizes$N, mean, na.rm =TRUE))

## Made a mistake in evaluating tree size in the Application scripts:
## I took #splits = nrow(tree$frame)-1, assuming tree$frame contains a row for
## each terminal node. It contains a row for each inner and terminal node.
## Thus, #splits = (nrow(tree$frame)-1)/2; original number should be divided by 2:
#math_sizes$longRPart <- math_sizes$longRPart/2 
#math_sizes$longRPart_s <- math_sizes$longRPart_s/2 
#read_sizes$longRPart <- read_sizes$longRPart/2 
#read_sizes$longRPart_s <- read_sizes$longRPart_s/2 
#scie_sizes$longRPart <- scie_sizes$longRPart/2 
#scie_sizes$longRPart_s <- scie_sizes$longRPart_s/2 

## tmp6 is math
tmp6 <- data.frame(stack(math_sizes[ , c(1:4,6)]),
                    dataset_id = factor(rep(1:nrow(math_sizes), times = 5)),
                   N = math_sizes$N)
names(tmp6)[1:2] <- c("splits", "method") 
tmp6$method2 <- as.character(tmp6$method)
tmp6$method2[tmp6$method2 == "LMtree_c"] <- "LM tree\n\n(cl.cov.)"
tmp6$method2[tmp6$method2 == "LMMtree"] <- "LMM tree"
tmp6$method2[tmp6$method2 %in% c("LMMtree_c", "LMMtree_sc")] <- "LMM tree\n\n(cl.cov.)"
tmp6$method2[tmp6$method2 == "LMMtree_r"] <- "LMM tree\n\n(ran.eff.)"
tmp6$random <- ifelse(tmp6$method == "LMtree_c", "atop(hat(sigma)[b[0]] == 0, hat(sigma)[b[1]] == 0)", 
                      ifelse(tmp6$method %in% c("LMMtree", "LMMtree_c", "LMMtree_r"), 
                             "atop(hat(sigma)[b[0]] > 0, hat(sigma)[b[1]] == 0)", 
                             ifelse(tmp6$method %in% c("LMMtree_sc"), 
                                "atop(hat(sigma)[b[0]] > 0, hat(sigma)[b[1]] > 0)", NA)))
tmp6$N <- factor(tmp6$N)
ggplot(tmp6) +
  geom_boxplot(aes(x=random, y=splits), 
               position=position_dodge(1), alpha = .5, width = .45, fill = "gray") +
  facet_grid(~method2, scales = "free", space = "free") +
  theme(axis.title.x=element_blank(), axis.text.x=element_blank(),
        axis.text.y = element_text(size = 8), axis.title.y = element_text(size = 10)) +
  labs(x = "", y = "no. of splits")
@
\end{subfigure}
\begin{subfigure}{\textwidth}
<<echo=FALSE, fig=TRUE, height=1.6, width=5>>=
## tmp7 is reading
tmp7 <- data.frame(stack(read_sizes[ , c(1:4,6)]),
                    dataset_id = factor(rep(1:nrow(read_sizes), times = 5)),
                   N = read_sizes$N)
names(tmp7)[1:2] <- c("splits", "method") 
tmp7$method2 <- as.character(tmp7$method)
tmp7$method2[tmp7$method2 == "LMtree_c"] <- "LM tree\n\n(cl.cov.)"
tmp7$method2[tmp7$method2 == "LMMtree"] <- "LMM tree"
tmp7$method2[tmp7$method2 %in% c("LMMtree_c", "LMMtree_sc")] <- "LMM tree\n\n(cl.cov.)"
tmp7$method2[tmp7$method2 == "LMMtree_r"] <- "LMM tree\n\n(ran.eff.)"
tmp7$random <- ifelse(tmp7$method == "LMtree_c", "atop(hat(sigma)[b[0]] == 0, hat(sigma)[b[1]] == 0)", 
                      ifelse(tmp7$method %in% c("LMMtree", "LMMtree_c", "LMMtree_r"), 
                             "atop(hat(sigma)[b[0]] > 0, hat(sigma)[b[1]] == 0)", 
                             ifelse(tmp7$method %in% c("LMMtree_sc"), 
                                "atop(hat(sigma)[b[0]] > 0, hat(sigma)[b[1]] > 0)", NA)))
tmp7$N <- factor(tmp7$N)
ggplot(tmp7) +
  geom_boxplot(aes(x=random, y=splits), 
               position=position_dodge(1), alpha = .5, width = .45, fill = "gray") +
  facet_grid(~method2, scales = "free", space = "free") +
  labs(x = "", y = "no. of splits") +
  theme(strip.background = element_blank(), strip.text.x = element_blank(),
        axis.title.x=element_blank(), axis.text.x=element_blank(),
        axis.text.y = element_text(size = 8), axis.title.y = element_text(size = 10))
@
\end{subfigure}
\begin{subfigure}{\textwidth}
<<echo=FALSE, fig=TRUE, height=2, width=5>>=
## tmp8 is science
tmp8 <- data.frame(stack(scie_sizes[ , c(1:4,6)]),
                    dataset_id = factor(rep(1:nrow(scie_sizes), times = 5)),
                   N = scie_sizes$N)
names(tmp8)[1:2] <- c("splits", "method") 
tmp8$method2 <- as.character(tmp8$method)
tmp8$method2[tmp8$method2 == "LMtree_c"] <- "LM tree\n\n(cl.cov.)"
tmp8$method2[tmp8$method2 == "LMMtree"] <- "LMM tree"
tmp8$method2[tmp8$method2 %in% c("LMMtree_c", "LMMtree_sc")] <- "LMM tree\n\n(cl.cov.)"
tmp8$method2[tmp8$method2 == "LMMtree_r"] <- "LMM tree\n\n(ran.eff.)"
tmp8$random <- ifelse(tmp8$method == "LMtree_c", "atop(hat(sigma)[b[0]] == 0, hat(sigma)[b[1]] == 0)", 
                      ifelse(tmp8$method %in% c("LMMtree", "LMMtree_c", "LMMtree_r"), 
                             "atop(hat(sigma)[b[0]] > 0, hat(sigma)[b[1]] == 0)", 
                             ifelse(tmp8$method %in% c("LMMtree_sc"), 
                                "atop(hat(sigma)[b[0]] > 0, hat(sigma)[b[1]] > 0)", NA)))
tmp8$N <- factor(tmp8$N)
ggplot(tmp8) +
  geom_boxplot(aes(x=random, y=splits), 
               position=position_dodge(1), alpha = .5, width = .45, fill = "gray") +
  facet_grid(~method2, scales = "free", space = "free") +
  theme(strip.background = element_blank(), strip.text.x = element_blank(),
        axis.text.y = element_text(size = 8), 
        axis.text.x = element_text(size = 9), axis.title.y = element_text(size = 10)) +
  labs(x = "", y = "no. of splits") +
  scale_x_discrete(labels = scales::parse_format()) +
  coord_cartesian(clip = 'off')
@
\end{subfigure}%
{\\\footnotesize \textit{Note.} Top, middle and bottom panels depict math, reading and science ability trajectories, respectively.}
\label{fig:application_sizes}
\end{figure}%





\FloatBarrier


\section{Discussion}

The simulations showed that the proposed extensions of GLMM trees are effective for partitioning LGCMs. Use of clustered covariances appears most effective and their good performance appears largely unaffected by (mis-)specification of the random effects; it may therefore be the safest choice in practice. Initializing estimation with the random effects was also effective, but only when the random-effects specification is kept simple (i.e., no estimation of random slopes). Combining cluster-level covariances and random-effects initialization worsened performance and is thus not recommended. 

Strong performance of clustered covariances was also observed in partitioning real-world academic trajectories. They provided substantially smaller trees for all outcomes and better or equal predictive accuracy. In comparison to cluster-level covariances, random-effects initialization resulted larger trees for all outcomes and worse performance for the reading and math outcomes.

The simulations showed comparable performance of LM(M) and SEM trees in partitioning LGCMs. SEM trees may however be more sensitive to mis-specification of the random effects, with under-specification resulting in too many splits. In line with results of \cite{ArnoyVoel21}, we found score-based SEM trees to have somewhat lower power than LRT-based SEM trees, but at a much lower computational cost. LongCART trees often selected the wrong partitioning variable for the first split, and were outperformed by LM(M) and SEM trees. The LongCART parameter stability tests \citep{KundyHare19} may be underpowered for detecting instability of the fixed intercept, or for detecting instability with respect to categorical covariates. 

The simulations clearly illustrated the lower computational burden of GLMM trees. This is in large part due to their local-global estimation approach, where fixed-effects parameters are estimated locally within a node and random-effects parameters are estimated globally, using all observations. In contrast, SEM trees and LongCART fit the full mixed-effects model in each node, which substantially increases computational load. The local-global estimation approach also reduces model complexity, because a lower number of random-effects parameters need to be estimated.

Yet, a possible downside of the local-global estimation approach is that it does not allow for recovering subgroups with differences in random-effects parameters. When there is a specific interest in partitioning the random effects, score-based SEM trees may be preferred. Alternatively, researchers may want to use the parameter stability tests for mixed-effects models developed by \cite{WangyMerk18} \citep[see also ][]{WangyMerk21,WangyGrav22}. This will be useful, for example, when the number of or distances between timepoints differ between respondents and SEM-based growth curve models cannot be applied \citep{NeisyMatt18}.

The current evaluations were limited to Gaussian responses and LGCMs. Future studies should assess performance of GLMM trees in partitioning longitudinal data with, for example, binomial or count responses. Also, whether our conclusions generalize to settings beyond LGCMs remains to be evaluated. We expect that the strong performance of cluster-level covariances in the parameter stability tests generalizes to other settings where covariates are measured at higher levels, either in longitudinal and/or otherwise nested data structures. Finally, we used the outer-product-of-gradients (OPG) estimator for computing (clustered) covariances. Though computationally more burdensome, future work could assess potential benefits of using the full sandwich estimator.





\bibliography{bib}

\newpage
\appendix


\section{Appendix A: Effects of data-generating parameters on tree size}
\label{sec:AppendixA}

For LM(M) trees (Figure~\ref{fig:LMM_sizes_interact}), use of cluster-level covariances provided the most robust improvement in split recovery. For the data-generating parameters, the effects on tree size were strongest for $\sigma_{b_0}$, followed by $N$, $p$, $\sigma_{b_1}$ and $\rho$. Higher values of $\sigma_{b_0}$, $N$, $p$ and $\sigma_{b_1}$ tend to yield higher numbers of splits, while the effect of $\rho$ is minimal. 

For SEM trees and LongCART (Figure~\ref{fig:tree_sizes_interact}), strongest effects were observed for $N$, followed by $\sigma_{b_0}$, $p_{noise}$, $\rho$ and $\sigma_{b_1}$. Results for $N$ were as expected for all methods: More splits are implemented with higher sample size. Both LRT- and score-based SEM trees seem only affected by levels of $\sigma_{b_0}$, $p_{noise}$ and $\rho$ under misspecification of the random effects. Especially when both random intercept and slope variances are fixed to 0, higher levels of $\sigma_{b_0}$ and $p_{noise}$ yield more splits with both SEM tree approached. LRT-based SEM trees seem unaffected by levels of $\rho$, while score-based SEM trees seemed to implement a larger number of splits with increasing values of $\rho$. This pattern seemed reversed for increased magnitude of $\sigma_{b_1}$, which yields a lower number of splits for both SEM tree approaches, but only when the random effects were correctly specified. LongCART implemented more splits with higher levels of $\rho$,  $\sigma_{b_1}$, $N$, $p_{noise}$ and $\sigma_{b_0}$.


\begin{figure}[!ht]
\caption{Effects of data-generating parameters on tree size for LM(M) trees.}
\begin{subfigure}{1.25\textwidth}
<<echo=FALSE, fig=TRUE, height=1.7>>=
theme_set(theme_bw(base_size = 8))
cols <- rep(rainbow_hcl(10)[c(3,5)], times = 5)
ggplot(tmp[LMM_ids, ], aes(x=setting, y=tree_size)) +
  geom_boxplot(aes(fill = sigma_int), 
               position=position_dodge(1), alpha = .5, width = .6, 
               outlier.shape = NA, coef = NULL) +
  geom_count(aes(group=sigma_int), position=position_dodge(1), colour = "black", 
             fill=NA, show.legend = FALSE, alpha = .15) +
  scale_y_continuous(trans = pseudo_log_trans(), breaks = breaks, lim = lim) +
  facet_grid(~ranef, scales = "free", space = "free", labeller = label_parsed) +
  labs(x = "", y = "no. of splits", col=expression(sigma^2~(b[0]))) + 
  geom_hline(yintercept=3, col = "darkgray") + 
  theme(axis.title.x=element_blank(), axis.text.x=element_blank()) +
  labs(fill=expression(sigma[b[0]]^2)) +
  stat_summary(aes(group=sigma_int), position=position_dodge(1), fun="mean", 
               col = "black", shape=16, size = .4) 
@
\end{subfigure}
\begin{subfigure}{1.25\textwidth}
<<echo=FALSE, fig=TRUE, height=1.5>>=
ggplot(tmp[LMM_ids, ], aes(x=setting, y=tree_size)) +
  geom_boxplot(aes(fill = N), 
               position=position_dodge(1), alpha = .5, width = .6, 
               outlier.shape = NA, coef=NULL) + 
  geom_count(aes(group=N), 
             position=position_dodge(1), colour = "black", fill=NA, show.legend = FALSE, alpha = .15) +
  scale_y_continuous(trans = pseudo_log_trans(), breaks = breaks, lim = lim) +
  facet_grid(~ranef, scales = "free", space = "free", labeller = label_parsed) +
  labs(x = "", y = "no. of splits") +
  geom_hline(yintercept=3, col = "darkgray") + 
  theme(axis.title.x=element_blank(), axis.text.x=element_blank(),
        strip.background = element_blank(), strip.text.x = element_blank()) +
  stat_summary(aes(group=N), position=position_dodge(1), fun="mean", 
               col = "black", shape=16, size = .4) 
@
\end{subfigure}
\begin{subfigure}{1.25\textwidth}
<<echo=FALSE, fig=TRUE, height=1.5>>=
ggplot(tmp[LMM_ids, ], aes(x=setting, y=tree_size)) +
  geom_boxplot(aes(fill = p_noise), position=position_dodge(1), 
               alpha = .5, width = .6, outlier.shape = NA, coef=NULL) + 
  geom_count(aes(group=p_noise), position=position_dodge(1), 
             colour = "black", fill=NA, show.legend = FALSE, alpha = .15) +
  scale_y_continuous(trans = pseudo_log_trans(), breaks = breaks, lim = lim) +
  facet_grid(~ranef, scales = "free", space = "free", labeller = label_parsed) +
  labs(x = "", y = "no. of splits", col = expression(p[noise])) +
  geom_hline(yintercept=3, col = "darkgray") +
  theme(axis.title.x=element_blank(),axis.text.x=element_blank(),
        strip.background = element_blank(), strip.text.x = element_blank()) +
  labs(fill=expression(p[noise])) +
  stat_summary(aes(group=p_noise), position=position_dodge(1), fun="mean", 
               col = "black", shape=16, size = .4) 
@
\end{subfigure}
\begin{subfigure}{1.25\textwidth}
<<echo=FALSE, fig=TRUE, height=1.5>>=
ggplot(tmp[LMM_ids, ], aes(x=setting, y=tree_size)) +
  geom_boxplot(aes(fill = sigma_slope), position=position_dodge(1), 
               alpha = .5, width = .6, outlier.shape = NA, coef=NULL) + 
  geom_count(aes(group=sigma_slope), position=position_dodge(1), 
             colour = "black", fill=NA, show.legend = FALSE, alpha = .15) +
  scale_y_continuous(trans = pseudo_log_trans(), breaks = breaks, lim = lim) +
  facet_grid(~ranef, scales = "free", space = "free", labeller = label_parsed) +
  labs(x = "", y = "no. of splits", col=expression(sigma^2~(b[1]))) +
  geom_hline(yintercept=3, col = "darkgray") + 
  theme(axis.title.x=element_blank(),axis.text.x=element_blank(),
        strip.background = element_blank(), strip.text.x = element_blank()) +
  labs(fill=expression(sigma[b[1]]^2)) +
  stat_summary(aes(group=sigma_slope), position=position_dodge(1), fun="mean", 
               col = "black", shape=16, size = .4) 
  
@
\end{subfigure}
\begin{subfigure}{1.25\textwidth}
<<echo=FALSE, fig=TRUE, height=1.8>>=
ggplot(tmp[LMM_ids, ], aes(x=setting, y=tree_size)) +
  geom_boxplot(aes(fill = rho), position=position_dodge(1), 
               alpha = .5, width = .6, outlier.shape = NA, coef=NULL) + 
  geom_count(aes(group=rho), position=position_dodge(1), 
             colour = "black", fill=NA, show.legend = FALSE, alpha = .15) +
  scale_y_continuous(trans = pseudo_log_trans(), breaks = breaks, lim = lim) +
  facet_grid(~ranef, scales = "free", space = "free", labeller = label_parsed) +
  labs(x = "", y = "no. of splits", col=expression(sigma^2~(b[1]))) +
  geom_hline(yintercept=3, col = "darkgray") + 
  theme(strip.background = element_blank(), strip.text.x = element_blank()) +
  labs(fill=expression(rho)) +
  stat_summary(aes(group=rho), position=position_dodge(1), fun="mean", 
               col = "black", shape=16, size = .4) 
theme_set(theme_bw(base_size = 11))
@
\end{subfigure}%
\vspace{-.6cm}%
{\singlespacing \footnotesize \textit{Note.} Black dots represent means, gray circles represent counts, dark gray horizontal lines represent true number of splits~3. Distances on $y$-axis are on log scale. $\sigma_{b_0}^2$ = variance of random intercept; $\sigma_{b_1}^2$ = variance of random slope; $N$ = sample size at level 2; $p_{noise}$ = number of noise variables; $\rho$ = correlation between partitioning variables.}
\label{fig:LMM_sizes_interact}
\end{figure}





\begin{figure}[!ht]
\caption{Effect of data-generating parameters on tree size for LM(M), SEM and LongCART trees.}
\begin{subfigure}{1.25\textwidth}
<<echo=FALSE, fig=TRUE, height=1.7>>=
lim <- c(0, 18)
theme_set(theme_bw(base_size = 8))
ggplot(tmp2, aes(x=random, y=tree_size)) +
  geom_boxplot(aes(fill = N), position=position_dodge(1), alpha = .5, width = .6,
               outlier.shape = NA, coef=NULL) + 
  geom_count(aes(group=N), 
             position=position_dodge(1), colour = "black", fill=NA, show.legend = FALSE, alpha = .15) +
  scale_y_continuous(trans = pseudo_log_trans(), breaks = breaks, lim = lim) +
  facet_grid(~method, scales = "free", space = "free") +
  labs(x = "", y = "no. of splits", col=expression(N)) + 
  geom_hline(yintercept=3, col = "darkgray") + 
  theme(axis.title.x=element_blank(), axis.text.x=element_blank()) +
  labs(fill=expression(N)) +
  stat_summary(aes(group=N), position=position_dodge(1), fun="mean", 
               col = "black", shape=16, size = .4) 
@
\end{subfigure}
\begin{subfigure}{1.25\textwidth}
<<echo=FALSE, fig=TRUE, height=1.5>>=
ggplot(tmp2, aes(x=random, y=tree_size)) +
  geom_boxplot(aes(fill = sigma_int), position=position_dodge(1), 
               alpha = .5, width = .6, outlier.shape = NA, coef=NULL) + 
  geom_count(aes(group=sigma_int), position=position_dodge(1), 
             colour = "black", fill=NA, show.legend = FALSE, alpha = .15) +
  scale_y_continuous(trans = pseudo_log_trans(), 
                     breaks = breaks, limits = lim) +
  facet_grid(~method, scales = "free", space = "free") +
  labs(x = "", y = "no. of splits", col=expression(sigma^2~(b[0]))) +
  geom_hline(yintercept=3, col = "darkgray") + 
  theme(axis.title.x=element_blank(),axis.text.x=element_blank(),
        strip.background = element_blank(), strip.text.x = element_blank()) +
  labs(fill=expression(sigma[b[0]]^2)) +
  stat_summary(aes(group=sigma_int), position=position_dodge(1), fun="mean", 
               col = "black", shape=16, size = .4)
@
\end{subfigure}
\begin{subfigure}{1.25\textwidth}
<<echo=FALSE, fig=TRUE, height=1.5>>=
ggplot(tmp2, aes(x=random, y=tree_size)) +
  geom_boxplot(aes(fill = p_noise), position=position_dodge(1), 
               alpha = .5, width = .6, outlier.shape = NA, coef=NULL) + 
  geom_count(aes(group=p_noise), position=position_dodge(1), 
             colour = "black", fill=NA, show.legend = FALSE, alpha = .15) +
  facet_grid(~method, scales = "free", space = "free") +
  labs(x = "", y = "no. of splits", col = expression(p[noise])) +
  geom_hline(yintercept=3, col = "darkgray") +
  theme(axis.title.x=element_blank(),axis.text.x=element_blank(),
        strip.background = element_blank(), strip.text.x = element_blank()) +
  labs(fill=expression(p[noise])) +
  scale_y_continuous(trans = pseudo_log_trans(), breaks = breaks, lim = lim) +
  stat_summary(aes(group=p_noise), position=position_dodge(1), fun="mean", 
               col = "black", shape=16, size = .4)
@
\end{subfigure}
\begin{subfigure}{1.25\textwidth}
<<echo=FALSE, fig=TRUE, height=1.5>>=
ggplot(tmp2, aes(x=random, y=tree_size)) +
  geom_boxplot(aes(fill = rho), position=position_dodge(1), 
               alpha = .5, width = .6, outlier.shape = NA, coef=NULL) + 
  geom_count(aes(group=rho), position=position_dodge(1), colour = "black", 
             fill=NA, show.legend = FALSE, alpha = .15) +
  scale_y_continuous(trans = pseudo_log_trans(), breaks = breaks, lim = lim) +
  facet_grid(~method, scales = "free", space = "free") +
  labs(x = "", y = "no. of splits", col=expression(rho)) +
  geom_hline(yintercept=3, col = "darkgray") + 
  theme(axis.title.x=element_blank(),axis.text.x=element_blank(),
        strip.background = element_blank(), strip.text.x = element_blank()) +
  labs(fill=expression(rho)) + 
  stat_summary(aes(group=rho), position=position_dodge(1), fun="mean", 
               col = "black", shape=16, size = .4)
@
\end{subfigure}
\begin{subfigure}{1.25\textwidth}
<<echo=FALSE, fig=TRUE, height=2.0>>=
ggplot(tmp2, aes(x=random, y=tree_size)) +
  geom_boxplot(aes(fill = sigma_slope), 
               position=position_dodge(1), alpha = .5, width = .6,
               outlier.shape = NA, coef=NULL) + 
  geom_count(aes(group=sigma_slope), 
             position=position_dodge(1), colour = "black", fill=NA, show.legend = FALSE, alpha = .15) +
  scale_y_continuous(trans = pseudo_log_trans(), 
                     breaks = breaks, lim = lim) +
  facet_grid(~method, scales = "free", space = "free") +
  labs(x = "", y = "no. of splits", col=expression(sigma[b[1]]^2)) +
  geom_hline(yintercept=3, col = "darkgray") +
  theme(strip.background = element_blank(), strip.text.x = element_blank()) +
  labs(fill=expression(sigma[b[1]]^2)) + 
  scale_x_discrete(labels = scales::parse_format()) +
  stat_summary(aes(group=sigma_slope), position=position_dodge(1), fun="mean", 
               col = "black", shape=16, size = .4)
theme_set(theme_bw(base_size = 11))
@
\end{subfigure}%
\vspace{-.6cm}%
{ \singlespacing \footnotesize \textit{Note.} Black dots represent means, gray circles represent counts, dark gray horizontal lines represent true number of splits~3. Distances on $y$-axis are on log scale. $N$ = sample size at level 2; $\sigma_{b_0}^2$ = variance of random intercept; $\sigma_{b_1}^2$ = variance of random slope; $p_{noise}$ = number of noise variables; $\rho$ = correlation between partitioning variables.}
\label{fig:tree_sizes_interact}
\end{figure}







\FloatBarrier


\end{document}






<<echo=FALSE, eval=FALSE>>=
###############################
##
## Check estimation problems
##  for longRPart
##

load("notes, N = 250, Math.Rda")
math_notes <- cbind(notes, N = 250)
load("notes, N = 1000, Math.Rda")
math_notes <- rbind(math_notes, cbind(notes, N = 1000))
sapply(math_notes, function(x) table(is.na(x)))
## 8 Warnings for LM(M)tree and longRPart: level of race removed from test data
## 76 Warnings for longRPart_s: 
sapply(math_notes, function(x) table(x, useNA = "ifany"))
## 4 warnings for race omitted from test data
## 76-4=72 warnings for 1, 2 or 3 nodes estimation not converged

load("notes, N = 250, Reading.Rda")
read_notes <- cbind(notes, N = 250)
load("notes, N = 1000, Reading.Rda")
read_notes <- rbind(read_notes, cbind(notes, N = 1000))
sapply(read_notes, function(x) table(is.na(x)))
## 18 Warnings for LM(M)tree and longRPart: level of race removed from test data
## 80 Warnings for longRPart_s: 
sapply(read_notes, function(x) table(x, useNA = "ifany"))
## 11 warnings for race omitted from test data
## 80-11=69 warnings for 1 or 2 nodes estimation not converged

load("notes, N = 250, Science.Rda")
scie_notes <- cbind(notes, N = 250)
load("notes, N = 1000, Science.Rda")
scie_notes <- rbind(scie_notes, cbind(notes, N = 1000))
sapply(scie_notes, function(x) table(is.na(x)))
## 18 Warnings for LM(M)tree and longRPart: level of race removed from test data
## 82 Warnings for longRPart_s: 
sapply(scie_notes, function(x) table(x, useNA = "ifany"))
## 12 warnings for race omitted from test data
## 82-12=70 warnings for 1, 2 or 3 nodes estimation not converged

## How does this correspond to MSEs?
sapply(math_MSEs, function(x) table(is.na(x), math_MSEs$N))
sapply(read_MSEs, function(x) table(is.na(x), read_MSEs$N))
sapply(scie_MSEs, function(x) table(is.na(x), scie_MSEs$N))
## Oops, we got MSE anyway

## Identify a tree where longRPart did not converge in a node
tapply(math_sizes$longRPart_s, factor(is.na(math_notes$longRPart_s)), 
       table, useNA = "ifany")
tapply(math_sizes$longRPart_s, factor(is.na(math_notes$longRPart_s)), 
       mean, na.rm = TRUE)
## Non-converged nodes are in larger trees, this makes sense (smaller nodes)
tapply(math_MSEs$longRPart_s, factor(is.na(math_notes$longRPart_s)), 
       table, useNA = "ifany")
tapply(math_MSEs$longRPart_s, factor(is.na(math_notes$longRPart_s)), 
       mean, na.rm = TRUE)
tapply(math_MSEs$longRPart_s, factor(is.na(math_notes$longRPart_s)), 
       sd, na.rm = TRUE)

## [6] "NANode 0.221767719908376did not converge. "      
math_sizes[6,]
math_MSEs[6,]

## Explanation: If lme model did not converge, no predictions were
##   generated and written to preds. Thus, predictions from last lme
##   were used for those nodes, which is always the lme with random 
## intercepts only

## Illustrate by refitting trees:
load("bag_ids, N = 250, Math.Rda")
library("longRPart")

## Prepare formulas and data
randomFormula <-  ~ 1 | CHILDID
randomFormula_s <- ~ (1 + months) | CHILDID
fixedFormula <- score ~ months
rPartFormula <- ~ GENDER + RACE + WKSESL + C1GMOTOR + C1FMOTOR + 
  T1INTERN + T1EXTERN + T1INTERP + T1CONTRO + P1FIRKDG + AGEBASELINE
resp <- "Math"
load(paste0(resp, " ability data.Rdata"))
resp_short <- ifelse(resp == "Math", "math", ifelse(resp == "Reading", "read", "scie"))
data <- get(paste0(resp_short, "data"))
data$CHILDID <- factor(data$CHILDID)
data$GENDER <- factor(data$GENDER)
data$asmtmm <- factor(data$asmtmm)

## Set up train and test data  
train <- data[data$CHILDID %in% bag_ids[ , 6], ]
test <- data[!data$CHILDID %in% bag_ids[ , 6], ]
## Remove obs with levels of race in test are also in train
levs <- unique(test$RACE) %in% unique(train$RACE)
if (!all(levs)) {
  test <- test[-which(!test$RACE %in% unique(test$RACE)[levs]), ]
  print(paste0(notes[i, method], "Levels or race omitted from test data: ", 
               unique(test$race)[!levs]))
}

## Fit longRPart
lrpdata <- train[ , c("CHILDID", "months", "score", "GENDER", "RACE",
                      "WKSESL", "C1GMOTOR", "C1FMOTOR", "T1INTERN",
                      "T1EXTERN", "T1INTERP", "T1CONTRO", "P1FIRKDG",
                      "AGEBASELINE")]
lrpdata$CHILDID <- as.numeric(lrpdata$CHILDID)
      
for (form in 1:2) {
  rForm <- list(randomFormula, randomFormula_s)[[form]]
  method <- c("longRPart", "longRPart_s")[form]
  system.time(
    tree <- try(longRPart(randomFormula = rForm, lmeFormula = fixedFormula,
                    rPartFormula = rPartFormula, data = lrpdata, 
                    control = rpart.control(maxdepth = 5))))["elapsed"]
  
  ## Check size
  (nrow(tree$frame)-1)/2
  if (form == 1L) math_sizes$longRPart[6] else if (form == 2L) 
    math_sizes$longRPart_s[6]

  if (class(tree) != "try-error") {
  
    ## Get predictions
    train_nodes <- factor(predict(tree))
    test_nodes <- factor(predict(tree, newdata = test),
                         levels = levels(train_nodes))
    preds <- rep(NA, times = nrow(test))
    ## Fit lme to training observations in each node and predict test observations
    for (j in unique(train_nodes)) {
        lme_mod <- try(lme(random = tree$randomFormula,
                           fixed = tree$lmeFormula,
                           data = tree$data[train_nodes == j, ]))
        if (class(lme_mod) != "try-error") {
          preds[test_nodes == j] <- predict(lme_mod, 
                                            newdata = test[test_nodes == j, ], 
                                            level = 0)
        } else {
          print(paste0("Node ", j, "did not converge. "))
        }
    }
    ## Check MSE:
    mean((test$score - preds)^2)  
    if (form == 1L) math_MSEs$longRPart[6] else if (form == 2L) math_MSEs$longRPart_s[6]
  }
}
## MSEs and tree sizes match up

## How many observations are affected?
table(is.na(math_sizes$longRPart), math_sizes$N)
table(is.na(math_sizes$longRPart_s), math_sizes$N)
table(is.na(read_sizes$longRPart), read_sizes$N)
table(is.na(read_sizes$longRPart_s), read_sizes$N)
table(is.na(scie_sizes$longRPart), scie_sizes$N)
table(is.na(scie_sizes$longRPart_s), scie_sizes$N)

table(is.na(math_sizes$longRPart_s), is.na(math_MSEs$longRPart_s))
table(is.na(read_sizes$longRPart_s), is.na(read_MSEs$longRPart_s))
table(is.na(scie_sizes$longRPart_s), is.na(scie_MSEs$longRPart_s))

## How strong is the influence?

## Let's say on average, 2 nodes are affected (mean of 1:3)

## Estimate total # of level-2 obs affected divide by total #:
math_affected <- sum(2 * math_sizes$N[!is.na(math_notes$longRPart_s)] / 
  (math_sizes$longRPart_s[!is.na(math_notes$longRPart_s)]+1))
math_affected / sum(math_sizes$N)
## [1] 0.06443202

read_affected <- sum(2 * read_sizes$N[!is.na(read_notes$longRPart_s)] / 
  (read_sizes$longRPart_s[!is.na(read_notes$longRPart_s)]+1))
read_affected / sum(read_sizes$N)
## [1] 0.111979

scie_affected <- sum(2 * scie_sizes$N[!is.na(scie_notes$longRPart_s)] / 
  (scie_sizes$longRPart_s[!is.na(scie_notes$longRPart_s)]+1), na.rm = TRUE)
scie_affected / sum(scie_sizes$N)
## [1] 0.2531524

## Ideally, would refit all longRPart_s trees which had convergence errors
## in one of the nodes. And then use NA for prediction, or refit a model
## without random slopes. This might paint a slightly difference picture.
## However, I do not expect dramatic differences, given that results
## for longRPart and longRPart_s are quite similar, and seem to differ
## most for science, which was most strongly affected by the mistake.
@



%\begin{figure}[h]
%\caption{Computation times for partitioning math, reading and science ability trajectories.}
%\begin{subfigure}{.5\textwidth}
<<eval=FALSE, echo=FALSE, fig=TRUE, height=3.25, width=3>>=
load("times, N = 250, Reading.Rda")
read_times <- cbind(times, N = 250)
load("times, N = 1000, Reading.Rda")
read_times <- rbind(read_times, cbind(times, N = 1000))

load("times, N = 250, Math.Rda")
math_times <- cbind(times, N = 250)
load("times, N = 1000, Math.Rda")
math_times <- rbind(math_times, cbind(times, N = 1000))

load("times, N = 250, Science.Rda")
scie_times <- cbind(times, N = 250)
load("times, N = 1000, Science.Rda")
scie_times <- rbind(scie_times, cbind(times, N = 1000))


## Combine all LMM trees
ECLSK_times <- rbind(stack(math_times[ , -c(6, 10)]),
                     stack(read_times[ , -c(6, 10)]),
                     stack(scie_times[ , -c(6, 10)]))
ECLSK_times$ind <- as.character(ECLSK_times$ind)
ECLSK_times$ind[grepl("LMMtree", ECLSK_times$ind)]<- "LMM tree"
ECLSK_times$ind[grepl("longRPart", ECLSK_times$ind)]<- "longRPart"
ECLSK_times$ind[ECLSK_times$ind == "LMtree_c"] <- "LM tree"
ECLSK_times$ind <- factor(ECLSK_times$ind)

## Compute summary stats
mean_times <- with(ECLSK_times, tapply(values, ind, mean, na.rm=TRUE))
#summ_stats <- data.frame(M = sprintf(mean_times, fmt = '%#.2f'),
#                         ind = names(mean_times), y = 11000)
## Plot
# ggplot(ECLSK_times) +
#   geom_violin(aes(x=ind, y=values), alpha = .5, fill = "gray", width=.7) +
#   scale_y_continuous(trans = "log", limits = c(0.1, 12000),
#                      breaks = c(.01, .1, 1, 10, 100, 1000)) +
#   labs(x = "", y = "Computation time (in seconds)") +
#   geom_text(data = summ_stats, aes(x=ind, y=y, label = M),
#             size = 3, col = "gray44") +
#   labs(tag = "M") + theme(plot.tag.position = c(.12, .95),
#                           plot.tag = element_text(size = 10))
@
%\end{subfigure}
%\label{fig:application_times}
%\end{figure}




\section{Notes}

\subsection{Fully local mixed-effects RPMs}

The LongCART, longRPart, longPart2, non-linear longitudinal IT and SEM trees methods estimate all model parameters (fixed and random) locally, that is, separately in every (terminal) node. SEM trees provide a possible exception, by allowing the user to restrict one or more parameters to be equal across nodes. Such local estimation of the full parametric model increases the total number of estimated parameters. On the one hand, this allows the node-specific models to flexibly accommodate the observed datapoints, which may be useful for example in case of heteroscedasticity, but at the same time may increase the likelihood of overfitting. 

LongRPart, non-linear longitudinal IT and SEM trees all employ likelihood-ratio tests for split selection. That is, in the current node, a likelihood-ratio is computed for every possible splitting variable and value pair. This brings a heavy computational load, as computing the likelihood ratios requires fitting the full parametric model for each of the possible splits, in each of the two resulting daughter nodes. This may also introduce variable selection bias towards variables with a larger number of possible split points \citep{ShihyTsai04, Shih04}. SEM trees \cite{BranyOert13} however allow for mitigating this bias through a two-stage procedure similar to that of \cite{LohyShih97} and \cite{KimyLoh01}.

MELT and LongCART also perform local estimation of all mixed-effects model parameters, but take a different approach to partitioning. MELT partitions the observations in the current node by fitting a basic and complex model: the basic model comprises an LMM with random intercepts and fixed slope of time (polynomial functions of time may also be included); the complex model additionally comprises random slope(s) of time. Node impurity is defined as the sum of the squared difference between the random slope from the complex model and the fixed slope from the basis model. The sign of the difference is used for splitting variable and value selection, following the approach of \cite{Loh09}. This likely reduces the computational burden compared to SEM trees, longRPart and IT, and also mitigates variable selection bias. Possible disadvantages are that MELT does not account for the precision of the random slope predictions, and does not allow for generating individual predictions, but only predicting the shape and magnitude of growth trajectories.   

LongCART takes an approach related to that of \cite{ZeilyHoth08}. It fits the full mixed-effects model in the current node, and employs score-based parameter stability tests to assess the instability of the fixed-effects parameters of the mixed-effects model w.r.t. each of the partitioning variables \citep{KundyHare19}. They propose different parameter stability tests for continuous and categorical partitioning variables. The p-values of the parameter stability tests are used to determine whether splitting should be continued, and if so, which variable should be selected for splitting. In the next step, the cut-point for the variable is selected that will yield the largest decrease in AIC. This two-step approach aims to mitigate overfitting as well as a selection bias towards variables with a larger number of possible split points. In the simulation study of \cite{KundyHare19}, LongCART outperforms GLMM trees, MVRPART, and rpart- and ctree-based RE-EM Trees, in terms of predictive accuracy and tree size. \footnote{GLMM performs pretty badly, is outperformed by MVPART also. Probably, cluster argument was not employed for GLMM tree. Also, they did not evaluate variable selection bias. Also, LongCART package only supports continuous outcomes.}

Finally, the GUIDE algorithm for piecewise linear models as proposed \cite{Loh02} can also be applied for fitting piecewise (i.e., local) linear mixed-effects models. The algorithm proposed by \cite{Loh02} takes the residuals from a linear regression model fitted in the current node. $\chi^2$ tests are used to quantify the assocation between the partitioning variables and the sign of the residuals (numerical partitioning variables are converted to categorical variables by cutting the variable into groups at the sample quartiles). The partitioning variable with the lowest p-value is selected for splitting. Although there is currently no evidence available on the performance of GUIDE for partitioning LGCMs, the GUIDE approach can be employed by simply taking the residuals from a linear mixed-effects regression model, instead of a linear regression model. 
 

\subsection{GEE-based RPMs}

Both longitudinal IT and GEE-based decision trees employ GEE instead of mixed-effects models. GEE-based decision trees \cite{Lee05} extend the generalized regression tree approach of \cite{ChauyLo95} to multivariate outcomes. A marginal GEE model is fitted in the current node and the average Pearson residual is computed for every level-2 unit. Units are grouped into two groups, one with non-negative and one with negative residuals, and two-sample t-test are performed to quantify differences between the two groups along each covariate axis. \footnote{The paper also mentions chi-square tests, but it is unclear if these are actually employed. They could be used for categorical covariates, but then p-values need to be used for selecting the splitting variable. \cite{ChauyLo95} transformed categorical covariates to ordered ones to allow for using t-tests.} The covariate selected to split the node is the one with the largest absolute t-statistic. The cut-point for the selected covariate is the weighted average of the two group means. Splitting is continued by repeating these steps in each of the resulting daughter nodes, until the p-value exceeds a pre-specified level.

Longitudinal IT \cite{SuyMene11} employs Wald tests for split selection. It fits a GEE model in the current node, and quantifies the strength of potential treatment-subgroup interactions for every possible split through a Wald statistic, which employs a sandwich estimator for the covariance matrix of the parameter estimates. The split with the highest Wald statistic is selected for splitting. After fitting a large initial tree, pruning is performed through cross validation to select a minimum value for the Wald statistic (i.e., a node's contribution to the interaction complexity of the tree).

Similar GEE approaches were developed for random forests. Both \cite{CalhyLevi20} and \cite{Mart15} found that the standard random-forest method favors splits based on covariates measured at level 2. \cite{CalhyLevi20} attribute this to the use of the Gini index, and successfully mitigate the bias by employing a robust Wald statistics for split selection instead.


\subsection{Partly global mixed-effects RPMs}

Several mixed-effects RPMs have been developed which take a partly global estimation approach \citep{HajjyBell11, HajjyLaro17, SelaySimo12, FuySimo15, SpeiyWolf18}. These methods estimate the random effects globally, using all observations, while estimating the fixed effects locally, i.e., separately in every subgroup. \cite{FokkySmit18} extended this approach to allow for GLMM-based recursive partitioning, which allows for estimating GLMs comprising one or more pre-specified predictor variables in the terminal nodes. This allows, for example, for partitioning LGCMs, while accounting for dependence of observations from the same subject through (global) estimation of random effects.

To estimate the full model, the partly global methods iterate between I) estimating the subgroup structure (recursive partition), given the current estimates of the random effects; and II) estimating the global effects, given the current subgroup structure. This procedure may require several iterations, but generally the computational load remains relatively low, as split selection does not require fitting a full parametric model for every possible split point. Note however, that if the partitioning algorithm employed in step I) exhibits variable selection bias, the full model will also exhibit this bias. 


\subsection{List of decision-tree methods for longitudinal data}

\cite{HajjyBell11} proposed regression trees for clustered data.

\cite{HajjyBell14} proposed mixed-effects random forests for clustered data.

\cite{HajjyLaro17} proposed generalized regression trees for clustered data.

\cite{EoyCho14} proposed tree-structured mixed-effects regression modeling for longitudinal data.

\cite{Simo13} proposed goodness-of-fit tests for mixed-effects regression tree models that can be used to test for non-linearity of the fixed effects or heteroscedasticity of the errors. 

\cite{SelaySimo12} proposed regression trees for clustered data.

\cite{FuySimo15} proposed unbiased regression trees for longitudinal and clustered data.

\cite{SpeiyWolf18} proposed a CART-based method for longitudinal and clustered data with binary outcomes. \cite{SpeiyWolf19} proposed random-forest method for longitudinal and clustered data with binary outcomes. The authors compared their tree and random forest methods also with GLMM trees, which were outperformed in the presence of strong random effects. However, this result may possibly be explained by the authors using the default settings of GLMM trees, where estimations is initialized by estimating the tree. Initializing estimation with the random effects may likely be beneficial in the presence of strong random effects.

For longitudinal data observed at very many times, \cite{YuyLamb99} treated each response vector as a random function and reduced the dimensionality of the data by fitting each trajectory with a spline curve. Then they used the estimated coefficients of the basis functions as multivariate responses to fit a regression tree model.

The method of \cite{StegyJaco18} allows for fitting non-linear mixed-effects models \cite{LindyBate90} in each of the tree nodes. The method of \cite{WeiyLiu20} allows for fitting splines \citep[i.e., natural cubic spline bases formed by B-splines, see][]{GyoryLasz06}.

\cite{Lee05} proposed generalized multivariate decision trees by using GEE.

From \cite{Loh14}: \cite{Sega92} first proposed a recursive partitioning method for longitudinal data by using as node impurity a function of the likelihood of an autoregressive or compound symmetry model. \cite{AbdoyLeBl02} used the same approach, but with a likelihood-ratio test statistic as impurity function. 

\cite{Zhan98} extended \cite{Sega92} to multiple binary response variables, using as node impurity the log-likelihood of an exponential family distribution that depends only on the linear terms and the sum of second-order products of the responses. \cite{ZhanyYe08} applied the technique to ordinal responses. Their approach requires covariance matrices to be computed at every node. 

\cite{WeiyLiu20} propose a tree-structured subgroup identification method IT-LT, that combines mixed-effects models with regression splines, in order to detect treatment subgroups with differential patterns of change over time. They find that in the absence of additive effects of partitioning variables, IT-LT and glmertree perform similarly. In the presence of additive effects, glmertree performs more poorly. 

\cite{DeAt02} avoided the problem of covariance estimation by using as node impurity the total sum of squared deviations from the mean across the response variables. \cite{LarsySpec04} used the Mahalanobis distance, but estimated the covariance matrix from the whole data set. (And whereas \cite{DeAt02} does not refer to the work of \cite{Sega92}, \cite{LarsySpec04} do.)

"Hsiao \& Shih (2007) showed that multivariate extensions of CART are biased toward selecting  variables  that  allow  more  splits.  They  proposed  using  chi-squared  tests  of  conditional independence (conditioning on the components of the response vector) of residual signs versus grouped X values to select the split variables. The method may lack power if the effects of the X variables are not in the same direction across all the Y variables. Lee (2005) applied the GUIDE approach to multiple responses with ordered X variables by  fitting  a  generalized  estimating  equation  model  to  the  data  in  each  node  and  taking  the average  of  the  Pearson  residuals  over  the  responses  variables,  for  each  observation.  The observations  are  classified  into  two  groups  according  to  the  signs  of  the  average  residuals, and  the X with  the  smallest p -value  from  two-sample t -tests  is  chosen  to  split  the  node. Although  unbiased,  the  method  is  not  sensitive  to  all  response  trajectory  shapes.  Loh  \& Zheng (2013) solved this problem by using the residual vector patterns, rather than their averages,  to  choose  the  split  variables.  The  solution  is  applicable  to  data  observed  at  random time points."

Further, work of Ciampi needs to be mentioned?!

\cite{DeAt02} extended CART to multivariate continuous outcomes. But this was actually already proposed by \cite{GillyShel74} "a new inductive statistical technique, MAID-M, performs predictive modeling for a multivariate criterion from a set of predictor variables. Based on additive multivariate measures of association, it identifies the smallest combination of predictor variables accounting for a maximal proportion of the variation space of a given set of criterion variables."
  
As noted by \cite{SelaySimo12}, the methods of \cite{DeAt02} and \cite{Sega92} have a substantial drawback for the analysis of longitudinal data, in that they require the same timepoints have been observed for every subject, and do not allow for extrapolation to future timepoints, i.e., prediction on timepoints not observed in the training data.


\cite{BurgyRits15} propose tree-based varying coefficient regression. It builds on the MOB algorithm, in order to fit a varying coefficient model. This redesign involves two adjustments to MOB: 1) Inspired by the algorithms of Hajjem et al. (2011) and Sela and Simonoff (2012), our algorithm builds a closed model that consists of a tree-structured fixed-effects component and a global random effect component. By doing so, the observations of an individual are connected with the single set of corresponding random coefficients, regardless of in which nodes these observations fall, allowing for splits on time-varying covariates. 2) The coefficient constancy tests for the variable and tree size selection of MOB are adjusted, so that a separate partition is fitted for each fixed-effects coefficient.

The algorithm does not include auto-correlated errors.

\cite{BurgyRits15} distinguish between unconnected mixed effects tree methods (where separate mixed models are fitted in each node) and connected mixed-effects tree methods (where fixed effects are estimated locally and random effects globally). 

\cite{BurgyRits17} do not have a mixed-effects part yet, only varying coefficients. The main difference with GLM trees is that for each coefficients of the GLM, a different model is fitted.

\cite{ChauyLo95} propose generalized regression trees, which fit a low-order polynomial in each node using maximum likelihood, and split each nodeusing a criterion based on the sign of the residuals. In principle, this allows for modeling longitudinal data, but the examples in the manuscript do not have repeated measures within subjects. Does, there is no correlated data.

\subsection{"Global" mixed-effects partitioning methods}

\cite{StegyJaco18} recently proposed an extension to the \cite{AbdoyLeBl02} to nonlinear mixed-effects models. Their method allows for partitioning based on cluster-level variables only. The implementation does currently not allow for specifying a within-group correlation structure, but defaults to assuming no within-group correlations. 

\cite{StegyJaco18} note that a limitation of \cite{AbdoyLeBl02}, SEM and glmertree do not allow for observation-level covariates. They also note that the  algorithms of \cite{HajjyBell11} and \cite{SelaySimo12} extend the work of \cite{AbdoyLeBl02} to simultaneously handle observation-level and cluster-level predictors. This is incorrect, as glmertree allows for partitioning on both individual and cluster-level variables. Furthermore, glmertree, \cite{HajjyBell11} and \cite{SelaySimo12} are different from the algorithms of \cite{AbdoyLeBl02} and \cite{StegyJaco18}, in that the latter two estimate the random effects locally, within every node, whereas the former three estimate the random effects globally. I believe this global estimation of random effects is what allows for splitting on observation- and cluster level covariates. If the whole model is re-estimated locally, I am not sure whether one could or should partition based on observation-level covariates. 

One advantage of RE-EMtree is that it also employs the nlme package, like \cite{AbdoyLeBl02} and \cite{StegyJaco18}. It thereby allows for specifying an autocorrelation structure within the errors (i.e., allows the error covariance matrix to be nondiagonal). glmertree employs lme4, which does not allow for specifying the correlation structure. The current implementation of \cite{StegyJaco18} employs nlme, but does not allow for specifying the correlation structure.

\cite{UsamyHaye17} performed a simulation study comparing the performance of LGCM-based SEM trees with LGCM mixtures, especially under model misspecification. They conclude that the Bonferroni method may outperform the 5-fold CV method for SEM trees, especially when the maximum number of possible partitions (subgroups?) and total sample size are small. Accurate recovery of the number of classes by SEM trees is strongly related to the agreement of the covariate with it true latent profile, and the influence of sample size is also notable (direction?). Agreement rates of .6 and .7 or up are needed for SEM trees to correctly detects classes, regardless of sample size. SEM trees might be very sensitive to model misspecification with respect to the template SEM. More separation between classes yields better class recovery for SEM trees. The impact of model misspecification on LGCMMs is smaller than that on SEM trees. This can be attributed to lower statistical power of oberved covariates in identifying classes with SEM trees.  

\cite{UsamyJaco19} investigated the performance of LGC model-based SEM trees in simulated data. They assessed the performance of SEM trees to correctly identify classes using linear and quadratic LGCM. They conclude that: correct identification of the number of classes are most strongly related to the agreement rate of (observed) covariates with the true latent profile. If covariates are correlated .70 or stronger to the true latent profiles, in many cases SEM trees can uncover heterogeneity more precisely than LGCM mixtures. The larger the number of true classes, the lower the likelihood that the true number of classes is recovered by LGCM-based SEM trees. LGCM-based SEM trees were more robust against model misspecification than latent-change-score SEM trees.

In this context, the finding of \cite{MartyOert15} may be of interest: Growth mixture models outperform simpler clustering algorithms when detecting longitudinal heterogeneity, even with small sample sizes. 




 
\subsection{Latent class growth trees}

\cite{BergySchm17} and \cite{BergyVerm18} used / proposed a method for latent class growth analysis to detect subpopulations that display different growth curves. The method recursively partitions observstions in a manner similar to divisive hierarchical clustering: classes are split until a certain criterion indicates that the fit can no longer be improved.


\subsection{Findings on random forests for multilevel data}

\cite{Mart15} proposed "an extension of the way in which variable importance measures are calculated for CART and random forests. In order to obtain more accurate estimates of variable importance, a simulated cross-validation sample for calculating variable importances is employed, rather than the out-of-bag sample. Simulation results \cite{Mart15} show that this indeed yields a more accurate ordering of variable importance than the traditional OOB approach." (from \cite{Finc15})

\cite{Mart15} Simulation phase results: "Both CART and conditional inference methods showed decreased performance in predictive accuracy and the identification of relevant variables when the ICC was moderate to large and predictors were measured at both levels of the analysis. In particular, both methods had a biased preference for level-2 variables, despite these variables having no simulated relationship with the outcome. While this is to be expected with conditional inference methods that utilize a permutation test framework built on independence assumptions, this finding is  unexpected for CART methods. If all variables are measured at the first level of analysis, however, both CART and conditional inference methods perform as expected, regardless of ICC values."

\cite{Mart15} Application phase results: "Results from three separate applications indicated  that forest methods did not massively outperform a main-effects only model in any application, but it did aid in the potential identification of small effects that deviated from linearity.

In the first application, a very small interaction effect was discovered between the SES and student minority status variables, such that non-minority students appeared to benefit more from having high SES compared to minority students, who appeared to benefit less. 

The second dataset had no evidence for anything more complex than a main effect, which is not surprising given the fact that the dataset was small ($N < 200$), making it more difficult to identify more complex model specifications that were likely to generalize to a future sample.  

The  third  dataset  found  a  small  non-linearity in student prior achievement predicting future achievement.While the deviation from nonlinearity was small, it contributed to the forest models outperforming the main effects only model due to the fact that the impact of prior achievement on future achievement was large. However, because prior achievement explained so much variation in future achievement, it left very little systematic variation to be explained by other measures"

\cite{KarpyHill09} "found that for multilevel data, the individual trees that make up the forest in RF are highly correlated with one another, and that this correlation increases concomitantly with increases in the ICC. In turn the inflated correlation among the trees results in an underestimate of the OOB error." (from \cite{Finc15})


\subsection{Regression trees with time-dependent covariates}

\cite{GaliyMont02} propose a CART-based approach to fitting regression trees (for continuous outcomes), where the split function (impurity measure) is adjusted so as to account for autocorrelation of the repeated observations on the same unit. The method allows for observations of the same unit to end up in different terminal nodes; the terminal nodes are piecewise-constant functions of (time-dependent) covariate.

\cite{PillayCalo03} combined this approach with radial basis function (RBF) networks, in order to smoothen the predictive model. In this approach, the location parameters of the RBF network are determined by fitting regression tree. 

\cite{GaliyPill07} proposed alternative split criteria, based on loss functions minimized by M-estimators \cite{Hube64}. These split criteria are more robust, by downweighing outliers when calculating the measure of within-node impurity. (But this is not really for repeated measures data.) 



\cite{NeisyMatt98} describe the use of mixed-effects models and latent-curve models to explore growth over time, and show that "one framework is often more advantageous to adopt". In a table, they provide an overview of when to prefer one approach over another. They summarize: "the ME approach tends to be most useful for straightforward models (e.g., a simple growth trajectory with one outcome variable), with complex data structures such as smaller samples, time-unstructured data, or multiple levels of nesting requiring more flexibility. Conversely, the LC approach is best suited to complex models with straightforward data structures, such as growth models embedded in larger models, assessments of global model fit, unconstrained time-varying covariates, and complex variance functions."


\subsection{Performance of traditional GEEs}

\cite{HubbyAher10}: "The one caveat to an otherwise straightforward approach is that the number of neighborhoods has to be sufficiently large; the inference is asymptotically correct but not necessarily accurate in smaller samples."

\cite{HubbyAher10} have pointed out that that mixed models involve unverifiable assumptions on the data-generating distribution, which lead to potentially misleading estimates and biased inference. They therefore favor a GEE-type approach, as it involves little assumptions, and just requires the number of level-2 observations to be sufficiently large for robust estimation of standard errors. They also point out the fact that GEE- or mixed-effects parameter estimates in the linear case are equivalent, but not in the binary case.

\subsection{Alternative application datasets}

\cite{Lee05} analyzed epileptic seizure data that is available in as the \verb|epil| dataset in package MASS. The data consist of the number of epileptic seizures in an eight-week baseline period before any treatment, and in each of four two-week treatment periods in which patients received either a placebo or the drug Progabide in addition to other therapy. 

<<eval=FALSE, echo=FALSE, fig.height=4, fig.width=4>>=
library("MASS")
library("glmertree")
gt1 <- glmertree(y ~ period | subject | base + age + trt, family = poisson,
                data = epil)
plot(gt1, "tree", fitted = "marginal")
gt2 <- glmertree(y ~ period | subject | base + age + trt, family = poisson,
                data = epil, ranefstart = TRUE)
plot(gt2, "tree", fitted = "marginal")
gt3 <- glmertree(y ~ period | subject | base + age + trt, family = poisson,
                data = epil, cluster = subject)
plot(gt3, "tree", fitted = "marginal")


library("MASS")
library("glmertree")
gt1 <- glmtree(y ~ period | base + age + trt, family = poisson,
                data = epil)
plot(gt1, "tree", fitted = "marginal")
gt2 <- glmtree(y ~ period | base + age + trt, family = poisson,
                data = epil, ranefstart = TRUE)
plot(gt2, "tree", fitted = "marginal")
gt3 <- glmtree(y ~ period | base + age + trt, family = poisson,
                data = epil, cluster = subject)
plot(gt3, "tree", fitted = "marginal")

@

\cite{Lee05} analyzed RCT data to compare a test treatment and placebo for a respiratory disorder. Patients in each of two centers were randomly assigned to groups receiving the active treatment or a placebo. During treatment, respiratory status was determined at four visits. The data is available as the \verb|resp| dataset in package sanon.

<<eval=FALSE, echo=FALSE, fig.height=4, fig.width=4>>=
library("sanon")
data("resp")
resp$treatment <- factor(resp$treatment)
resp$sex <- factor(resp$sex)
Resp <- data.frame(y = with(resp, c(baseline, visit1, visit2, visit3, visit4)),
                   time = rep(0:4, each = nrow(resp)), age = resp$age, 
                   center = factor(resp$center), 
                   treatment = factor(resp$treatment),
                   sex = factor(resp$sex), subject = 1:nrow(resp))
gt1 <- lmertree(y ~ time | subject | age + treatment + center + sex,
                data = Resp)
plot(gt1, "tree", fitted = "marginal")
gt2 <- lmertree(y ~ time | subject | age + treatment + center + sex,
                data = Resp, cluster = subject)
plot(gt2, "tree", fitted = "marginal")
gt3 <- lmertree(y ~ time | subject | age + treatment + center + sex,
                data = Resp, ranefstart = TRUE)
plot(gt3, "tree", fitted = "marginal")
@


<<eval=FALSE, echo=FALSE>>=
library("semtree")
library("lavaan")


lav_mod <- '
  intercept =~ 1*baseline + 1*visit1 + 1*visit2 + 1*visit3 + 1*visit4
  slope =~ visit1 + 2*visit2 + 3*visit3 + 4*visit4
'
lav_fit <- growth(lav_mod, data = resp, do.fit = TRUE)
tree_data <- resp[ , c("baseline", "visit1", "visit2", "visit3", 
                       "visit4", "age", "center", "treatment", "sex")]
st1 <- semtree(lav_fit, data = resp, 
              predictors = c("age", "center", "treatment", "sex"))
st1
st2 <- semtree(lav_fit, data = resp, 
              predictors = c("age", "center", "treatment", "sex"), 
              control = semtree.control(method = "fair"))
st2
st3 <- semtree(lav_fit, data = resp, 
              predictors = c("age", "center", "treatment", "sex"), 
              control = semtree.control(method = "fair3"))
st3
st4 <- semtree(lav_fit, data = resp, 
              predictors = c("age", "center", "treatment", "sex"), 
              control = semtree.control(method = "naive"))
st4
st5 <- semtree(lav_fit, data = resp, 
              predictors = c("age", "center", "treatment", "sex"), 
              control = semtree.control(method = "cv"))
st5
st6 <- semtree(lav_fit, data = resp, 
              predictors = c("age", "center", "treatment", "sex"), 
              control = semtree.control(method = "score"))
st6

library("OpenMx")
manifests <- c("baseline", "visit1", "visit2", "visit3", "visit4")
latents <- c("I", "S")
mx_mod <- mxModel(model = "LGC", type = "RAM", manifestVars = manifests,
                  latentVars = latents,
                  mxPath(from = "I", to=manifests, arrows = 1, free = FALSE, values = 1),
                  mxPath(from = "S", to=manifests, arrows = 1, free = FALSE, values = 0:4),
                  mxPath(from = latents, to = latents, arrows = 2, free = TRUE, 
                           values = c(.8, .8), labels = c("VarI", "VarS")),
                  mxPath(from = "I", to = "S", arrows = 2, free = TRUE, values = .6, 
                         labels = "Cov_IS"),
                  mxPath(from = manifests, to = manifests, arrows = 2, free = TRUE, values = .8,
                         labels = c("base", "v1", "v2", "v3", "v4")),
                  mxPath(from = "one", to = latents, arrows = 1, free = TRUE, values = .2, 
                         labels = c("mI", "mS")),
                  mxData(observed = resp, type = "raw"))
mx_fit <- mxRun(mx_mod)  
summary(mx_fit)
st7 <- semtree(mx_fit, data = resp, predictors = c("age", "center", "treatment", "sex"), 
               control = semtree.control(method = "score"))
st7

##undebug(semtree)
##debug(growTree)
##undebug(semtree:::naiveSplitScoreTest)

@

\cite{BurgyRits15} analyzed data derived from the British Household Panel Study, to show how the effect of unemployment on self-reported happiness varies across individual life circumstances. The data is included as supplementary material to their paper?

\cite{FuySimo15} analyzed "the wages data, obtained from the UCLA Academic Technology Service website. It contains data on 888 individuals' hourly log wage (response variable) information and corresponding covariate values. It was previously studied by Singer and Willett (2003) and Eo and Cho (2014). The number of observations of each individual range from 1 to 13, so the data are highly unbalanced, with a total of 6402 observations. Eo and Cho (2014) were limited to time-invariant covariates in their applications of GUIDE and MELT to these data, using race (White, Black and Hispanic) and hgc (highest degree completed by each individual). The CART-based REEMtree is also given in Eo and Cho (2014), and it only splits once, on the hgc variable. Fig. 16 ... gives the unbiased REEM tree, which has a more complex structure, broadly similar to that of GUIDE (as presented in Eo and Cho, 2014)."



\subsection{Auto correlation structures}


https://bbolker.github.io/mixedmodels-misc/notes/corr\_braindump.html
