\documentclass[nojss]{jss}
\usepackage{amssymb, amsmath, amsthm, booktabs, thumbpdf}

%% Sweave/vignette information and metadata
%% need no \usepackage{Sweave}
\SweaveOpts{engine = R, eps = FALSE, keep.source = TRUE}
%\VignetteIndexEntry{Evolutionary Learning of Globally Optimal Classification and Regression Trees in R}
%\VignetteDepends{stats,evtree,partykit,mlbench,plotrix,lme4,multcomp,rpart,ctree,xtable}
%\VignetteKeywords{machine learning, classification trees, regression trees, evolutionary algorithms, R}
%\VignettePackage{evtree}

\author{Thomas Grubinger\\ Innsbruck Medical University 
   \And Achim Zeileis\\ Universit\"at Innsbruck 
   \And Karl-Peter Pfeiffer\\ Innsbruck Medical University
}
\Plainauthor{Thomas Grubinger, Achim Zeileis, Karl-Peter Pfeiffer}

\title{\pkg{evtree}: Evolutionary Learning of Globally Optimal Classification and Regression Trees in \proglang{R}}
\Plaintitle{evtree: Evolutionary Learning of Globally Optimal Classification and Regression Trees in R}
\Shorttitle{\pkg{evtree}: Evolutionary Learning of Globally Optimal Trees in \proglang{R}}

\Abstract{
  Commonly used classification and regression tree methods like the CART
  algorithm are recursive partitioning methods that build the model in a forward
  stepwise search. Although this approach is known to be an efficient heuristic,
  the results of recursive tree methods are only locally optimal, as splits are
  chosen to maximize homogeneity at the next step only. An alternative way to
  search over the parameter space of trees is to use global optimization methods
  like evolutionary algorithms. This paper describes the \pkg{evtree} package,
  which implements an evolutionary algorithm for learning globally optimal
  classification and regression trees in \proglang{R}. CPU and memory-intensive
  tasks are fully computed in \proglang{C++} while the \pkg{partykit}
  \citep{hothorn2009partykit} package is leveraged to represent the resulting
  trees in \proglang{R}, providing unified infrastructure for summaries,
  visualizations, and predictions. \code{evtree} is compared to \code{rpart}
  \citep{therneau1997introduction}, the open-source CART implementation, and
  \code{ctree} \citep{hothorn2006urp} (conditional inference trees). On 20
  evaluated data-sets, including several benchmark problems from the UCI machine
  learning repository, \code{evtree} models offered at least similar and most of
  the time increased predictive accuracy.  
}

\Keywords{machine learning, classification trees, regression trees, evolutionary algorithms, \proglang{R}}
\Plainkeywords{machine learning, classification trees, regression trees, evolutionary algorithms, R}

\Address{
  Thomas Grubinger, Karl-Peter Pfeiffer\\
  Innsbruck Medical University\\
  Department for Medical Statistics, Informatics and Health Economics\\
  6020, Innsbruck, Austria\\
  E-mail: \email{thomas.grubinger@i-med.ac.at}\\
  URL: \url{http://www.i-med.ac.at/msig/mitarbeiter/grubinger}\\

  Achim Zeileis \\
  Universit\"at Innsbruck\\
  Department of Statistics\\
  6020, Innsbruck, Austria\\
  E-mail: \email{Achim.Zeileis@R-project.org}\\
  URL: \url{http://eeecon.uibk.ac.at/~zeileis/}
}


\begin{document}
 
\section{Introduction}

Classification and regression trees are commonly applied for exploration and modeling of complex data. They are able to handle strongly nonlinear relationships with high order interactions and different variable types. The resulting model can be interpreted as a tree structure, which gives a compact and intuitive representation. Commonly used classification and regression tree algorithms, including \textit{CART} \citep{breiman1984cart} and \textit{C4.5} \citep{quinlan1993}, use a greedy heuristic, where split rules are selected in a forward stepwise search. The data is recursively split into two groups. The split rule at each internal node is selected to maximize the homogeneity of its two child nodes, without consideration of nodes further down in the tree. Split rules selected by this top-down heuristic are only locally optimal, as splits are chosen to maximize homogeneity at the next step only. Nonetheless, the greedy heuristic is computationally efficient and often yields reasonably good results \citep{murthy1995dti}. However, for some problems, greedily induced trees can be far from the optimal solution, and a global search over the tree's parameter space can lead to much more compact and accurate models.

One possibility to build globally optimal trees is to use stochastic optimization methods like evolutionary algorithms. Evolutionary algorithms are methods that are inspired by natural Darwinian evolution. Important concepts include inheritance, mutation, and natural selection. They are population based, i.e., a whole collection of candidate solutions are processed simultaneously. The initial population is either initialized randomly, or by a problem dependent algorithm. The individuals in the populations are then iteratively modified by \textit{variation operators}. For tree algorithms variation operators can be applied to modify the tree structure and the tests that are applied in the internal nodes. Variation operators that apply to single individuals are called \textit{mutation}, while operators that merge information from different solutions are called \textit{crossover}. The survivor selection process favors individuals that perform well according to some quality criteria, which is called \textit{fitness function} or \textit{evaluation function}. In this evolutionary process the mean quality of the population increases over time \citep{baeck,eiben2003iec}.  

In practice, evolutionary algorithm and other stochastic optimization methods are rarely used in decision tree induction. One reason is that they are computationally much more demanding than a forward stepwise search. Perhaps another important reason is their lack of availability in major software packages. \proglang{R} \citep{team2011r} provides several packages with tree-based methods. \pkg{rpart} \citep{therneau1997introduction} is an \proglang{R} implementations of CART. Package \pkg{party} provides two tree-based algorithms with unbiased variable selection and statistical stopping criteria. Several tree algorithms are available in \proglang{Weka} \citep{weka}. Package \pkg{RWeka} \citep{rweka} provides an interface to \proglang{Weka} for \proglang{R} users. Decision tree methods in \proglang{Weka} include implementations of C4.5 and \textit{M5} \citep{quinlan1992lcc}. However, all of these methods are based on a top-down heuristic. An exception is the \pkg{LogicReg} package, which implements logic regression. This method is applicable for regression problems with binary covariates. The resulting models of logic regression can be visualized as logic trees. 

In this article, we present \pkg{evtree}, a software package that uses evolutionary methods for learning globally optimal classification and regression trees in \proglang{R}. The package is available from the Comprehensive \proglang{R} Archive Network at \url{http://CRAN.R-project.org/package=evtree}. An important difference to comparable algorithms is the survivor selection mechanism, where we use a simple $(1+1)$ selection strategy. That is, one parent solution competes with one offspring for a place in the subsequent population. As further discussed in Section~\ref{evo}, this design offers computational advantages for the application of classification and regression trees. Furthermore, we argue that the chosen selection mechanism helps to avoid premature convergence. 

The remainder of this paper is structured as follows. Section~\ref{got} describes the problem of learning globally optimal decision trees and contrasts it to the only locally optimal top-down heuristic that is utilized by recursive partitioning algorithms. In Section~\ref{desc} the \code{evtree} algorithm will be introduced. Section~\ref{impl} addresses implementation details and gives an overview of the implemented functions. In Section~\ref{comp} \code{evtree} is compared to the two commonly used algorithms \code{rpart} and \code{ctree}, from package \pkg{party}, which are the implementations of CART and conditional inference trees in \proglang{R}. The comparison comprises 14 benchmark data-sets, 3 real-world data-sets and 3 simulated data-sets. It will be shown that \code{evtree} models are often significantly more accurate compared to models from the two recursive partitioning methods. Finally, in Section~\ref{conc}, we will summarize and draw conclusions on the implementation and the performance of our algorithm.


\section{Globally and locally optimal decision trees}
\label{got}

The aim of classification and regression tree analysis can be stated by explaining a response variable $Y$ by a vector of $P$ predictor variables $X=(X_1,...,X_P)$. For classification trees the output variable $Y$ is qualitative and for regression trees $Y$ is quantitative. Tree-based methods first partition the input space $X$ into a set of $M$ rectangular regions $R_m$ where $m = 1,...,M$. In the second step a model is fit to each region. The model fitted to the distribution of $\{Y \in R_m\}$ can be any proper measure of the location, e.g. the median, or dispersion, e.g. the variance, of a random variable. Typically the mode is used for classification trees and the arithmetic mean is applied for regression trees. 

In this work only binary tree models are considered, where regions are split into two halves at a time. However, this is not a big issue in global tree induction. As binary splits are searched globally and any multiway split can be represented by a series of binary splits. Based on a set of split rules, binary classification and regression trees recursively partition the input space $X$ into two regions. Samples that fulfill the criterion of a split rule propagate down into the descendant left node and the other instances into the right node. Split points of a tree are termed \textit{internal nodes} and nodes without successors are called \textit{terminal nodes}. A binary tree with $M$ terminal nodes has $M-1$ internal nodes. Generally, the maximum number of terminal nodes $M_{max}=\infty$. In practice, $M_{max}$ is restricted by the maximum tree depth $D_{max}$ as $M_{max} = 2^{D_{max}}-1$. When the trees are constrained to have a minimum number of observations in the internal and/or terminal nodes, $M_{max}$ is also restricted by the finiteness of the training data. 

A binary tree model can be uniquely defined by the positions of the internal nodes, being a subset of $\{1,...,M_{max}-1\}$, and the corresponding split rules. Let the positions of the internal nodes of a tree with $M-1$ terminal nodes be indicated by $n_1,...,n_{M-1}$. The positions $\{n_1,...,n_{M-1}\} \in \{1,...,M_{max}-1\}$ and $n_i < n_j \Leftrightarrow i < j$. Let the variable selected for splitting, at a new position $r \in n_1,...,n_{M_{max}-1}$, be denoted as $v_r$, where $v_r=1,...,P$. Depending on the domain of $X_{v_r}$, a split point or a set of categories has to be selected, which we denote as $s_r$. If $X_{v_r}$ is numerical or ordinal, a binary split rule is defined as $X_{v_r} < s_r$. If $X_{v_r}$ has $u$ unique values, then there are $u-1$ possible split points and $s_r=1,...,u-1$. In the case of a nominal split variable $X_{v_r}$, a split rule is defined as $X_{v_r} \in s_r$, where $s_r \subset \{1,...,K\}$ and $s_r \neq \{\emptyset\}$. If $X_{v_r}$ has $K$ categories there are $2^{K-1}-1$ possible splits. 


\subsection{The parameter space of globally optimal decision trees}

As done by \cite{breiman1984cart}, let the complexity of a tree be measured by a function of the number of terminal nodes, without further considering the depth or the shape of trees. Then the overall parameter space $\Theta$ can be written as $\Theta =\bigcup_{M=1}^{M_{max}} \Theta_{M}$, where $\Theta_M$ denotes the parameter space of models with $M$ terminal nodes. Let $\theta$ be a generic element of $\Theta$, then $\theta$ is fully defined by:

\begin{equation}
	 \theta=(v_{n_1}, s_{n_1},...,v_{n_{M-1}},s_{n_{M-1}} ).
	\label{treeRep}
\end{equation}

The overall goal of classification and regression tree analysis can be defined as finding a set of parameters $\hat{\theta}$, such that

\begin{equation}
	\hat{\theta} = \operatorname*{\arg \,\min}_{\theta \in \Theta} \; loss(Y, f(X,\theta)) + comp(\theta).
	\label{generalcostfunction}
\end{equation}

Based on the input data $X$ and the paramters $\theta$, $f(X,\theta)$ gives predictions for the output $Y$. $loss$ is a suitable loss function for the domain of $Y$. Commonly, the misclassification error for classification problems, or the squared error for regression problems, is used. $comp$ quantifies the complexity of trees and penalizes for more complex models in the tree selection process. In CART, the complexity is a non-negative function of the terminal-nodes. Note that finding $\hat{\theta}$ requires a search over all $\Theta_m$.

The parameter space $\Theta$ becomes large for already medium sized problems and a complete search for larger problems is computationally intractable. In fact, \cite{hyafil1976cob} showed that building optimal binary decision trees, such that the expected number of tests required to classify an unknown sample is minimized, is NP-complete. \cite{zantema2000finding} proved that finding a decision tree of minimal size that is decision equivalent to a given decision tree is an NP-hard problem. As a consequence the search space is usually limited by heuristics. 


\subsection{The parameter space of locally optimal decision trees}

Instead of searching all combination of $\Theta$ simultaneously, recursive partitioning algorithms only consider one split at a time. At each internal node $r \in \{n_1,..., n_{M-1}\}$, the split variable $X_{v_r}$ and the corresponding split point $s_r$ is selected such that it minimizes the impurity in the two resulting terminal nodes. Let the parameters of a tree, constructed in $i$-steps, be denoted as $\tilde{\theta}_{i}$ and $\tilde{\theta}_{0} = (\emptyset)$. Then, for a tree with parameters $\tilde{\theta}_{i}$, the position of the new internal node $r$, as well as the corresponding split variable $v_r$ and split point $s_r$, are chosen such that:

\begin{equation}
		\min_{v_r, s_r} \; loss(Y, f(X, \tilde{\theta}_{i-1} \cup (v_r, s_r) ))
	\label{eq:whichsplit}
\end{equation}

CART's standard measures for node impurity are the Gini index, for classification problems and the mean squared error, for regression problems. After the selection of the $i^{th}$ split $\tilde{\theta}_{i}$ is given by $\tilde{\theta}_{i} = \tilde{\theta}_{i-1}  \cup (\hat{v}_r, \hat{s}_r)$. This process of recursively selecting and executing splits continues until the homogeneity of the nodes cannot be further improved. Additionally, often additional stopping criteria are defined, e.g. minimum sample sizes in the internal nodes and terminal nodes.

In CART, and many other recursive tree methods, the tree growing step is followed by a pruning step. The idea behind pruning is to first grow a large tree without or with little restriction on the magnitude of impurity reduction. Starting from the bottom of the tree pruning then removes worthless splits. The method proposed by Breiman \textit{et al.} \cite[Chapter 2]{breiman1984cart} and used by CART is called \textit{cost-complexity pruning}. Cost-complexity pruning iteratively prunes the internal node that:

\begin{equation}
		\min_{r=1,...,n_{M-1}} \; loss(Y, f(X,\tilde{\theta}_i \setminus (v_r, s_r )))  + comp(\tilde{\theta}_i \setminus (v_r, s_r)).
	\label{eq:pruning}
\end{equation}

The subsequent solution $\tilde{\theta}_{i+j+1} = \tilde{\theta}_{i+j} \setminus (\hat{v}_r, \hat{s}_r)$, where $j$ is the number of completed pruning steps.

For nontrivial problems, recursive partitioning methods only search a small fraction of the global search space $(v_1, s_1, ..., v_{M_{max}-1}, s_{M_{max}-1} )$. They only search each $(v_r, s_r)$ once, and independently of the other split rules. The selection of $v_r$ and $s_r$ at each step $i$ requires a search of the locally optimal split over $M=i$ terminal nodes. The pruning step requires $j$ selections of an internal node $r$.


In contrast to CART and C4.5, conditional inference trees utilize an unbiased variable selection approach that is based on permutation testing. Bias in the split variable selection process arises from a preference towards variables with many potential split points. The conditional inference framework overcomes this problem by separating the variable and split point selection. Although this distinction influences the selection of split rules, conditional inference tree solve the same minimization problem (Equation \ref{eq:whichsplit}). The \code{ctree} implementation of conditional inference trees does not employ pruning, but uses a statistical stopping criterion.



\subsection{An illustration showing the limitations of locally optimal decision trees}

A very simple example that illustrates the limitation of recursive partitioning methods is depicted in Figure~\ref{fig:illustration}. The example only contains two independent variables and can be solved with only three splits that partition the input space into four regions. As expected the recursive partitioning methods \code{rpart} and \code{ctree} fail to find any split at all, as the impurity of the resulting subsets cannot be reduced by the first split. For methods that explore $\Theta$ in a more global fashion it is straightforward to find an optimal solution to this problem. One solution, which is the tree constructed by \code{evtree} is given by: 
%
<< echo = FALSE, split= TRUE>>=
library("evtree")
X1 <- c(rep(0.25, 4), rep(0.75, 4), rep(1.25 ,4),  rep(1.75, 4))
X2 <- c(rep(seq(0.25, 1.75 ,0.5),4))
Y <- array(1,16)
Y[X1 < 1 & X2 < 1]=2
Y[X1 > 1 & X2 > 1]=2
chess22 <- as.data.frame(cbind(X1, X2, Y))
chess22[,3] <- as.factor( chess22[,3] )
levels(chess22[,3]) = c("class: o","class: x")
print(evtree(Y~.,data=chess22, minbucket = 1, minsplit=2))
@

\setkeys{Gin}{width=0.4\textwidth}
\begin{figure}[tbh!]
\centering
<<fig = TRUE, echo = FALSE>>=
par(cex.axis=1.5, cex.lab=1.5)
plot(chess22[chess22[,3]=="class: o",1], chess22[chess22[,3]=="class: o",2], type="p",  pch= c("x"), col=colors()[320], xlab="X1", ylab="X2", xlim=c(0,2), ylim=c(0,2), cex=2)
points(chess22[chess22[,3]=="class: x",1], chess22[chess22[,3]=="class: x",2], pch= c("o"), cex=2)
par(cex.axis=1.5, cex.lab=1.5)
@
\caption{\label{fig:illustration} Class distribution of  the $(X_1, X_2)$-plane. The two classes are indicated by black $(o)$ and grey $(x)$.}
\end{figure}

All instances all are classified correctly. Each of the terminal nodes 3 and 7 contain four instances of the class $(x)$. Four instances of class $(o)$ are assigned to each of the terminal nodes 4 and 6.


\subsection{Approaches for learning globally optimal decision trees}

When compared with the described forward stepwise search, a less greedy approach is to calculate the effects of the choice of the split rule deeper down in the tree. In this way optimal trees can be found for simple problems. However, feature selection at a node of greedily induced trees has complexity  $O(PN)$. Through a search of the optimal split up to $d-1$ levels the complexity increases to $O(P^dN^d)$ \citep{papagelis2001bdt}. Therefore, for more difficult problems the search space at each node must be limited by the depth of the search. Additionally, the search space can be reduced, limiting the number of possible split points of the selected variables and/or other heuristics. These methods are known as \textit{look-ahead algorithms} \citep{esmeir2007ald}. Another class of algorithms is given by stochastic optimization methods. Given an initial tree, these methods seek improved solutions through stochastic changes to the tree structure. Besides evolutionary algorithms \citep{koza1991concept}, \textit{Bayesian CART} \citep{denison1998bca} and \textit{simulated annealing} \citep{sutton1992ict} were used successfully to solve difficult classification and regression tree problems. \cite{koza1991concept} first formulated the concept of using evolutionary algorithms as a stochastic optimization method to build classification and regression trees. \cite{papagelis2001bdt} presented a classification tree algorithm and provided results on several data-sets from the UCI machine learning repository \citep{Frank+Asuncion:2010}. A method for the construction of regression trees via evolutionary algorithms was introduced by \cite{fan2005rta}.  \cite{cantu2003inducing} used an evolutionary algorithm to induce oblique classification trees. \cite{gray2008cta} introduced a classification tree algorithm that allows univariate splits, but also splits defined by linear combinations of two or three variables. 

\section[The evtree algorithm]{The \code{evtree} algorithm}
\label{desc}

The general framework of evolutionary algorithms emerged from different representatives. \cite{Holland} called his method \textit{genetic algorithms}, \cite{rechenberg} invented \textit{evolution strategies}, and \cite{Fogel} introduced \textit{evolutionary programming}. More recently, \cite{Koza} introduced a fourth stream and called it \textit{genetic programming}. All four representatives only differ in the technical details, for example the encoding of the individual solutions, but follow the same general outline \citep{eiben2003iec}. Evolutionary algorithms are being increasingly widely applied to a variety of optimization and search problems. Common areas of application include data mining \citep{freitas2002sea,cano2003uea}, statistics \citep{de34glmulti}, signal and image processing \citep{man1997gac}, and planning and scheduling \citep{jensen2001raf}. 
 
The pseudocode for the general scheme of an evolutionary algorithm is depicted in Table~\ref{pc:genericEA}. With the description of these components together with the definition of individuals, the most important aspects of an evolutionary algorithm are specified. The representation of an individual is given by a binary tree structure, which is fully defined with the definition of $\theta$ in Equation~\ref{treeRep}. This section continues with the description of the remaining components following the general outline displayed in Table~\ref{pc:genericEA}. 

\begin{table}[tbh!]
\begin{center}
\begin{tabular}{l}
\toprule
1. initialize the population \\
2. evaluate each individual  \\
3. while(termination condition is not satisfied) do  \\
\hspace{8mm} a. select parents  \\
\hspace{8mm} b. alter selected individuals via variation operators \\
\hspace{8mm} c. evaluate new solutions  \\
\hspace{8mm} d. select survivors for the next generation  \\
\hspace{3.8mm} od  \\
\bottomrule
\end{tabular}
\end{center}
\caption{Pseudocode of the general schema of an evolutionary algorithm.} 
\label{pc:genericEA}
\end{table}

This section describes the general functioning of the \code{evtree} algorithm. Implementation details as user-specified parameters, including their default values and their possible ranges, are discussed in the subsequent section. These implementation details also include parameters that specify a minimum number of observation that are required in each internal node and  terminal node of a tree. In the following, the term \textit{invalid node} refers to a node that do not contain enough observations. 


\subsection{Initialization}
\label{init}

Each tree of the population is initialized with a valid, randomly generated, split rule in the root node. First, $v_1$ is selected with uniform probability from $1,...,P$. Second, a split point $s_1$ is selected. If $X_{v_1}$ is numeric or ordinal with $u$ unique values, a split point $s_1$ is selected with uniform probability from the $u-1$ possible spit points of $X_{v_1}$. If $X_{v_1}$ is nominal and has $K$ categories, each $k=1,...,K$ has a probability of $50\%$ to be assigned to the left or the right terminal node. In cases where all $k$ are allocated to the same terminal node, the assignment of one category is flipped to the other terminal node. If this procedure results in a non-valid split rule, the two steps of random split variable selection and split point selection are repeated. With the definition of $r=1$ and the selection of $v_1$ and $s_1$, the initialized tree is fully described by $\tilde{\theta}_{1}= (v_1, s_1)$. 


\subsection{Parent selection}
\label{parent}

In every iteration, each tree is selected once to be modified by one of the variation operators. In cases where the crossover operator is applied, the second parent is selected randomly from the remaining population. In this way, some trees are selected more than once in each iteration. 


\subsection{Variation operators}
\label{vari}

Four types of mutation operators and one crossover operator are utilized by our algorithm. In each modification step, one of the variation operators is randomly selected for each tree. The mutation and crossover operators are described below. 

\subsubsection{Split}

\textit{Split} selects a random terminal-node and assigns a valid, randomly generated, split rule to it. As a consequence, the selected terminal node becomes an internal node $r$ and two new terminal nodes are generated. 

The split rule of the new internal node is chosen as follows. First, $v_r$ is selected with uniform probability from $1,...,P$. Second, a split point $s_r$ is selected. If $X_{v_r}$ is numeric or ordinal, a split point $s_r$ is selected with uniform probability from the $u-1$ possible spit points of $X_{v_r}$. If $X_{v_r}$ is nominal, each category of $X_{v_r}$, has a probability of $50\%$ to be assigned to the left or the right terminal node. In cases where all $k$ are allocated to the same terminal node, one of them is changed to the other terminal node. If this procedure results in a non-valid split rule, the two steps of random split variable selection and split point selection are repeated for a maximum of $P$ iterations. 

In cases where no valid split rule can be assigned to the internal node at position $r$, the search for a valid split rule is carried out on another randomly selected terminal node. If, after $10$ attempts, no valid split rule can be found, then $\tilde{\theta}_{i+1} = \tilde{\theta}_{i}$. Otherwise, the set of parameters in iteration $i+1$ are given by $\tilde{\theta}_{i+1}= \tilde{\theta}_i \cup (v_{r},s_{r})$.

\subsubsection{Prune}
\textit{Prune} chooses a random internal node $r$, where $r>1$, which has two terminal nodes as successors and prunes it into a terminal node. The tree's parameters at iteration $i+1$ are $\tilde{\theta}_{i+1}=\tilde{\theta}_{i} \setminus (v_{r},s_{r})$. If $\tilde{\theta}_{i}$ only comprises one internal node, i.e., the root node, than $\tilde{\theta}_{i+1} = \tilde{\theta}_{i}$.

\subsubsection{Major split rule mutation}
\textit{Major split rule mutation} selects a random internal node $r$ and changes the split rule, defined by the corresponding split variable $v_r$, and the split point $s_r$. With a probability of $50\%$, a value from the range $1,...,P$ is assigned to $v_r$. Otherwise $v_r$ remains unchanged and only $s_r$ is modified. Again, depending on the domain of $X_{v_r}$, either a random split point from the range of possible values of $X_{v_r}$ is selected, or a non-empty set of categories is assigned to each of the two terminal nodes. If the split rule at $r$ becomes invalid, the mutation operation is reversed and the procedure, starting with the selection of $r$, is repeated for a maximum of $3$ attempts. Subsequent nodes that become invalid are pruned. 

If no pruning occurs, $\tilde{\theta}_{i}$ and $\tilde{\theta}_{i+1}$ contain the same set of parameters. Otherwise, the set of parameters $(v_{m_1}, s_{m_1}, ..., v_{m_f}, s_{m_f})$, corresponding to invalid nodes, is removed from $\tilde{\theta}_i$. Thus, $\tilde{\theta}_{i+1} = \tilde{\theta}_{i} \setminus (v_{m_1}, s_{m_1}, ..., v_{m_f}, s_{m_f})$. 

\subsubsection{Minor split rule mutation}

\textit{minor split rule mutation} is similar to the \textit{major split rule mutation} operator, but it does not alter $v_r$ and only changes the split point $s_r$ by a minor degree.
If $X_{v_r}$ is numerical or ordinal the split point $s_r$ is changed by a non-zero number of unique values of $X_{v_r}$. In cases where $X_{v_r}$ has less then $20$ unique values, the split point is change to to the next larger, or the next lower, unique value of $X_{v_r}$. Otherwise, $s_r$ is randomly shifted by a number of unique values that is not larger than $10\%$ of the range of unique values of $X_{v_r}$. If $X_{v_r}$ is a nominal variable, with less than $20$ categories, one of the categories is randomly modified. Otherwise, at least one and at most $10\%$ of the variable's categories are changed. In cases where subsequent nodes become invalid, further split points are searched that preserve the tree's topology. After five non-successful attempts at finding a topology preserving split point, the non-valid nodes are pruned.

Equivalently to the \textit{major split rule mutation} operator the subsequent solution $\tilde{\theta}_{i+1} = \tilde{\theta}_{i} \setminus (v_{m_1}, s_{m_1}, ..., v_{m_f}, s_{m_f})$.

\subsubsection{Crossover}

\textit{Crossover} exchanges, randomly selected, subtrees between two trees. Let $\tilde{\theta}_{i}^1$ and $\tilde{\theta}_{i}^2$ be the parameters of the trees that are chosen for crossover. First, an internal node is selected randomly from each of $\tilde{\theta}_{i}^1$ and $\tilde{\theta}_{i}^2$, which we denote as $n_k^1$ and $n_k^2$. Let $sub1(\tilde{\theta}_i^j, n^j_k)$ denote the subtrees rooted by $n_k^j$, where $j=1,2$. Then, $sub1(\tilde{\theta}_i^j)$ contains the parameters $(v_{n^j_k}, s_{n^j_k},  v_{n^j_{k1}}, s_{n^j_{k1}}, ...,v_{n^j_{km}}, s_{n^j_{km}})$, where $(n^j_{k1}, ..., n^j_{km})$ are the set of descendent node positions of $n^j_k$. Now, the two selected subtrees can be written as $sub1(\tilde{\theta}_{i}^1, n^1_k) $ and $sub1(\tilde{\theta}_{i}^2, n^2_k)$. Furthermore, $sub2(\tilde{\theta}_{i}^1, n^1_k) = \tilde{\theta}_{i}^1 \setminus sub1(\tilde{\theta}_{i}^1, n^1_k)$ and $sub2(\tilde{\theta}_{i}^2, n^2_k) = \tilde{\theta}_{i}^2 \setminus sub1(\tilde{\theta}_{i}^2, n^2_k)$. 

After the application of the \textit{crossover} operator, the trees' parameter spaces are given by $\tilde{\theta}_{i+1}^1= sub2(\tilde{\theta}_{i}^1, n^1_k) \cup sub1(\tilde{\theta}_{i}^2, n^2_k) \setminus (v_{m^2_{1}}, s_{m^2_{1}}, ..., v_{m^2_{f}}, s_{m^2_{f}})$ and $\tilde{\theta}_{i+1}^2= sub2(\tilde{\theta}_{i}^2, n^2_k) \cup sub1(\tilde{\theta}_{i}^1, n^1_k) \setminus (v_{m^1_{1}}, s_{m^1_{1}}, ..., v_{m^1_{f}}, s_{m^1_{f}})$. Here, $(v_{m^1_{1}}, s_{m^1_{1}}, ..., v_{m^1_{f}}, s_{m^1_{f}}) \subset sub1(\tilde{\theta}_i^1, n^1_k)$ and $(v_{m^2_{1}}, s_{m^2_{1}}, ..., v_{m^2_{f}}, s_{m^2_{f}}) \subset sub1(\tilde{\theta}_i^2, n^2_k)$ are the parameters of the invalid nodes.


\subsection{Evaluation function}
\label{eval}

The evaluation function represents the requirements the population should adapt to. In general, these requirements are formulated by Equation~\ref{generalcostfunction}. A suitable evaluation function for classification and regression trees minimizes the models' accuracy on the training data, and the models' complexity. This subsection describes the currently implemented choices of evaluation functions for classification and for regression. 

\subsubsection{Classification}

The quality of a classification tree is most commonly measured as a function of its misclassification loss $MC(Y, f(X, \theta)) = \sum_{n=1}^N I(Y_n \neq \hat{f}(X_{\cdot n}, \theta))$, and the complexity of a tree by a function of the number of its terminal nodes $M$. \code{evtree} uses $2 \cdot MC(Y, f(X, \theta))$ as a loss function. The number of terminal nodes, weighted by $\ln N$ and a user-specified parameter $\alpha$, measures the complexity of trees. With these particular choices, the general definition of the optimization problem in Equation~\ref{generalcostfunction} can be written as:

\begin{equation}
	 \hat{\theta} = \operatorname*{\arg \,\min}_{\theta \in \Theta} \; 2 \cdot MC(Y, f(X, \theta))+\alpha \cdot M \cdot \ln N.
\label{eq:classification}
\end{equation}

Equation~\ref{eq:classification} seeks parameters $\hat{\theta}$ that minimize the misclassification loss and the number of terminal nodes. Increasing values of $\alpha$ encourage decreasing tree sizes. 

Other, existing and commonly used choices of evaluation functions include the \textit{Bayesian Information Criteria (BIC)} \citep{gray2008cta} and \textit{Minimum Description Length (MDL)} \citep{quinlan1989inferring}. For both evaluation functions deviance is used for accuracy estimation. Deviance is usually preferred over the misclassification rate in recursive partitioning methods, as it is more sensitive to changes in the node probabilities \citep[pp.~308--310]{hastie}. However this is not necessarily an advantage for global tree building methods like evolutionary algorithms.

\subsubsection{Regression}

For regression trees, accuracy is usually measured by the mean squared error $MSE(Y, f(X, \theta))= \sum_{n=1}^N (Y_n - f(X_{\cdot n}, \theta)^2)$. Using $N \cdot \ln MSE(Y, f(X, \theta))$ as a loss function and $\alpha \cdot 4 \cdot (M+1) \cdot \ln N$ as the complexity part, the general formulation of the optimization problem in Equation~\ref{generalcostfunction} can be rewritten as:

\begin{equation}
	 \hat{\theta} = \operatorname*{\arg \,\min}_{\theta \in \Theta} \; N \cdot \ln MSE(Y, f(X, \theta))+\alpha \cdot 4 \cdot (M+1) \cdot \ln N.
\label{eq:regression}
\end{equation} 

Here, $M+1$ is the effective number of estimated parameters, taking into account the estimates of a mean parameter in each of the terminal nodes and the constant error variance term. With $\alpha = 0.25$ the criteria is, up to a constant, equivalent to the BIC used by \cite{fan2005rta}. However, the effective number of parameters estimated for is actually much higher than $M+1$ due to the selection of parameters in the split variable and the selection of the variable itself. It is however unclear how these should be counted \citep[p.~222]{gray2008cta, ripley}. Therefore, a more conservative default value of $\alpha=1$ is assumed.


\subsection{Survivor selection}
\label{evo}

The population size stays constant during the evolution and only a fixed subset of the candidate solution can be kept in memory. A common strategy is the $(\mu + \lambda)$ selection, where $\mu$ survivors for the next generation are selected from the union of $\mu$ parents and $\lambda$ offsprings. An alternative approach is the $(\mu,\lambda)$ strategy. In this schema $\mu$ survivors for the next generation are selected from $\lambda$ offsprings.

Our algorithm uses a $(1+1)$ selection schema, where one parent solution competes with one offspring for a place in the population. In the case of a mutation operator either the solution before $\tilde{\theta}_i$ or after modification $\tilde{\theta}_{i+1}$ is kept in memory. In the case of the crossover operator, the initial solutions of $\tilde{\theta}_i^1$ competes with its subsequent solutions $\tilde{\theta}_{i+1}^1$. Correspondingly, one of the two solutions $\tilde{\theta}_i^2$ and $\tilde{\theta}_{i+1}^2$ is rejected. The survivor selection is done deterministically. The tree with lower fitness, according to the evaluation function, is rejected. Note that, due to the definition of the crossover operator, that some trees are selected more than once in an iteration. Correspondingly, these trees undergo the survival selection process more than once in an iteration.

As in classification and regression tree analysis the individual solutions are represented by trees. This design offers computational advantages over $(\mu + \lambda)$, with $\mu>1 \;  \& \; \lambda>1$, and $(\mu,\lambda)$ strategies. In particular, for the application of mutation operators no new trees have to be constructed. The tree after modification is simply accepted or reversed to the previous solution.

There are two important issues in the evolution process of an evolutionary algorithm: population diversity and selective pressure \citep{michalewicz1994genetic}. These factors are related, as with increasing selective pressure the search is focused more around the currently best solutions. An overly strong selective pressure can cause the algorithm to converge early in a local optima. On the other hand, an overly weak selective pressure can make the search ineffective. Using a $(\mu + \lambda)$ strategy, a strong selective pressure can occur in situations as follows. Suppose the $b^{th}$ tree of the population is one of the fittest trees in iteration $i$, and in iteration $i$  one split rule of the $b^{th}$ tree is changed only by a minor degree. Then very few instances are classified differently and the overall misclassification might not even change. However, as they represent one of the best solutions in iteration $i$, they are both selected for the subsequent population. This situation can occur frequently, especially when a fine-tuning operator like the \textit{minor split rule mutation} is used. Then, the diversity of different trees is lost quickly and the algorithm likely terminates in a local optimum. The $(1+1)$ selection mechanism clearly avoids these situations, as only the parent or the offspring can be part of the subsequent population.


\subsection{Termination}
\label{term}

The algorithm terminates when the quality of the best $5\%$ of trees stabilizes for $100$ iterations, but not before $1000$ iterations. If the run does not converge the algorithm terminates after a user-specified number of iterations. In cases where the algorithm does not converge, a warning message is written to the command line. The tree with the highest quality according to the evaluation function is returned.


\section{Implementation}
\label{impl}

\code{evtree} provides an efficient implementation of an evolutionary algorithm that builds classification trees in \proglang{R}. CPU- and memory- intensive tasks are fully computed in \proglang{C++}, while the user interfaces and plot functions are written in \proglang{R}. The \code{.C()} interface \citep{chambers2008software} was used to pass arguments between the two languages. \code{evtree} depends on the \pkg{partykit} package  \citep{hothorn2009partykit}, which provides an infrastructure for representing, summarizing, and visualizing tree-structured models. The supplied functionality includes standard \code{print()}, \code{summary()}, and \code{plot()} functions to display trees and a \code{predict()} function to predict the class labels of new data. The arguments of  \code{evtree()} are
%
\begin{Code}
  evtree(formula, data = list(), weights = NULL, subset = NULL,
    control = evtree.control(...), ...)
\end{Code}
%
where \code{formula} and \code{data} are a standard \proglang{R} formula of type \code{y ~ x1 + x2} with the respective data. \code{weights} is an optional vector specifying weights to be used in the fitting process. The option vector subset specifies a subset of observations to be used in the fitting process. \code{control} comprises a list of control parameters
%
\begin{Code}
  evtree.control(minbucket = 7L, minsplit = 20L, maxdepth = 9L, 
    niterations = 10000L, ntrees = 100L, alpha = 1,
    operatorprob = list(pmutatemajor = 0.2, pmutateminor = 0.2,
      pcrossover = 0.2, psplit = 0.2, pprune = 0.2),
    seed = NULL, ...)
\end{Code}
%
where the parameters \code{minbucket}, \code{minsplit}, and \code{maxdepth} constrain the solution to a minimum number of observations in each terminal node, a minimum number of observation in each internal node and a maximum tree depth. Note that the memory requirements increase by the square of the maximum tree depth. Parameter \code{alpha} regulates the complexity part of the evaluation function and refers to $\alpha$ in Equation~\ref{eq:classification} for classification trees and Equation~\ref{eq:regression} for regression trees. \code{niterations} and \code{ntrees} specify the maximum number of iterations and the number of trees in the population. With the argument \code{operatorprob}, user-specified probabilities for the variation operators can be defined. Argument \code{seed} is an optional seed for the random number generator, which allows models to be reproduced by re-using the same seed. If not specified, the random number generator is initialized by the system time. In this case, iterative calls of \code{evtree} will possibly result in different models.
%
<<>>=
library("evtree")
data("iris")
model <- evtree(Species ~ ., iris, control = evtree.control(seed = 1000))
@
%
The fitted model can be examined using the \code{print()} and \code{plot()} functions. 
%
<<>>=
print(model)
@

\setkeys{Gin}{width=1\textwidth}
\begin{figure}[t!]
\centering
<<echo= FALSE>>=
par(cex.axis=1.5, cex.lab=1.5)
<<fig=TRUE, echo=TRUE, width = 13.5, height= 8.5>>=
plot(model)
@
\caption{\label{fig:example} Example of a tree plotted by the \code{plot()} function.}
\end{figure}

The resulting plot is displayed in Figure~\ref{fig:example}. \code{predict()} can be used for the prediction of the response.
%
<<>>=
predict(model, newdata = iris[c(10, 35, 60, 85, 110, 135), -5])
@

\section{Performance comparison}
\label{comp}

In this Section we compare \code{evtree} with \code{rpart} and \code{ctree}. All models are constrained to a minimum number of 7 observations per terminal node, 20 observations per internal node and a maximum tree depth of 9. Apart from that, the default settings of the algorithms are used. 

In the first part of the analysis (Section~\ref{comp1}) the algorithms are compared on 14 benchmark data-sets that are publicly available and $3$ real-world data-sets from the Austrian \textit{Diagnosis Related Group (DRG)} system \citep{LKF2009}. The analysis is based on the evaluation of 250 bootstrap samples for each of the 20 data-sets. The misclassification rate on the \textit{out-of-bag} \citep{hothorn2005design} samples is used as a measure of predictive accuracy. Furthermore, the complexity is estimated by the number of terminal nodes. Missing values are currently not supported by \code{evtree}. The only data-set with missing values is the \textit{Breast Cancer Database} with a total of 16 missing values, which were removed before analysis. Dunnett's correction from \proglang{R} package \pkg{multcomp} \citep{multcomp} was used to calculate simultaneous $95\%$ confidence intervals for the individual data-sets .

In the second part (Section~\ref{comp2}) the algorithms' performances are assessed on an artificial problem that is simulated with different noise levels. The estimation of predictive accuracy and the number of terminal nodes is based on $250$ realizations for each simulation.
 

\subsection{Benchmark and real-world problems}
\label{comp1}

In Table~\ref{tab:dataDescription} the benchmark and real-world data-sets from the Austrian DRG-system are described. In the Austrian DRG-system, resources are allocated to hospitals by simple rules regarding diagnosis, selected individual services, and patient characteristics. Within this system regression tree analysis is performed to classify hospital patients into clinically meaningful and comprehensible groups that consume similar hospital resources. The data-sets MEL0101 (intervention on the skull), HDG0202 (malignant neoplasms), and HDG0502 (acute affections of the respiratory tract and middle atelectasis) contain patient groups with the same main diagnosis or procedure. The task of the regression tree analysis is to further divide these groups with the intention of finding groups with similar resource consumption. Possible split-variables are the patients' diagnoses, procedures, age, and gender. The dependent variable is the patients' length of stay, which has a good relationship with the total costs and is available \citep{theurl2007ihf}. Permission to use the data was granted by the \citet{BMG2009}. A more detailed description of the application can be found in \citet{grubinger2010regression}.

\begin{table}
\begin{center}
\begin{tabular}{lrrrrrr}
\toprule
\textsc{Data-set} & \textsc{Instances} &  \multicolumn{5}{l}{\textsc{Attributes}}   \\ 
                  &                    &  binary              & nominal          &  ordered      &  metric     & classes \\ 
\midrule
Glass Identification$^\#$  &   214   &  - &  - &  - & 9 &  6      	 \\
Statlog Heart*         &   270   &  3 &   3 & 1 & 6 &   2     	 \\
Ionosphere$^\#$ &   351   &  2 &  - &  - & 32 & 2  	             \\
Musk$^+$  &  476  & - &  - &  - & 166 & 2                     	 \\
Breast Cancer Database$^\#$ &   685   &  - &  4 & 5 &  - &  2  	 \\
Pima Indians Diabetes$^\#$ &   768   &  - &   - & - & 8 &  2      	 \\
Vowel$^\#$  &  990  & - &  1 &  - & 9 & 11     	                     \\
Statlog German Credit* &  1000   &  2 &  10 & 1 & 7 &  2      	 \\
Contraceptive Method* &   1437   & 3 &  - &  4 &  2 &  3  	     \\
DNA$^\#$  &  3186  & 180 &  - &  - & - & 3   	                       \\
Spam$^+$  &  4601  & - &  - &  - & 57 & 2                      	 \\
MAGIC Gamma Telescope*  &  19020  & - &  - &  - & 10 & 2     	   \\
Servo$^\#$ & 167 &  - & 4 & - &  -  &  -											   \\
Boston Housing$^\#$ & 506 & 1 & - & - &  12  & -                 \\
MEL0101$^\diamond$ &   875 &  1 & 4 & 1 &  108  & -              \\
HDG0202$^\diamond$ &  3933 &  1 & 7 & 1 &   46  & -              \\
HDG0502$^\diamond$ & 8251 &  1 & 7 & 1 &   91  & -               \\
\bottomrule
\end{tabular}
\end{center}
\caption{Description of the evaluated benchmark data-sets. The data-sets marked with a $(^*)$ are from the UCI machine learning repository \citep{Frank+Asuncion:2010}. Data-sets marked with a $(^\#)$ and $(^{+})$ are from the \proglang{R} packages \pkg{mlbench} \citep{mlbench} and \pkg{kernlab} \citep{kernlab}. The $3$ real-world data-sets from the Austrian DRG system are marked with a $(^\diamond)$. } 
\label{tab:dataDescription}
\end{table}

The relative performance of \code{evtree} and \code{rpart} is summarized in Figure~\ref{fig:crpart}. Performance differences are displayed relative to \code{evtree}'s performance. For example, on the \textit{Glass} data-set, the average misclassification rate of \code{rpart} is $2.8\%$ higher than the misclassification rate of \code{evtree}. It can be observed that on 13 out of 17 data-sets \code{evtree} significantly outperforms \code{rpart} in terms of predictive accuracy. Only on the \textit{Contraceptive Method} data-set \code{evtree} performs slightly worse.  In terms of complexity, \code{evtree} models are significantly more complex on 9 and less complex on 7 data-sets. 

Figure~\ref{fig:cptree} summarizes the relative performance of \code{evtree} and \code{ctree}. For 14 out of 17 data-sets \code{evtree} shows a better predictive performance. The algorithms' performances on the 3 other data-sets is similar. The relative complexity of \code{evtree} models is significantly smaller for 15 and larger for 1 data-set. 

%% vorberechnungen
<<label=figciplots, echo = FALSE>>=
rm(list = ls())
library("plotrix")
library("lme4")
library("multcomp")
load(file="./results/breastcancer.RData")
load(file="./results/glass.RData")
load(file="./results/credit.RData")
load(file="./results/heart.RData")
load(file="./results/pima.RData")
load(file="./results/contraceptive.RData")
load(file="./results/dna.RData")
load(file="./results/musk.RData")
load(file="./results/spam.RData")
load(file="./results/vowel.RData")
load(file="./results/ionosphere.RData")
load(file="./results/magicgamma.RData")
load(file="./results/hdg0202.RData")
load(file="./results/hdg0502.RData")
load(file="./results/mel0101.RData")
load(file="./results/bostonhousing.RData")
load(file="./results/servo.RData")

preprocess <- function(d, dname = "datasetname", isclassification = TRUE){
    if(isclassification == TRUE)
        d[,1:3] <- 1-d[,1:3]
    d <- as.data.frame(d)
    colnames(d) <- c("evtree", "rpart", "ctree","evtree", "rpart", "ctree")
    d[,2] <- d[,2] / d[,1] *100
    d[,3] <- d[,3] / d[,1] *100
    d[,1] <- d[,1] / d[,1] *100
    d[,5] <- d[,5] / d[,4] *100
    d[,6] <- d[,6] / d[,4] *100
    d[,4] <- d[,4] / d[,4] *100
    x <- d[ ,1:3]
    y <- d[ ,4:6]
    x <- reshape(x, idvar="samp", times=names(x), timevar = "alg",varying= list(names(x)), direction="long")
    y <- reshape(y, idvar="samp", times=names(y), timevar = "alg",varying= list(names(y)), direction="long")
    x$ds <- dname
    y$ds <- dname
    return( cbind(x,y) )
}

r <- preprocess(d = rglass, dname = "Glass Identification", isclassification = T)
r <- rbind(r, preprocess(d = rheart, dname = "Statlog Heart", isclassification = T))
r <- rbind(r, preprocess(d = rionosphere, dname = "Ionosphere", isclassification = T))
r <- rbind(r, preprocess(d = rmusk, dname = "Musk", isclassification = T))
r <- rbind(r, preprocess(d = rbreastcancer, dname = "Breast Cancer Database", isclassification = T))
r <- rbind(r, preprocess(d = rpima, dname = "Pima Indians Diabetes", isclassification = T))
r <- rbind(r, preprocess(d = rvowel, dname = "Vowel", isclassification = T))
r <- rbind(r, preprocess(d = rcredit, dname = "Statlog German Credit", isclassification = T))
r <- rbind(r, preprocess(d = rcontraceptive, dname = "Contraceptive Method", isclassification = T))
r <- rbind(r, preprocess(d = rdna, dname = "DNA", isclassification = T))
r <- rbind(r, preprocess(d = rspam, dname = "Spam", isclassification = T))
r <- rbind(r, preprocess(d = rmagic, dname = "Magic Gamma Telescope", isclassification = T))
r <- rbind(r, preprocess(d = rservo, dname = "Servo", isclassification = F))
r <- rbind(r, preprocess(d = rbostonhousing, dname = "Boston Housing", isclassification = F))
r <- rbind(r, preprocess(d = mel0101, dname = "MEL0101", isclassification = F))
r <- rbind(r, preprocess(d = hdg0202, dname = "HDG0202", isclassification = F))
r <- rbind(r, preprocess(d = rhdg0502, dname = "HDG0502", isclassification = F))

ra <- r[,1:4]
rc <- r[,5:8]

tf <- function(x){
    names(x)[[2]] <- "value"
    x$samp <- factor(x$samp) 
    x$ds <- factor(x$ds)
    x$dssamp <- x$ds:x$samp # unique levels of dataset and bootstrap sample
    x$alg <- factor(x$alg)
    x$alg <- relevel(x$alg, "evtree") # change order of algorithms
    x
}
ra <- tf(ra)
rc <- tf(rc)

uniquedatasetlabels <- unique(ra$ds)[length(unique(ra$ds)):1]

cstats <- function(x= ra, alg= "rpart"){
    mean  <- array(0,17)
    lower <- array(0,17)
    upper <- array(0,17)
    for(i in 1:length(uniquedatasetlabels)){
        mod1 <- lm(value ~ alg, data = subset(x, x$ds == uniquedatasetlabels[i]))
        pt <- glht(mod1, linfct = mcp(alg = "Dunnett"))
        w <- confint(pt)$confint
        if(alg == "rpart")
            d <- 1
        else
            d <- 0
        mean[i]  <-  w[1+d]
        lower[i] <-  w[3+d]
        upper[i] <-  w[5+d]
    }
    return(as.data.frame(cbind(mean,lower,upper)))
}

plotd <- function(alg){
    par(las=1, cex=1.5, cex.lab=1.3, mfrow= c(1,2), omd=c(0.16,1,0,1), mar=c(5, 0, 1, 2) + 0.1)
    dummy <- c(1:17)
    names(dummy) <- uniquedatasetlabels
    stats <- cstats(ra, alg)
    plotCI(stats$mean, dummy, li=stats$lower, ui=stats$upper, err="x", xlim=c(-12,45), axes=F, ann=T, ylab= "", xlab= "difference in predictive accuracy [%]", lwd=1.5 )
    axis(side = 2, at = c(1:length(dummy)), labels= names(dummy), lwd=0.1)
    axis(side = 1, lwd=0.1, lwd.ticks=1, at=c(-10,-5,0,5,10,15,20,25,30,35,40))
    box(lwd=1)
    abline(v=0, lty=2)
    abline(h=5.5, lty=1, lwd=0.5)
    stats <- cstats(rc, alg)
    plotCI(stats$mean, dummy, li= stats$lower, ui= stats$upper, err="x", xlim=c(-160,210), axes=F, ann=T, ylab= "", xlab= "difference in complexity [%]", lwd=1.5 )
    axis(side = 1, lwd=0.1, lwd.ticks=1, at=c(-150,-100,-50,0,50,100,150,200))
    box(lwd=1)
    abline(v=0, lty=2)
    abline(h=5.5, lty=1, lwd=0.5)
}
@

\setkeys{Gin}{width=1\textwidth}
\begin{figure}[t!]
\centering
<<fig = TRUE, echo = FALSE, width=14, height=7.9>>=
plotd("rpart")
@
\caption{\label{fig:crpart} Performance comparison of \code{evtree} and \code{rpart}. Prediction error is compared by the relative difference of the misclassification rate or the mean-squared error. The complexity is compared by the relative difference of the number of terminal nodes.}
\end{figure}

\begin{figure}[tbh!]
\centering
<<fig = TRUE, echo = FALSE, width=14, height=7.9>>=
plotd("ctree")
@
\caption{\label{fig:cptree} Performance comparison of \code{evtree} and \code{ctree}. Prediction error is compared by the relative difference of the misclassification rate or the mean-squared error. The complexity is compared by the relative difference of the number of terminal nodes.}
\end{figure}

The disadvantages of our algorithm are computation time and memory requirements. While the smallest of the analyzed data-sets, \textit{Glass Identification}, only needed approximately 4--6 seconds to fit, larger data-sets demanded several minutes. The fit of a model from the largest data-set, \textit{MAGIC Gamma Telescope}, required approximately 40--50 minutes and a main memory of 400 Mbit. The required resources were measured on an Intel Core 2 Duo with 2.2 GHz and 2 GB RAM using the 64-bit version of Ubuntu 10.10.

\subsection{Artificial Problem}
\label{comp2}

In this section we demonstrate the ability of \code{evtree} to solve an artificial problem that is difficult to solve for most recursive classification tree algorithms \citep{loh2009improving}. The data was simulated with $2000$ instances for both the training-set and the test-set. Predictor variables $X_1$ and $X_2$ are simulated to be uniformly distributed in the interval $[0,4]$. The classes are distributed in alternating squares forming a $4 \times 4$ chessboard in the $(X_1, X_2)$-plane. One realization of the simulated data is shown in Figure~\ref{fig:chess}. Furthermore, variables $X_3-X_8$ are noise variables that are uniformly distributed on the interval [0, 1]. The ideal model for this problem only uses variables $X_1$ and $X_2$ and has 16 terminal nodes, whereas each terminal node comprises the observations that are in the region of one square. Two further simulations are done in the same way, but $5\%$ and $10\%$ percent of the class labels are randomly changed to the other class. 

<<echo = FALSE>>=
chessboard44 <- function(totalpoints = 4000, noisevariables = 6, noise = 0){
    chess44 <- array(0,c(totalpoints,noisevariables+3))
    for(i in 1:(noisevariables+2))
        chess44[,i] <- as.numeric(runif(dim(chess44)[1]))*4

     x <- chess44[,1]
     y <- chess44[,2]
     chess44[,ncol(chess44)] <- 0
     for(k in 1:4)  
        chess44[(x <= k & x > k-1 & y <= k & y > k-1), ncol(chess44)] <- 1
     for(k in 1:2)  
        chess44[(x <= k & x > k-1 & y <= k+2 & y > k+1), ncol(chess44)] <- 1
     for(k in 1:2)  
        chess44[(y <= k & y > k-1 & x <= k+2 & x > k+1), ncol(chess44)] <- 1

     if( noise > 0){
        flipclasslist <- sample(totalpoints, totalpoints * (noise / 100), replace = F)

        for(i in 1:length(flipclasslist)){
            if(chess44[flipclasslist[i], ncol(chess44)] == 1)
                chess44[flipclasslist[i], ncol(chess44)] = 0
            else if(chess44[flipclasslist[i], ncol(chess44)] == 0)
                chess44[flipclasslist[i], ncol(chess44)] = 1
        }
    }

    chess44 <- as.data.frame(chess44)
    chess44[,ncol(chess44)] <- as.factor(chess44[,ncol(chess44)])
    names(chess44)[9] <- "class"
    chess44
}
@

\setkeys{Gin}{width=0.4\textwidth}
\begin{figure}[tbh!]
\centering
<<fig = TRUE, echo = FALSE>>=
par(cex.axis=1.5, cex.lab=1.5)
ch <- chessboard44(totalpoints= 2000)
par(cex.axis=1.5, cex.lab=1.5)
plot(ch[ch[,9]==1,1], ch[ch[,9]==1,2], type="p",  pch= c("x"), col=colors()[320], xlab="X1", ylab="X2", cex=1)
points(ch[ch[,9]==0,1], ch[ch[,9]==0,2], pch= c("o"))
@
\caption{\label{fig:chess} Class distribution of the simulated $4 \times 4$ chess-board problem plotted on the $(X_1, X_2)$-plane. The two classes are indicated by black $(o)$ and grey $(x)$.}
\end{figure}

The results are summarized in Table~\ref{tab:resultsChessboard}. It can be seen that, in the absence of noise, \code{rpart} models on average classify $67.2\%$ of the data points correctly and had $13.0$ terminal nodes. An average \code{ctree} model only has $1.1$ terminal nodes and a classification accuracy of $50.2\%$. In contrast, \code{evtree} classifies $92.8\%$ of the instances correctly and requires 14.0 terminal nodes. In the presence of $5\%$ and $10\%$ noise, \code{evtree} classifies $88.7\%$ and $84.2\%$ of the data correctly. 

<<label =  tab:resultsChessboard,  echo = FALSE, results=tex>>=
library("xtable")
load(file = "./results/chessboard44_0.RData")
load(file = "./results/chessboard44_5.RData")
load(file = "./results/chessboard44_10.RData")

chesstable_means  <- as.data.frame( rbind(apply(rchessboard44_0,2,mean), apply(rchessboard44_5,2,mean) , apply(rchessboard44_10,2,mean) )) 
names(chesstable_means) <-  c("evtree", "rpart", "ctree", "evtree", "rpart", "ctree")
chesstable_means[,1:3] <-  format(chesstable_means[,1:3]*100, digits=1, nsmall=1)
chesstable_means[,4:6] <-  format(chesstable_means[,4:6], digits=1, nsmall=1)

chesstable_sd  <- as.data.frame( rbind(apply(rchessboard44_0,2,sd), apply(rchessboard44_5,2,sd) , apply(rchessboard44_10,2,sd) )) 
names(chesstable_sd) <-  c("evtree", "rpart", "ctree", "evtree", "rpart", "ctree")
chesstable_sd[,1:3] <-  format(chesstable_sd[,1:3]*100, digits=1, nsmall=1)
chesstable_sd[,4:6] <-  format(chesstable_sd[,4:6], digits=1, nsmall=1)

chesstable <- chesstable_means
for(j in 1:ncol(chesstable_means)){
	for(i in 1:nrow(chesstable_means)){
		chesstable[i,j] <- paste(chesstable_means[i,j] ,  "(", chesstable_sd[i,j], ")",  sep="")	
	}
}

chesstable <- cbind(as.integer(rbind(0,5,10)), chesstable)
colnames(chesstable)[1] = ""

print(xtable(chesstable,
caption = "Comparison of accuracy on simulated $4 \\times 4$ chess-board examples. Accuracy and the number of terminal nodes are displayed as $mean(std)$.",
caption.placement= "bottom",
label= "tab:resultsChessboard"), 
include.rownames = FALSE, allign= "rllllll", hline.after=NULL,
add.to.row=list(pos=list(-1,-1, 0, 3), command=c(
"\\toprule", 
c("\\multicolumn{1}{l}{Noise [\\%]} & \\multicolumn{3}{l}{Accuracy}  & \\multicolumn{3}{l}{Terminal Nodes}\\\\",
"\\midrule",
"\\bottomrule"
)))
)
@

\section{Conclusions}
\label{conc}

In this paper, we presented the \pkg{evtree} package, which implements classification and regression trees that are evolved by through an evolutionary algorithm. The package uses standard \code{print()}, \code{summary()}, and \code{plot()} functions to display trees and a \code{predict()} function to predict the class labels of new data from the \pkg{partykit} package. As the learning step of our algorithm is computationally demanding, most calculations are conducted in \proglang{C++}. At the moment our algorithm does not support parallelism. However, we intend to extend \pkg{evtree} to parallel computing.

In Section~\ref{comp} we compared \code{evtree} with \code{rpart} and \code{ctree} on 14 benchmark and 3 real-world data-sets. The data-sets varied by sample size, the number of classes, and by the number and types of attributes. \code{evtree} models offered at least comparable and, for the majority of the data-sets, improved predictive accuracy. In an artificial problem of a $4 \times 4$ chessboard we demonstrated \code{evtree}'s performance in the presence of a higher order of interactions and noise. 

The goal of this package is not to replace the well-established algorithms like \code{rpart} and \code{ctree}. We rather want to provide \proglang{R} users with an alternative which, given sufficient amounts of time and main memory, is often more accurate than recursive partitioning methods. By the nature of the algorithm it is able to discover patterns which cannot be modeled by a greedy algorithm. As \code{evtree} models can be substantially different to recursively fitted models, it can be beneficial to use both approaches, as this may reveal additional relationships in the data.  

\bibliography{evtree} 

\end{document}

